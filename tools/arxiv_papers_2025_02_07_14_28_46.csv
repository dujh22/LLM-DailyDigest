Paper ID,Title,URL,Summary,First Author,Publish Date,Update Date,Code URL,Stars,Categories
2502.04330v1,"Geometrical frustration, power law tunneling and non-local gauge fields from scattered light",http://arxiv.org/abs/2502.04330v1,"Designing the amplitude and range of couplings in quantum systems is afundamental tool for exploring a large variety of quantum mechanical effects.Here, we consider off-resonant photon scattering processes on a geometricallyshaped molecular cloud. Our analysis shows that such a setup is properlymodeled by a Bose-Hubbard Hamiltonian where the range, amplitude and sign ofthe tunneling processes of the scattered photonic modes can be accuratelytuned. Specifically, by varying the molecular distribution, we demonstrate thatdifferent configurations characterized by geometrical frustration, long-rangepower law hopping processes, and non-local gauge fields can be achieved. Ourresults thus represent a powerful and alternative approach to perform anaccurate Hamiltonian engineering of quantum systems with non trivial couplingstructures.",Pavel P. Popov,2025-02-06,2025-02-06,,N/A,"['quant-ph', 'cond-mat.quant-gas']"
2502.04329v1,SMART: Advancing Scalable Map Priors for Driving Topology Reasoning,http://arxiv.org/abs/2502.04329v1,"Topology reasoning is crucial for autonomous driving as it enablescomprehensive understanding of connectivity and relationships between lanes andtraffic elements. While recent approaches have shown success in perceivingdriving topology using vehicle-mounted sensors, their scalability is hinderedby the reliance on training data captured by consistent sensor configurations.We identify that the key factor in scalable lane perception and topologyreasoning is the elimination of this sensor-dependent feature. To address this,we propose SMART, a scalable solution that leverages easily availablestandard-definition (SD) and satellite maps to learn a map prior model,supervised by large-scale geo-referenced high-definition (HD) maps independentof sensor settings. Attributed to scaled training, SMART alone achievessuperior offline lane topology understanding using only SD and satelliteinputs. Extensive experiments further demonstrate that SMART can be seamlesslyintegrated into any online topology reasoning methods, yielding significantimprovements of up to 28% on the OpenLane-V2 benchmark.",Junjie Ye,2025-02-06,2025-02-06,,N/A,"['cs.CV', 'cs.RO']"
2502.04328v1,Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment,http://arxiv.org/abs/2502.04328v1,"Recent advances in large language models, particularly following GPT-4o, havesparked increasing interest in developing omni-modal models capable ofunderstanding more modalities. While some open-source alternatives haveemerged, there is still a notable lag behind specialized single-modality modelsin performance. In this paper, we present Ola, an Omni-modal language modelthat achieves competitive performance across image, video, and audiounderstanding compared to specialized counterparts. The core design of Ola liesin its progressive modality alignment strategy that extends the supportingmodality of the language model progressively. Our training pipeline begins withthe most distinct modalities: image and text, then gradually expands the skillsets of the model using speech data that connects language and audio knowledge,and video data that connects all modalities. The progressive learning pipelinealso enables us to maintain a relatively small size of the cross-modalalignment data, making developing omni-modal from existing vision-languagemodels easy and less costly. Moreover, to unlock an advanced interactiveexperience like GPT-4o, we further design a sentence-wise decoding solution forstreaming speech generation. Extensive experiments demonstrate that Olasurpasses existing open omni-modal LLMs across all modalities while achievinghighly competitive performance compared to state-of-the-art specialized modelsof similar sizes. We aim to make Ola a fully open omni-modal understandingsolution to advance future research in this emerging field. Model weights,code, and data are open-sourced at https://github.com/Ola-Omni/Ola.",Zuyan Liu,2025-02-06,2025-02-06,,N/A,"['cs.CV', 'cs.CL', 'cs.MM', 'cs.SD', 'eess.AS', 'eess.IV']"
2502.04327v1,Value-Based Deep RL Scales Predictably,http://arxiv.org/abs/2502.04327v1,"Scaling data and compute is critical to the success of machine learning.However, scaling demands predictability: we want methods to not only performwell with more compute or data, but also have their performance be predictablefrom small-scale runs, without running the large-scale experiment. In thispaper, we show that value-based off-policy RL methods are predictable despitecommunity lore regarding their pathological behavior. First, we show that dataand compute requirements to attain a given performance level lie on a Paretofrontier, controlled by the updates-to-data (UTD) ratio. By estimating thisfrontier, we can predict this data requirement when given more compute, andthis compute requirement when given more data. Second, we determine the optimalallocation of a total resource budget across data and compute for a givenperformance and use it to determine hyperparameters that maximize performancefor a given budget. Third, this scaling behavior is enabled by first estimatingpredictable relationships between hyperparameters, which is used to manageeffects of overfitting and plasticity loss unique to RL. We validate ourapproach using three algorithms: SAC, BRO, and PQL on DeepMind Control, OpenAIgym, and IsaacGym, when extrapolating to higher levels of data, compute,budget, or performance.",Oleh Rybkin,2025-02-06,2025-02-06,,N/A,['cs.LG']
2502.04326v1,WorldSense: Evaluating Real-world Omnimodal Understanding for Multimodal LLMs,http://arxiv.org/abs/2502.04326v1,"In this paper, we introduce WorldSense, the first benchmark to assess themulti-modal video understanding, that simultaneously encompasses visual, audio,and text inputs. In contrast to existing benchmarks, our WorldSense has severalfeatures: (i) collaboration of omni-modality, we design the evaluation tasks tofeature a strong coupling of audio and video, requiring models to effectivelyutilize the synergistic perception of omni-modality; (ii) diversity of videosand tasks, WorldSense encompasses a diverse collection of 1,662 audio-visualsynchronised videos, systematically categorized into 8 primary domains and 67fine-grained subcategories to cover the broad scenarios, and 3,172 multi-choiceQA pairs across 26 distinct tasks to enable the comprehensive evaluation; (iii)high-quality annotations, all the QA pairs are manually labeled by 80 expertannotators with multiple rounds of correction to ensure quality. Based on ourWorldSense, we extensively evaluate various state-of-the-art models. Theexperimental results indicate that existing models face significant challengesin understanding real-world scenarios (48.0% best accuracy). We hope ourWorldSense can provide a platform for evaluating the ability in constructingand understanding coherent contexts from omni-modality.",Jack Hong,2025-02-06,2025-02-06,,N/A,"['cs.CV', 'cs.AI']"
2502.04324v1,Can Grammarly and ChatGPT accelerate language change? AI-powered technologies and their impact on the English language: wordiness vs. conciseness,http://arxiv.org/abs/2502.04324v1,"The proliferation of NLP-powered language technologies, AI-based naturallanguage generation models, and English as a mainstream means of communicationamong both native and non-native speakers make the output of AI-powered toolsespecially intriguing to linguists. This paper investigates how Grammarly andChatGPT affect the English language regarding wordiness vs. conciseness. A casestudy focusing on the purpose subordinator in order to is presented toillustrate the way in which Grammarly and ChatGPT recommend shorter grammaticalstructures instead of longer and more elaborate ones. Although the analysedsentences were produced by native speakers, are perfectly correct, and wereextracted from a language corpus of contemporary English, both Grammarly andChatGPT suggest more conciseness and less verbosity, even for relatively shortsentences. The present article argues that technologies such as Grammarly notonly mirror language change but also have the potential to facilitate oraccelerate it.",Karolina Rudnicka,2025-02-06,2025-02-06,,N/A,"['cs.CL', 'cs.CY']"
2502.04323v1,The Uniformly Rotated Mondrian Kernel,http://arxiv.org/abs/2502.04323v1,"First proposed by Rahimi and Recht, random features are used to decrease thecomputational cost of kernel machines in large-scale problems. The Mondriankernel is one such example of a fast random feature approximation of theLaplace kernel, generated by a computationally efficient hierarchical randompartition of the input space known as the Mondrian process. In this work, westudy a variation of this random feature map by using uniformly randomlyrotated Mondrian processes to approximate a kernel that is invariant underrotations. We obtain a closed-form expression for this isotropic kernel, aswell as a uniform convergence rate of the uniformly rotated Mondrian kernel tothis limit. To this end, we utilize techniques from the theory of stationaryrandom tessellations in stochastic geometry and prove a new result on thegeometry of the typical cell of the superposition of uniformly random rotationsof Mondrian tessellations. Finally, we test the empirical performance of thisrandom feature map on both synthetic and real-world datasets, demonstrating itsimproved performance over the Mondrian kernel on a debiased dataset.",Calvin Osborne,2025-02-06,2025-02-06,,N/A,"['cs.LG', 'math.PR']"
2502.04321v1,Variation of sentence length across time and genre,http://arxiv.org/abs/2502.04321v1,"The goal of this paper is threefold: i) to present some practical aspects ofusing full-text version of Corpus of Historical American English (COHA), thelargest diachronic multi-genre corpus of the English language, in theinvestigation of a linguistic trend of change; ii) to test a widely heldassumption that sentence length in written English has been steadily decreasingover the past few centuries; iii) to point to a possible link between thechanges in sentence length and changes in the English syntactic usage. Theempirical proof of concept for iii) is provided by the decline in the frequencyof the non-finite purpose subordinator in order to. Sentence length, genre andthe likelihood of occurrence of in order to are shown to be interrelated.",Karolina Rudnicka,2025-02-06,2025-02-06,,N/A,['cs.CL']
2502.04322v1,Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions,http://arxiv.org/abs/2502.04322v1,"Despite extensive safety alignment efforts, large language models (LLMs)remain vulnerable to jailbreak attacks that elicit harmful behavior. Whileexisting studies predominantly focus on attack methods that require technicalexpertise, two critical questions remain underexplored: (1) Are jailbrokenresponses truly useful in enabling average users to carry out harmful actions?(2) Do safety vulnerabilities exist in more common, simple human-LLMinteractions? In this paper, we demonstrate that LLM responses most effectivelyfacilitate harmful actions when they are both actionable and informative--twoattributes easily elicited in multi-step, multilingual interactions. Using thisinsight, we propose HarmScore, a jailbreak metric that measures how effectivelyan LLM response enables harmful actions, and Speak Easy, a simple multi-step,multilingual attack framework. Notably, by incorporating Speak Easy into directrequest and jailbreak baselines, we see an average absolute increase of 0.319in Attack Success Rate and 0.426 in HarmScore in both open-source andproprietary LLMs across four safety benchmarks. Our work reveals a critical yetoften overlooked vulnerability: Malicious users can easily exploit commoninteraction patterns for harmful intentions.",Yik Siu Chan,2025-02-06,2025-02-06,,N/A,"['cs.LG', 'cs.AI', 'cs.CL', 'cs.CY']"
2502.04320v1,ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features,http://arxiv.org/abs/2502.04320v1,"Do the rich representations of multi-modal diffusion transformers (DiTs)exhibit unique properties that enhance their interpretability? We introduceConceptAttention, a novel method that leverages the expressive power of DiTattention layers to generate high-quality saliency maps that precisely locatetextual concepts within images. Without requiring additional training,ConceptAttention repurposes the parameters of DiT attention layers to producehighly contextualized concept embeddings, contributing the major discovery thatperforming linear projections in the output space of DiT attention layersyields significantly sharper saliency maps compared to commonly usedcross-attention mechanisms. Remarkably, ConceptAttention even achievesstate-of-the-art performance on zero-shot image segmentation benchmarks,outperforming 11 other zero-shot interpretability methods on theImageNet-Segmentation dataset and on a single-class subset of PascalVOC. Ourwork contributes the first evidence that the representations of multi-modal DiTmodels like Flux are highly transferable to vision tasks like segmentation,even outperforming multi-modal foundation models like CLIP.",Alec Helbling,2025-02-06,2025-02-06,,N/A,"['cs.CV', 'cs.LG']"
