Paper ID,Title,URL,Summary,First Author,Publish Date,Update Date,Code URL,Stars,Categories
2508.06623v1,ContextGuard-LVLM: Enhancing News Veracity through Fine-grained Cross-modal Contextual Consistency Verification,http://arxiv.org/abs/2508.06623v1,"The proliferation of digital news media necessitates robust methods forverifying content veracity, particularly regarding the consistency betweenvisual and textual information. Traditional approaches often fall short inaddressing the fine-grained cross-modal contextual consistency (FCCC) problem,which encompasses deeper alignment of visual narrative, emotional tone, andbackground information with text, beyond mere entity matching. To address this,we propose ContextGuard-LVLM, a novel framework built upon advancedVision-Language Large Models (LVLMs) and integrating a multi-stage contextualreasoning mechanism. Our model is uniquely enhanced through reinforced oradversarial learning paradigms, enabling it to detect subtle contextualmisalignments that evade zero-shot baselines. We extend and augment threeestablished datasets (TamperedNews-Ent, News400-Ent, MMG-Ent) with newfine-grained contextual annotations, including ""contextual sentiment,"" ""visualnarrative theme,"" and ""scene-event logical coherence,"" and introduce acomprehensive CTXT (Contextual Coherence) entity type. Extensive experimentsdemonstrate that ContextGuard-LVLM consistently outperforms state-of-the-artzero-shot LVLM baselines (InstructBLIP and LLaVA 1.5) across nearly allfine-grained consistency tasks, showing significant improvements in complexlogical reasoning and nuanced contextual understanding. Furthermore, our modelexhibits superior robustness to subtle perturbations and a higher agreementrate with human expert judgments on challenging samples, affirming its efficacyin discerning sophisticated forms of context detachment.",Sihan Ma,2025/8/8,2025/8/8,,,['cs.CV']
2508.04163v1,Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork,http://arxiv.org/abs/2508.04163v1,"AI agents deployed in assistive roles often have to collaborate with otheragents (humans, AI systems) without prior coordination. Methods consideredstate of the art for such ad hoc teamwork often pursue a data-driven approachthat needs a large labeled dataset of prior observations, lacks transparency,and makes it difficult to rapidly revise existing knowledge in response tochanges. As the number of agents increases, the complexity of decision-makingmakes it difficult to collaborate effectively. This paper advocates leveragingthe complementary strengths of knowledge-based and data-driven methods forreasoning and learning for ad hoc teamwork. For any given goal, ourarchitecture enables each ad hoc agent to determine its actions throughnon-monotonic logical reasoning with: (a) prior commonsense domain-specificknowledge; (b) models learned and revised rapidly to predict the behavior ofother agents; and (c) anticipated abstract future goals based on genericknowledge of similar situations in an existing foundation model. Weexperimentally evaluate our architecture's capabilities in VirtualHome, arealistic physics-based 3D simulation environment.",Hasra Dodampegama,2025/8/6,2025/8/6,,,"['cs.AI', 'cs.LO', 'cs.MA']"
2507.23541v2,Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via Progressive Reinforcement Learning,http://arxiv.org/abs/2507.23541v2,"In medical scenarios, effectively retrieving external knowledge andleveraging it for rigorous logical reasoning is of significant importance.Despite their potential, existing work has predominantly focused on enhancingeither retrieval or reasoning capabilities of the models in isolation, withlittle attention given to their joint optimization, which leads to limitedcoordination between the two processes. Additionally, current methods relyheavily on supervised fine-tuning (SFT), which can cause models to memorizeexisting problem-solving pathways, thereby restricting their generalizationability when confronted with novel problem contexts. Furthermore, while somestudies have explored to improve retrieval-augmented reasoning in generaldomains via reinforcement learning, their reward function designs do notadequately capture the specific demands of the medical domain. To address thesechallenges, we introduce **Med-R$^3$**, a **Med**ical **R**etrieval-augmented**R**easoning framework driven by progressive **R**einforcement learning. Inthis framework, we first develop the model's ability to perform logicalreasoning over medical problems. Subsequently, on the basis of this foundation,we adaptively optimize the retrieval capability to better align with thecharacteristics of knowledge corpus and external information utilizationthroughout the reasoning process. Finally, we conduct joint optimization of themodel's retrieval and reasoning coordination. Extensive experiments indicatethat **Med-R$^3$** could achieve state-of-the-art performances, withLLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by3.93\% at a comparable parameter scale, while Qwen2.5-14B augmented withMed-R$^3$ shows a more substantial gain of 13.53\%.",Keer Lu,2025/7/31,2025/8/2,,,['cs.CL']
2507.20999v1,LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning,http://arxiv.org/abs/2507.20999v1,"Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefitsubstantially from chain-of-thought (CoT) reasoning, yet pushing theirperformance typically requires vast data, large model sizes, and full-parameterfine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost,most existing approaches primarily address domain adaptation or layer-wiseallocation rather than explicitly tailoring data and parameters to differentresponse demands. Inspired by ""Thinking, Fast and Slow,"" which characterizestwo distinct modes of thought-System 1 (fast, intuitive, often automatic) andSystem 2 (slower, more deliberative and analytic)-we draw an analogy thatdifferent ""subregions"" of an LLM's parameters might similarly specialize fortasks that demand quick, intuitive responses versus those requiring multi-steplogical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA frameworkthat partitions both data and parameters by System 1 or System 2 demands, usingfewer yet more focused parameters for each task. Specifically, we classify taskdata via multi-model role-playing and voting, and partition parameters based onimportance scoring, then adopt a two-stage fine-tuning strategy of trainingSystem 1 tasks with supervised fine-tuning (SFT) to enhance knowledge andintuition and refine System 2 tasks with reinforcement learning (RL) toreinforce deeper logical deliberation next. Extensive experiments show that thetwo-stage fine-tuning strategy, SFT and RL, lowers active parameter usage whilematching or surpassing SOTA PEFT baselines.",Yining Huang,2025/7/28,2025/7/28,,,"['cs.LG', 'cs.CL']"
2507.20960v1,On the Limits of Hierarchically Embedded Logic in Classical Neural Networks,http://arxiv.org/abs/2507.20960v1,"We propose a formal model of reasoning limitations in large neural net modelsfor language, grounded in the depth of their neural architecture. By treatingneural networks as linear operators over logic predicate space we show thateach layer can encode at most one additional level of logical reasoning. Weprove that a neural network of depth a particular depth cannot faithfullyrepresent predicates in a one higher order logic, such as simple counting overcomplex predicates, implying a strict upper bound on logical expressiveness.This structure induces a nontrivial null space during tokenization andembedding, excluding higher-order predicates from representability. Ourframework offers a natural explanation for phenomena such as hallucination,repetition, and limited planning, while also providing a foundation forunderstanding how approximations to higher-order logic may emerge. Theseresults motivate architectural extensions and interpretability strategies infuture development of language models.",Bill Cochran,2025/7/28,2025/7/28,,,['cs.AI']
2507.20529v1,Enhancing Spatial Reasoning through Visual and Textual Thinking,http://arxiv.org/abs/2507.20529v1,"The spatial reasoning task aims to reason about the spatial relationships in2D and 3D space, which is a fundamental capability for Visual QuestionAnswering (VQA) and robotics. Although vision language models (VLMs) havedeveloped rapidly in recent years, they are still struggling with the spatialreasoning task. In this paper, we introduce a method that can enhance Spatialreasoning through Visual and Textual thinking Simultaneously (SpatialVTS). Inthe spatial visual thinking phase, our model is trained to generatelocation-related specific tokens of essential targets automatically. Not onlyare the objects mentioned in the problem addressed, but also the potentialobjects related to the reasoning are considered. During the spatial textualthinking phase, Our model conducts long-term thinking based on visual cues anddialogues, gradually inferring the answers to spatial reasoning problems. Toeffectively support the model's training, we perform manual corrections to theexisting spatial reasoning dataset, eliminating numerous incorrect labelsresulting from automatic annotation, restructuring the data input format toenhance generalization ability, and developing thinking processes with logicalreasoning details. Without introducing additional information (such as masks ordepth), our model's overall average level in several spatial understandingtasks has significantly improved compared with other models.",Xun Liang,2025/7/28,2025/7/28,,,"['cs.CV', 'cs.AI']"
2507.19992v1,NIRS: An Ontology for Non-Invasive Respiratory Support in Acute Care,http://arxiv.org/abs/2507.19992v1,"Objective: Develop a Non Invasive Respiratory Support (NIRS) ontology tosupport knowledge representation in acute care settings.  Materials and Methods: We developed the NIRS ontology using Web OntologyLanguage (OWL) semantics and Protege to organize clinical concepts andrelationships. To enable rule-based clinical reasoning beyond hierarchicalstructures, we added Semantic Web Rule Language (SWRL) rules. We evaluatedlogical reasoning by adding 17 hypothetical patient clinical scenarios. We usedSPARQL queries and data from the Electronic Intensive Care Unit (eICU)Collaborative Research Database to retrieve and test targeted inferences.  Results: The ontology has 132 classes, 12 object properties, and 17 dataproperties across 882 axioms that establish concept relationships. Tostandardize clinical concepts, we added 350 annotations, including descriptivedefinitions based on controlled vocabularies. SPARQL queries successfullyvalidated all test cases (rules) by retrieving appropriate patient outcomes,for instance, a patient treated with HFNC (high-flow nasal cannula) for 2 hoursdue to acute respiratory failure may avoid endotracheal intubation.  Discussion: The NIRS ontology formally represents domain-specific concepts,including ventilation modalities, patient characteristics, therapy parameters,and outcomes. SPARQL query evaluations on clinical scenarios confirmed theability of the ontology to support rule based reasoning and therapyrecommendations, providing a foundation for consistent documentation practices,integration into clinical data models, and advanced analysis of NIRS outcomes.  Conclusion: We unified NIRS concepts into an ontological framework anddemonstrated its applicability through the evaluation of hypothetical patientscenarios and alignment with standardized vocabularies.",Md Fantacher Islam,2025/7/26,2025/7/26,,,"['q-bio.OT', 'cs.AI']"
2507.22933v1,Augmented Vision-Language Models: A Systematic Review,http://arxiv.org/abs/2507.22933v1,"Recent advances in visual-language machine learning models have demonstratedexceptional ability to use natural language and understand visual scenes bytraining on large, unstructured datasets. However, this training paradigmcannot produce interpretable explanations for its outputs, requires retrainingto integrate new information, is highly resource-intensive, and struggles withcertain forms of logical reasoning. One promising solution involves integratingneural networks with external symbolic information systems, forming neuralsymbolic systems that can enhance reasoning and memory abilities. These neuralsymbolic systems provide more interpretable explanations to their outputs andthe capacity to assimilate new information without extensive retraining.Utilizing powerful pre-trained Vision-Language Models (VLMs) as the core neuralcomponent, augmented by external systems, offers a pragmatic approach torealizing the benefits of neural-symbolic integration. This systematicliterature review aims to categorize techniques through which visual-languageunderstanding can be improved by interacting with external symbolic informationsystems.",Anthony C Davis,2025/7/24,2025/7/24,,,"['cs.CL', 'cs.AI']"
2507.17512v1,Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning,http://arxiv.org/abs/2507.17512v1,"Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as apowerful paradigm for enhancing the reasoning capabilities of LLMs. Existingresearch has predominantly concentrated on isolated reasoning domains such asmathematical problem-solving, coding tasks, or logical reasoning. However, realworld reasoning scenarios inherently demand an integrated application ofmultiple cognitive skills. Despite this, the interplay among these reasoningskills under reinforcement learning remains poorly understood. To bridge thisgap, we present a systematic investigation of multi-domain reasoning within theRLVR framework, explicitly focusing on three primary domains: mathematicalreasoning, code generation, and logical puzzle solving. We conduct acomprehensive study comprising four key components: (1) Leveraging the GRPOalgorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates themodels' in-domain improvements and cross-domain generalization capabilitieswhen trained on single-domain datasets. (2) Additionally, we examine theintricate interactions including mutual enhancements and conflicts that emergeduring combined cross-domain training. (3) To further understand the influenceof SFT on RL, we also analyze and compare performance differences between baseand instruct models under identical RL configurations. (4) Furthermore, wedelve into critical RL training details, systematically exploring the impactsof curriculum learning strategies, variations in reward design, andlanguage-specific factors. Through extensive experiments, our results offersignificant insights into the dynamics governing domain interactions, revealingkey factors influencing both specialized and generalizable reasoningperformance. These findings provide valuable guidance for optimizing RLmethodologies to foster comprehensive, multi-domain reasoning capabilities inLLMs.",Yu Li,2025/7/23,2025/7/23,,,"['cs.AI', 'cs.LG']"
2507.07498v2,Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code,http://arxiv.org/abs/2507.07498v2,"Enhancing reasoning capabilities remains a central focus in the LLM reasearchcommunity. A promising direction involves requiring models to simulate codeexecution step-by-step to derive outputs for given inputs. However, as code isoften designed for large-scale systems, direct application leads toover-reliance on complex data structures and algorithms, even for simple cases,resulting in overfitting to algorithmic patterns rather than core reasoningstructures. To address this, we propose TeaR, which aims at teaching LLMs toreason better. TeaR leverages careful data curation and reinforcement learningto guide models in discovering optimal reasoning paths through code-relatedtasks, thereby improving general reasoning abilities. We conduct extensiveexperiments using two base models and three long-CoT distillation models, withmodel sizes ranging from 1.5 billion to 32 billion parameters, and across 17benchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The resultsconsistently show significant performance improvements. Notably, TeaR achievesa 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.",Keqin Bao,2025/7/10,2025/7/14,,,"['cs.CL', 'cs.LG']"
2508.00004v1,Reasoning under uncertainty in the game of Cops and Robbers,http://arxiv.org/abs/2508.00004v1,"The game of Cops and Robbers is an important model for studying computationalqueries in pursuit-evasion environments, among others. As recent logicalexplorations have shown, its structure exhibits appealing analogies with modallogic. In this paper, we enrich the game with a setting in which players mayhave imperfect information. We propose a new formal framework, Epistemic Logicof Cops and Robbers (ELCR), to make the core notions of the game precise, forinstance, players' positions, observational power and inference. Applying ELCRto analyze the game, we obtain an automated way to track interactions betweenplayers and characterize their information updates during the game. The updatemechanism is defined by a novel dynamic operator, and we compare it with somerelevant paradigms from the game and logic perspectives. We study variousproperties of ELCR including axiomatization and decidability. To our knowledge,this is the first attempt to explore these games from a formal point of viewwhere (partial) information available to players is taken into account.",Dazhu Li,2025/7/9,2025/7/9,,,"['cs.LO', 'math.LO']"
2507.04770v1,FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System,http://arxiv.org/abs/2507.04770v1,"Furniture decoration is an important task in various industrial applications.However, achieving a high-quality decorative result is often time-consuming andrequires specialized artistic expertise. To tackle these challenges, we explorehow multi-agent systems can assist in automating the decoration process. Wepropose FurniMAS, a multi-agent system for automatic furniture decoration.Specifically, given a human prompt and a household furniture item such as aworking desk or a TV stand, our system suggests relevant assets withappropriate styles and materials, and arranges them on the item, ensuring thedecorative result meets functionality, aesthetic, and ambiance preferences.FurniMAS assembles a hybrid team of LLM-based and non-LLM agents, eachfulfilling distinct roles in a typical decoration project. These agentscollaborate through communication, logical reasoning, and validation totransform the requirements into the final outcome. Extensive experimentsdemonstrate that our FurniMAS significantly outperforms other baselines ingenerating high-quality 3D decor.",Toan Nguyen,2025/7/7,2025/7/7,,,"['cs.AI', 'cs.CV']"
2507.02170v1,"Synergizing Logical Reasoning, Knowledge Management and Collaboration in Multi-Agent LLM System",http://arxiv.org/abs/2507.02170v1,"This paper explores the integration of advanced Multi-Agent Systems (MAS)techniques to develop a team of agents with enhanced logical reasoning,long-term knowledge retention, and Theory of Mind (ToM) capabilities. Byuniting these core components with optimized communication protocols, we createa novel framework called SynergyMAS, which fosters collaborative teamwork andsuperior problem-solving skills. The system's effectiveness is demonstratedthrough a product development team case study, where our approach significantlyenhances performance and adaptability. These findings highlight SynergyMAS'spotential to tackle complex, real-world challenges.",Adam Kostka,2025/7/2,2025/7/2,,,['cs.MA']
2506.22434v1,MiCo: Multi-image Contrast for Reinforcement Visual Reasoning,http://arxiv.org/abs/2506.22434v1,"This work explores enabling Chain-of-Thought (CoT) reasoning to link visualcues across multiple images. A straightforward solution is to adapt rule-basedreinforcement learning for Vision-Language Models (VLMs). However, such methodstypically rely on manually curated question-answer pairs, which can beparticularly challenging when dealing with fine grained visual details andcomplex logic across images. Inspired by self-supervised visual representationlearning, we observe that images contain inherent constraints that can serve assupervision. Based on this insight, we construct image triplets comprising twoaugmented views of the same image and a third, similar but distinct image.During training, the model is prompted to generate a reasoning process tocompare these images (i.e., determine same or different). Then we optimize themodel with rule-based reinforcement learning. Due to the high visual similarityand the presence of augmentations, the model must attend to subtle visualchanges and perform logical reasoning to succeed. Experiments show that,although trained solely on visual comparison tasks, the learned reasoningability generalizes effectively to a wide range of questions. Without relyingon any human-annotated question-answer pairs, our method achieves significantimprovements on multi-image reasoning benchmarks and shows strong performanceon general vision tasks.",Xi Chen,2025/6/27,2025/6/27,,,['cs.CV']
2506.21656v1,Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs,http://arxiv.org/abs/2506.21656v1,"Current Vision-Language Models (VLMs) struggle with fine-grained spatialreasoning, particularly when multi-step logic and precise spatial alignment arerequired. In this work, we introduce SpatialReasoner-R1, a vision-languagereasoning model designed to address these limitations. To constructhigh-quality supervision for spatial reasoning, we design a Multi-Model MonteCarlo Tree Search (M3CTS) method that generates diverse, logically consistentLong Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we proposefine-grained Direct Preference Optimization (fDPO), which introducessegment-specific preference granularity for descriptive grounding and logicalreasoning, guided by a spatial reward mechanism that evaluates candidateresponses based on visual consistency, spatial grounding, and logicalcoherence. Experimental results demonstrate that fDPO achieves an averageimprovement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets anew SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% inaverage accuracy, while maintaining competitive performance on generalvision-language tasks.",Yifan Shen,2025/6/26,2025/6/26,,,"['cs.CV', 'cs.CL']"
2506.18383v1,LOGICPO: Efficient Translation of NL-based Logical Problems to FOL using LLMs and Preference Optimization,http://arxiv.org/abs/2506.18383v1,"Logical reasoning is a key task for artificial intelligence due to it's rolein major downstream tasks such as Question Answering, Summarization. Recentmethods in improving the reasoning ability of LLMs fall short in correctlyconverting a natural language reasoning problem to an equivalent logicalformulation, which hinders the framework's overall ability to reason. Towardsthis, we propose to use finetuning on a preference optimization dataset tolearn to parse and represent a natural language problem as a whole to aconsistent logical program by 1) introducing a new supervised and preferenceoptimization dataset LogicPO, and 2) adopting popular techniques such as DirectPreference Optimization (DPO), Kahneman-Tversky optimization (KTO) to finetuneopen-source LLMs. Our best model with Phi-3.5 consistently outperformsGPT-3.5-turbo's (8-shot) by producing 10% more logically correct and with 14%less syntax errors. Through the framework and our improved evaluation metrics,we offer a promising direction in improving the logical reasoning of LLMs bybetter representing them in their logical formulations.",Koushik Viswanadha,2025/6/23,2025/6/23,,,"['cs.LG', 'cs.AI']"
2506.14373v2,Discrete JEPA: Learning Discrete Token Representations without Reconstruction,http://arxiv.org/abs/2506.14373v2,"The cornerstone of cognitive intelligence lies in extracting hidden patternsfrom observations and leveraging these principles to systematically predictfuture outcomes. However, current image tokenization methods demonstratesignificant limitations in tasks requiring symbolic abstraction and logicalreasoning capabilities essential for systematic inference. To address thischallenge, we propose Discrete-JEPA, extending the latent predictive codingframework with semantic tokenization and novel complementary objectives tocreate robust tokenization for symbolic reasoning tasks. Discrete-JEPAdramatically outperforms baselines on visual symbolic prediction tasks, whilestriking visual evidence reveals the spontaneous emergence of deliberatesystematic patterns within the learned semantic token space. Though an initialmodel, our approach promises a significant impact for advancing Symbolic worldmodeling and planning capabilities in artificial intelligence systems.",Junyeob Baek,2025/6/17,2025/6/22,,,['cs.CV']
2506.17294v1,AI-Generated Game Commentary: A Survey and a Datasheet Repository,http://arxiv.org/abs/2506.17294v1,"AI-Generated Game Commentary (AIGGC) has gained increasing attention due toits market potential and inherent technical challenges. As a comprehensivemultimodal Natural Language Processing (NLP) task, AIGGC imposes substantialdemands on language models, including factual accuracy, logical reasoning,expressive text generation, generation speed, and context management. In thispaper, we introduce a general framework for AIGGC and present a comprehensivesurvey of 45 existing game commentary dataset and methods according to keychallenges they aim to address in this domain. We further classify and comparevarious evaluation metrics commonly used in this domain. To support futureresearch and benchmarking, we also provide a structured datasheet summarizingthe essential attributes of these datasets in appendix, which is meanwhilepublicly available in an open repository.",Qirui Zheng,2025/6/17,2025/6/17,,,"['cs.CL', 'cs.AI', 'cs.LG']"
2506.13331v1,Mixture of Cognitive Reasoners: Modular Reasoning with Brain-Like Specialization,http://arxiv.org/abs/2506.13331v1,"Human intelligence emerges from the interaction of specialized brainnetworks, each dedicated to distinct cognitive functions such as languageprocessing, logical reasoning, social understanding, and memory retrieval.Inspired by this biological observation, we introduce the Mixture of CognitiveReasoners (MiCRo) architecture and training paradigm: a modulartransformer-based language model with a training curriculum that encourages theemergence of functional specialization among different modules. Inspired bystudies in neuroscience, we partition the layers of a pretrained transformermodel into four expert modules, each corresponding to a well-studied cognitivebrain network. Our Brain-Like model has three key benefits over the state ofthe art: First, the specialized experts are highly interpretable andfunctionally critical, where removing a module significantly impairsperformance on domain-relevant benchmarks. Second, our model outperformscomparable baselines that lack specialization on seven reasoning benchmarks.And third, the model's behavior can be steered at inference time by selectivelyemphasizing certain expert modules (e.g., favoring social over logicalreasoning), enabling fine-grained control over the style of its response. Ourfindings suggest that biologically inspired inductive biases involved in humancognition lead to significant modeling gains in interpretability, performance,and controllability.",Badr AlKhamissi,2025/6/16,2025/6/16,,,['cs.LG']
2506.12849v1,CAPO: Reinforcing Consistent Reasoning in Medical Decision-Making,http://arxiv.org/abs/2506.12849v1,"In medical visual question answering (Med-VQA), achieving accurate responsesrelies on three critical steps: precise perception of medical imaging data,logical reasoning grounded in visual input and textual questions, and coherentanswer derivation from the reasoning process. Recent advances in generalvision-language models (VLMs) show that large-scale reinforcement learning (RL)could significantly enhance both reasoning capabilities and overall modelperformance. However, their application in medical domains is hindered by twofundamental challenges: 1) misalignment between perceptual understanding andreasoning stages, and 2) inconsistency between reasoning pathways and answergeneration, both compounded by the scarcity of high-quality medical datasetsfor effective large-scale RL. In this paper, we first introduce Med-Zero-17K, acurated dataset for pure RL-based training, encompassing over 30 medical imagemodalities and 24 clinical tasks. Moreover, we propose a novel large-scale RLframework for Med-VLMs, Consistency-Aware Preference Optimization (CAPO), whichintegrates rewards to ensure fidelity between perception and reasoning,consistency in reasoning-to-answer derivation, and rule-based accuracy forfinal responses. Extensive experiments on both in-domain and out-of-domainscenarios demonstrate the superiority of our method over strong VLM baselines,showcasing strong generalization capability to 3D Med-VQA benchmarks andR1-like training paradigms.",Songtao Jiang,2025/6/15,2025/6/15,,,['cs.CV']
2506.10209v1,TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games,http://arxiv.org/abs/2506.10209v1,"Large reasoning models (LRMs) have demonstrated impressive reasoningcapabilities across a broad range of tasks including Olympiad-levelmathematical problems, indicating evidence of their complex reasoningabilities. While many reasoning benchmarks focus on the STEM domain, theability of LRMs to reason correctly in broader task domains remainsunderexplored. In this work, we introduce \textbf{TTT-Bench}, a new benchmarkthat is designed to evaluate basic strategic, spatial, and logical reasoningabilities in LRMs through a suite of four two-player Tic-Tac-Toe-style gamesthat humans can effortlessly solve from a young age. We propose a simple yetscalable programmatic approach for generating verifiable two-player gameproblems for TTT-Bench. Although these games are trivial for humans, theyrequire reasoning about the intentions of the opponent, as well as the gameboard's spatial configurations, to ensure a win. We evaluate a diverse set ofstate-of-the-art LRMs, and \textbf{discover that the models that excel at hardmath problems frequently fail at these simple reasoning games}. Further testingreveals that our evaluated reasoning models score on average $\downarrow$ 41\%\& $\downarrow$ 5\% lower on TTT-Bench compared to MATH 500 \& AIME 2024respectively, with larger models achieving higher performance using shorterreasoning traces, where most of the models struggle on long-term strategicreasoning situations on simple and new TTT-Bench tasks.",Prakamya Mishra,2025/6/11,2025/6/11,,,"['cs.CL', 'cs.AI']"
2506.11128v1,Stronger Language Models Produce More Human-Like Errors,http://arxiv.org/abs/2506.11128v1,"Do language models converge toward human-like reasoning patterns as theyimprove? We provide surprising evidence that while overall reasoningcapabilities increase with model sophistication, the nature of errorsincreasingly mirrors predictable human reasoning fallacies: a previouslyunobserved inverse scaling phenomenon. To investigate this question, we applythe Erotetic Theory of Reasoning (ETR), a formal cognitive framework withempirical support for predicting human reasoning outcomes. Using theopen-source package PyETR, we generate logical reasoning problems where humanspredictably err, evaluating responses from 38 language models across 383reasoning tasks. Our analysis indicates that as models advance in generalcapability (as measured by Chatbot Arena scores), the proportion of theirincorrect answers that align with ETR-predicted human fallacies tends toincrease ($\rho = 0.360, p = 0.0265$). Notably, as we observe no correlationbetween model sophistication and logical correctness on these tasks, this shiftin error patterns toward human-likeness occurs independently of error rate.These findings challenge the prevailing view that scaling language modelsnaturally obtains normative rationality, suggesting instead a convergencetoward human-like cognition inclusive of our characteristic biases andlimitations, as we further confirm by demonstrating order-effects in languagemodel reasoning.",Andrew Keenan Richardson,2025/6/10,2025/6/10,,,"['cs.CL', 'cs.AI']"
2506.07288v3,EVINET: Towards Open-World Graph Learning via Evidential Reasoning Network,http://arxiv.org/abs/2506.07288v3,"Graph learning has been crucial to many real-world tasks, but they are oftenstudied with a closed-world assumption, with all possible labels of data knowna priori. To enable effective graph learning in an open and noisy environment,it is critical to inform the model users when the model makes a wrongprediction to in-distribution data of a known class, i.e., misclassificationdetection or when the model encounters out-of-distribution from novel classes,i.e., out-of-distribution detection. This paper introduces Evidential ReasoningNetwork (EVINET), a framework that addresses these two challenges byintegrating Beta embedding within a subjective logic framework. EVINET includestwo key modules: Dissonance Reasoning for misclassification detection andVacuity Reasoning for out-of-distribution detection. Extensive experimentsdemonstrate that EVINET outperforms state-of-the-art methods across multiplemetrics in the tasks of in-distribution classification, misclassificationdetection, and out-of-distribution detection. EVINET demonstrates the necessityof uncertainty estimation and logical reasoning for misclassification detectionand out-of-distribution detection and paves the way for open-world graphlearning. Our code and data are available at https://github.com/SSSKJ/EviNET.",Weijie Guan,2025/6/8,2025/8/1,,,['cs.LG']
2506.06881v1,KnowCoder-V2: Deep Knowledge Analysis,http://arxiv.org/abs/2506.06881v1,"Deep knowledge analysis tasks always involve the systematic extraction andassociation of knowledge from large volumes of data, followed by logicalreasoning to discover insights. However, to solve such complex tasks, existingdeep research frameworks face three major challenges: 1) They lack systematicorganization and management of knowledge; 2) They operate purely online, makingit inefficient for tasks that rely on shared and large-scale knowledge; 3) Theycannot perform complex knowledge computation, limiting their abilities toproduce insightful analytical results. Motivated by these, in this paper, wepropose a \textbf{K}nowledgeable \textbf{D}eep \textbf{R}esearch (\textbf{KDR})framework that empowers deep research with deep knowledge analysis capability.Specifically, it introduces an independent knowledge organization phase topreprocess large-scale, domain-relevant data into systematic knowledge offline.Based on this knowledge, it extends deep research with an additional kind ofreasoning steps that perform complex knowledge computation in an online manner.To enhance the abilities of LLMs to solve knowledge analysis tasks in the aboveframework, we further introduce \textbf{\KCII}, an LLM that bridges knowledgeorganization and reasoning via unified code generation. For knowledgeorganization, it generates instantiation code for predefined classes,transforming data into knowledge objects. For knowledge computation, itgenerates analysis code and executes on the above knowledge objects to obtaindeep analysis results. Experimental results on more than thirty datasets acrosssix knowledge analysis tasks demonstrate the effectiveness of \KCII. Moreover,when integrated into the KDR framework, \KCII can generate high-quality reportswith insightful analytical results compared to the mainstream deep researchframework.",Zixuan Li,2025/6/7,2025/6/7,,,['cs.AI']
2506.03984v1,Around the World in 24 Hours: Probing LLM Knowledge of Time and Place,http://arxiv.org/abs/2506.03984v1,"Reasoning over time and space is essential for understanding our world.However, the abilities of language models in this area are largely unexploredas previous work has tested their abilities for logical reasoning in terms oftime and space in isolation or only in simple or artificial environments. Inthis paper, we present the first evaluation of the ability of language modelsto jointly reason over time and space. To enable our analysis, we createGeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37time zones. Using GeoTemp, we evaluate eight open chat models of threedifferent model families for different combinations of temporal and geographicknowledge. We find that most models perform well on reasoning tasks involvingonly temporal knowledge and that overall performance improves with scale.However, performance remains constrained in tasks that require connectingtemporal and geographical information. We do not find clear correlations ofperformance with specific geographic regions. Instead, we find a significantperformance increase for location names with low model perplexity, suggestingtheir repeated occurrence during model training. We further demonstrate thattheir performance is heavily influenced by prompt formulation - a directinjection of geographical knowledge leads to performance gains, whereas,surprisingly, techniques like chain-of-thought prompting decrease performanceon simpler tasks.",Carolin Holtermann,2025/6/4,2025/6/4,,,['cs.CL']
2506.03771v1,Toward Entailment Checking: Explore Eigenmarking Search,http://arxiv.org/abs/2506.03771v1,"Logic entailment is essential to reasoning,  but entailment checking has the worst-case complexity of an exponential ofthe variable size.  With recent development, quantum computing when mature  may allow an effective approach for various combinatorial problems,  including entailment checking.  Grover algorithm uses Grover operations, selective phase inversion andamplitude amplification  to address a search over unstructured data with quadratic improvement from aclassical method.  Its original form is intended to a single-winner scenario: exactly one matchis promised.  Its extension to multiple-winner cases employs probabilistic control over  a number of applications of Grover operations, while a no-winner case ishandled by time-out.  Our study explores various schemes of ``eigenmarking'' approach.  Still relying on Grover operations, but the approach introduces additionalqubits  to tag the eigenstates.  The tagged eigenstates are to facilitate an interpretation of the measuredresults and  enhance identification of a no-winner case (related to no logic violation inentailment context).  Our investigation experiments three variations of eigenmarking on a two-qubitsystem  using an IBM Aer simulator.  The results show strong distinguishability in all schemes with the bestrelative distinguishabilities of 19 and 53 in worst case  and in average case, respectively.  Our findings reveal a viable quantum mechanism to differentiate a no-winnercase  from other scenarios, which could play a pivot role in entailment checkingand logic reasoning in general.",Tatpong Katanyukul,2025/6/4,2025/6/4,,,"['quant-ph', 'cs.ET', 'cs.LO']"
2506.03295v2,Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem,http://arxiv.org/abs/2506.03295v2,"We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possessimmense reasoning potential inherited from the pre-training stage. Withreinforcement learning (RL), these models can improve dramatically on reasoningtasks. Recent studies have shown that even RL on a single problem can unleashthese models' reasoning capabilities. However, RL is not only expensive butalso unstable. Even one-shot RL requires hundreds of GPU hours. This raises acritical question: Is there a more efficient way to unleash the reasoningpotential of these powerful base LLMs? In this work, we demonstrate thatCritique Fine-Tuning (CFT) on only one problem can effectively unleash thereasoning potential of LLMs. Our method constructs critique data by collectingdiverse model-generated solutions to a single problem and using teacher LLMs toprovide detailed critiques. We fine-tune Qwen and Llama family models, rangingfrom 1.5B to 14B parameters, on the CFT data and observe significantperformance gains across diverse reasoning tasks. For example, with just 5 GPUhours of training, Qwen-Math-7B-CFT show an average improvement of 15% on sixmath benchmarks and 16% on three logic reasoning benchmarks. These results arecomparable to or even surpass the results from RL with 20x less compute.Ablation studies reveal the robustness of one-shot CFT across different promptproblems. These results highlight one-shot CFT as a simple, general, andcompute-efficient approach to unleashing the reasoning capabilities of modernLLMs.",Yubo Wang,2025/6/3,2025/6/5,,,"['cs.CL', 'cs.LG']"
2506.02690v1,Towards Geometry Problem Solving in the Large Model Era: A Survey,http://arxiv.org/abs/2506.02690v1,"Geometry problem solving (GPS) represents a critical frontier in artificialintelligence, with profound applications in education, computer-aided design,and computational graphics. Despite its significance, automating GPS remainschallenging due to the dual demands of spatial understanding and rigorouslogical reasoning. Recent advances in large models have enabled notablebreakthroughs, particularly for SAT-level problems, yet the field remainsfragmented across methodologies, benchmarks, and evaluation frameworks. Thissurvey systematically synthesizes GPS advancements through three coredimensions: (1) benchmark construction, (2) textual and diagrammatic parsing,and (3) reasoning paradigms. We further propose a unified analytical paradigm,assess current limitations, and identify emerging opportunities to guide futureresearch toward human-level geometric reasoning, including automated benchmarkgeneration and interpretable neuro-symbolic integration.",Yurui Zhao,2025/6/3,2025/6/3,,,"['cs.CV', 'math.GT']"
2505.23977v1,VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL,http://arxiv.org/abs/2505.23977v1,"Vision language models (VLMs) are expected to perform effective multimodalreasoning and make logically coherent decisions, which is critical to taskssuch as diagram understanding and spatial problem solving. However, current VLMreasoning lacks large-scale and well-structured training datasets. To bridgethis gap, we propose VisualSphinx, a first-of-its-kind large-scale syntheticvisual logical reasoning training data. To tackle the challenge of imagesynthesis with grounding answers, we propose a rule-to-image synthesispipeline, which extracts and expands puzzle rules from seed questions andgenerates the code of grounding synthesis image synthesis for puzzle sampleassembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinxbenefit from logical coherence and readability of our dataset and exhibitimproved performance on logical reasoning tasks. The enhanced reasoningcapabilities developed from VisualSphinx also benefit other reasoning taskssuch as algebraic reasoning, arithmetic reasoning and geometry reasoning.",Yichen Feng,2025/5/29,2025/5/29,,,"['cs.CV', 'cs.AI', 'cs.LG']"
2505.23648v1,Continuous Chain of Thought Enables Parallel Exploration and Reasoning,http://arxiv.org/abs/2505.23648v1,"Current language models generate chain-of-thought traces by autoregressivelysampling tokens from a finite vocabulary. While this discrete sampling hasachieved remarkable success, conducting chain-of-thought withcontinuously-valued tokens (CoT2) offers a richer and more expressivealternative. Our work examines the benefits of CoT2 through logical reasoningtasks that inherently require search capabilities and provide optimization andexploration methods for CoT2. Theoretically, we show that CoT2 allows the modelto track multiple traces in parallel and quantify its benefits for inferenceefficiency. Notably, one layer transformer equipped with CoT2 can provablysolve the combinatorial ""subset sum problem"" given sufficient embeddingdimension. These insights lead to a novel and effective supervision strategywhere we match the softmax outputs to the empirical token distributions of aset of target traces. Complementing this, we introduce sampling strategies thatunlock policy optimization and self-improvement for CoT2. Our first strategysamples and composes $K$ discrete tokens at each decoding step to control thelevel of parallelism, and reduces to standard CoT when $K=1$. Our secondstrategy relies on continuous exploration over the probability simplex.Experiments confirm that policy optimization with CoT2 indeed improves theperformance of the model beyond its initial discrete or continuous supervision.",Halil Alperen Gozeten,2025/5/29,2025/5/29,,,['cs.LG']
2505.21398v1,A Structured Unplugged Approach for Foundational AI Literacy in Primary Education,http://arxiv.org/abs/2505.21398v1,"Younger generations are growing up in a world increasingly shaped byintelligent technologies, making early AI literacy crucial for developing theskills to critically understand and navigate them. However, education in thisfield often emphasizes tool-based learning, prioritizing usage overunderstanding the underlying concepts. This lack of knowledge leavesnon-experts, especially children, prone to misconceptions, unrealisticexpectations, and difficulties in recognizing biases and stereotypes. In thispaper, we propose a structured and replicable teaching approach that fostersfoundational AI literacy in primary students, by building upon coremathematical elements closely connected to and of interest in primarycurricula, to strengthen conceptualization, data representation, classificationreasoning, and evaluation of AI. To assess the effectiveness of our approach,we conducted an empirical study with thirty-one fifth-grade students across twoclasses, evaluating their progress through a post-test and a satisfactionsurvey. Our results indicate improvements in terminology understanding andusage, features description, logical reasoning, and evaluative skills, withstudents showing a deeper comprehension of decision-making processes and theirlimitations. Moreover, the approach proved engaging, with students particularlyenjoying activities that linked AI concepts to real-world reasoning. Materials:https://github.com/tail-unica/ai-literacy-primary-ed.",Maria Cristina Carrisi,2025/5/27,2025/5/27,,,"['cs.AI', 'cs.ET']"
2505.19797v3,The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants,http://arxiv.org/abs/2505.19797v3,"Proprietary giants are increasingly dominating the race for ever-largerlanguage models. Can open-source, smaller models remain competitive across abroad range of tasks? In this paper, we present the Avengers -- a simple recipethat leverages the collective intelligence of these smaller models. TheAvengers builds upon four lightweight operations: (i) embedding: encode queriesusing a text embedding model; (ii) clustering: group queries based on theirsemantic similarity; (iii) scoring: scores each model's performance within eachcluster; and (iv) voting: improve outputs via repeated sampling and voting. Atinference time, each query is embedded and assigned to its nearest cluster. Thetop-performing model(s) within that cluster are selected to generate theresponse with repeated sampling. Remarkably, with 10 open-source models (~7Bparameters each), the Avengers surpasses GPT-4o, 4.1, and 4.5 in averageperformance across 15 diverse datasets spanning mathematics, coding, logicalreasoning, general knowledge, and affective tasks. In particular, it surpassesGPT-4.1 on mathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore,the Avengers delivers superior out-of-distribution generalization, and remainsrobust across various embedding models, clustering algorithms, ensemblestrategies, and values of its sole parameter -- the number of clusters.",Yiqun Zhang,2025/5/26,2025/6/18,,,['cs.CL']
2505.15817v2,Learning to Reason via Mixture-of-Thought for Logical Reasoning,http://arxiv.org/abs/2505.15817v2,"Human beings naturally utilize multiple reasoning modalities to learn andsolve logical problems, i.e., different representational formats such asnatural language, code, and symbolic logic. In contrast, most existingLLM-based approaches operate with a single reasoning modality during training,typically natural language. Although some methods explored modality selectionor augmentation at inference time, the training process remains modality-blind,limiting synergy among modalities. To fill in this gap, we proposeMixture-of-Thought (MoT), a framework that enables LLMs to reason across threecomplementary modalities: natural language, code, and a newly introducedsymbolic modality, truth-table, which systematically enumerates logical casesand partially mitigates key failure modes in natural language reasoning. MoTadopts a two-phase design: (1) self-evolving MoT training, which jointly learnsfrom filtered, self-generated rationales across modalities; and (2) MoTinference, which fully leverages the synergy of three modalities to producebetter predictions. Experiments on logical reasoning benchmarks including FOLIOand ProofWriter demonstrate that our MoT framework consistently andsignificantly outperforms strong LLM baselines with single-modalitychain-of-thought approaches, achieving up to +11.7pp average accuracy gain.Further analyses show that our MoT framework benefits both training andinference stages; that it is particularly effective on harder logical reasoningproblems; and that different modalities contribute complementary strengths,with truth-table reasoning helping to overcome key bottlenecks in naturallanguage inference.",Tong Zheng,2025/5/21,2025/6/9,,,['cs.CL']
2505.12766v1,Reasoning-OCR: Can Large Multimodal Models Solve Complex Logical Reasoning Problems from OCR Cues?,http://arxiv.org/abs/2505.12766v1,"Large Multimodal Models (LMMs) have become increasingly versatile,accompanied by impressive Optical Character Recognition (OCR) relatedcapabilities. Existing OCR-related benchmarks emphasize evaluating LMMs'abilities of relatively simple visual question answering, visual-text parsing,etc. However, the extent to which LMMs can deal with complex logical reasoningproblems based on OCR cues is relatively unexplored. To this end, we introducethe Reasoning-OCR benchmark, which challenges LMMs to solve complex reasoningproblems based on the cues that can be extracted from rich visual-text.Reasoning-OCR covers six visual scenarios and encompasses 150 meticulouslydesigned questions categorized into six reasoning challenges. Additionally,Reasoning-OCR minimizes the impact of field-specialized knowledge. Ourevaluation offers some insights for proprietary and open-source LMMs indifferent reasoning challenges, underscoring the urgent to improve thereasoning performance. We hope Reasoning-OCR can inspire and facilitate futureresearch on enhancing complex reasoning ability based on OCR cues.Reasoning-OCR is publicly available athttps://github.com/Hxyz-123/ReasoningOCR.",Haibin He,2025/5/19,2025/5/19,,,['cs.CV']
2505.12307v1,LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?,http://arxiv.org/abs/2505.12307v1,"Recent advances in Large Multimodal Models (LMMs) have significantly improvedtheir reasoning and Optical Character Recognition (OCR) capabilities. However,their performance on complex logical reasoning tasks involving text-rich imagesremains underexplored. To bridge this gap, we introduce LogicOCR, a benchmarkcomprising 1,100 multiple-choice questions designed to evaluate LMMs' logicalreasoning abilities on text-rich images, while minimizing reliance ondomain-specific knowledge (e.g., mathematics). We construct LogicOCR bycurating a text corpus from the Chinese National Civil Servant Examination anddevelop a scalable, automated pipeline to convert it into multimodal samples.First, we design prompt templates to steer GPT-Image-1 to generate images withdiverse backgrounds, interleaved text-illustration layouts, and varied fonts,ensuring contextual relevance and visual realism. Then, the generated imagesare manually verified, with low-quality examples discarded. We evaluate a rangeof representative open-source and proprietary LMMs under both Chain-of-Thought(CoT) and direct-answer settings. Our multi-dimensional analysis reveals keyinsights, such as the impact of test-time scaling, input modality differences,and sensitivity to visual-text orientation. Notably, LMMs still lag inmultimodal reasoning compared to text-only inputs, indicating that they havenot fully bridged visual reading with reasoning. We hope LogicOCR will serve asa valuable resource for advancing multimodal reasoning research. The dataset isavailable at https://github.com/MiliLab/LogicOCR.",Maoyuan Ye,2025/5/18,2025/5/18,,,"['cs.CV', 'cs.CL']"
2505.12284v1,Efficient RL Training for Reasoning Models via Length-Aware Optimization,http://arxiv.org/abs/2505.12284v1,"Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstratedremarkable performance on reasoning tasks but often incur a long reasoning pathwith significant memory and time costs. Existing methods primarily aim toshorten reasoning paths by introducing additional training data and stages. Inthis paper, we propose three critical reward designs integrated directly intothe reinforcement learning process of large reasoning models, which reduce theresponse length without extra training stages. Experiments on four settingsshow that our method significantly decreases response length while maintainingor even improving performance. Specifically, in a logic reasoning setting, weachieve a 40% reduction in response length averaged by steps alongside a 14%gain in performance. For math problems, we reduce response length averaged bysteps by 33% while preserving performance.",Danlong Yuan,2025/5/18,2025/5/18,,,"['cs.AI', 'cs.CL']"
2505.12275v1,Curriculum Abductive Learning,http://arxiv.org/abs/2505.12275v1,"Abductive Learning (ABL) integrates machine learning with logical reasoningin a loop: a learning model predicts symbolic concept labels from raw inputs,which are revised through abduction using domain knowledge and then fed backfor retraining. However, due to the nondeterminism of abduction, the trainingprocess often suffers from instability, especially when the knowledge base islarge and complex, resulting in a prohibitively large abduction space. Whileprior works focus on improving candidate selection within this space, theytypically treat the knowledge base as a static black box. In this work, wepropose Curriculum Abductive Learning (C-ABL), a method that explicitlyleverages the internal structure of the knowledge base to address the ABLtraining challenges. C-ABL partitions the knowledge base into a sequence ofsub-bases, progressively introduced during training. This reduces the abductionspace throughout training and enables the model to incorporate logic in astepwise, smooth way. Experiments across multiple tasks show that C-ABLoutperforms previous ABL implementations, significantly improves trainingstability, convergence speed, and final accuracy, especially under complexknowledge setting.",Wen-Chao Hu,2025/5/18,2025/5/18,,,"['cs.LG', 'cs.AI']"
2505.13529v1,BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs,http://arxiv.org/abs/2505.13529v1,"Recent advances in Large Reasoning Models (LRMs) have shown impressivecapabilities in mathematical and logical reasoning. However, current LRMsrarely admit ignorance or respond with ""I don't know"". Instead, they oftenproduce incorrect answers while showing undue confidence, raising concernsabout their factual reliability. In this work, we identify two pathologicalreasoning patterns characterized by overthinking that contribute to theoverconfident and incorrect answers: last-minute guessing and second-thoughtspiraling. To address these issues, we propose BARREL-a novel framework thatpromotes concise and boundary-aware factual reasoning. Our experiments showthat BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8Bfrom 39.33% to 61.48%, while still achieving accuracy comparable to modelsfinetuned on reasoning data generated by R1. These results demonstrate that ourpilot study is inspiring to build more reliable and factual System 2 LRMs.",Junxiao Yang,2025/5/18,2025/5/18,,,"['cs.AI', 'cs.CL', 'cs.LG']"
2505.11854v1,Evaluating the Logical Reasoning Abilities of Large Reasoning Models,http://arxiv.org/abs/2505.11854v1,"Large reasoning models, often post-trained on long chain-of-thought (longCoT) data with reinforcement learning, achieve state-of-the-art performance onmathematical, coding, and domain-specific reasoning benchmarks. However, theirlogical reasoning capabilities - fundamental to human cognition and independentof domain knowledge - remain understudied. To address this gap, we introduceLogiEval, a holistic benchmark for evaluating logical reasoning in largereasoning models. LogiEval spans diverse reasoning types (deductive, inductive,analogical, and abductive) and task formats (e.g., logical sequence, argumentanalysis), sourced from high-quality human examinations (e.g., LSAT, GMAT). Ourexperiments demonstrate that modern reasoning models excel at 4-choice argumentanalysis problems and analogical reasoning, surpassing human performance, yetexhibit uneven capabilities across reasoning types and formats, highlightinglimitations in their generalization. Our analysis reveals that humanperformance does not mirror model failure distributions. To foster furtherresearch, we curate LogiEval-Hard, a challenging subset identified through anovel screening paradigm where small-model failures (Qwen3-30B-A3B) reliablypredict difficulties for larger models. Modern models show striking, consistentfailures on LogiEval-Hard. This demonstrates that fundamental reasoningbottlenecks persist across model scales, and establishes LogiEval-Hard as botha diagnostic tool and a rigorous testbed for advancing logical reasoning inLLMs.",Hanmeng Liu,2025/5/17,2025/5/17,,,['cs.AI']
2505.09862v1,Rhetorical XAI: Explaining AI's Benefits as well as its Use via Rhetorical Design,http://arxiv.org/abs/2505.09862v1,"This paper explores potential benefits of incorporating Rhetorical Designinto the design of Explainable Artificial Intelligence (XAI) systems. While XAIis traditionally framed around explaining individual predictions or overallsystem behavior, explanations also function as a form of argumentation, shapinghow users evaluate system perceived usefulness, credibility, and fosterappropriate trust. Rhetorical Design offers a useful framework to analyze thecommunicative role of explanations between AI systems and users, focusing on:(1) logical reasoning conveyed through different types of explanations, (2)credibility projected by the system and its developers, and (3) emotionalresonance elicited in users. Together, these rhetorical appeals help usunderstand how explanations influence user perceptions and facilitate AIadoption. This paper synthesizes design strategies from prior XAI work thatalign with these three rhetorical appeals and highlights both opportunities andchallenges of integrating rhetorical design into XAI design.",Houjiang Liu,2025/5/14,2025/5/14,,,"['cs.HC', 'cs.CY']"
2505.03668v1,Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time,http://arxiv.org/abs/2505.03668v1,"This paper proposes an integration of temporal logical reasoning andPartially Observable Markov Decision Processes (POMDPs) to achieveinterpretable decision-making under uncertainty with macro-actions. Our methodleverages a fragment of Linear Temporal Logic (LTL) based on Event Calculus(EC) to generate \emph{persistent} (i.e., constant) macro-actions, which guideMonte Carlo Tree Search (MCTS)-based POMDP solvers over a time horizon,significantly reducing inference time while ensuring robust performance. Suchmacro-actions are learnt via Inductive Logic Programming (ILP) from a fewtraces of execution (belief-action pairs), thus eliminating the need formanually designed heuristics and requiring only the specification of the POMDPtransition model. In the Pocman and Rocksample benchmark scenarios, our learnedmacro-actions demonstrate increased expressiveness and generality when comparedto time-independent heuristics, indeed offering substantial computationalefficiency improvements.",Celeste Veronese,2025/5/6,2025/5/6,,,['cs.AI']
2504.21088v1,Temperature Mapping in Urban Biomes Using an Infrared Thermometer: An Investigative Approach to Physics Education,http://arxiv.org/abs/2504.21088v1,"In recent years, the frequency of extreme weather events on Earth hasincreased significantly. This phenomenon is driven by the intensification ofthe greenhouse effect caused by anthropogenic activities, leading totemperature variations in urban environments that affect thermal comfort andquality of life. Given this context, the present study investigates temperaturemapping in urban biomes using an infrared thermometer, conducted as part of ahands-on workshop offered during the 21st National Science and Technology Week.The initiative involved students from the public school system and was groundedin Physics education, aiming to foster scientific enculturation. Participantsengaged in a problem-based learning experience, actively contributing to allstages of the knowledge construction process. The objective was to examine therelationship between vegetation presence and its impact on temperature in urbanenvironments. A qualitative and quantitative methodological approach wasadopted, enabling the identification of scientific literacy indicators such asinformation sequencing, data organization, logical reasoning, hypothesisformulation, justification, and explanation of observed phenomena. The analysisof students' statements and activity guidelines provided insights into theircritical thinking development. The findings indicate that students developedessential skills for understanding physical and environmental phenomena,effectively linking collected data to scientific concepts and proposingwell-supported interpretations. Moreover, the experience reinforced theperception of science as a dynamic and investigative process, fosteringcuriosity and enhancing students' argumentative abilities. Thus, the workshopproved to be an effective strategy for promoting scientific literacy andengaging participants in the study of environmental impacts in urban contexts.",Welington Fabrcio dos Santos Costa,2025/4/29,2025/4/29,,,['physics.ed-ph']
2504.19852v2,A Formal Framework for Naturally Specifying and Verifying Sequential Algorithms,http://arxiv.org/abs/2504.19852v2,"Current approaches for formal verification of algorithms face importantlimitations. For specification, they cannot express algorithms naturally andconcisely, especially for algorithms with states and flexible control flow. Forverification, formal proof based on Hoare logic cannot reflect the logicalstructure of natural proof. To address these challenges, we introduce a formalframework for naturally specifying and verifying sequential algorithms in Coq.We use the state relation monad to integrate Coq's expressive type system withthe flexible control flow of imperative languages. It supports nondeterministicoperations and customizable program states, enabling specifying algorithms atan appropriate level of abstraction. For verification, we build a Hoare logicfor the monad and propose a novel two-stage proof approach that separatesnatural logical reasoning from mechanical composition. It reflects the logicalstructure of natural proof, enhancing modularity and readability. We evaluatethe framework by formalizing the Depth-First Search (DFS) algorithm andverifying the Knuth-Morris-Pratt (KMP) algorithm.",Chengxi Yang,2025/4/28,2025/4/30,,,['cs.PL']
2504.16117v1,Context-Awareness and Interpretability of Rare Occurrences for Discovery and Formalization of Critical Failure Modes,http://arxiv.org/abs/2504.16117v1,"Vision systems are increasingly deployed in critical domains such assurveillance, law enforcement, and transportation. However, theirvulnerabilities to rare or unforeseen scenarios pose significant safety risks.To address these challenges, we introduce Context-Awareness andInterpretability of Rare Occurrences (CAIRO), an ontology-based human-assistivediscovery framework for failure cases (or CP - Critical Phenomena) detectionand formalization. CAIRO by design incentivizes human-in-the-loop for testingand evaluation of criticality that arises from misdetections, adversarialattacks, and hallucinations in AI black-box models. Our robust analysis ofobject detection model(s) failures in automated driving systems (ADS) showcasesscalable and interpretable ways of formalizing the observed gaps between cameraperception and real-world contexts, resulting in test cases stored as explicitknowledge graphs (in OWL/XML format) amenable for sharing, downstream analysis,logical reasoning, and accountability.",Sridevi Polavaram,2025/4/18,2025/4/18,,,"['cs.CV', 'cs.AI', 'cs.HC']"
2504.12749v1,LAD-Reasoner: Tiny Multimodal Models are Good Reasoners for Logical Anomaly Detection,http://arxiv.org/abs/2504.12749v1,"Recent advances in industrial anomaly detection have highlighted the need fordeeper logical anomaly analysis, where unexpected relationships among objects,counts, and spatial configurations must be identified and explained. Existingapproaches often rely on large-scale external reasoning modules or elaboratepipeline designs, hindering practical deployment and interpretability. Toaddress these limitations, we introduce a new task, Reasoning Logical AnomalyDetection (RLAD), which extends traditional anomaly detection by incorporatinglogical reasoning. We propose a new framework, LAD-Reasoner, a customized tinymultimodal language model built on Qwen2.5-VL 3B. Our approach leverages atwo-stage training paradigm that first employs Supervised Fine-Tuning (SFT) forfine-grained visual understanding, followed by Group Relative PolicyOptimization (GRPO) to refine logical anomaly detection and enforce coherent,human-readable reasoning. Crucially, reward signals are derived from both thedetection accuracy and the structural quality of the outputs, obviating theneed for building chain of thought (CoT) reasoning data. Experiments on theMVTec LOCO AD dataset show that LAD-Reasoner, though significantly smaller,matches the performance of Qwen2.5-VL-72B in accuracy and F1 score, and furtherexcels in producing concise and interpretable rationales. This unified designreduces reliance on large models and complex pipelines, while offeringtransparent and interpretable insights into logical anomaly detection. Code anddata will be released.",Weijia Li,2025/4/17,2025/4/17,,,['cs.CV']
2504.11008v2,MediSee: Reasoning-based Pixel-level Perception in Medical Images,http://arxiv.org/abs/2504.11008v2,"Despite remarkable advancements in pixel-level medical image perception,existing methods are either limited to specific tasks or heavily rely onaccurate bounding boxes or text labels as input prompts. However, the medicalknowledge required for input is a huge obstacle for general public, whichgreatly reduces the universality of these methods. Compared with thesedomain-specialized auxiliary information, general users tend to rely on oralqueries that require logical reasoning. In this paper, we introduce a novelmedical vision task: Medical Reasoning Segmentation and Detection (MedSD),which aims to comprehend implicit queries about medical images and generate thecorresponding segmentation mask and bounding box for the target object. Toaccomplish this task, we first introduce a Multi-perspective, Logic-drivenMedical Reasoning Segmentation and Detection (MLMR-SD) dataset, whichencompasses a substantial collection of medical entity targets along with theircorresponding reasoning. Furthermore, we propose MediSee, an effective baselinemodel designed for medical reasoning segmentation and detection. Theexperimental results indicate that the proposed method can effectively addressMedSD with implicit colloquial queries and outperform traditional medicalreferring segmentation methods.",Qinyue Tong,2025/4/15,2025/4/23,,,"['cs.CV', 'cs.AI']"
2504.10885v1,PuzzleBench: A Fully Dynamic Evaluation Framework for Large Multimodal Models on Puzzle Solving,http://arxiv.org/abs/2504.10885v1,"Large Multimodal Models (LMMs) have demonstrated impressive capabilitiesacross a wide range of multimodal tasks, achieving ever-increasing performanceon various evaluation benchmarks. However, existing benchmarks are typicallystatic and often overlap with pre-training datasets, leading to fixedcomplexity constraints and substantial data contamination issues. Meanwhile,manually annotated datasets are labor-intensive, time-consuming, and subject tohuman bias and inconsistency, leading to reliability and reproducibilityissues. To address these problems, we propose a fully dynamic multimodalevaluation framework, named Open-ended Visual Puzzle Generation (OVPG), whichaims to generate fresh, diverse, and verifiable evaluation data automaticallyin puzzle-solving tasks. Specifically, the OVPG pipeline consists of a rawmaterial sampling module, a visual content generation module, and a puzzle ruledesign module, which ensures that each evaluation instance is primitive, highlyrandomized, and uniquely solvable, enabling continual adaptation to theevolving capabilities of LMMs. Built upon OVPG, we construct PuzzleBench, adynamic and scalable benchmark comprising 11,840 VQA samples. It features sixcarefully designed puzzle tasks targeting three core LMM competencies, visualrecognition, logical reasoning, and context understanding. PuzzleBench differsfrom static benchmarks that quickly become outdated. It enables ongoing datasetrefreshing through OVPG and a rich set of open-ended puzzle designs, allowingseamless adaptation to the evolving capabilities of LMMs.",Zeyu Zhang,2025/4/15,2025/4/15,,,"['cs.CV', 'cs.AI']"
2504.09058v1,Towards Stepwise Domain Knowledge-Driven Reasoning Optimization and Reflection Improvement,http://arxiv.org/abs/2504.09058v1,"Recently, stepwise supervision on Chain of Thoughts (CoTs) presents anenhancement on the logical reasoning tasks such as coding and math, with thehelp of Monte Carlo Tree Search (MCTS). However, its contribution to tasksrequiring domain-specific expertise and knowledge remains unexplored. Motivatedby the interest, we identify several potential challenges of vanilla MCTSwithin this context, and propose the framework of Stepwise DomainKnowledge-Driven Reasoning Optimization, employing the MCTS algorithm todevelop step-level supervision for problems that require essentialcomprehension, reasoning, and specialized knowledge. Additionally, we alsointroduce the Preference Optimization towards Reflection Paths, whichiteratively learns self-reflection on the reasoning thoughts from betterperspectives. We have conducted extensive experiments to evaluate the advantageof the methodologies. Empirical results demonstrate the effectiveness onvarious legal-domain problems. We also report a diverse set of valuablefindings, hoping to encourage the enthusiasm to the research of domain-specificLLMs and MCTS.",Chengyuan Liu,2025/4/12,2025/4/12,,,"['cs.AI', 'cs.CL']"
2504.05419v1,Reasoning Models Know When They're Right: Probing Hidden States for Self-Verification,http://arxiv.org/abs/2504.05419v1,"Reasoning models have achieved remarkable performance on tasks like math andlogical reasoning thanks to their ability to search during reasoning. However,they still suffer from overthinking, often performing unnecessary reasoningsteps even after reaching the correct answer. This raises the question: canmodels evaluate the correctness of their intermediate answers during reasoning?In this work, we study whether reasoning models encode information about answercorrectness through probing the model's hidden states. The resulting probe canverify intermediate answers with high accuracy and produces highly calibratedscores. Additionally, we find models' hidden states encode correctness offuture answers, enabling early prediction of the correctness before theintermediate answer is fully formulated. We then use the probe as a verifier todecide whether to exit reasoning at intermediate answers during inference,reducing the number of inference tokens by 24\% without compromisingperformance. These findings confirm that reasoning models do encode a notion ofcorrectness yet fail to exploit it, revealing substantial untapped potential toenhance their efficiency.",Anqi Zhang,2025/4/7,2025/4/7,,,"['cs.AI', 'cs.CL']"
2504.04702v1,Provable Failure of Language Models in Learning Majority Boolean Logic via Gradient Descent,http://arxiv.org/abs/2504.04702v1,"Recent advancements in Transformer-based architectures have led to impressivebreakthroughs in natural language processing tasks, with models such as GPT-4,Claude, and Gemini demonstrating human-level reasoning abilities. However,despite their high performance, concerns remain about the inherent limitationsof these models, especially when it comes to learning basic logical functions.While complexity-theoretic analyses indicate that Transformers can representsimple logic functions (e.g., $\mathsf{AND}$, $\mathsf{OR}$, and majoritygates) by its nature of belonging to the $\mathsf{TC}^0$ class, these resultsassume ideal parameter settings and do not account for the constraints imposedby gradient descent-based training methods. In this work, we investigatewhether Transformers can truly learn simple majority functions when trainedusing gradient-based methods. We focus on a simplified variant of theTransformer architecture and consider both $n=\mathrm{poly}(d)$ and$n=\exp(\Omega(d))$ number of training samples, where each sample is a $d$-sizebinary string paired with the output of a basic majority function. Our analysisdemonstrates that even after $\mathrm{poly}(d)$ gradient queries, thegeneralization error of the Transformer model still remains substantiallylarge, growing exponentially with $d$. This work highlights fundamentaloptimization challenges in training Transformers for the simplest logicalreasoning tasks and provides new insights into their theoretical limitations.",Bo Chen,2025/4/7,2025/4/7,,,"['cs.LG', 'cs.AI', 'cs.CC']"
2504.02826v4,Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing,http://arxiv.org/abs/2504.02826v4,"Large Multi-modality Models (LMMs) have made significant progress in visualunderstanding and generation, but they still face challenges in General VisualEditing, particularly in following complex instructions, preserving appearanceconsistency, and supporting flexible input formats. To study this gap, weintroduce RISEBench, the first benchmark for evaluating Reasoning-InformedviSual Editing (RISE). RISEBench focuses on four key reasoning categories:Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality testcases for each category and propose an robust evaluation framework thatassesses Instruction Reasoning, Appearance Consistency, and Visual Plausibilitywith both human judges and the LMM-as-a-judge approach. We conductedexperiments evaluating nine prominent visual editing models, comprising bothopen-source and proprietary models. The evaluation results demonstrate thatcurrent models face significant challenges in reasoning-based editing tasks.Even the most powerful model evaluated, GPT-4o-Image, achieves an accuracy ofmerely 28.8%. RISEBench effectively highlights the limitations of contemporaryediting models, provides valuable insights, and indicates potential futuredirections for the field of reasoning-aware visual editing. Our code and datahave been released at https://github.com/PhoenixZ810/RISEBench.",Xiangyu Zhao,2025/4/3,2025/5/27,,,['cs.CV']
2503.23064v2,VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models,http://arxiv.org/abs/2503.23064v2,"Large Vision-Language Models (LVLMs) struggle with puzzles, which requireprecise perception, rule comprehension, and logical reasoning. Assessing andenhancing their performance in this domain is crucial, as it reflects theirability to engage in structured reasoning - an essential skill for real-worldproblem-solving. However, existing benchmarks primarily evaluate pre-trainedmodels without additional training or fine-tuning, often lack a dedicated focuson reasoning, and fail to establish a systematic evaluation framework. Toaddress these limitations, we introduce VGRP-Bench, a Visual Grid ReasoningPuzzle Benchmark featuring 20 diverse puzzles. VGRP-Bench spans multipledifficulty levels, and includes extensive experiments not only on existing chatLVLMs (e.g., GPT-4o), but also on reasoning LVLMs (e.g., Gemini-Thinking). Ourresults reveal that even the state-of-the-art LVLMs struggle with thesepuzzles, highlighting fundamental limitations in their puzzle-solvingcapabilities. Most importantly, through systematic experiments, we identify andanalyze key factors influencing LVLMs' puzzle-solving performance, includingthe number of clues, grid size, and rule complexity. Furthermore, we exploretwo Supervised Fine-Tuning (SFT) strategies that can be used in post-training:SFT on solutions (S-SFT) and SFT on synthetic reasoning processes (R-SFT).While both methods significantly improve performance on trained puzzles, theyexhibit limited generalization to unseen ones. We will release VGRP-Bench tofacilitate further research on LVLMs for complex, real-world problem-solving.Project page: https://yufan-ren.com/subpage/VGRP-Bench/.",Yufan Ren,2025/3/29,2025/4/2,,,['cs.CV']
2503.22738v1,ShieldAgent: Shielding Agents via Verifiable Safety Policy Reasoning,http://arxiv.org/abs/2503.22738v1,"Autonomous agents powered by foundation models have seen widespread adoptionacross various real-world applications. However, they remain highly vulnerableto malicious instructions and attacks, which can result in severe consequencessuch as privacy breaches and financial losses. More critically, existingguardrails for LLMs are not applicable due to the complex and dynamic nature ofagents. To tackle these challenges, we propose ShieldAgent, the first guardrailagent designed to enforce explicit safety policy compliance for the actiontrajectory of other protected agents through logical reasoning. Specifically,ShieldAgent first constructs a safety policy model by extracting verifiablerules from policy documents and structuring them into a set of action-basedprobabilistic rule circuits. Given the action trajectory of the protectedagent, ShieldAgent retrieves relevant rule circuits and generates a shieldingplan, leveraging its comprehensive tool library and executable code for formalverification. In addition, given the lack of guardrail benchmarks for agents,we introduce ShieldAgent-Bench, a dataset with 3K safety-related pairs of agentinstructions and action trajectories, collected via SOTA attacks across 6 webenvironments and 7 risk categories. Experiments show that ShieldAgent achievesSOTA on ShieldAgent-Bench and three existing benchmarks, outperforming priormethods by 11.3% on average with a high recall of 90.1%. Additionally,ShieldAgent reduces API queries by 64.7% and inference time by 58.2%,demonstrating its high precision and efficiency in safeguarding agents.",Zhaorun Chen,2025/3/26,2025/3/26,,,"['cs.LG', 'cs.CR']"
2503.18213v1,A Study on Neuro-Symbolic Artificial Intelligence: Healthcare Perspectives,http://arxiv.org/abs/2503.18213v1,"Over the last few decades, Artificial Intelligence (AI) scientists have beenconducting investigations to attain human-level performance by a machine inaccomplishing a cognitive task. Within machine learning, the ultimateaspiration is to attain Artificial General Intelligence (AGI) through amachine. This pursuit has led to the exploration of two distinct AI paradigms.Symbolic AI, also known as classical or GOFAI (Good Old-Fashioned AI) andConnectionist (Sub-symbolic) AI, represented by Neural Systems, are twomutually exclusive paradigms. Symbolic AI excels in reasoning, explainability,and knowledge representation but faces challenges in processing complexreal-world data with noise. Conversely, deep learning (Black-Box systems)research breakthroughs in neural networks are notable, yet they lack reasoningand interpretability. Neuro-symbolic AI (NeSy), an emerging area of AIresearch, attempts to bridge this gap by integrating logical reasoning intoneural networks, enabling them to learn and reason with symbolicrepresentations. While a long path, this strategy has made significant progresstowards achieving common sense reasoning by systems. This article conducts anextensive review of over 977 studies from prominent scientific databases (DBLP,ACL, IEEExplore, Scopus, PubMed, ICML, ICLR), thoroughly examining themultifaceted capabilities of Neuro-Symbolic AI, with a particular focus on itshealthcare applications, particularly in drug discovery, and Proteinengineering research. The survey addresses vital themes, including reasoning,explainability, integration strategies, 41 healthcare-related use cases,benchmarking, datasets, current approach limitations from both healthcare andbroader perspectives, and proposed novel approaches for future experiments.",Delower Hossain,2025/3/23,2025/3/23,,,['cs.AI']
2503.18050v1,(G)I-DLE: Generative Inference via Distribution-preserving Logit Exclusion with KL Divergence Minimization for Constrained Decoding,http://arxiv.org/abs/2503.18050v1,"We propose (G)I-DLE, a new approach to constrained decoding that leverages KLdivergence minimization to preserve the intrinsic conditional probabilitydistribution of autoregressive language models while excluding undesirabletokens. Unlike conventional methods that naively set banned tokens' logits to$-\infty$, which can distort the conversion from raw logits to posteriorprobabilities and increase output variance, (G)I-DLE re-normalizes the allowedtoken probabilities to minimize such distortion. We validate our method on theK2-Eval dataset, specifically designed to assess Korean language fluency,logical reasoning, and cultural appropriateness. Experimental results onQwen2.5 models (ranging from 1.5B to 14B) demonstrate that G-IDLE not onlyboosts mean evaluation scores but also substantially reduces the variance ofoutput quality.",Hanwool Lee,2025/3/23,2025/3/23,,,"['cs.CE', 'cs.CL']"
2503.17860v1,Enhancing Retrieval Systems with Inference-Time Logical Reasoning,http://arxiv.org/abs/2503.17860v1,"Traditional retrieval methods rely on transforming user queries into vectorrepresentations and retrieving documents based on cosine similarity within anembedding space. While efficient and scalable, this approach often fails tohandle complex queries involving logical constructs such as negations,conjunctions, and disjunctions. In this paper, we propose a novelinference-time logical reasoning framework that explicitly incorporates logicalreasoning into the retrieval process. Our method extracts logical reasoningstructures from natural language queries and then composes the individualcosine similarity scores to formulate the final document scores. This approachenables the retrieval process to handle complex logical reasoning withoutcompromising computational efficiency. Our results on both synthetic andreal-world benchmarks demonstrate that the proposed method consistentlyoutperforms traditional retrieval methods across different models and datasets,significantly improving retrieval performance for complex queries.",Felix Faltings,2025/3/22,2025/3/22,,,['cs.CL']
2503.18968v3,MedAgent-Pro: Towards Evidence-based Multi-modal Medical Diagnosis via Reasoning Agentic Workflow,http://arxiv.org/abs/2503.18968v3,"In modern medicine, clinical diagnosis relies on the comprehensive analysisof primarily textual and visual data, drawing on medical expertise to ensuresystematic and rigorous reasoning. Recent advances in large Vision-LanguageModels (VLMs) and agent-based methods hold great potential for medicaldiagnosis, thanks to the ability to effectively integrate multi-modal patientdata. However, they often provide direct answers and draw empirical-drivenconclusions without quantitative analysis, which reduces their reliability andclinical usability. We propose MedAgent-Pro, a new agentic reasoning paradigmthat follows the diagnosis principle in modern medicine, to decouple theprocess into sequential components for step-by-step, evidence-based reasoning.Our MedAgent-Pro workflow presents a hierarchical diagnostic structure tomirror this principle, consisting of disease-level standardized plan generationand patient-level personalized step-by-step reasoning. To support disease-levelplanning, an RAG-based agent is designed to retrieve medical guidelines toensure alignment with clinical standards. For patient-level reasoning, wepropose to integrate professional tools such as visual models to enablequantitative assessments. Meanwhile, we propose to verify the reliability ofeach step to achieve evidence-based diagnosis, enforcing rigorous logicalreasoning and a well-founded conclusion. Extensive experiments across a widerange of anatomical regions, imaging modalities, and diseases demonstrate thesuperiority of MedAgent-Pro to mainstream VLMs, agentic systems andstate-of-the-art expert models. Ablation studies and human evaluation byclinical experts further validate its robustness and clinical relevance. Codeis available at https://github.com/jinlab-imvr/MedAgent-Pro.",Ziyue Wang,2025/3/21,2025/7/2,,,['cs.AI']
2503.17125v5,LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in Reinforcement Learning,http://arxiv.org/abs/2503.17125v5,"Deep Reinforcement Learning (DRL) has demonstrated strong performance inrobotic control but remains susceptible to out-of-distribution (OOD) states,often resulting in unreliable actions and task failure. While previous methodshave focused on minimizing or preventing OOD occurrences, they largely neglectrecovery once an agent encounters such states. Although the latest research hasattempted to address this by guiding agents back to in-distribution states,their reliance on uncertainty estimation hinders scalability in complexenvironments. To overcome this limitation, we introduce Language Models forOut-of-Distribution Recovery (LaMOuR), which enables recovery learning withoutrelying on uncertainty estimation. LaMOuR generates dense reward codes thatguide the agent back to a state where it can successfully perform its originaltask, leveraging the capabilities of LVLMs in image description, logicalreasoning, and code generation. Experimental results show that LaMOuRsubstantially enhances recovery efficiency across diverse locomotion tasks andeven generalizes effectively to complex environments, including humanoidlocomotion and mobile manipulation, where existing methods struggle. The codeand supplementary materials are available at https://lamour-rl.github.io/.",Chan Kim,2025/3/21,2025/3/28,,,"['cs.RO', 'cs.AI']"
2503.14499v2,Measuring AI Ability to Complete Long Tasks,http://arxiv.org/abs/2503.14499v2,"Despite rapid progress on AI benchmarks, the real-world meaning of benchmarkperformance remains unclear. To quantify the capabilities of AI systems interms of human capabilities, we propose a new metric: 50%-task-completion timehorizon. This is the time humans typically take to complete tasks that AImodels can complete with 50% success rate. We first timed humans with relevantdomain expertise on a combination of RE-Bench, HCAST, and 66 novel shortertasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnethave a 50% time horizon of around 50 minutes. Furthermore, frontier AI timehorizon has been doubling approximately every seven months since 2019, thoughthe trend may have accelerated in 2024. The increase in AI models' timehorizons seems to be primarily driven by greater reliability and ability toadapt to mistakes, combined with better logical reasoning and tool usecapabilities. We discuss the limitations of our results -- including theirdegree of external validity -- and the implications of increased autonomy fordangerous capabilities. If these results generalize to real-world softwaretasks, extrapolation of this trend predicts that within 5 years, AI systemswill be capable of automating many software tasks that currently take humans amonth.",Thomas Kwa,2025/3/18,2025/3/30,,,"['cs.AI', 'cs.LG']"
2503.15551v2,Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting Attack,http://arxiv.org/abs/2503.15551v2,"Batch prompting, which combines a batch of multiple queries sharing the samecontext in one inference, has emerged as a promising solution to reduceinference costs. However, our study reveals a significant securityvulnerability in batch prompting: malicious users can inject attackinstructions into a batch, leading to unwanted interference across all queries,which can result in the inclusion of harmful content, such as phishing links,or the disruption of logical reasoning. In this paper, we constructBATCHSAFEBENCH, a comprehensive benchmark comprising 150 attack instructions oftwo types and 8k batch instances, to study the batch prompting vulnerabilitysystematically. Our evaluation of both closed-source and open-weight LLMsdemonstrates that all LLMs are susceptible to batch-prompting attacks. We thenexplore multiple defending approaches. While the prompting-based defense showslimited effectiveness for smaller LLMs, the probing-based approach achievesabout 95% accuracy in detecting attacks. Additionally, we perform a mechanisticanalysis to understand the attack and identify attention heads that areresponsible for it.",Murong Yue,2025/3/18,2025/6/20,,,"['cs.CR', 'cs.AI', 'cs.LG']"
2503.12161v1,Aristotle's Original Idea: For and Against Logic in the era of AI,http://arxiv.org/abs/2503.12161v1,"Aristotle is generally accepted as the father of logic. The ideas that heraised in his study of logical reasoning carried the development of scienceover the centuries. Today, in the era of AI, this title of the fatherhood oflogic has a renewed significance. Behind it lies his original idea that humanreasoning could be studied as a process and that perhaps there exist universalsystems of reasoning that underly all human reasoning irrespective of thecontent of what we are reasoning about. In this article, we look intoAristotle's work on human thought, his work on reasoning itself but also on howit relates to science and human endeavor more generally, from a modernperspective of Artificial Intelligence and ask if this can help enlighten ourunderstanding of AI and Science more generally.",Antonis C. Kakas,2025/3/15,2025/3/15,,,"['cs.AI', 'I.2.0 General']"
2503.10691v2,Reasoning is All You Need for Video Generalization: A Counterfactual Benchmark with Sub-question Evaluation,http://arxiv.org/abs/2503.10691v2,"Counterfactual reasoning is crucial for robust video understanding butremains underexplored in existing multimodal benchmarks. In this paper, weintroduce \textbf{COVER} (\textbf{\underline{CO}}unterfactual\textbf{\underline{V}}id\textbf{\underline{E}}o\textbf{\underline{R}}easoning), a multidimensional multimodal benchmark thatsystematically evaluates MLLMs across the abstract-concrete andperception-cognition dimensions. Beyond prior multimodal benchmarks, COVERdecomposes complex queries into structured sub-questions, enabling fine-grainedreasoning analysis. Experiments on commercial and open-source models reveal astrong correlation between sub-question accuracy and counterfactual reasoningperformance, highlighting the role of structured inference in videounderstanding. Furthermore, our results suggest a key insight: enhancing thereasoning capability of models is essential for improving the robustness ofvideo understanding. COVER establishes a new standard for assessing MLLMs'logical reasoning abilities in dynamic environments. Our work is available athttps://github.com/gongyifan-hash/COVER-Benchmark.",Qiji Zhou,2025/3/12,2025/6/4,,,['cs.CV']
2503.07536v2,LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL,http://arxiv.org/abs/2503.07536v2,"Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challengesfrom the complex interplay between visual perception and logical reasoning,particularly in compact 3B-parameter architectures where architecturalconstraints limit reasoning capacity and modality alignment.  While rule-based reinforcement learning (RL) excels in text-only domains, itsmultimodal extension confronts two critical barriers: (1) data limitations dueto ambiguous answers and scarce complex reasoning examples, and (2) degradedfoundational reasoning induced by multimodal pretraining. To address thesechallenges, we propose \textbf{LMM-R1}, a two-stage framework adaptingrule-based RL for multimodal reasoning through \textbf{Foundational ReasoningEnhancement (FRE)} followed by \textbf{Multimodal Generalization Training(MGT)}. The FRE stage first strengthens reasoning abilities using text-onlydata with rule-based RL, then the MGT stage generalizes these reasoningcapabilities to multimodal domains.  Experiments on Qwen2.5-VL-Instruct-3B demonstrate that LMM-R1 achieves 4.83\%and 4.5\% average improvements over baselines in multimodal and text-onlybenchmarks, respectively, with a 3.63\% gain in complex Football Game tasks.These results validate that text-based reasoning enhancement enables effectivemultimodal generalization, offering a data-efficient paradigm that bypassescostly high-quality multimodal training data.",Yingzhe Peng,2025/3/10,2025/3/11,,,"['cs.CL', 'cs.AI']"
2503.06518v1,Towards Superior Quantization Accuracy: A Layer-sensitive Approach,http://arxiv.org/abs/2503.06518v1,"Large Vision and Language Models have exhibited remarkable human-likeintelligence in tasks such as natural language comprehension, problem-solving,logical reasoning, and knowledge retrieval. However, training and serving thesemodels require substantial computational resources, posing a significantbarrier to their widespread application and further research. To mitigate thischallenge, various model compression techniques have been developed to reducecomputational requirements. Nevertheless, existing methods often employ uniformquantization configurations, failing to account for the varying difficultiesacross different layers in quantizing large neural network models. This papertackles this issue by leveraging layer-sensitivity features, such as activationsensitivity and weight distribution Kurtosis, to identify layers that arechallenging to quantize accurately and allocate additional memory budget. Theproposed methods, named SensiBoost and KurtBoost, respectively, demonstratenotable improvement in quantization accuracy, achieving up to 9% lowerperplexity with only a 2% increase in memory budget on LLama models compared tothe baseline.",Feng Zhang,2025/3/9,2025/3/9,,,"['cs.LG', 'cs.AI']"
2503.06427v1,Pre-Training Meta-Rule Selection Policy for Visual Generative Abductive Learning,http://arxiv.org/abs/2503.06427v1,"Visual generative abductive learning studies jointly training symbol-groundedneural visual generator and inducing logic rules from data, such that afterlearning, the visual generation process is guided by the induced logic rules. Amajor challenge for this task is to reduce the time cost of logic abductionduring learning, an essential step when the logic symbol set is large and thelogic rule to induce is complicated. To address this challenge, we propose apre-training method for obtaining meta-rule selection policy for the recentlyproposed visual generative learning approach AbdGen [Peng et al., 2023], aimingat significantly reducing the candidate meta-rule set and pruning the searchspace. The selection model is built based on the embedding representation ofboth symbol grounding of cases and meta-rules, which can be effectivelyintegrated with both neural model and logic reasoning system. The pre-trainingprocess is done on pure symbol data, not involving symbol grounding learning ofraw visual inputs, making the entire learning process low-cost. An additionalinteresting observation is that the selection policy can rectify symbolgrounding errors unseen during pre-training, which is resulted from thememorization ability of attention mechanism and the relative stability ofsymbolic patterns. Experimental results show that our method is able toeffectively address the meta-rule selection problem for visual abduction,boosting the efficiency of visual generative abductive learning. Code isavailable at https://github.com/future-item/metarule-select.",Yu Jin,2025/3/9,2025/3/9,,,"['cs.LG', 'cs.AI', 'cs.CV']"
2503.04378v2,HelpSteer3: Human-Annotated Feedback and Edit Data to Empower Inference-Time Scaling in Open-Ended General-Domain Tasks,http://arxiv.org/abs/2503.04378v2,"Inference-Time Scaling has been critical to the success of recent models suchas OpenAI o1 and DeepSeek R1. However, many techniques used to train models forinference-time scaling require tasks to have answers that can be verified,limiting their application to domains such as math, coding and logicalreasoning. We take inspiration from how humans make first attempts, ask fordetailed feedback from others and make improvements based on such feedbackacross a wide spectrum of open-ended endeavors. To this end, we collectHelpSteer3 data to train dedicated Feedback and Edit Models that are capable ofperforming inference-time scaling for open-ended general-domain tasks. In oursetup, one model generates an initial response, which are given feedback by asecond model, that are then used by a third model to edit the response. We showthat performance on Arena Hard, a benchmark strongly predictive of ChatbotArena Elo can be boosted by scaling the number of initial response drafts,effective feedback and edited responses. When scaled optimally, our setup basedon 70B models from the Llama 3 family can reach SoTA performance on Arena Hardat 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 andDeepSeek R1 with 92.3.",Zhilin Wang,2025/3/6,2025/5/30,,,"['cs.CL', 'cs.AI', 'cs.LG']"
2503.04848v2,Three tiers of computation in transformers and in brain architectures,http://arxiv.org/abs/2503.04848v2,"Human language and logic abilities are computationally quantified within thewell-studied grammar-automata hierarchy. We identify three hierarchical tiersand two corresponding transitions and show their correspondence to specificabilities in transformer-based language models (LMs). These emergent abilitieshave often been described in terms of scaling; we show that it is thetransition between tiers, rather than scaled size itself, that determines asystem's capabilities. Specifically, humans effortlessly process language yetrequire critical training to perform arithmetic or logical reasoning tasks; andLMs possess language abilities absent from predecessor systems, yet stillstruggle with logical processing. We submit a novel benchmark of computationalpower, provide empirical evaluations of humans and fifteen LMs, and, mostsignificantly, provide a theoretically grounded framework to promote carefulthinking about these crucial topics. The resulting principled analyses provideexplanatory accounts of the abilities and shortfalls of LMs, and suggestactionable insights into the expansion of their logic abilities.",E Graham,2025/3/5,2025/3/12,,,"['cs.CL', 'cs.NE', 'q-bio.NC']"
2503.02172v1,KGCompiler: Deep Learning Compilation Optimization for Knowledge Graph Complex Logical Query Answering,http://arxiv.org/abs/2503.02172v1,"Complex Logical Query Answering (CLQA) involves intricate multi-hop logicalreasoning over large-scale and potentially incomplete Knowledge Graphs (KGs).Although existing CLQA algorithms achieve high accuracy in answering suchqueries, their reasoning time and memory usage scale significantly with thenumber of First-Order Logic (FOL) operators involved, creating seriouschallenges for practical deployment. In addition, current research primarilyfocuses on algorithm-level optimizations for CLQA tasks, often overlookingcompiler-level optimizations, which can offer greater generality andscalability. To address these limitations, we introduce a Knowledge GraphCompiler, namely KGCompiler, the first deep learning compiler specificallydesigned for CLQA tasks. By incorporating KG-specific optimizations proposed inthis paper, KGCompiler enhances the reasoning performance of CLQA algorithmswithout requiring additional manual modifications to their implementations. Atthe same time, it significantly reduces memory usage. Extensive experimentsdemonstrate that KGCompiler accelerates CLQA algorithms by factors ranging from1.04x to 8.26x, with an average speedup of 3.71x. We also provide an interfaceto enable hands-on experience with KGCompiler.",Hongyu Lin,2025/3/4,2025/3/4,,,"['cs.AI', 'cs.SE']"
2502.16965v3,Autoregressive Image Generation with Vision Full-view Prompt,http://arxiv.org/abs/2502.16965v3,"In autoregressive (AR) image generation, models based on the 'next-tokenprediction' paradigm of LLMs have shown comparable performance to diffusionmodels by reducing inductive biases. However, directly applying LLMs to compleximage generation can struggle with reconstructing the image's structure anddetails, impacting the generation's accuracy and stability. Additionally, the'next-token prediction' paradigm in the AR model does not align with thecontextual scanning and logical reasoning processes involved in human visualperception, limiting effective image generation. Prompt engineering, as a keytechnique for guiding LLMs, leverages specifically designed prompts to improvemodel performance on complex natural language processing (NLP) tasks, enhancingaccuracy and stability of generation while maintaining contextual coherence andlogical consistency, similar to human reasoning. Inspired by prompt engineeringfrom the field of NLP, we propose Vision Full-view prompt (VF prompt) toenhance autoregressive image generation. Specifically, we design specializedimage-related VF prompts for AR image generation to simulate the process ofhuman image creation. This enhances contextual logic ability by allowing themodel to first perceive overall distribution information before generating theimage, and improve generation stability by increasing the inference steps.Compared to the AR method without VF prompts, our method shows outstandingperformance and achieves an approximate improvement of 20%.",Miaomiao Cai,2025/2/24,2025/3/12,,,['cs.CV']
2502.14373v1,CrossVTON: Mimicking the Logic Reasoning on Cross-category Virtual Try-on guided by Tri-zone Priors,http://arxiv.org/abs/2502.14373v1,"Despite remarkable progress in image-based virtual try-on systems, generatingrealistic and robust fitting images for cross-category virtual try-on remains achallenging task. The primary difficulty arises from the absence of human-likereasoning, which involves addressing size mismatches between garments andmodels while recognizing and leveraging the distinct functionalities of variousregions within the model images. To address this issue, we draw inspirationfrom human cognitive processes and disentangle the complex reasoning requiredfor cross-category try-on into a structured framework. This frameworksystematically decomposes the model image into three distinct regions: try-on,reconstruction, and imagination zones. Each zone plays a specific role inaccommodating the garment and facilitating realistic synthesis. To endow themodel with robust reasoning capabilities for cross-category scenarios, wepropose an iterative data constructor. This constructor encompasses diversescenarios, including intra-category try-on, any-to-dress transformations(replacing any garment category with a dress), and dress-to-any transformations(replacing a dress with another garment category). Utilizing the generateddataset, we introduce a tri-zone priors generator that intelligently predictsthe try-on, reconstruction, and imagination zones by analyzing how the inputgarment is expected to align with the model image. Guided by these tri-zonepriors, our proposed method, CrossVTON, achieves state-of-the-art performance,surpassing existing baselines in both qualitative and quantitative evaluations.Notably, it demonstrates superior capability in handling cross-category virtualtry-on, meeting the complex demands of real-world applications.",Donghao Luo,2025/2/20,2025/2/20,,,['cs.CV']
2502.12919v1,A Smooth Transition Between Induction and Deduction: Fast Abductive Learning Based on Probabilistic Symbol Perception,http://arxiv.org/abs/2502.12919v1,"Abductive learning (ABL) that integrates strengths of machine learning andlogical reasoning to improve the learning generalization, has been recentlyshown effective. However, its efficiency is affected by the transition betweennumerical induction and symbolical deduction, leading to high computationalcosts in the worst-case scenario. Efforts on this issue remain to be limited.In this paper, we identified three reasons why previous optimization algorithmsfor ABL were not effective: insufficient utilization of prediction, symbolrelationships, and accumulated experience in successful abductive processes,resulting in redundant calculations to the knowledge base. To address thesechallenges, we introduce an optimization algorithm named as ProbabilisticSymbol Perception (PSP), which makes a smooth transition between induction anddeduction and keeps the correctness of ABL unchanged. We leverage probabilityas a bridge and present an efficient data structure, achieving the transferfrom a continuous probability sequence to discrete Boolean sequences with lowcomputational complexity. Experiments demonstrate the promising results.",Lin-Han Jia,2025/2/18,2025/2/18,,,['cs.LG']
2502.12304v1,Warmup Generations: A Task-Agnostic Approach for Guiding Sequence-to-Sequence Learning with Unsupervised Initial State Generation,http://arxiv.org/abs/2502.12304v1,"Traditional supervised fine-tuning (SFT) strategies for sequence-to-sequencetasks often train models to directly generate the target output. Recent workhas shown that guiding models with intermediate steps, such as keywords,outlines, or reasoning chains, can significantly improve performance,coherence, and interpretability. However, these methods often depend onpredefined intermediate formats and annotated data, limiting their scalabilityand generalizability. In this work, we introduce a task-agnostic framework thatenables models to generate intermediate ""warmup"" sequences. These warmupsequences, serving as an initial state for subsequent generation, are optimizedto enhance the probability of generating the target sequence without relying onexternal supervision or human-designed structures. Drawing inspiration fromreinforcement learning principles, our method iteratively refines theseintermediate steps to maximize their contribution to the final output, similarto reward-driven optimization in reinforcement learning with human feedback.Experimental results across tasks such as translation, summarization, andmulti-choice question answering for logical reasoning show that our approachoutperforms traditional SFT methods, and offers a scalable and flexiblesolution for sequence-to-sequence tasks.",Senyu Li,2025/2/17,2025/2/17,,,"['cs.CL', 'cs.AI']"
2502.11291v1,Dialogue-based Explanations for Logical Reasoning using Structured Argumentation,http://arxiv.org/abs/2502.11291v1,"The problem of explaining inconsistency-tolerant reasoning in knowledge bases(KBs) is a prominent topic in Artificial Intelligence (AI). While there is somework on this problem, the explanations provided by existing approaches oftenlack critical information or fail to be expressive enough for non-binaryconflicts. In this paper, we identify structural weaknesses of thestate-of-the-art and propose a generic argumentation-based approach to addressthese problems. This approach is defined for logics involving reasoning withmaximal consistent subsets and shows how any such logic can be translated toargumentation. Our work provides dialogue models as dialectic-proof proceduresto compute and explain a query answer wrt inconsistency-tolerant semantics.This allows us to construct dialectical proof trees as explanations, which aremore expressive and arguably more intuitive than existing explanationformalisms.",Loan Ho,2025/2/16,2025/2/16,,,"['cs.AI', 'cs.DB', 'cs.HC', 'cs.LO']"
2502.09457v1,The Multilingual Mind : A Survey of Multilingual Reasoning in Language Models,http://arxiv.org/abs/2502.09457v1,"While reasoning and multilingual capabilities in Language Models (LMs) haveachieved remarkable progress in recent years, their integration into a unifiedparadigm, multilingual reasoning, is at a nascent stage. Multilingual reasoningrequires language models to handle logical reasoning across languages whileaddressing misalignment, biases, and challenges in low-resource settings. Thissurvey provides the first in-depth review of multilingual reasoning in LMs. Inthis survey, we provide a systematic overview of existing methods that leverageLMs for multilingual reasoning, specifically outlining the challenges,motivations, and foundational aspects of applying language models to reasonacross diverse languages. We provide an overview of the standard data resourcesused for training multilingual reasoning in LMs and the evaluation benchmarksemployed to assess their multilingual capabilities. Next, we analyze variousstate-of-the-art methods and their performance on these benchmarks. Finally, weexplore future research opportunities to improve multilingual reasoning in LMs,focusing on enhancing their ability to handle diverse languages and complexreasoning tasks.",Akash Ghosh,2025/2/13,2025/2/13,,,['cs.CL']
2502.09022v2,Mechanistic Unveiling of Transformer Circuits: Self-Influence as a Key to Model Reasoning,http://arxiv.org/abs/2502.09022v2,"Transformer-based language models have achieved significant success; however,their internal mechanisms remain largely opaque due to the complexity ofnon-linear interactions and high-dimensional operations. While previous studieshave demonstrated that these models implicitly embed reasoning trees, humanstypically employ various distinct logical reasoning mechanisms to complete thesame task. It is still unclear which multi-step reasoning mechanisms are usedby language models to solve such tasks. In this paper, we aim to address thisquestion by investigating the mechanistic interpretability of language models,particularly in the context of multi-step reasoning tasks. Specifically, weemploy circuit analysis and self-influence functions to evaluate the changingimportance of each token throughout the reasoning process, allowing us to mapthe reasoning paths adopted by the model. We apply this methodology to theGPT-2 model on a prediction task (IOI) and demonstrate that the underlyingcircuits reveal a human-interpretable reasoning process used by the model.",Lin Zhang,2025/2/13,2025/2/14,,,['cs.AI']
2502.07591v1,DMWM: Dual-Mind World Model with Long-Term Imagination,http://arxiv.org/abs/2502.07591v1,"Imagination in world models is crucial for enabling agents to learnlong-horizon policy in a sample-efficient manner. Existing recurrentstate-space model (RSSM)-based world models depend on single-step statisticalinference to capture the environment dynamics, and, hence, they are unable toperform long-term imagination tasks due to the accumulation of predictionerrors. Inspired by the dual-process theory of human cognition, we propose anovel dual-mind world model (DMWM) framework that integrates logical reasoningto enable imagination with logical consistency. DMWM is composed of twocomponents: an RSSM-based System 1 (RSSM-S1) component that handles statetransitions in an intuitive manner and a logic-integrated neural network-basedSystem 2 (LINN-S2) component that guides the imagination process throughhierarchical deep logical reasoning. The inter-system feedback mechanism isdesigned to ensure that the imagination process follows the logical rules ofthe real environment. The proposed framework is evaluated on benchmark tasksthat require long-term planning from the DMControl suite. Extensiveexperimental results demonstrate that the proposed framework yields significantimprovements in terms of logical coherence, trial efficiency, data efficiencyand long-term imagination over the state-of-the-art world models.",Lingyi Wang,2025/2/11,2025/2/11,,,"['cs.LG', 'cs.AI']"
2502.02135v1,Standard Neural Computation Alone Is Insufficient for Logical Intelligence,http://arxiv.org/abs/2502.02135v1,"Neural networks, as currently designed, fall short of achieving true logicalintelligence. Modern AI models rely on standard neuralcomputation-inner-product-based transformations and nonlinear activations-toapproximate patterns from data. While effective for inductive learning, thisarchitecture lacks the structural guarantees necessary for deductive inferenceand logical consistency. As a result, deep networks struggle with rule-basedreasoning, structured generalization, and interpretability without extensivepost-hoc modifications. This position paper argues that standard neural layersmust be fundamentally rethought to integrate logical reasoning. We advocate forLogical Neural Units (LNUs)-modular components that embed differentiableapproximations of logical operations (e.g., AND, OR, NOT) directly withinneural architectures. We critique existing neurosymbolic approaches, highlightthe limitations of standard neural computation for logical inference, andpresent LNUs as a necessary paradigm shift in AI. Finally, we outline a roadmapfor implementation, discussing theoretical foundations, architecturalintegration, and key challenges for future research.",Youngsung Kim,2025/2/4,2025/2/4,,,"['cs.AI', 'cs.LG']"
2502.15725v1,Town Hall Debate Prompting: Enhancing Logical Reasoning in LLMs through Multi-Persona Interaction,http://arxiv.org/abs/2502.15725v1,"Debate is a commonly used form of human communication catered towardsproblem-solving because of its efficiency. Debate fundamentally allows multipleviewpoints to be brought up in problem-solving, and for complex problems, eachviewpoint opens a new path for problem-solving. In this work, we apply thisconcept to LLM decision-making by proposing town hall-style debate prompting(THDP), a prompting method that splices a language model into multiple personasthat will debate one another to reach a conclusion. Our experimental pipelinevaries both the number of personas and the personality types of each persona tofind the optimum town hall size and personality for benchmark performance asmeasured by ZebraLogic bench, a reasoning-intensive benchmark characterized byboth multiple-choice and fill-in-the-blank questions. Our experimental resultsdemonstrate that a town hall size of 5 personas with LLM-determined personalitytypes performs optimally on ZebraLogic, achieving a 13\% improvement overone-shot CoT baselines in per-cell accuracy in GPT-4o, 9% puzzle accuracyincrease in Claude 3.5 Sonnet, and an improvement in hard puzzle accuracy from10-15%.",Vivaan Sandwar,2025/1/28,2025/1/28,,,['cs.CL']
2501.16220v2,DBRouting: Routing End User Queries to Databases for Answerability,http://arxiv.org/abs/2501.16220v2,"Enterprise level data is often distributed across multiple sources andidentifying the correct set-of data-sources with relevant information for aknowledge request is a fundamental challenge. In this work, we define the noveltask of routing an end-user query to the appropriate data-source, where thedata-sources are databases. We synthesize datasets by extending existingdatasets designed for NL-to-SQL semantic parsing. We create baselines on thesedatasets by using open-source LLMs, using both pre-trained and task specificembeddings fine-tuned using the training data. With these baselines wedemonstrate that open-source LLMs perform better than embedding based approach,but suffer from token length limitations. Embedding based approaches benefitfrom task specific fine-tuning, more so when there is availability of data interms of database specific questions for training. We further find that thetask becomes more difficult (i) with an increase in the number of data-sources,(ii) having data-sources closer in terms of their domains,(iii) havingdatabases without external domain knowledge required to interpret its entitiesand (iv) with ambiguous and complex queries requiring more fine-grainedunderstanding of the data-sources or logical reasoning for routing to anappropriate source. This calls for the need for developing more sophisticatedsolutions to better address the task.",Priyangshu Mandal,2025/1/27,2025/1/28,,,['cs.CL']
2501.08168v1,LeapVAD: A Leap in Autonomous Driving via Cognitive Perception and Dual-Process Thinking,http://arxiv.org/abs/2501.08168v1,"While autonomous driving technology has made remarkable strides, data-drivenapproaches still struggle with complex scenarios due to their limited reasoningcapabilities. Meanwhile, knowledge-driven autonomous driving systems haveevolved considerably with the popularization of visual language models. In thispaper, we propose LeapVAD, a novel method based on cognitive perception anddual-process thinking. Our approach implements a human-attentional mechanism toidentify and focus on critical traffic elements that influence drivingdecisions. By characterizing these objects through comprehensive attributes -including appearance, motion patterns, and associated risks - LeapVAD achievesmore effective environmental representation and streamlines the decision-makingprocess. Furthermore, LeapVAD incorporates an innovative dual-processdecision-making module miming the human-driving learning process. The systemconsists of an Analytic Process (System-II) that accumulates driving experiencethrough logical reasoning and a Heuristic Process (System-I) that refines thisknowledge via fine-tuning and few-shot learning. LeapVAD also includesreflective mechanisms and a growing memory bank, enabling it to learn from pastmistakes and continuously improve its performance in a closed-loop environment.To enhance efficiency, we develop a scene encoder network that generatescompact scene representations for rapid retrieval of relevant drivingexperiences. Extensive evaluations conducted on two leading autonomous drivingsimulators, CARLA and DriveArena, demonstrate that LeapVAD achieves superiorperformance compared to camera-only approaches despite limited training data.Comprehensive ablation studies further emphasize its effectiveness incontinuous learning and domain adaptation. Project page:https://pjlab-adg.github.io/LeapVAD/.",Yukai Ma,2025/1/14,2025/1/14,,,['cs.AI']
2501.07214v1,TimeLogic: A Temporal Logic Benchmark for Video QA,http://arxiv.org/abs/2501.07214v1,"Temporal logical understanding, a core facet of human cognition, plays apivotal role in capturing complex sequential events and their temporalrelationships within videos. This capability is particularly crucial in taskslike Video Question Answering (VideoQA), where the goal is to process visualdata over time together with textual data to provide coherent answers. However,current VideoQA benchmarks devote little focus to evaluating this criticalskill due to the challenge of annotating temporal logic. Despite theadvancement of vision-language models, assessing their temporal logicalreasoning powers remains a challenge, primarily due to the lack QA pairs thatdemand formal, complex temporal reasoning. To bridge this gap, we introduce theTimeLogic QA (TLQA) framework to automatically generate the QA pairs,specifically designed to evaluate the temporal logical understanding. To thisend, TLQA leverages temporal annotations from existing video datasets togetherwith temporal operators derived from logic theory to construct questions thattest understanding of event sequences and their temporal relationships. TLQAframework is generic and scalable, capable of leveraging both, existing videoaction datasets with temporal action segmentation annotations, or videodatasets with temporal scene graph annotations, to automatically generatetemporal logical questions. We leverage 4 datasets, STAR, Breakfast, AGQA, andCrossTask, and generate two VideoQA dataset variants - small (TLQA-S) and large(TLQA-L) - containing 2k and 10k QA pairs for each category, resulting in 32kand 160k total pairs per dataset. We undertake a comprehensive evaluation ofleading-edge VideoQA models, employing the TLQA to benchmark their temporallogical understanding capabilities. We assess the VideoQA model's temporalreasoning performance on 16 categories of temporal logic with varying temporalcomplexity.",Sirnam Swetha,2025/1/13,2025/1/13,,,['cs.CV']
2501.07021v2,Neural Probabilistic Circuits: Enabling Compositional and Interpretable Predictions through Logical Reasoning,http://arxiv.org/abs/2501.07021v2,"End-to-end deep neural networks have achieved remarkable success acrossvarious domains but are often criticized for their lack of interpretability.While post hoc explanation methods attempt to address this issue, they oftenfail to accurately represent these black-box models, resulting in misleading orincomplete explanations. To overcome these challenges, we propose an inherentlytransparent model architecture called Neural Probabilistic Circuits (NPCs),which enable compositional and interpretable predictions through logicalreasoning. In particular, an NPC consists of two modules: an attributerecognition model, which predicts probabilities for various attributes, and atask predictor built on a probabilistic circuit, which enables logicalreasoning over recognized attributes to make class predictions. To train NPCs,we introduce a three-stage training algorithm comprising attribute recognition,circuit construction, and joint optimization. Moreover, we theoreticallydemonstrate that an NPC's error is upper-bounded by a linear combination of theerrors from its modules. To further demonstrate the interpretability of NPC, weprovide both the most probable explanations and the counterfactualexplanations. Empirical results on four benchmark datasets show that NPCsstrike a balance between interpretability and performance, achieving resultscompetitive even with those of end-to-end black-box models while providingenhanced interpretability.",Weixin Chen,2025/1/13,2025/1/20,,,"['cs.LG', 'cs.AI']"
2501.01767v2,LogicAD: Explainable Anomaly Detection via VLM-based Text Feature Extraction,http://arxiv.org/abs/2501.01767v2,"Logical image understanding involves interpreting and reasoning about therelationships and consistency within an image's visual content. This capabilityis essential in applications such as industrial inspection, where logicalanomaly detection is critical for maintaining high-quality standards andminimizing costly recalls. Previous research in anomaly detection (AD) hasrelied on prior knowledge for designing algorithms, which often requiresextensive manual annotations, significant computing power, and large amounts ofdata for training. Autoregressive, multimodal Vision Language Models (AVLMs)offer a promising alternative due to their exceptional performance in visualreasoning across various domains. Despite this, their application to logical ADremains unexplored. In this work, we investigate using AVLMs for logical AD anddemonstrate that they are well-suited to the task. Combining AVLMs with formatembedding and a logic reasoner, we achieve SOTA performance on publicbenchmarks, MVTec LOCO AD, with an AUROC of 86.0% and F1-max of 83.7%, alongwith explanations of anomalies. This significantly outperforms the existingSOTA method by a large margin.",Er Jin,2025/1/3,2025/1/8,,,['cs.CV']
2501.00541v1,Formalization of Biological Circuit Block Diagrams for formally analyzing Biomedical Control Systems in pHRI Applications,http://arxiv.org/abs/2501.00541v1,"The control of Biomedical Systems in Physical Human-Robot Interaction (pHRI)plays a pivotal role in achieving the desired behavior by ensuring the intendedtransfer function and stability of subsystems within the overall system.Traditionally, the control aspects of biomedical systems have been analyzedusing manual proofs and computer based analysis tools. However, theseapproaches provide inaccurate results due to human error in manual proofs andunverified algorithms and round-off errors in computer-based tools. We argueusing Interactive reasoning, or frequently called theorem proving, to analyzecontrol systems of biomedical engineering applications, specifically in thecontext of Physical Human-Robot Interaction (pHRI). Our methodology involvesconstructing mathematical models of the control components using Higher-orderLogic (HOL) and analyzing them through deductive reasoning in the HOL Lighttheorem prover. We propose to model these control systems in terms of theirblock diagram representations, which in turn utilize the correspondingdifferential equations and their transfer function-based representation usingthe Laplace Transform (LT). These formally represented block diagrams are thenanalyzed through logical reasoning in the trusted environment of a theoremprover to ensure the correctness of the results. For illustration, we present areal-world case study by analyzing the control system of the ultrafilterationdialysis process.",Adnan Rashid,2024/12/31,2024/12/31,,,['cs.LO']
2412.16689v1,Formal Language Knowledge Corpus for Retrieval Augmented Generation,http://arxiv.org/abs/2412.16689v1,"The integration of retrieval-augmented techniques with LLMs has shown promisein improving performance across various domains. However, their utility intasks requiring advanced reasoning, such as generating and evaluatingmathematical statements and proofs, remains underexplored. This study exploresthe use of Lean, a programming language for writing mathematical proofs, topopulate the knowledge corpus used by RAG systems. We hope for this to lay thefoundation to exploring different methods of using RAGs to improve theperformance of LLMs in advanced logical reasoning tasks.",Majd Zayyad,2024/12/21,2024/12/21,,,['cs.AI']
2412.09951v2,WiseAD: Knowledge Augmented End-to-End Autonomous Driving with Vision-Language Model,http://arxiv.org/abs/2412.09951v2,"The emergence of general human knowledge and impressive logical reasoningcapacity in rapidly progressed vision-language models (VLMs) have drivenincreasing interest in applying VLMs to high-level autonomous driving tasks,such as scene understanding and decision-making. However, an in-depth study onthe relationship between knowledge proficiency, especially essential drivingexpertise, and closed-loop autonomous driving performance requires furtherexploration. In this paper, we investigate the effects of the depth and breadthof fundamental driving knowledge on closed-loop trajectory planning andintroduce WiseAD, a specialized VLM tailored for end-to-end autonomous drivingcapable of driving reasoning, action justification, object recognition, riskanalysis, driving suggestions, and trajectory planning across diversescenarios. We employ joint training on driving knowledge and planning datasets,enabling the model to perform knowledge-aligned trajectory planningaccordingly. Extensive experiments indicate that as the diversity of drivingknowledge extends, critical accidents are notably reduced, contributing 11.9%and 12.4% improvements in the driving score and route completion on the Carlaclosed-loop evaluations, achieving state-of-the-art performance. Moreover,WiseAD also demonstrates remarkable performance in knowledge evaluations onboth in-domain and out-of-domain datasets.",Songyan Zhang,2024/12/13,2024/12/17,,,['cs.CV']
2412.07752v3,FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware,http://arxiv.org/abs/2412.07752v3,"While Transformers and other sequence-parallelizable neural networkarchitectures seem like the current state of the art in sequence modeling, theyspecifically lack state-tracking capabilities. These are important fortime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,as well as modern variants like sLSTM do have these capabilities at the cost ofstrictly sequential processing. While this is often seen as a stronglimitation, we show how fast these networks can get with ourhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to theregister level on modern GPUs. We extend traditional RNNs with aparallelization variant that processes multiple RNNs of smaller hidden state inparallel, similar to the head-wise processing in Transformers. To enableflexibility on different GPU variants, we introduce a new optimizationframework for hardware-internal cache sizes, memory and compute handling. Itmodels the hardware in a setting using polyhedral-like constraints, includingthe notion of divisibility. This speeds up the solution process in ourConstrINT library for general integer constraint satisfaction problems (integerCSPs). We show that our kernels can achieve 50x speed-ups over a vanillaPyTorch implementation and allow 40x larger hidden sizes compared to our Tritonimplementation. Our open-source kernels and the optimization library arereleased here to boost research in the direction of state-tracking enabled RNNsand sequence modeling: https://github.com/NX-AI/flashrnn",Korbinian Pppel,2024/12/10,2025/3/13,,,"['cs.LG', 'cs.AI']"
2412.05753v1,Can OpenAI o1 outperform humans in higher-order cognitive thinking?,http://arxiv.org/abs/2412.05753v1,"This study evaluates the performance of OpenAI's o1-preview model inhigher-order cognitive domains, including critical thinking, systematicthinking, computational thinking, data literacy, creative thinking, logicalreasoning, and scientific reasoning. Using established benchmarks, we comparedthe o1-preview models's performance to human participants from diverseeducational levels. o1-preview achieved a mean score of 24.33 on the Ennis-WeirCritical Thinking Essay Test (EWCTET), surpassing undergraduate (13.8) andpostgraduate (18.39) participants (z = 1.60 and 0.90, respectively). Insystematic thinking, it scored 46.1, SD = 4.12 on the Lake Urmia Vignette,significantly outperforming the human mean (20.08, SD = 8.13, z = 3.20). Fordata literacy, o1-preview scored 8.60, SD = 0.70 on Merk et al.'s ""Use Data""dimension, compared to the human post-test mean of 4.17, SD = 2.02 (z = 2.19).On creative thinking tasks, the model achieved originality scores of 2.98, SD =0.73, higher than the human mean of 1.74 (z = 0.71). In logical reasoning(LogiQA), it outperformed humans with average 90%, SD = 10% accuracy versus86%, SD = 6.5% (z = 0.62). For scientific reasoning, it achieved near-perfectperformance (mean = 0.99, SD = 0.12) on the TOSLS,, exceeding the highest humanscores of 0.85, SD = 0.13 (z = 1.78). While o1-preview excelled in structuredtasks, it showed limitations in problem-solving and adaptive reasoning. Theseresults demonstrate the potential of AI to complement education in structuredassessments but highlight the need for ethical oversight and refinement forbroader applications.",Ehsan Latif,2024/12/7,2024/12/7,,,"['cs.CY', 'cs.AI']"
2411.18932v1,ScratchEval: Are GPT-4o Smarter than My Child? Evaluating Large Multimodal Models with Visual Programming Challenges,http://arxiv.org/abs/2411.18932v1,"Recent advancements in large multimodal models (LMMs) have showcasedimpressive code generation capabilities, primarily evaluated throughimage-to-code benchmarks. However, these benchmarks are limited to specificvisual programming scenarios where the logic reasoning and the multimodalunderstanding capacities are split apart. To fill this gap, we proposeScratchEval, a novel benchmark designed to evaluate the visual programmingreasoning ability of LMMs. ScratchEval is based on Scratch, a block-basedvisual programming language widely used in children's programming education. Byintegrating visual elements and embedded programming logic, ScratchEvalrequires the model to process both visual information and code structure,thereby comprehensively evaluating its programming intent understandingability. Our evaluation approach goes beyond the traditional image-to-codemapping and focuses on unified logical thinking and problem-solving abilities,providing a more comprehensive and challenging framework for evaluating thevisual programming ability of LMMs. ScratchEval not only fills the gap inexisting evaluation methods, but also provides new insights for the futuredevelopment of LMMs in the field of visual programming. Our benchmark can beaccessed at https://github.com/HKBUNLP/ScratchEval .",Rao Fu,2024/11/28,2024/11/28,,,"['cs.CL', 'cs.AI', 'cs.CV']"
2411.18201v1,Learning for Long-Horizon Planning via Neuro-Symbolic Abductive Imitation,http://arxiv.org/abs/2411.18201v1,"Recent learning-to-imitation methods have shown promising results in planningvia imitating within the observation-action space. However, their ability inopen environments remains constrained, particularly in long-horizon tasks. Incontrast, traditional symbolic planning excels in long-horizon tasks throughlogical reasoning over human-defined symbolic spaces but struggles to handleobservations beyond symbolic states, such as high-dimensional visual inputsencountered in real-world scenarios. In this work, we draw inspiration fromabductive learning and introduce a novel framework \textbf{AB}ductive\textbf{I}mitation \textbf{L}earning (ABIL) that integrates the benefits ofdata-driven learning and symbolic-based reasoning, enabling long-horizonplanning. Specifically, we employ abductive reasoning to understand thedemonstrations in symbolic space and design the principles of sequentialconsistency to resolve the conflicts between perception and reasoning. ABILgenerates predicate candidates to facilitate the perception from rawobservations to symbolic space without laborious predicate annotations,providing a groundwork for symbolic planning. With the symbolic understanding,we further develop a policy ensemble whose base policies are built withdifferent logical objectives and managed through symbolic reasoning.Experiments show that our proposal successfully understands the observationswith the task-relevant symbolics to assist the imitation learning. Importantly,ABIL demonstrates significantly improved data efficiency and generalizationacross various long-horizon tasks, highlighting it as a promising solution forlong-horizon planning. Project website:\url{https://www.lamda.nju.edu.cn/shaojj/KDD25_ABIL/}.",Jie-Jing Shao,2024/11/27,2024/11/27,,,"['cs.LG', 'cs.AI']"
2411.17438v2,Object-centric proto-symbolic behavioural reasoning from pixels,http://arxiv.org/abs/2411.17438v2,"Autonomous intelligent agents must bridge computational challenges atdisparate levels of abstraction, from the low-level spaces of sensory input andmotor commands to the high-level domain of abstract reasoning and planning. Akey question in designing such agents is how best to instantiate therepresentational space that will interface between these two levels -- ideallywithout requiring supervision in the form of expensive data annotations. Theseobjectives can be efficiently achieved by representing the world in terms ofobjects (grounded in perception and action). In this work, we present a novel,brain-inspired, deep-learning architecture that learns from pixels tointerpret, control, and reason about its environment, using object-centricrepresentations. We show the utility of our approach through tasks in syntheticenvironments that require a combination of (high-level) logical reasoning and(low-level) continuous control. Results show that the agent can learn emergentconditional behavioural reasoning, such as $(A \to B) \land (\neg A \to C)$, aswell as logical composition $(A \to B) \land (A \to C) \vdash A \to (B \landC)$ and XOR operations, and successfully controls its environment to satisfyobjectives deduced from these logical rules. The agent can adapt online tounexpected changes in its environment and is robust to mild violations of itsworld model, thanks to dynamic internal desired goal generation. While thepresent results are limited to synthetic settings (2D and 3D activated versionsof dSprites), which fall short of real-world levels of complexity, the proposedarchitecture shows how to manipulate grounded object representations, as a keyinductive bias for unsupervised learning, to enable behavioral reasoning.",Ruben van Bergen,2024/11/26,2025/2/11,,,"['cs.AI', 'cs.CV', 'cs.LG', 'cs.NE', 'I.2.0; I.2.6; I.2.10']"
2411.15509v1,Interactive Visual Assessment for Text-to-Image Generation Models,http://arxiv.org/abs/2411.15509v1,"Visual generation models have achieved remarkable progress in computergraphics applications but still face significant challenges in real-worlddeployment. Current assessment approaches for visual generation tasks typicallyfollow an isolated three-phase framework: test input collection, model outputgeneration, and user assessment. These fashions suffer from fixed coverage,evolving difficulty, and data leakage risks, limiting their effectiveness incomprehensively evaluating increasingly complex generation models. To addressthese limitations, we propose DyEval, an LLM-powered dynamic interactive visualassessment framework that facilitates collaborative evaluation between humansand generative models for text-to-image systems. DyEval features an intuitivevisual interface that enables users to interactively explore and analyze modelbehaviors, while adaptively generating hierarchical, fine-grained, and diversetextual inputs to continuously probe the capability boundaries of the modelsbased on their feedback. Additionally, to provide interpretable analysis forusers to further improve tested models, we develop a contextual reflectionmodule that mines failure triggers of test inputs and reflects model potentialfailure patterns supporting in-depth analysis using the logical reasoningability of LLM. Qualitative and quantitative experiments demonstrate thatDyEval can effectively help users identify max up to 2.56 times generationfailures than conventional methods, and uncover complex and rare failurepatterns, such as issues with pronoun generation and specific cultural contextgeneration. Our framework provides valuable insights for improving generativemodels and has broad implications for advancing the reliability andcapabilities of visual generation systems across various domains.",Xiaoyue Mi,2024/11/23,2024/11/23,,,"['cs.CV', 'cs.AI']"
2411.08463v2,Symbolic-AI-Fusion Deep Learning (SAIF-DL): Encoding Knowledge into Training with Answer Set Programming Loss Penalties by a Novel Loss Function Approach,http://arxiv.org/abs/2411.08463v2,"This paper presents a hybrid methodology that enhances the training processof deep learning (DL) models by embedding domain expert knowledge usingontologies and answer set programming (ASP). By integrating these symbolic AImethods, we encode domain-specific constraints, rules, and logical reasoningdirectly into the model's learning process, thereby improving both performanceand trustworthiness. The proposed approach is flexible and applicable to bothregression and classification tasks, demonstrating generalizability acrossvarious fields such as healthcare, autonomous systems, engineering, and batterymanufacturing applications. Unlike other state-of-the-art methods, the strengthof our approach lies in its scalability across different domains. The designallows for the automation of the loss function by simply updating the ASPrules, making the system highly scalable and user-friendly. This facilitatesseamless adaptation to new domains without significant redesign, offering apractical solution for integrating expert knowledge into DL models inindustrial settings such as battery manufacturing.",Fadi Al Machot,2024/11/13,2024/12/18,,,"['cs.AI', 'cs.ET']"
2411.06253v1,"Knowledge Authoring with Factual English, Rules, and Actions",http://arxiv.org/abs/2411.06253v1,"Knowledge representation and reasoning systems represent knowledge ascollections of facts and rules. KRRs can represent complex concepts andrelations, and they can query and manipulate information in sophisticated ways.Unfortunately, the KRR technology has been hindered by the fact that specifyingthe requisite knowledge requires skills that most domain experts do not have,and professional knowledge engineers are hard to find. Some recent CNL-basedapproaches, such as the Knowledge Authoring Logic Machine (KALM), have shown tohave very high accuracy compared to others, and a natural question is to whatextent the CNL restrictions can be lifted. Besides the CNL restrictions, KALMhas limitations in terms of the types of knowledge it can represent. To addressthese issues, we propose an extension of KALM called KALM for Factual Language(KALMF). KALMF uses a neural parser for natural language, MS, to parse what wecall factual English sentences, which require little grammar training to use.Building upon KALMF, we propose KALM for Rules and Actions (KALMR), torepresent and reason with rules and actions. Furthermore, we identify thereasons behind the slow speed of KALM and make optimizations to address thisissue. Our evaluation using multiple benchmarks shows that our approachesachieve a high level of correctness on fact and query authoring (95%) and onrule authoring (100%). When used for authoring and reasoning with actions, ourapproach achieves more than 99.3% correctness, demonstrating its effectivenessin enabling more sophisticated knowledge representation and reasoning. We alsoillustrate the logical reasoning capabilities of our approach by drawingattention to the problems faced by the famous AI, ChatGPT. Finally, theevaluation of the newly proposed speed optimization points not only to a 68%runtime improvement but also yields better accuracy of the overall system.",Yuheng Wang,2024/11/9,2024/11/9,,,['cs.AI']
2411.03231v2,Formal Logic-guided Robust Federated Learning against Poisoning Attacks,http://arxiv.org/abs/2411.03231v2,"Federated Learning (FL) offers a promising solution to the privacy concernsassociated with centralized Machine Learning (ML) by enabling decentralized,collaborative learning. However, FL is vulnerable to various security threats,including poisoning attacks, where adversarial clients manipulate the trainingdata or model updates to degrade overall model performance. Recognizing thisthreat, researchers have focused on developing defense mechanisms to counteractpoisoning attacks in FL systems. However, existing robust FL methodspredominantly focus on computer vision tasks, leaving a gap in addressing theunique challenges of FL with time series data. In this paper, we presentFLORAL, a defense mechanism designed to mitigate poisoning attacks in federatedlearning for time-series tasks, even in scenarios with heterogeneous clientdata and a large number of adversarial participants. Unlike traditionalmodel-centric defenses, FLORAL leverages logical reasoning to evaluate clienttrustworthiness by aligning their predictions with global time-series patterns,rather than relying solely on the similarity of client updates. Our approachextracts logical reasoning properties from clients, then hierarchically infersglobal properties, and uses these to verify client updates. Through formallogic verification, we assess the robustness of each client contribution,identifying deviations indicative of adversarial behavior. Experimental resultson two datasets demonstrate the superior performance of our approach comparedto existing baseline methods, highlighting its potential to enhance therobustness of FL to time series applications. Notably, FLORAL reduced theprediction error by 93.27% in the best-case scenario compared to thesecond-best baseline. Our code is available athttps://anonymous.4open.science/r/FLORAL-Robust-FTS.",Dung Thuy Nguyen,2024/11/5,2024/11/6,,,"['cs.CR', 'cs.AI', 'cs.DC', 'cs.LO']"
2411.02265v3,Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent,http://arxiv.org/abs/2411.02265v3,"In this paper, we introduce Hunyuan-Large, which is currently the largestopen-source Transformer-based mixture of experts model, with a total of 389billion parameters and 52 billion activation parameters, capable of handling upto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superiorperformance across various benchmarks including language understanding andgeneration, logical reasoning, mathematical problem-solving, coding,long-context, and aggregated tasks, where it outperforms LLama3.1-70B andexhibits comparable performance when compared to the significantly largerLLama3.1-405B model. Key practice of Hunyuan-Large include large-scalesynthetic data that is orders larger than in previous literature, a mixedexpert routing strategy, a key-value cache compression technique, and anexpert-specific learning rate strategy. Additionally, we also investigate thescaling laws and learning rate schedule of mixture of experts models, providingvaluable insights and guidances for future model development and optimization.The code and checkpoints of Hunyuan-Large are released to facilitate futureinnovations and applications.  Codes: https://github.com/Tencent/Hunyuan-Large  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",Xingwu Sun,2024/11/4,2024/11/6,,,"['cs.CL', 'cs.AI']"
2410.23123v2,On Memorization of Large Language Models in Logical Reasoning,http://arxiv.org/abs/2410.23123v2,"Large language models (LLMs) achieve good performance on challengingreasoning benchmarks, yet could also make basic reasoning mistakes. Thiscontrasting behavior is puzzling when it comes to understanding the mechanismsbehind LLMs' reasoning capabilities. One hypothesis is that the increasinglyhigh and nearly saturated performance on common reasoning benchmarks could bedue to the memorization of similar problems. In this paper, we systematicallyinvestigate this hypothesis with a quantitative measurement of memorization inreasoning tasks, using a dynamically generated logical reasoning benchmarkbased on Knights and Knaves (K&K) puzzles. We find that LLMs could interpolateand memorize the training puzzles (achieving near-perfect accuracy) afterfine-tuning, yet they struggle with slight variations of these puzzles. On theother hand, we show that while fine-tuning leads to heavy memorization, it alsoconsistently improves generalization performance. Through in-depth analyseswith perturbation tests, cross difficulty-level transferability, probing modelinternals, and fine-tuning with wrong answers, we establish that LLMs developreasoning skills on K&K puzzles alongside memorization. Finally, our analysisbased on a per-sample memorization score sheds light on how LLMs switch betweenreasoning and memorization when solving logical puzzles. Our code and data areavailable at https://memkklogic.github.io.",Chulin Xie,2024/10/30,2025/3/4,,,['cs.CL']
2410.20957v1,Neuro-symbolic Learning Yielding Logical Constraints,http://arxiv.org/abs/2410.20957v1,"Neuro-symbolic systems combine the abilities of neural perception and logicalreasoning. However, end-to-end learning of neuro-symbolic systems is still anunsolved challenge. This paper proposes a natural framework that fuses neuralnetwork training, symbol grounding, and logical constraint synthesis into acoherent and efficient end-to-end learning process. The capability of thisframework comes from the improved interactions between the neural and thesymbolic parts of the system in both the training and inference stages.Technically, to bridge the gap between the continuous neural network and thediscrete logical constraint, we introduce a difference-of-convex programmingtechnique to relax the logical constraints while maintaining their precision.We also employ cardinality constraints as the language for logical constraintlearning and incorporate a trust region method to avoid the degeneracy oflogical constraint in learning. Both theoretical analyses and empiricalevaluations substantiate the effectiveness of the proposed framework.",Zenan Li,2024/10/28,2024/10/28,,,"['cs.AI', 'cs.LG']"
2410.20695v2,Combining Domain-Specific Models and LLMs for Automated Disease Phenotyping from Survey Data,http://arxiv.org/abs/2410.20695v2,"This exploratory pilot study investigated the potential of combining adomain-specific model, BERN2, with large language models (LLMs) to enhanceautomated disease phenotyping from research survey data. Motivated by the needfor efficient and accurate methods to harmonize the growing volume of surveydata with standardized disease ontologies, we employed BERN2, a biomedicalnamed entity recognition and normalization model, to extract diseaseinformation from the ORIGINS birth cohort survey data. After rigorouslyevaluating BERN2's performance against a manually curated ground truth dataset,we integrated various LLMs using prompt engineering, Retrieval-AugmentedGeneration (RAG), and Instructional Fine-Tuning (IFT) to refine the model'soutputs. BERN2 demonstrated high performance in extracting and normalizingdisease mentions, and the integration of LLMs, particularly with Few ShotInference and RAG orchestration, further improved accuracy. This approach,especially when incorporating structured examples, logical reasoning prompts,and detailed context, offers a promising avenue for developing tools to enableefficient cohort profiling and data harmonization across large, heterogeneousresearch datasets.",Gal Beeri,2024/10/28,2024/12/20,,,['cs.CL']
2410.15463v1,MedLogic-AQA: Enhancing Medical Question Answering with Abstractive Models Focusing on Logical Structures,http://arxiv.org/abs/2410.15463v1,"In Medical question-answering (QA) tasks, the need for effective systems ispivotal in delivering accurate responses to intricate medical queries. However,existing approaches often struggle to grasp the intricate logical structuresand relationships inherent in medical contexts, thus limiting their capacity tofurnish precise and nuanced answers. In this work, we address this gap byproposing a novel Abstractive QA system MedLogic-AQA that harnesses First OrderLogic (FOL) based rules extracted from both context and questions to generatewell-grounded answers. Through initial experimentation, we identified sixpertinent first-order logical rules, which were then used to train aLogic-Understanding (LU) model capable of generating logical triples for agiven context, question, and answer. These logic triples are then integratedinto the training of MedLogic-AQA, enabling effective and coherent reasoningduring answer generation. This distinctive fusion of logical reasoning withabstractive QA equips our system to produce answers that are logically sound,relevant, and engaging. Evaluation with respect to both automated andhuman-based demonstrates the robustness of MedLogic-AQA against strongbaselines. Through empirical assessments and case studies, we validate theefficacy of MedLogic-AQA in elevating the quality and comprehensiveness ofanswers in terms of reasoning as well as informativeness",Aizan Zafar,2024/10/20,2024/10/20,,,"['cs.CL', 'cs.LO']"
2410.15173v1,Uncovering Autoregressive LLM Knowledge of Thematic Fit in Event Representation,http://arxiv.org/abs/2410.15173v1,"The thematic fit estimation task measures the compatibility between apredicate (typically a verb), an argument (typically a noun phrase), and aspecific semantic role assigned to the argument. Previous state-of-the-art workhas focused on modeling thematic fit through distributional or neural models ofevent representation, trained in a supervised fashion with indirect labels. Inthis work, we assess whether pre-trained autoregressive LLMs possessconsistent, expressible knowledge about thematic fit. We evaluate both closedand open state-of-the-art LLMs on several psycholinguistic datasets, alongthree axes: (1) Reasoning Form: multi-step logical reasoning (chain-of-thoughtprompting) vs. simple prompting. (2) Input Form: providing context (generatedsentences) vs. raw tuples <predicate, argument, role>. (3) Output Form:categorical vs. numeric. Our results show that chain-of-thought reasoning ismore effective on datasets with self-explanatory semantic role labels,especially Location. Generated sentences helped only in few settings, andlowered results in many others. Predefined categorical (compared to numeric)output raised GPT's results across the board with few exceptions, but loweredLlama's. We saw that semantically incoherent generated sentences, which themodels lack the ability to consistently filter out, hurt reasoning and overallperformance too. Our GPT-powered methods set new state-of-the-art on all testeddatasets.",Safeyah Khaled Alshemali,2024/10/19,2024/10/19,,,"['cs.CL', 'cs.AI']"
2410.13259v1,From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition,http://arxiv.org/abs/2410.13259v1,"We examine the language capabilities of language models (LMs) from thecritical perspective of human language acquisition. Building on classicallanguage development theories, we propose a three-stage framework to assess theabilities of LMs, ranging from preliminary word understanding to complexgrammar and complex logical reasoning. Using this framework, we evaluate thegenerative capacities of LMs using methods from linguistic research. Resultsindicate that although recent LMs outperform earlier models in overallperformance, their developmental trajectory does not strictly follow the pathof human language acquisition. Notably, in generation tasks, LMs are moresimilar to human performance in areas where information is easier to extractfrom the corpus, such as average word length, clauses, and auxiliary verbs.Newer LMs did not exhibit significant progress in terms of specific dimensions,such as clauses and auxiliary verbs, where the variation across corpora isrelatively limited. Register theory offers a plausible explanation for theseobservations, suggesting that the linguistic features of the training data havea substantial impact on the models' abilities.",Qiyuan Yang,2024/10/17,2024/10/17,,,['cs.CL']
2410.21287v1,A Systematic Assessment of OpenAI o1-Preview for Higher Order Thinking in Education,http://arxiv.org/abs/2410.21287v1,"As artificial intelligence (AI) continues to advance, it demonstratescapabilities comparable to human intelligence, with significant potential totransform education and workforce development. This study evaluates OpenAIo1-preview's ability to perform higher-order cognitive tasks across 14dimensions, including critical thinking, systems thinking, computationalthinking, design thinking, metacognition, data literacy, creative thinking,abstract reasoning, quantitative reasoning, logical reasoning, analogicalreasoning, and scientific reasoning. We used validated instruments like theEnnis-Weir Critical Thinking Essay Test and the Biological Systems ThinkingTest to compare the o1-preview's performance with human performancesystematically. Our findings reveal that o1-preview outperforms humans in mostcategories, achieving 150% better results in systems thinking, computationalthinking, data literacy, creative thinking, scientific reasoning, and abstractreasoning. However, compared to humans, it underperforms by around 25% inlogical reasoning, critical thinking, and quantitative reasoning. In analogicalreasoning, both o1-preview and humans achieved perfect scores. Despite thesestrengths, the o1-preview shows limitations in abstract reasoning, where humanpsychology students outperform it, highlighting the continued importance ofhuman oversight in tasks requiring high-level abstraction. These results havesignificant educational implications, suggesting a shift toward developinghuman skills that complement AI, such as creativity, abstract reasoning, andcritical thinking. This study emphasizes the transformative potential of AI ineducation and calls for a recalibration of educational goals, teaching methods,and curricula to align with an AI-driven world.",Ehsan Latif,2024/10/11,2024/10/11,,,"['cs.CY', 'cs.AI']"
2410.08390v1,KnowGraph: Knowledge-Enabled Anomaly Detection via Logical Reasoning on Graph Data,http://arxiv.org/abs/2410.08390v1,"Graph-based anomaly detection is pivotal in diverse security applications,such as fraud detection in transaction networks and intrusion detection fornetwork traffic. Standard approaches, including Graph Neural Networks (GNNs),often struggle to generalize across shifting data distributions. Meanwhile,real-world domain knowledge is more stable and a common existing component ofreal-world detection strategies. To explicitly integrate such knowledge intodata-driven models such as GCNs, we propose KnowGraph, which integrates domainknowledge with data-driven learning for enhanced graph-based anomaly detection.KnowGraph comprises two principal components: (1) a statistical learningcomponent that utilizes a main model for the overarching detection task,augmented by multiple specialized knowledge models that predict domain-specificsemantic entities; (2) a reasoning component that employs probabilisticgraphical models to execute logical inferences based on model outputs, encodingdomain knowledge through weighted first-order logic formulas. Extensiveexperiments on these large-scale real-world datasets show that KnowGraphconsistently outperforms state-of-the-art baselines in both transductive andinductive settings, achieving substantial gains in average precision whengeneralizing to completely unseen test graphs. Further ablation studiesdemonstrate the effectiveness of the proposed reasoning component in improvingdetection performance, especially under extreme class imbalance. These resultshighlight the potential of integrating domain knowledge into data-driven modelsfor high-stakes, graph-based security applications.",Andy Zhou,2024/10/10,2024/10/10,,,"['cs.CR', 'cs.AI', 'cs.LG']"
2410.08130v2,Think Beyond Size: Adaptive Prompting for More Effective Reasoning,http://arxiv.org/abs/2410.08130v2,"Pretrained large language models (LLMs) are increasingly utilized across awide range of natural language processing (NLP) tasks due to their impressivecapabilities as few-shot learners. Recent techniques, such as chain-of-thought(CoT) prompting, have significantly advanced multi-step reasoning byintroducing step-by-step decomposition, achieving state-of-the-art results oncomplex reasoning benchmarks. However, these approaches often rely on staticprompting templates that do not adapt to task complexity or errors during thereasoning process. In this work, we introduce Adaptive Prompting, a dynamic anditerative framework designed to enhance reasoning by incorporating real-timeadjustments to prompt structures and validation mechanisms.Experimental resultsdemonstrate that Adaptive Prompting significantly improves performance ondiverse reasoning benchmarks, including arithmetic reasoning (GSM8K,MultiArith), logical reasoning and commonsense tasks, achieving substantialaccuracy gains compared to static prompting baselines. By integrating guidedprompts, intermediate validation, and self-corrective steps, our approachenables smaller models to achieve competitive performance with largercounterparts, such as GPT-4, while maintaining computational efficiency. Theframework achieves this without requiring fine-tuning or task-specific trainingdata, highlighting the untapped potential of iterative reasoning methods.",Kamesh R,2024/10/10,2024/11/29,,,"['cs.LG', 'cs.CL']"
2410.08047v2,Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning,http://arxiv.org/abs/2410.08047v2,"Complex logical reasoning tasks require a long sequence of reasoning, which alarge language model (LLM) with chain-of-thought prompting still falls short.To alleviate this issue, neurosymbolic approaches incorporate a symbolicsolver. Specifically, an LLM only translates a natural language problem into asatisfiability (SAT) problem that consists of first-order logic formulas, and asound symbolic solver returns a mathematically correct solution. However, wediscover that LLMs have difficulties to capture complex logical semanticshidden in the natural language during translation. To resolve this limitation,we propose a Compositional First-Order Logic Translation. An LLM first parses anatural language sentence into newly defined logical dependency structures thatconsist of an atomic subsentence and its dependents, then sequentiallytranslate the parsed subsentences. Since multiple logical dependency structuresand sequential translations are possible for a single sentence, we alsointroduce two Verification algorithms to ensure more reliable results. Weutilize an SAT solver to rigorously compare semantics of generated first-orderlogic formulas and select the most probable one. We evaluate the proposedmethod, dubbed CLOVER, on seven logical reasoning benchmarks and show that itoutperforms the previous neurosymbolic approaches and achieves newstate-of-the-art results.",Hyun Ryu,2024/10/10,2025/2/25,,,['cs.CL']
2410.09112v1,HLM-Cite: Hybrid Language Model Workflow for Text-based Scientific Citation Prediction,http://arxiv.org/abs/2410.09112v1,"Citation networks are critical in modern science, and predicting whichprevious papers (candidates) will a new paper (query) cite is a criticalproblem. However, the roles of a paper's citations vary significantly, rangingfrom foundational knowledge basis to superficial contexts. Distinguishing theseroles requires a deeper understanding of the logical relationships amongpapers, beyond simple edges in citation networks. The emergence of LLMs withtextual reasoning capabilities offers new possibilities for discerning theserelationships, but there are two major challenges. First, in practice, a newpaper may select its citations from gigantic existing papers, where the textsexceed the context length of LLMs. Second, logical relationships between papersare implicit, and directly prompting an LLM to predict citations may result insurface-level textual similarities rather than the deeper logical reasoning. Inthis paper, we introduce the novel concept of core citation, which identifiesthe critical references that go beyond superficial mentions. Thereby, weelevate the citation prediction task from a simple binary classification todistinguishing core citations from both superficial citations andnon-citations. To address this, we propose $\textbf{HLM-Cite}$, a$\textbf{H}$ybrid $\textbf{L}$anguage $\textbf{M}$odel workflow for citationprediction, which combines embedding and generative LMs. We design a curriculumfinetune procedure to adapt a pretrained text embedding model to coarselyretrieve high-likelihood core citations from vast candidates and then design anLLM agentic workflow to rank the retrieved papers through one-shot reasoning,revealing the implicit relationships among papers. With the pipeline, we canscale the candidate sets to 100K papers. We evaluate HLM-Cite across 19scientific fields, demonstrating a 17.6% performance improvement comparing SOTAmethods.",Qianyue Hao,2024/10/10,2024/10/10,,,"['cs.DL', 'cs.AI', 'cs.CL', 'I.2.7']"
2410.07432v2,Can Transformers Reason Logically? A Study in SAT Solving,http://arxiv.org/abs/2410.07432v2,"We formally study the logical reasoning capabilities of decoder-onlyTransformers in the context of the boolean satisfiability (SAT) problem. First,we prove by construction that decoder-only Transformers can decide 3-SAT, in anon-uniform model of computation, using backtracking and deduction viaChain-of-Thought (CoT). %We prove its correctness by showing trace equivalenceto the well-known DPLL SAT-solving algorithm. Second, we implement ourconstruction as a PyTorch model with a tool (PARAT) that we designed toempirically demonstrate its correctness and investigate its properties. Third,rather than \textit{programming} a transformer to reason, we evaluateempirically whether it can be \textit{trained} to do so by learning directlyfrom algorithmic traces (``reasoning paths'') from our theoreticalconstruction. The trained models demonstrate strong out-of-distributiongeneralization on problem sizes seen during training but has limited lengthgeneralization, which is consistent with the implications of our theoreticalresult",Leyan Pan,2024/10/9,2025/2/8,,,"['cs.LG', 'cs.AI', 'cs.LO']"
2410.04492v5,Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification,http://arxiv.org/abs/2410.04492v5,"Vision models excel in image classification but struggle to generalize tounseen data, such as classifying images from unseen domains or discoveringnovel categories. In this paper, we explore the relationship between logicalreasoning and deep learning generalization in visual classification. A logicalregularization termed L-Reg is derived which bridges a logical analysisframework to image classification. Our work reveals that L-Reg reduces thecomplexity of the model in terms of the feature distribution and classifierweights. Specifically, we unveil the interpretability brought by L-Reg, as itenables the model to extract the salient features, such as faces to persons,for classification. Theoretical analysis and experiments demonstrate that L-Regenhances generalization across various scenarios, including multi-domaingeneralization and generalized category discovery. In complex real-worldscenarios where images span unknown classes and unseen domains, L-Regconsistently improves generalization, highlighting its practical efficacy.",Zhaorui Tan,2024/10/6,2025/1/27,,,"['cs.CV', 'cs.AI', 'cs.LG']"
2410.03136v3,Deliberate Reasoning in Language Models as Structure-Aware Planning with an Accurate World Model,http://arxiv.org/abs/2410.03136v3,"Enhancing the reasoning capabilities of language models (LMs) remains a keychallenge, especially for tasks that require complex, multi-stepdecision-making where existing Chain-of-Thought (CoT) approaches struggle withconsistency and verification. In this paper, we propose a novel reasoningframework, referred to as Structure-aware Planning with an Accurate World Model(SWAP), that integrates structured knowledge representation with learnedplanning. Unlike prior methods that rely purely on natural language reasoning,SWAP leverages entailment graphs to encode structured dependencies and enablesymbolic verification of intermediate steps. To systematically construct andupdate the graph, SWAP employs a policy model to propose candidate expansionsand a world model to predict structural updates. To improve accuracy, the worldmodel generates multiple alternative updates, and a discriminator re-ranks thembased on plausibility. To encourage diverse exploration, we introduceDiversity-based Modelling (DM), which samples candidates from the remainingprobability mass after removing previously sampled candidates from the originalpolicy distribution. Additionally, SWAP improves the discrimination accuracythrough Contrastive Ranking (CR), which directly compares candidates withinprompts and incorporates meta-knowledge to improve ranking quality. We evaluateSWAP across diverse reasoning-intensive benchmarks including math reasoning,logical reasoning, and coding tasks. Extensive experiments demonstrate thatSWAP significantly improves upon the base models and consistently outperformsexisting reasoning methods.",Siheng Xiong,2024/10/4,2025/2/18,,,['cs.CL']
2410.02203v3,GraphIC: A Graph-Based In-Context Example Retrieval Model for Multi-Step Reasoning,http://arxiv.org/abs/2410.02203v3,"In-context learning (ICL) enhances large language models (LLMs) byincorporating demonstration examples, yet its effectiveness heavily depends onthe quality of selected examples. Current methods typically use text embeddingsto measure semantic similarity, which often introduces bias in multi-stepreasoning tasks. This occurs because text embeddings contain irrelevantsemantic information and lack deeper reasoning structures. To address this, wepropose GraphIC, a graph-based retrieval model that leverages reasoning-awarerepresentation and specialized similarity metric for in-context exampleretrieval. GraphIC first constructs thought graphs-directed, node-attributedgraphs that explicitly model reasoning steps and their dependencies-forcandidate examples and queries. This approach filters out superficial semanticswhile preserving essential reasoning processes. Next, GraphIC retrievesexamples using a novel similarity metric tailored for these graphs, capturingsequential reasoning patterns and asymmetry between examples. Comprehensiveevaluations across mathematical reasoning, code generation, and logicalreasoning tasks demonstrate that GraphIC outperforms 10 baseline methods. Ourresults highlight the importance of reasoning-aware retrieval in ICL, offeringa robust solution for enhancing LLM performance in multi-step reasoningscenarios.",Jiale Fu,2024/10/3,2025/2/25,,,['cs.AI']
2409.17539v2,Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models,http://arxiv.org/abs/2409.17539v2,"Large Language Models (LLMs) have demonstrated remarkable capabilities acrossvarious tasks but their performance in complex logical reasoning tasks remainsunsatisfactory. Although some prompting methods, such as Chain-of-Thought, canimprove the reasoning ability of LLMs to some extent, they suffer from anunfaithful issue where derived conclusions may not align with the generatedreasoning chain. To address this issue, some studies employ the approach ofpropositional logic to further enhance logical reasoning abilities of LLMs.However, the potential omissions in the extraction of logical expressions inthese methods can cause information loss in the logical reasoning process,thereby generating incorrect results. To this end, we propose Logic-of-Thought(LoT) prompting which employs propositional logic to generate expanded logicalinformation descriptions and utilizes them as an additional augmentation tooriginal contexts, thereby ensuring information completeness and enhancinglogical reasoning ability. LoT is orthogonal to existing prompting methods andcan be seamlessly integrated with them. Extensive experiments demonstrate thatLoT boosts the performance of various prompting methods with a striking marginacross five logical reasoning tasks. In particular, LoT enhancesChain-of-Thought's performance on the ReClor dataset by +4.35%, improvesChain-of-Thought with Self-Consistency's performance on the RuleTaker datasetby +3.52%, and boosts performance of Tree-of-Thoughts on the ProofWriterdataset by +8%.",Tongxuan Liu,2024/9/26,2025/2/7,,,['cs.CL']
2409.16045v1,LTNtorch: PyTorch Implementation of Logic Tensor Networks,http://arxiv.org/abs/2409.16045v1,"Logic Tensor Networks (LTN) is a Neuro-Symbolic framework that effectivelyincorporates deep learning and logical reasoning. In particular, LTN allowsdefining a logical knowledge base and using it as the objective of a neuralmodel. This makes learning by logical reasoning possible as the parameters ofthe model are optimized by minimizing a loss function composed of a set oflogical formulas expressing facts about the learning task. The framework learnsvia gradient-descent optimization. Fuzzy logic, a relaxation of classical logicpermitting continuous truth values in the interval [0,1], makes this learningpossible. Specifically, the training of an LTN consists of three steps.Firstly, (1) the training data is used to ground the formulas. Then, (2) theformulas are evaluated, and the loss function is computed. Lastly, (3) thegradients are back-propagated through the logical computational graph, and theweights of the neural model are changed so the knowledge base is maximallysatisfied. LTNtorch is the fully documented and tested PyTorch implementationof Logic Tensor Networks. This paper presents the formalization of LTN and howLTNtorch implements it. Moreover, it provides a basic binary classificationexample.",Tommaso Carraro,2024/9/24,2024/9/24,,,['cs.AI']
2409.14495v3,Thought-Path Contrastive Learning via Premise-Oriented Data Augmentation for Logical Reading Comprehension,http://arxiv.org/abs/2409.14495v3,"Logical reading comprehension is a challenging task that entails grasping theunderlying semantics of text and applying reasoning to deduce the correctanswer. Prior researches have primarily focused on enhancing logical reasoningcapabilities through Chain-of-Thought (CoT) or data augmentation. However,previous work constructing chain-of-thought rationales concentrates solely onanalyzing correct options, neglecting the incorrect alternatives. Addtionally,earlier efforts on data augmentation by altering contexts rely on rule-basedmethods, which result in generated contexts that lack diversity and coherence.To address these issues, we propose a Premise-Oriented Data Augmentation (PODA)framework. This framework can generate CoT rationales including analyses forboth correct and incorrect options, while constructing diverse and high-qualitycounterfactual contexts from incorrect candidate options. We integratesummarizing premises and identifying premises for each option into rationales.Subsequently, we employ multi-step prompts with identified premises toconstruct counterfactual context. To facilitate the model's capabilities tobetter differentiate the reasoning process associated with each option, weintroduce a novel thought-path contrastive learning method that comparesreasoning paths between the original and counterfactual samples. Experimentalresults on three representative LLMs demonstrate that our method can improvethe baselines substantially across two challenging logical reasoning benchmarks(ReClor and LogiQA 2.0). The data and code are released athttps://github.com/lalalamdbf/TPReasoner.",Chenxu Wang,2024/9/22,2025/2/6,,,"['cs.CL', 'cs.AI']"
2409.12929v2,LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning,http://arxiv.org/abs/2409.12929v2,"In this paper, we propose a new data synthesis method called\textbf{LogicPro}, which leverages LeetCode-style algorithm\underline{Pro}blems and their corresponding \underline{Pro}gram solutions tosynthesize Complex \underline{Logic}al Reasoning data in text format. First, wesynthesize complex reasoning problems through source algorithm problems andtest cases. Then, standard answers and intermediate variable outputs areobtained for each problem based on standard python solutions and test cases.Finally, with the guidance of code intermediate variables, we synthesize thetext reasoning process for each reasoning problems. Through this method, we cansynthesize data that is difficult, scalable, effective, and comes with goldenstandard answers and high-quality reasoning processes. As a result, with our540K synthesized dataset constructed solely from 2,360 algorithm problems, ourapproach  Code and data are publicly available athttps://github.com/jiangjin1999/LogicPro achieves significant improvements inmultiple models for the datasets \textit{BBH$^{27}$}, \textit{LogicBench},\textit{DROP}, \textit{AR-LSAT}, and \textit{GSM8K}, etc. outperforming a widerange of existing reasoning datasets.",Jin Jiang,2024/9/19,2025/2/17,,,['cs.CL']
2409.12437v2,Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data,http://arxiv.org/abs/2409.12437v2,"Despite recent advances in training and prompting strategies for LargeLanguage Models (LLMs), these models continue to face challenges with complexlogical reasoning tasks that involve long reasoning chains. In this work, weexplore the potential and limitations of using graph-based synthetic reasoningdata as training signals to enhance LLMs' reasoning capabilities. Our extensiveexperiments, conducted on two established natural language reasoning tasks --inductive reasoning and spatial reasoning -- demonstrate that supervisedfine-tuning (SFT) with synthetic graph-based reasoning data effectivelyenhances LLMs' reasoning performance without compromising their effectivenesson other standard evaluation benchmarks.",Jiaming Zhou,2024/9/19,2024/12/16,,,"['cs.CL', 'cs.LG']"
2409.04793v1,Action is the primary key: a categorical framework for episode description and logical reasoning,http://arxiv.org/abs/2409.04793v1,"This research presents a computational framework for describing andrecognizing episodes and for logical reasoning. This framework, namedcognitive-logs, consists of a set of relational and graph databases.Cognitive-logs record knowledge, particularly in episodes that consist of""actions"" represented by verbs in natural languages and ""participants"" whoperform the actions. These objects are connected by arrows (morphisms) thatlink each action to its participant and link cause to effect. Operations basedon category theory enable comparisons between episodes and deductiveinferences, including abstractions of stories. One of the goals of this studyis to develop a database-driven artificial intelligence. This artificialintelligence thinks like a human but possesses the accuracy and rigour of amachine. The vast capacities of databases (up to petabyte scales in currenttechnologies) enable the artificial intelligence to store a greater volume ofknowledge than neural-network based artificial intelligences. Cognitive-logsserve as a model of human cognition and designed with references to cognitivelinguistics. Cognitive-logs also have the potential to model various human mindactivities.",Yoshiki Fukada,2024/9/7,2024/9/7,,,['cs.AI']
2407.19666v1,Take A Step Back: Rethinking the Two Stages in Visual Reasoning,http://arxiv.org/abs/2407.19666v1,"Visual reasoning, as a prominent research area, plays a crucial role in AI byfacilitating concept formation and interaction with the world. However, currentworks are usually carried out separately on small datasets thus lackinggeneralization ability. Through rigorous evaluation of diverse benchmarks, wedemonstrate the shortcomings of existing ad-hoc methods in achievingcross-domain reasoning and their tendency to data bias fitting. In this paper,we revisit visual reasoning with a two-stage perspective: (1) symbolization and(2) logical reasoning given symbols or their representations. We find that thereasoning stage is better at generalization than symbolization. Thus, it ismore efficient to implement symbolization via separated encoders for differentdata domains while using a shared reasoner. Given our findings, we establishdesign principles for visual reasoning frameworks following the separatedsymbolization and shared reasoning. The proposed two-stage framework achievesimpressive generalization ability on various visual reasoning tasks, includingpuzzles, physical prediction, and visual question answering (VQA), encompassingboth 2D and 3D modalities. We believe our insights will pave the way forgeneralizable visual reasoning.",Mingyu Zhang,2024/7/29,2024/7/29,,,['cs.CV']
2407.16974v1,SelfPiCo: Self-Guided Partial Code Execution with LLMs,http://arxiv.org/abs/2407.16974v1,"Code executability plays a vital role in software debugging and testing(e.g., detecting runtime exceptions or assertion violations). However, codeexecution, especially partial or arbitrary code execution, is a non-trivialtask due to missing definitions and complex third-party dependencies. To makepartial code (such as code snippets posted on the web or code fragments deepinside complex software projects) executable, the existing study has proposed amachine learning model to predict the undefined element types and inject thepre-defined dummy values into execution. However, the performance of their toolis limited due to its simply designed dummy values and the inability tocontinue learning. In this paper, we design and implement a novel framework,named SelfPiCo (Self Guided Partial Code Executor), to dynamically guidepartial code execution by incorporating the open-source LLM (i.e., Code Llama)within an interactive loop. Particularly, SelfPiCo leverages few-shotin-context learning and chain-of-thought reasoning to elicit human knowledgeand logical reasoning based on fine-tuning the Code Llama model. SelfPiCocontinuously learns from code execution results and refines its predictionsstep after step. Our evaluations demonstrate that SelfPiCo can execute 72.7%and 83.3% of all lines in the open-source code and Stack Overflow snippets,outperforming the most recent state-of-the-art Lexecutor by 37.9% and 33.5%,respectively. Moreover, SelfPiCo successfully detected 18 and 33 runtime typeerror issues by executing the partial code from eight GitHub software projectsand 43 Stack Overflow posts, demonstrating the practical usage and potentialapplication of our framework in practice.",Zhipeng Xue,2024/7/24,2024/7/24,,,['cs.SE']
2407.16831v1,Networks of Networks: Complexity Class Principles Applied to Compound AI Systems Design,http://arxiv.org/abs/2407.16831v1,"As practitioners seek to surpass the current reliability and quality frontierof monolithic models, Compound AI Systems consisting of many language modelinference calls are increasingly employed. In this work, we construct systems,which we call Networks of Networks (NoNs) organized around the distinctionbetween generating a proposed answer and verifying its correctness, afundamental concept in complexity theory that we show empirically extends toLanguage Models (LMs). We introduce a verifier-based judge NoN with Kgenerators, an instantiation of ""best-of-K"" or ""judge-based"" compound AIsystems. Through experiments on synthetic tasks such as prime factorization,and core benchmarks such as the MMLU, we demonstrate notable performance gains.For instance, in factoring products of two 3-digit primes, a simple NoNimproves accuracy from 3.7\% to 36.6\%. On MMLU, a verifier-based judgeconstruction with only 3 generators boosts accuracy over individual GPT-4-Turbocalls by 2.8\%. Our analysis reveals that these gains are most pronounced indomains where verification is notably easier than generation--acharacterization which we believe subsumes many reasoning and proceduralknowledge tasks, but doesn't often hold for factual and declarativeknowledge-based settings. For mathematical and formal logic reasoning-basedsubjects of MMLU, we observe a 5-8\% or higher gain, whilst no gain on otherssuch as geography and religion. We provide key takeaways for ML practitioners,including the importance of considering verification complexity, the impact ofwitness format on verifiability, and a simple test to determine the potentialbenefit of this NoN approach for a given problem distribution. This work aimsto inform future research and practice in the design of compound AI systems.",Jared Quincy Davis,2024/7/23,2024/7/23,,,['cs.AI']
2407.15569v2,An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought,http://arxiv.org/abs/2407.15569v2,"Since the launch of ChatGPT at the end of 2022, generative dialogue modelsrepresented by ChatGPT have quickly become essential tools in daily life. Asuser expectations increase, enhancing the capability of generative dialoguemodels to solve complex problems has become a focal point of current research.This paper delves into the effectiveness of the RAFT (Retrieval AugmentedFine-Tuning) method in improving the performance of Generative dialogue models.RAFT combines chain-of-thought with model supervised fine-tuning (SFT) andretrieval augmented generation (RAG), which significantly enhanced the model'sinformation extraction and logical reasoning abilities. We evaluated the RAFTmethod across multiple datasets and analysed its performance in variousreasoning tasks, including long-form QA and short-form QA tasks, tasks in bothChinese and English, and supportive and comparison reasoning tasks. Notably, itaddresses the gaps in previous research regarding long-form QA tasks andChinese datasets. Moreover, we also evaluate the benefit of thechain-of-thought (CoT) in the RAFT method. This work offers valuable insightsfor studies focused on enhancing the performance of generative dialogue models.",Yuetong Zhao,2024/7/22,2024/8/30,,,['cs.CL']
2407.14865v1,An Explainable Fast Deep Neural Network for Emotion Recognition,http://arxiv.org/abs/2407.14865v1,"In the context of artificial intelligence, the inherent human attribute ofengaging in logical reasoning to facilitate decision-making is mirrored by theconcept of explainability, which pertains to the ability of a model to providea clear and interpretable account of how it arrived at a particular outcome.This study explores explainability techniques for binary deep neuralarchitectures in the framework of emotion classification through videoanalysis. We investigate the optimization of input features to binaryclassifiers for emotion recognition, with face landmarks detection using animproved version of the Integrated Gradients explainability method. The maincontribution of this paper consists in the employment of an innovativeexplainable artificial intelligence algorithm to understand the crucial faciallandmarks movements during emotional feeling, using this information also forimproving the performances of deep learning-based emotion classifiers. By meansof explainability, we can optimize the number and the position of the faciallandmarks used as input features for facial emotion recognition, lowering theimpact of noisy landmarks and thus increasing the accuracy of the developedmodels. In order to test the effectiveness of the proposed approach, weconsidered a set of deep binary models for emotion classification trainedinitially with a complete set of facial landmarks, which are progressivelyreduced based on a suitable optimization procedure. The obtained results provethe robustness of the proposed explainable approach in terms of understandingthe relevance of the different facial points for the different emotions, alsoimproving the classification accuracy and diminishing the computational cost.",Francesco Di Luzio,2024/7/20,2024/7/20,,,"['cs.CV', 'cs.LG']"
2407.07671v1,Why should we ever automate moral decision making?,http://arxiv.org/abs/2407.07671v1,"While people generally trust AI to make decisions in various aspects of theirlives, concerns arise when AI is involved in decisions with significant moralimplications. The absence of a precise mathematical framework for moralreasoning intensifies these concerns, as ethics often defies simplisticmathematical models. Unlike fields such as logical reasoning, reasoning underuncertainty, and strategic decision-making, which have well-definedmathematical frameworks, moral reasoning lacks a broadly accepted framework.This absence raises questions about the confidence we can place in AI's moraldecision-making capabilities.  The environments in which AI systems are typically trained today seeminsufficiently rich for such a system to learn ethics from scratch, and even ifwe had an appropriate environment, it is unclear how we might bring about suchlearning. An alternative approach involves AI learning from human moraldecisions. This learning process can involve aggregating curated humanjudgments or demonstrations in specific domains, or leveraging a foundationmodel fed with a wide range of data. Still, concerns persist, given theimperfections in human moral decision making.  Given this, why should we ever automate moral decision making -- is it notbetter to leave all moral decision making to humans? This paper lays out anumber of reasons why we should expect AI systems to engage in decisions with amoral component, with brief discussions of the associated risks.",Vincent Conitzer,2024/7/10,2024/7/10,,,['cs.AI']
2407.05557v1,$R^2$-Guard: Robust Reasoning Enabled LLM Guardrail via Knowledge-Enhanced Logical Reasoning,http://arxiv.org/abs/2407.05557v1,"As LLMs become increasingly prevalent across various applications, it iscritical to establish safety guardrails to moderate input/output content ofLLMs. Existing guardrail models treat various safety categories independentlyand fail to explicitly capture the intercorrelations among them. This has ledto limitations such as ineffectiveness due to inadequate training on long-taildata from correlated safety categories, susceptibility to jailbreaking attacks,and inflexibility regarding new safety categories. To address theselimitations, we propose $R^2$-Guard, a robust reasoning enabled LLM guardrailvia knowledge-enhanced logical reasoning. Specifically, $R^2$-Guard comprisestwo parts: data-driven category-specific learning and reasoning components. Thedata-driven guardrail models provide unsafety probabilities of moderatedcontent on different safety categories. We then encode safety knowledge amongdifferent categories as first-order logical rules and embed them into aprobabilistic graphic model (PGM) based reasoning component. The unsafetyprobabilities of different categories from data-driven guardrail models aresent to the reasoning component for final inference. We employ two types ofPGMs: Markov logic networks (MLNs) and probabilistic circuits (PCs), andoptimize PCs to achieve precision-efficiency balance via improved graphstructure. To further perform stress tests for guardrail models, we employ apairwise construction method to construct a new safety benchmark TwinSafety,which features principled categories. We demonstrate the effectiveness of$R^2$-Guard by comparisons with eight strong guardrail models on six safetybenchmarks, and demonstrate the robustness of $R^2$-Guard against four SOTAjailbreaking attacks. $R^2$-Guard significantly surpasses SOTA methodLlamaGuard by 30.2% on ToxicChat and by 59.5% against jailbreaking attacks.",Mintong Kang,2024/7/8,2024/7/8,,,['cs.AI']
2407.00514v1,Combining Classical and Probabilistic Independence Reasoning to Verify the Security of Oblivious Algorithms (Extended Version),http://arxiv.org/abs/2407.00514v1,"We consider the problem of how to verify the security of probabilisticoblivious algorithms formally and systematically. Unfortunately, prior programlogics fail to support a number of complexities that feature in the semanticsand invariant needed to verify the security of many practical probabilisticoblivious algorithms. We propose an approach based on reasoning over perfectlyoblivious approximations, using a program logic that combines both classicalHoare logic reasoning and probabilistic independence reasoning to support allthe needed features. We formalise and prove our new logic sound in Isabelle/HOLand apply our approach to formally verify the security of several challengingcase studies beyond the reach of prior methods for proving obliviousness.",Pengbo Yan,2024/6/29,2024/6/29,,,['cs.PL']
2407.00401v1,PUZZLES: A Benchmark for Neural Algorithmic Reasoning,http://arxiv.org/abs/2407.00401v1,"Algorithmic reasoning is a fundamental cognitive ability that plays a pivotalrole in problem-solving and decision-making processes. Reinforcement Learning(RL) has demonstrated remarkable proficiency in tasks such as motor control,handling perceptual input, and managing stochastic environments. Theseadvancements have been enabled in part by the availability of benchmarks. Inthis work we introduce PUZZLES, a benchmark based on Simon Tatham's PortablePuzzle Collection, aimed at fostering progress in algorithmic and logicalreasoning in RL. PUZZLES contains 40 diverse logic puzzles of adjustable sizesand varying levels of complexity; many puzzles also feature a diverse set ofadditional configuration parameters. The 40 puzzles provide detailedinformation on the strengths and generalization capabilities of RL agents.Furthermore, we evaluate various RL algorithms on PUZZLES, providing baselinecomparisons and demonstrating the potential for future research. All thesoftware, including the environment, is available athttps://github.com/ETH-DISCO/rlp.",Benjamin Estermann,2024/6/29,2024/6/29,,,"['cs.LG', 'cs.AI']"
2406.19237v2,FlowVQA: Mapping Multimodal Logic in Visual Question Answering with Flowcharts,http://arxiv.org/abs/2406.19237v2,"Existing benchmarks for visual question answering lack in visual groundingand complexity, particularly in evaluating spatial reasoning skills. Weintroduce FlowVQA, a novel benchmark aimed at assessing the capabilities ofvisual question-answering multimodal language models in reasoning withflowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated andhuman-verified flowchart images from three distinct content sources, along with22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,including information localization, decision-making, and logical progression.We conduct a thorough baseline evaluation on a suite of both open-source andproprietary multimodal language models using various strategies, followed by ananalysis of directional bias. The results underscore the benchmark's potentialas a vital tool for advancing the field of multimodal modeling, providing afocused and challenging environment for enhancing model performance in visualand logical reasoning tasks.",Shubhankar Singh,2024/6/27,2024/6/28,,,"['cs.CL', 'cs.CV', 'cs.IR', 'cs.LG']"
2406.14880v1,Pathformer: Recursive Path Query Encoding for Complex Logical Query Answering,http://arxiv.org/abs/2406.14880v1,"Complex Logical Query Answering (CLQA) over incomplete knowledge graphs is achallenging task. Recently, Query Embedding (QE) methods are proposed to solveCLQA by performing multi-hop logical reasoning. However, most of them onlyconsider historical query context information while ignoring futureinformation, which leads to their failure to capture the complex dependenciesbehind the elements of a query. In recent years, the transformer architecturehas shown a strong ability to model long-range dependencies between words. Thebidirectional attention mechanism proposed by the transformer can solve thelimitation of these QE methods regarding query context. Still, as a sequencemodel, it is difficult for the transformer to model complex logical querieswith branch structure computation graphs directly. To this end, we propose aneural one-point embedding method called Pathformer based on the tree-likecomputation graph, i.e., query computation tree. Specifically, Pathformerdecomposes the query computation tree into path query sequences by branches andthen uses the transformer encoder to recursively encode these path querysequences to obtain the final query embedding. This allows Pathformer to fullyutilize future context information to explicitly model the complex interactionsbetween various parts of the path query. Experimental results show thatPathformer outperforms existing competitive neural QE methods, and we foundthat Pathformer has the potential to be applied to non-one-point embeddingspace.",Chongzhi Zhang,2024/6/21,2024/6/21,,,"['cs.LG', 'cs.LO']"
2406.11334v1,Program Synthesis Benchmark for Visual Programming in XLogoOnline Environment,http://arxiv.org/abs/2406.11334v1,"Large language and multimodal models have shown remarkable successes onvarious benchmarks focused on specific skills such as general-purposeprogramming, natural language understanding, math word problem-solving, andvisual question answering. However, it is unclear how well these models performon tasks that require a combination of these skills. In this paper, we curate anovel program synthesis benchmark based on the XLogoOnline visual programmingenvironment. The benchmark comprises 85 real-world tasks from the Mini-level ofthe XLogoOnline environment, each requiring a combination of different skillssuch as spatial planning, basic programming, and logical reasoning. Ourevaluation shows that current state-of-the-art models like GPT-4V andLlama3-70B struggle to solve these tasks, achieving only 20% and 2.35% successrates. Next, we develop a fine-tuning pipeline to boost the performance ofmodels by leveraging a large-scale synthetic training dataset with over 80000tasks. Moreover, we showcase how emulator-driven feedback can be used to designa curriculum over training data distribution. We showcase that a fine-tunedLlama3-8B drastically outperforms GPT-4V and Llama3-70B models, and provide anin-depth analysis of the models' expertise across different skill dimensions.We will publicly release the benchmark for future research on program synthesisin visual programming.",Chao Wen,2024/6/17,2024/6/17,,,['cs.AI']
2406.11303v1,VideoVista: A Versatile Benchmark for Video Understanding and Reasoning,http://arxiv.org/abs/2406.11303v1,"Despite significant breakthroughs in video analysis driven by the rapiddevelopment of large multimodal models (LMMs), there remains a lack of aversatile evaluation benchmark to comprehensively assess these models'performance in video understanding and reasoning. To address this, we presentVideoVista, a video QA benchmark that integrates challenges across diversecontent categories, durations, and abilities. Specifically, VideoVistacomprises 25,000 questions derived from 3,400 videos spanning 14 categories(e.g., Howto, Film, and Entertainment) with durations ranging from a fewseconds to over 10 minutes. Besides, it encompasses 19 types of understandingtasks (e.g., anomaly detection, interaction understanding) and 8 reasoningtasks (e.g., logical reasoning, causal reasoning). To achieve this, we presentan automatic data construction framework, leveraging powerful GPT-4o alongsideadvanced analysis tools (e.g., video splitting, object segmenting, andtracking). We also utilize this framework to construct training data to enhancethe capabilities of video-related LMMs (Video-LMMs). Through a comprehensiveand quantitative evaluation of cutting-edge models, we reveal that: 1)Video-LMMs face difficulties in fine-grained video tasks involving temporallocation, object tracking, and anomaly detection; 2) Video-LMMs presentinferior logical and relation reasoning abilities; 3) Open-source Video-LMMs'performance is significantly lower than GPT-4o and Gemini-1.5, lagging by 20points. This highlights the crucial role VideoVista will play in advancing LMMsthat can accurately understand videos and perform precise reasoning.",Yunxin Li,2024/6/17,2024/6/17,,,"['cs.CV', 'cs.AI', 'cs.CL']"
2406.11035v1,Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars,http://arxiv.org/abs/2406.11035v1,"Logical reasoning remains a challenge for natural language processing, but itcan be improved by training language models to mimic theorem provers onprocedurally generated problems. Previous work used domain-specific proofgeneration algorithms, which biases reasoning toward specific proof traces andlimits auditability and extensibility. We present a simpler and more generaldeclarative framework with flexible context-sensitive rules binding multiplelanguages (specifically, simplified English and the TPTP theorem-provinglanguage). We construct first-order logic problems by selecting up to 32premises and one hypothesis. We demonstrate that using semantic constraintsduring generation and careful English verbalization of predicates enhanceslogical reasoning without hurting natural English tasks. We use relativelysmall DeBERTa-v3 models to achieve state-of-the-art accuracy on the FOLIOhuman-authored logic dataset, surpassing GPT-4 in accuracy with or without anexternal solver by 12%.",Damien Sileo,2024/6/16,2024/6/16,,,"['cs.CL', 'I.2.7']"
2406.07034v1,Improving Multi-hop Logical Reasoning in Knowledge Graphs with Context-Aware Query Representation Learning,http://arxiv.org/abs/2406.07034v1,"Multi-hop logical reasoning on knowledge graphs is a pivotal task in naturallanguage processing, with numerous approaches aiming to answer First-OrderLogic (FOL) queries. Recent geometry (e.g., box, cone) and probability (e.g.,beta distribution)-based methodologies have effectively addressed complex FOLqueries. However, a common challenge across these methods lies in determiningaccurate geometric bounds or probability parameters for these queries. Thechallenge arises because existing methods rely on linear sequential operationswithin their computation graphs, overlooking the logical structure of the queryand the relation-induced information that can be gleaned from the relations ofthe query, which we call the context of the query. To address the problem, wepropose a model-agnostic methodology that enhances the effectiveness ofexisting multi-hop logical reasoning approaches by fully integrating thecontext of the FOL query graph. Our approach distinctively discerns (1) thestructural context inherent to the query structure and (2) the relation-inducedcontext unique to each node in the query graph as delineated in thecorresponding knowledge graph. This dual-context paradigm helps nodes within aquery graph attain refined internal representations throughout the multi-hopreasoning steps. Through experiments on two datasets, our method consistentlyenhances the three multi-hop reasoning foundation models, achieving performanceimprovements of up to 19.5%. Our code is available athttps://github.com/kjh9503/caqr.",Jeonghoon Kim,2024/6/11,2024/6/11,,,"['cs.AI', 'cs.CL']"
2406.04472v1,On the Hardness of Probabilistic Neurosymbolic Learning,http://arxiv.org/abs/2406.04472v1,"The limitations of purely neural learning have sparked an interest inprobabilistic neurosymbolic models, which combine neural networks withprobabilistic logical reasoning. As these neurosymbolic models are trained withgradient descent, we study the complexity of differentiating probabilisticreasoning. We prove that although approximating these gradients is intractablein general, it becomes tractable during training. Furthermore, we introduceWeightME, an unbiased gradient estimator based on model sampling. Under mildassumptions, WeightME approximates the gradient with probabilistic guaranteesusing a logarithmic number of calls to a SAT solver. Lastly, we evaluate thenecessity of these guarantees on the gradient. Our experiments indicate thatthe existing biased approximations indeed struggle to optimize even when exactsolving is still feasible.",Jaron Maene,2024/6/6,2024/6/6,,,['cs.LG']
2406.01895v1,Explicitly Encoding Structural Symmetry is Key to Length Generalization in Arithmetic Tasks,http://arxiv.org/abs/2406.01895v1,"Despite the success of Transformers on language understanding, codegeneration, and logical reasoning, they still fail to generalize over length onbasic arithmetic tasks such as addition and multiplication. A major reasonbehind this failure is the vast difference in structure between numbers andtext; For example, the numbers are typically parsed from right to left, andthere is a correspondence between digits at the same position across differentnumbers. In contrast, for text, such symmetries are quite unnatural. In thiswork, we propose to encode these semantics explicitly into the model viamodified number formatting and custom positional encodings. Empirically, ourmethod allows a Transformer trained on numbers with at most 5-digits foraddition and multiplication to generalize up to 50-digit numbers, without usingadditional data for longer sequences. We further demonstrate that traditionalabsolute positional encodings (APE) fail to generalize to longer sequences,even when trained with augmented data that captures task symmetries. Toelucidate the importance of explicitly encoding structure, we prove thatexplicit incorporation of structure via positional encodings is necessary forout-of-distribution generalization. Finally, we pinpoint other challengesinherent to length generalization beyond capturing symmetries, in particularcomplexity of the underlying task, and propose changes in the trainingdistribution to address them.",Mahdi Sabbaghi,2024/6/4,2024/6/4,,,"['cs.LG', 'cs.CL', 'stat.ML']"
2406.01140v2,Logical Reasoning with Relation Network for Inductive Knowledge Graph Completion,http://arxiv.org/abs/2406.01140v2,"Inductive knowledge graph completion (KGC) aims to infer the missing relationfor a set of newly-coming entities that never appeared in the training set.Such a setting is more in line with reality, as real-world KGs are constantlyevolving and introducing new knowledge. Recent studies have shown promisingresults using message passing over subgraphs to embed newly-coming entities forinductive KGC. However, the inductive capability of these methods is usuallylimited by two key issues. (i) KGC always suffers from data sparsity, and thesituation is even exacerbated in inductive KGC where new entities often havefew or no connections to the original KG. (ii) Cold-start problem. It is overcoarse-grained for accurate KG reasoning to generate representations for newentities by gathering the local information from few neighbors. To this end, wepropose a novel iNfOmax RelAtion Network, namely NORAN, for inductive KGcompletion. It aims to mine latent relation patterns for inductive KGcompletion. Specifically, by centering on relations, NORAN provides a hyperview towards KG modeling, where the correlations between relations can benaturally captured as entity-independent logical evidence to conduct inductiveKGC. Extensive experiment results on five benchmarks show that our frameworksubstantially outperforms the state-of-the-art KGC methods.",Qinggang Zhang,2024/6/3,2024/7/22,,,['cs.AI']
2406.00938v1,A Synergistic Approach In Network Intrusion Detection By Neurosymbolic AI,http://arxiv.org/abs/2406.00938v1,"The prevailing approaches in Network Intrusion Detection Systems (NIDS) areoften hampered by issues such as high resource consumption, significantcomputational demands, and poor interpretability. Furthermore, these systemsgenerally struggle to identify novel, rapidly changing cyber threats. Thispaper delves into the potential of incorporating Neurosymbolic ArtificialIntelligence (NSAI) into NIDS, combining deep learning's data-driven strengthswith symbolic AI's logical reasoning to tackle the dynamic challenges incybersecurity, which also includes detailed NSAI techniques introduction forcyber professionals to explore the potential strengths of NSAI in NIDS. Theinclusion of NSAI in NIDS marks potential advancements in both the detectionand interpretation of intricate network threats, benefiting from the robustpattern recognition of neural networks and the interpretive prowess of symbolicreasoning. By analyzing network traffic data types and machine learningarchitectures, we illustrate NSAI's distinctive capability to offer moreprofound insights into network behavior, thereby improving both detectionperformance and the adaptability of the system. This merging of technologiesnot only enhances the functionality of traditional NIDS but also sets the stagefor future developments in building more resilient, interpretable, and dynamicdefense mechanisms against advanced cyber threats. The continued progress inthis area is poised to transform NIDS into a system that is both responsive toknown threats and anticipatory of emerging, unseen ones.",Alice Bizzarri,2024/6/3,2024/6/3,,,"['cs.CR', 'cs.AI', 'cs.SC']"
2405.09711v1,STAR: A Benchmark for Situated Reasoning in Real-World Videos,http://arxiv.org/abs/2405.09711v1,"Reasoning in the real world is not divorced from situations. How to capturethe present knowledge from surrounding situations and perform reasoningaccordingly is crucial and challenging for machine intelligence. This paperintroduces a new benchmark that evaluates the situated reasoning ability viasituation abstraction and logic-grounded question answering for real-worldvideos, called Situated Reasoning in Real-World Videos (STAR Benchmark). Thisbenchmark is built upon the real-world videos associated with human actions orinteractions, which are naturally dynamic, compositional, and logical. Thedataset includes four types of questions, including interaction, sequence,prediction, and feasibility. We represent the situations in real-world videosby hyper-graphs connecting extracted atomic entities and relations (e.g.,actions, persons, objects, and relationships). Besides visual perception,situated reasoning also requires structured situation comprehension and logicalreasoning. Questions and answers are procedurally generated. The answeringlogic of each question is represented by a functional program based on asituation hyper-graph. We compare various existing video reasoning models andfind that they all struggle on this challenging situated reasoning task. Wefurther propose a diagnostic neuro-symbolic model that can disentangle visualperception, situation abstraction, language understanding, and functionalreasoning to understand the challenges of this benchmark.",Bo Wu,2024/5/15,2024/5/15,,,"['cs.AI', 'cs.CL', 'cs.CV']"
2405.04872v1,Logical Negation Augmenting and Debiasing for Prompt-based Methods,http://arxiv.org/abs/2405.04872v1,"Prompt-based methods have gained increasing attention on NLP and shownvalidity on many downstream tasks. Many works have focused on mining thesemethods' potential for knowledge extraction, but few explore their ability tomake logical reasoning. In this work, we focus on the effectiveness of theprompt-based methods on first-order logical reasoning and find that thebottleneck lies in logical negation. Based on our analysis, logical negationtends to result in spurious correlations to negative answers, whilepropositions without logical negation correlate to positive answers. To solvethe problem, we propose a simple but effective method, Negation Augmenting andNegation Debiasing (NAND), which introduces negative propositions toprompt-based methods without updating parameters. Specifically, these negativepropositions can counteract spurious correlations by providing ""not"" for allinstances so that models cannot make decisions only by whether expressionscontain a logical negation. Experiments on three datasets show that NAND notonly solves the problem of calibrating logical negation but also significantlyenhances prompt-based methods of logical reasoning without model retraining.",Yitian Li,2024/5/8,2024/5/8,,,"['cs.CL', 'cs.AI', 'cs.LO']"
2404.16884v1,Aligning Knowledge Graphs Provided by Humans and Generated from Neural Networks in Specific Tasks,http://arxiv.org/abs/2404.16884v1,"This paper develops an innovative method that enables neural networks togenerate and utilize knowledge graphs, which describe their concept-levelknowledge and optimize network parameters through alignment with human-providedknowledge. This research addresses a gap where traditionally, network-generatedknowledge has been limited to applications in downstream symbolic analysis orenhancing network transparency. By integrating a novel autoencoder design withthe Vector Symbolic Architecture (VSA), we have introduced auxiliary tasks thatsupport end-to-end training. Our approach eschews traditional dependencies onontologies or word embedding models, mining concepts from neural networks anddirectly aligning them with human knowledge. Experiments show that our methodconsistently captures network-generated concepts that align closely with humanknowledge and can even uncover new, useful concepts not previously identifiedby humans. This plug-and-play strategy not only enhances the interpretabilityof neural networks but also facilitates the integration of symbolic logicalreasoning within these systems.",Tangrui Li,2024/4/23,2024/4/23,,,"['cs.LG', 'cs.AI']"
2404.09305v2,OWLOOP: Interfaces for Mapping OWL Axioms into OOP Hierarchies,http://arxiv.org/abs/2404.09305v2,"The paper tackles the issue of mapping logic axioms formalised in theOntology Web Language (OWL) within the Object-Oriented Programming (OOP)paradigm. The issues of mapping OWL axioms hierarchies and OOP objectshierarchies are due to OWL-based reasoning algorithms, which might change anOWL hierarchy at runtime; instead, OOP hierarchies are usually defined asstatic structures. Although programming paradigms based on reflection allowchanging the OOP hierarchies at runtime and mapping OWL axioms dynamically,there are no currently available mechanisms that do not limit the reasoningalgorithms. Thus, the factory-based paradigm is typically used since itdecouples the OWL and OOP hierarchies. However, the factory inhibits OOPpolymorphism and introduces a paradigm shift with respect to widely acceptedOOP paradigms. We present the OWLOOP API, which exploits the factory to notlimit reasoning algorithms, and it provides novel OOP interfaces concerning theaxioms in an ontology. OWLOOP is designed to limit the paradigm shift requiredfor using ontologies while improving, through OOP-like polymorphism, themodularity of software architectures that exploit logic reasoning. The paperdetails our OWL to OOP mapping mechanism, and it shows the benefits andlimitations of OWLOOP through examples concerning a robot in a smartenvironment.",Luca Buoncompagni,2024/4/14,2024/4/19,,,"['cs.AI', 'cs.LO', 'cs.RO', 'cs.SE', '68T27 (Primary) 68T30, 68N19, 68T40 (Secondary)', 'D.2.11; D.1.5; D.1.6; E.2; I.2.4']"
2404.03820v2,CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues,http://arxiv.org/abs/2404.03820v2,"Recent advancements in instruction-tuning datasets have predominantly focusedon specific tasks like mathematical or logical reasoning. There has been anotable gap in data designed for aligning language models to maintain topicrelevance in conversations - a critical aspect for deploying chatbots toproduction. We introduce the CantTalkAboutThis dataset to help language modelsremain focused on the subject at hand during task-oriented interactions. Itconsists of synthetic dialogues on a wide range of conversation topics fromdifferent domains. These dialogues are interspersed with distractor turns thatintentionally divert the chatbot from the predefined topic. Fine-tuninglanguage models on this dataset helps make them resilient to deviating from therole assigned and improves their ability to maintain topical coherence comparedto general-purpose instruction-tuned LLMs like GPT-4-turbo andMixtral-Instruct. Additionally, preliminary observations suggest that trainingmodels on this dataset also enhance their performance on fine-grainedinstruction following tasks, including safety alignment.",Makesh Narsimhan Sreedhar,2024/4/4,2024/6/21,,,['cs.CL']
2404.01591v1,Language Model Guided Interpretable Video Action Reasoning,http://arxiv.org/abs/2404.01591v1,"While neural networks have excelled in video action recognition tasks, theirblack-box nature often obscures the understanding of their decision-makingprocesses. Recent approaches used inherently interpretable models to analyzevideo actions in a manner akin to human reasoning. These models, however,usually fall short in performance compared to their black-box counterparts. Inthis work, we present a new framework named Language-guided InterpretableAction Recognition framework (LaIAR). LaIAR leverages knowledge from languagemodels to enhance both the recognition capabilities and the interpretability ofvideo models. In essence, we redefine the problem of understanding video modeldecisions as a task of aligning video and language models. Using the logicalreasoning captured by the language model, we steer the training of the videomodel. This integrated approach not only improves the video model'sadaptability to different domains but also boosts its overall performance.Extensive experiments on two complex video action datasets, Charades & CAD-120,validates the improved performance and interpretability of our LaIAR framework.The code of LaIAR is available at https://github.com/NingWang2049/LaIAR.",Ning Wang,2024/4/2,2024/4/2,,,['cs.CV']
2403.20194v2,ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Capability for Large Vision-Language Models,http://arxiv.org/abs/2403.20194v2,"This paper presents ConvBench, a novel multi-turn conversation evaluationbenchmark tailored for Large Vision-Language Models (LVLMs). Unlike existingbenchmarks that assess individual capabilities in single-turn dialogues,ConvBench adopts a three-level multimodal capability hierarchy, mimicking humancognitive processes by stacking up perception, reasoning, and creativity. Eachlevel focuses on a distinct capability, mirroring the cognitive progressionfrom basic perception to logical reasoning and ultimately to advancedcreativity. ConvBench comprises 577 meticulously curated multi-turnconversations encompassing 215 tasks reflective of real-world demands.Automatic evaluations quantify response performance at each turn and overallconversation level. Leveraging the capability hierarchy, ConvBench enablesprecise attribution of conversation mistakes to specific levels. Experimentalresults reveal a performance gap between multi-modal models, including GPT4-V,and human performance in multi-turn conversations. Additionally, weakfine-grained perception in multi-modal models contributes to reasoning andcreation failures. ConvBench serves as a catalyst for further research aimed atenhancing visual dialogues.",Shuo Liu,2024/3/29,2024/4/25,,,['cs.MM']
2403.15637v1,CoNVOI: Context-aware Navigation using Vision Language Models in Outdoor and Indoor Environments,http://arxiv.org/abs/2403.15637v1,"We present ConVOI, a novel method for autonomous robot navigation inreal-world indoor and outdoor environments using Vision Language Models (VLMs).We employ VLMs in two ways: first, we leverage their zero-shot imageclassification capability to identify the context or scenario (e.g., indoorcorridor, outdoor terrain, crosswalk, etc) of the robot's surroundings, andformulate context-based navigation behaviors as simple text prompts (e.g.``stay on the pavement""). Second, we utilize their state-of-the-art semanticunderstanding and logical reasoning capabilities to compute a suitabletrajectory given the identified context. To this end, we propose a novelmulti-modal visual marking approach to annotate the obstacle-free regions inthe RGB image used as input to the VLM with numbers, by correlating it with alocal occupancy map of the environment. The marked numbers ground imagelocations in the real-world, direct the VLM's attention solely to navigablelocations, and elucidate the spatial relationships between them and terrainsdepicted in the image to the VLM. Next, we query the VLM to select numbers onthe marked image that satisfy the context-based behavior text prompt, andconstruct a reference path using the selected numbers. Finally, we propose amethod to extrapolate the reference trajectory when the robot's environmentalcontext has not changed to prevent unnecessary VLM queries. We use thereference trajectory to guide a motion planner, and demonstrate that it leadsto human-like behaviors (e.g. not cutting through a group of people, usingcrosswalks, etc.) in various real-world indoor and outdoor scenarios.",Adarsh Jagan Sathyamoorthy,2024/3/22,2024/3/22,,,['cs.RO']
2403.11348v1,COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits,http://arxiv.org/abs/2403.11348v1,"Conformal prediction has shown spurring performance in constructingstatistically rigorous prediction sets for arbitrary black-box machine learningmodels, assuming the data is exchangeable. However, even small adversarialperturbations during the inference can violate the exchangeability assumption,challenge the coverage guarantees, and result in a subsequent decline inempirical coverage. In this work, we propose a certifiably robustlearning-reasoning conformal prediction framework (COLEP) via probabilisticcircuits, which comprise a data-driven learning component that trainsstatistical models to learn different semantic concepts, and a reasoningcomponent that encodes knowledge and characterizes the relationships among thetrained models for logic reasoning. To achieve exact and efficient reasoning,we employ probabilistic circuits (PCs) within the reasoning component.Theoretically, we provide end-to-end certification of prediction coverage forCOLEP in the presence of bounded adversarial perturbations. We also providecertified coverage considering the finite size of the calibration set.Furthermore, we prove that COLEP achieves higher prediction coverage andaccuracy over a single model as long as the utilities of knowledge models arenon-trivial. Empirically, we show the validity and tightness of our certifiedcoverage, demonstrating the robust conformal prediction of COLEP on variousdatasets, including GTSRB, CIFAR10, and AwA2. We show that COLEP achieves up to12% improvement in certified coverage on GTSRB, 9% on CIFAR-10, and 14% onAwA2.",Mintong Kang,2024/3/17,2024/3/17,,,"['cs.LG', 'cs.AI', 'stat.ML']"
2403.11314v1,Reasoning in Transformers -- Mitigating Spurious Correlations and Reasoning Shortcuts,http://arxiv.org/abs/2403.11314v1,"Transformer language models are neural networks used for a wide variety oftasks concerning natural language, including some that also require logicalreasoning. However, a transformer model may easily learn spurious patterns inthe data, short-circuiting actual reasoning. In this paper we investigate towhat extent transformers can be trained to a) approximate reasoning inpropositional logic while b) avoiding known reasoning shortcuts via spuriouscorrelations in the training data. To do so, we use a dataset with knownspurious correlation between truth and e.g. the number of rules in the problem.We augment the data with proofs, and train two models: a generativetransformer, WP-BART, trained on problems and their whole proofs, and aneuro-symbolic model, SIP-BART, trained on individual proof steps and combiningthe generative transformer model BART with a symbolic proof checker. We findthat SIP-BART succeeds in avoiding reasoning shortcuts, while WP-BART does not.For SIP-BART, we then identify a few remaining reasoning errors, not previouslydescribed in the literature, arising from using a pre-trained language model.These are qualitatively analysed to create a taxonomy of four different typesof additional pitfalls.",Daniel Enstrm,2024/3/17,2024/3/17,,,"['cs.LG', 'cs.CL']"
2403.04017v1,Learning Guided Automated Reasoning: A Brief Survey,http://arxiv.org/abs/2403.04017v1,"Automated theorem provers and formal proof assistants are general reasoningsystems that are in theory capable of proving arbitrarily hard theorems, thussolving arbitrary problems reducible to mathematics and logical reasoning. Inpractice, such systems however face large combinatorial explosion, andtherefore include many heuristics and choice points that considerably influencetheir performance. This is an opportunity for trained machine learningpredictors, which can guide the work of such reasoning systems. Conversely,deductive search supported by the notion of logically valid proof allows one totrain machine learning systems on large reasoning corpora. Such bodies of proofare usually correct by construction and when combined with more and moreprecise trained guidance they can be boostrapped into very large corpora, withincreasingly long reasoning chains and possibly novel proof ideas. In thispaper we provide an overview of several automated reasoning and theorem provingdomains and the learning and AI methods that have been so far developed forthem. These include premise selection, proof guidance in several settings, AIsystems and feedback loops iterating between reasoning and learning, andsymbolic classification problems.",Lasse Blaauwbroek,2024/3/6,2024/3/6,,,"['cs.AI', 'cs.LG', 'cs.LO', 'cs.NE', 'cs.SC']"
2403.02933v1,Fuzzy Datalog$^\exists$ over Arbitrary t-Norms,http://arxiv.org/abs/2403.02933v1,"One of the main challenges in the area of Neuro-Symbolic AI is to performlogical reasoning in the presence of both neural and symbolic data. Thisrequires combining heterogeneous data sources such as knowledge graphs, neuralmodel predictions, structured databases, crowd-sourced data, and many more. Toallow for such reasoning, we generalise the standard rule-based languageDatalog with existential rules (commonly referred to as tuple-generatingdependencies) to the fuzzy setting, by allowing for arbitrary t-norms in theplace of classical conjunctions in rule bodies. The resulting formalism allowsus to perform reasoning about data associated with degrees of uncertainty whilepreserving computational complexity results and the applicability of reasoningtechniques established for the standard Datalog setting. In particular, weprovide fuzzy extensions of Datalog chases which produce fuzzy universal modelsand we exploit them to show that in important fragments of the language,reasoning has the same complexity as in the classical setting.",Matthias Lanzinger,2024/3/5,2024/3/5,,,"['cs.AI', 'cs.LO']"
2403.01811v2,Enhancing Multi-Domain Automatic Short Answer Grading through an Explainable Neuro-Symbolic Pipeline,http://arxiv.org/abs/2403.01811v2,"Grading short answer questions automatically with interpretable reasoningbehind the grading decision is a challenging goal for current transformerapproaches. Justification cue detection, in combination with logical reasoners,has shown a promising direction for neuro-symbolic architectures in ASAG. But,one of the main challenges is the requirement of annotated justification cuesin the students' responses, which only exist for a few ASAG datasets. Toovercome this challenge, we contribute (1) a weakly supervised annotationprocedure for justification cues in ASAG datasets, and (2) a neuro-symbolicmodel for explainable ASAG based on justification cues. Our approach improvesupon the RMSE by 0.24 to 0.3 compared to the state-of-the-art on the ShortAnswer Feedback dataset in a bilingual, multi-domain, and multi-questiontraining setup. This result shows that our approach provides a promisingdirection for generating high-quality grades and accompanying explanations forfuture research in ASAG and educational NLP.",Felix Knnecke,2024/3/4,2024/3/19,,,['cs.CL']
2403.01185v1,Balancing Exploration and Exploitation in LLM using Soft RLLF for Enhanced Negation Understanding,http://arxiv.org/abs/2403.01185v1,"Finetuning approaches in NLP often focus on exploitation rather thanexploration, which may lead to suboptimal models. Given the vast search spaceof natural language, this limited exploration can restrict their performance incomplex, high-stakes domains, where accurate negation understanding and logicalreasoning abilities are crucial. To address this issue, we leverageReinforcement Learning from Logical Feedback (RLLF) to create an effectivebalance between exploration and exploitation in LLMs. Our approach employs anappropriate benchmark dataset for training and evaluation, highlighting theimportance of exploration in enhancing negation understanding capabilities. Wecompare the performance of our RLLF-enhanced LLMs with baseline models trainedwithout RLLF, demonstrating the value of this balanced approach. Furthermore,we showcase the potential of our method in legal AI applications by employingtransfer learning and evaluating its impact on negation understanding. Ourexperimental results exhibit the effectiveness of balancing exploration andexploitation with RLLF in improving LLMs' negation capabilities. This hasimplications for the development of more accurate, reliable, and logicallyconsistent language models in high-stakes domains.",Ha-Thanh Nguyen,2024/3/2,2024/3/2,,,"['cs.CL', 'cs.AI']"
2402.13897v2,Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning,http://arxiv.org/abs/2402.13897v2,"Information retrieval is a rapidly evolving field. However it still facessignificant limitations in the scientific and industrial vast amounts ofinformation, such as semantic divergence and vocabulary gaps in sparseretrieval, low precision and lack of interpretability in semantic search, orhallucination and outdated information in generative models. In this paper, weintroduce a two-block approach to tackle these hurdles for long documents. Thefirst block enhances language understanding in sparse retrieval by queryexpansion to retrieve relevant documents. The second block deepens the resultby providing comprehensive and informative answers to the complex questionusing only the information spread in the long document, enabling bidirectionalengagement. At various stages of the pipeline, intermediate results arepresented to users to facilitate understanding of the system's reasoning. Webelieve this bidirectional approach brings significant advancements in terms oftransparency, logical thinking, and comprehensive understanding in the field ofscientific information retrieval.",Loc Rakotoson,2024/2/21,2024/3/14,,,"['cs.IR', 'cs.AI', 'cs.CL', 'cs.LG', 'H.3.1; H.3.3; I.7; K.4']"
2402.14047v2,Simple and Effective Transfer Learning for Neuro-Symbolic Integration,http://arxiv.org/abs/2402.14047v2,"Deep Learning (DL) techniques have achieved remarkable successes in recentyears. However, their ability to generalize and execute reasoning tasks remainsa challenge. A potential solution to this issue is Neuro-Symbolic Integration(NeSy), where neural approaches are combined with symbolic reasoning. Most ofthese methods exploit a neural network to map perceptions to symbols and alogical reasoner to predict the output of the downstream task. These methodsexhibit superior generalization capacity compared to fully neuralarchitectures. However, they suffer from several issues, including slowconvergence, learning difficulties with complex perception tasks, andconvergence to local minima. This paper proposes a simple yet effective methodto ameliorate these problems. The key idea involves pretraining a neural modelon the downstream task. Then, a NeSy model is trained on the same task viatransfer learning, where the weights of the perceptual part are injected fromthe pretrained network. The key observation of our work is that the neuralnetwork fails to generalize only at the level of the symbolic part while beingperfectly capable of learning the mapping from perceptions to symbols. We havetested our training strategy on various SOTA NeSy methods and datasets,demonstrating consistent improvements in the aforementioned problems.",Alessandro Daniele,2024/2/21,2024/7/15,,,"['cs.LG', 'cs.AI']"
2402.13744v1,Reasoning Algorithmically in Graph Neural Networks,http://arxiv.org/abs/2402.13744v1,"The development of artificial intelligence systems with advanced reasoningcapabilities represents a persistent and long-standing research question.Traditionally, the primary strategy to address this challenge involved theadoption of symbolic approaches, where knowledge was explicitly represented bymeans of symbols and explicitly programmed rules. However, with the advent ofmachine learning, there has been a paradigm shift towards systems that canautonomously learn from data, requiring minimal human guidance. In light ofthis shift, in latest years, there has been increasing interest and efforts atendowing neural networks with the ability to reason, bridging the gap betweendata-driven learning and logical reasoning. Within this context, NeuralAlgorithmic Reasoning (NAR) stands out as a promising research field, aiming tointegrate the structured and rule-based reasoning of algorithms with theadaptive learning capabilities of neural networks, typically by tasking neuralmodels to mimic classical algorithms. In this dissertation, we providetheoretical and practical contributions to this area of research. We explorethe connections between neural networks and tropical algebra, deriving powerfularchitectures that are aligned with algorithm execution. Furthermore, wediscuss and show the ability of such neural reasoners to learn and manipulatecomplex algorithmic and combinatorial optimization concepts, such as theprinciple of strong duality. Finally, in our empirical efforts, we validate thereal-world utility of NAR networks across different practical scenarios. Thisincludes tasks as diverse as planning problems, large-scale edge classificationtasks and the learning of polynomial-time approximate algorithms for NP-hardcombinatorial problems. Through this exploration, we aim to showcase thepotential integrating algorithmic reasoning in machine learning models.",Danilo Numeroso,2024/2/21,2024/2/21,,,['cs.LG']
2402.13440v1,A Neuro-Symbolic Approach to Multi-Agent RL for Interpretability and Probabilistic Decision Making,http://arxiv.org/abs/2402.13440v1,"Multi-agent reinforcement learning (MARL) is well-suited for runtimedecision-making in optimizing the performance of systems where multiple agentscoexist and compete for shared resources. However, applying common deeplearning-based MARL solutions to real-world problems suffers from issues ofinterpretability, sample efficiency, partial observability, etc. To addressthese challenges, we present an event-driven formulation, where decision-makingis handled by distributed co-operative MARL agents using neuro-symbolicmethods. The recently introduced neuro-symbolic Logical Neural Networks (LNN)framework serves as a function approximator for the RL, to train a rules-basedpolicy that is both logical and interpretable by construction. To enabledecision-making under uncertainty and partial observability, we developed anovel probabilistic neuro-symbolic framework, Probabilistic Logical NeuralNetworks (PLNN), which combines the capabilities of logical reasoning withprobabilistic graphical models. In PLNN, the upward/downward inferencestrategy, inherited from LNN, is coupled with belief bounds by setting theactivation function for the logical operator associated with each neuralnetwork node to a probability-respecting generalization of the Fr\'echetinequalities. These PLNN nodes form the unifying element that combinesprobabilistic logic and Bayes Nets, permitting inference for variables withunobserved states. We demonstrate our contributions by addressing key MARLchallenges for power sharing in a system-on-chip application.",Chitra Subramanian,2024/2/21,2024/2/21,,,"['cs.AI', 'cs.NE', 'I.2.6']"
2402.12954v2,Conditional Logical Message Passing Transformer for Complex Query Answering,http://arxiv.org/abs/2402.12954v2,"Complex Query Answering (CQA) over Knowledge Graphs (KGs) is a challengingtask. Given that KGs are usually incomplete, neural models are proposed tosolve CQA by performing multi-hop logical reasoning. However, most of themcannot perform well on both one-hop and multi-hop queries simultaneously.Recent work proposes a logical message passing mechanism based on thepre-trained neural link predictors. While effective on both one-hop andmulti-hop queries, it ignores the difference between the constant and variablenodes in a query graph. In addition, during the node embedding update stage,this mechanism cannot dynamically measure the importance of different messages,and whether it can capture the implicit logical dependencies related to a nodeand received messages remains unclear. In this paper, we propose ConditionalLogical Message Passing Transformer (CLMPT), which considers the differencebetween constants and variables in the case of using pre-trained neural linkpredictors and performs message passing conditionally on the node type. Weempirically verified that this approach can reduce computational costs withoutaffecting performance. Furthermore, CLMPT uses the transformer to aggregatereceived messages and update the corresponding node embedding. Through theself-attention mechanism, CLMPT can assign adaptive weights to elements in aninput set consisting of received messages and the corresponding node andexplicitly model logical dependencies between various elements. Experimentalresults show that CLMPT is a new state-of-the-art neural CQA model.https://github.com/qianlima-lab/CLMPT.",Chongzhi Zhang,2024/2/20,2024/8/10,,,"['cs.LG', 'cs.AI', 'cs.LO']"
2402.09609v1,LogicPrpBank: A Corpus for Logical Implication and Equivalence,http://arxiv.org/abs/2402.09609v1,"Logic reasoning has been critically needed in problem-solving anddecision-making. Although Language Models (LMs) have demonstrated capabilitiesof handling multiple reasoning tasks (e.g., commonsense reasoning), theirability to reason complex mathematical problems, specifically propositionallogic, remains largely underexplored. This lack of exploration can beattributed to the limited availability of annotated corpora. Here, we present awell-labeled propositional logic corpus, LogicPrpBank, containing 7093Propositional Logic Statements (PLSs) across six mathematical subjects, tostudy a brand-new task of reasoning logical implication and equivalence. Webenchmark LogicPrpBank with widely-used LMs to show that our corpus offers auseful resource for this challenging task and there is ample room for modelimprovement.",Zhexiong Liu,2024/2/14,2024/2/14,,,"['cs.CL', 'cs.AI']"
2402.07872v1,PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs,http://arxiv.org/abs/2402.07872v1,"Vision language models (VLMs) have shown impressive capabilities across avariety of tasks, from logical reasoning to visual understanding. This opensthe door to richer interaction with the world, for example robotic control.However, VLMs produce only textual outputs, while robotic control and otherspatial tasks require outputting continuous coordinates, actions, ortrajectories. How can we enable VLMs to handle such settings withoutfine-tuning on task-specific data?  In this paper, we propose a novel visual prompting approach for VLMs that wecall Prompting with Iterative Visual Optimization (PIVOT), which casts tasks asiterative visual question answering. In each iteration, the image is annotatedwith a visual representation of proposals that the VLM can refer to (e.g.,candidate robot actions, localizations, or trajectories). The VLM then selectsthe best ones for the task. These proposals are iteratively refined, allowingthe VLM to eventually zero in on the best available answer. We investigatePIVOT on real-world robotic navigation, real-world manipulation from images,instruction following in simulation, and additional spatial inference taskssuch as localization. We find, perhaps surprisingly, that our approach enableszero-shot control of robotic systems without any robot training data,navigation in a variety of environments, and other capabilities. Althoughcurrent performance is far from perfect, our work highlights potentials andlimitations of this new regime and shows a promising approach forInternet-Scale VLMs in robotic and spatial reasoning domains. Website:pivot-prompt.github.io and HuggingFace:https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.",Soroush Nasiriany,2024/2/12,2024/2/12,,,"['cs.RO', 'cs.CL', 'cs.CV', 'cs.LG']"
2402.06608v2,"TIC: Translate-Infer-Compile for accurate ""text to plan"" using LLMs and Logical Representations",http://arxiv.org/abs/2402.06608v2,"We study the problem of generating plans for given natural language planningtask requests. On one hand, LLMs excel at natural language processing but donot perform well on planning. On the other hand, classical planning tools excelat planning tasks but require input in a structured language such as thePlanning Domain Definition Language (PDDL). We leverage the strengths of boththe techniques by using an LLM for generating the PDDL representation (taskPDDL) of planning task requests followed by using a classical planner forcomputing a plan. Unlike previous approaches that use LLMs for generating taskPDDLs directly, our approach comprises of (a) translate: using an LLM only forgenerating a logically interpretable intermediate representation of naturallanguage task description, (b) infer: deriving additional logically dependentinformation from the intermediate representation using a logic reasoner(currently, Answer Set Programming solver), and (c) compile: generating thetarget task PDDL from the base and inferred information. We observe that usingan LLM to only output the intermediate representation significantly reduces LLMerrors. Consequently, TIC approach achieves, for at least one LLM, highaccuracy on task PDDL generation for all seven domains of our evaluationdataset.",Sudhir Agarwal,2024/2/9,2024/6/29,,,"['cs.CL', 'cs.AI']"
2402.03663v1,Symbol Correctness in Deep Neural Networks Containing Symbolic Layers,http://arxiv.org/abs/2402.03663v1,"To handle AI tasks that combine perception and logical reasoning, recent workintroduces Neurosymbolic Deep Neural Networks (NS-DNNs), which contain -- inaddition to traditional neural layers -- symbolic layers: symbolic expressions(e.g., SAT formulas, logic programs) that are evaluated by symbolic solversduring inference. We identify and formalize an intuitive, high-level principlethat can guide the design and analysis of NS-DNNs: symbol correctness, thecorrectness of the intermediate symbols predicted by the neural layers withrespect to a (generally unknown) ground-truth symbolic representation of theinput data. We demonstrate that symbol correctness is a necessary property forNS-DNN explainability and transfer learning (despite being in generalimpossible to train for). Moreover, we show that the framework of symbolcorrectness provides a precise way to reason and communicate about modelbehavior at neural-symbolic boundaries, and gives insight into the fundamentaltradeoffs faced by NS-DNN training algorithms. In doing so, we both identifysignificant points of ambiguity in prior work, and provide a framework tosupport further NS-DNN developments.",Aaron Bembenek,2024/2/6,2024/2/6,,,"['cs.LG', 'cs.AI']"
2402.03268v3,Understanding Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation,http://arxiv.org/abs/2402.03268v3,"Pre-trained language models (LMs) are able to perform complex reasoningwithout explicit fine-tuning. To understand how pre-training with a next-tokenprediction objective contributes to the emergence of such reasoning capability,we propose that we can view an LM as deriving new conclusions by aggregatingindirect reasoning paths seen at pre-training time. We found this perspectiveeffective in two important cases of reasoning: logic reasoning with knowledgegraphs (KGs) and chain-of-thought (CoT) reasoning. More specifically, weformalize the reasoning paths as random walk paths on the knowledge/reasoninggraphs. Analyses of learned LM distributions suggest that a weighted sum ofrelevant random walk path probabilities is a reasonable way to explain how LMsreason. Experiments and analysis on multiple KG and CoT datasets reveal theeffect of training on random walk paths and suggest that augmenting unlabeledrandom walk reasoning paths can improve real-world multi-step reasoningperformance. code: https://github.com/WANGXinyiLinda/LM_random_walk",Xinyi Wang,2024/2/5,2024/6/20,,,"['cs.LG', 'cs.AI', 'cs.CL']"
2401.11800v1,Revisiting Document-Level Relation Extraction with Context-Guided Link Prediction,http://arxiv.org/abs/2401.11800v1,"Document-level relation extraction (DocRE) poses the challenge of identifyingrelationships between entities within a document as opposed to the traditionalRE setting where a single sentence is input. Existing approaches rely onlogical reasoning or contextual cues from entities. This paper reframesdocument-level RE as link prediction over a knowledge graph with distinctbenefits: 1) Our approach combines entity context with document-derived logicalreasoning, enhancing link prediction quality. 2) Predicted links betweenentities offer interpretability, elucidating employed reasoning. We evaluateour approach on three benchmark datasets: DocRED, ReDocRED, and DWIE. Theresults indicate that our proposed method outperforms the state-of-the-artmodels and suggests that incorporating context-based link prediction techniquescan enhance the performance of document-level relation extraction models.",Monika Jain,2024/1/22,2024/1/22,,,['cs.IR']
2401.10695v2,LangBridge: Multilingual Reasoning Without Multilingual Supervision,http://arxiv.org/abs/2401.10695v2,"We introduce LangBridge, a zero-shot approach to adapt language models formultilingual reasoning tasks without multilingual supervision. LangBridgeoperates by bridging two models, each specialized in different aspects: (1) onespecialized in understanding multiple languages (e.g., mT5 encoder) and (2) onespecialized in reasoning (e.g., MetaMath). LangBridge connects the two modelsby introducing minimal trainable parameters between them. Despite utilizingonly English data for training, LangBridge considerably enhances theperformance of language models on low-resource languages across mathematicalreasoning, code completion, logical reasoning, and commonsense reasoning. Ouranalysis suggests that the efficacy of LangBridge stems from thelanguage-agnostic characteristics of multilingual representations. We publiclyrelease our code and models.",Dongkeun Yoon,2024/1/19,2024/6/3,,,['cs.CL']
2401.07448v2,Formal Logic Enabled Personalized Federated Learning Through Property Inference,http://arxiv.org/abs/2401.07448v2,"Recent advancements in federated learning (FL) have greatly facilitated thedevelopment of decentralized collaborative applications, particularly in thedomain of Artificial Intelligence of Things (AIoT). However, a critical aspectmissing from the current research landscape is the ability to enabledata-driven client models with symbolic reasoning capabilities. Specifically,the inherent heterogeneity of participating client devices poses a significantchallenge, as each client exhibits unique logic reasoning properties. Failingto consider these device-specific specifications can result in criticalproperties being missed in the client predictions, leading to suboptimalperformance. In this work, we propose a new training paradigm that leveragestemporal logic reasoning to address this issue. Our approach involves enhancingthe training process by incorporating mechanically generated logic expressionsfor each FL client. Additionally, we introduce the concept of aggregationclusters and develop a partitioning algorithm to effectively group clientsbased on the alignment of their temporal reasoning properties. We evaluate theproposed method on two tasks: a real-world traffic volume prediction taskconsisting of sensory data from fifteen states and a smart city multi-taskprediction utilizing synthetic data. The evaluation results exhibit clearimprovements, with performance accuracy improved by up to 54% across allsequential prediction models.",Ziyan An,2024/1/15,2024/1/24,,,"['cs.AI', 'cs.LG']"
2312.16012v1,Detection-based Intermediate Supervision for Visual Question Answering,http://arxiv.org/abs/2312.16012v1,"Recently, neural module networks (NMNs) have yielded ongoing success inanswering compositional visual questions, especially those involving multi-hopvisual and logical reasoning. NMNs decompose the complex question into severalsub-tasks using instance-modules from the reasoning paths of that question andthen exploit intermediate supervisions to guide answer prediction, therebyimproving inference interpretability. However, their performance may behindered due to sketchy modeling of intermediate supervisions. For instance,(1) a prior assumption that each instance-module refers to only one groundedobject yet overlooks other potentially associated grounded objects, impedingfull cross-modal alignment learning; (2) IoU-based intermediate supervisionsmay introduce noise signals as the bounding box overlap issue might guide themodel's focus towards irrelevant objects. To address these issues, a novelmethod, \textbf{\underline{D}}etection-based \textbf{\underline{I}}ntermediate\textbf{\underline{S}}upervision (DIS), is proposed, which adopts a generativedetection framework to facilitate multiple grounding supervisions via sequencegeneration. As such, DIS offers more comprehensive and accurate intermediatesupervisions, thereby boosting answer prediction performance. Furthermore, byconsidering intermediate results, DIS enhances the consistency in answeringcompositional questions and their sub-questions.Extensive experimentsdemonstrate the superiority of our proposed DIS, showcasing both improvedaccuracy and state-of-the-art reasoning consistency compared to priorapproaches.",Yuhang Liu,2023/12/26,2023/12/26,,,"['cs.CV', 'cs.AI']"
2312.15816v2,TEILP: Time Prediction over Knowledge Graphs via Logical Reasoning,http://arxiv.org/abs/2312.15816v2,"Conventional embedding-based models approach event time prediction intemporal knowledge graphs (TKGs) as a ranking problem. However, they often fallshort in capturing essential temporal relationships such as order and distance.In this paper, we propose TEILP, a logical reasoning framework that naturallyintegrates such temporal elements into knowledge graph predictions. We firstconvert TKGs into a temporal event knowledge graph (TEKG) which has a moreexplicit representation of time in term of nodes of the graph. The TEKG equipsus to develop a differentiable random walk approach to time prediction.Finally, we introduce conditional probability density functions, associatedwith the logical rules involving the query interval, using which we arrive atthe time prediction. We compare TEILP with state-of-the-art methods on fivebenchmark datasets. We show that our model achieves a significant improvementover baselines while providing interpretable explanations. In particular, weconsider several scenarios where training samples are limited, event types areimbalanced, and forecasting the time of future events based on only past eventsis desired. In all these cases, TEILP outperforms state-of-the-art methods interms of robustness.",Siheng Xiong,2023/12/25,2024/1/29,,,['cs.CL']
2312.15643v3,Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation,http://arxiv.org/abs/2312.15643v3,"Abductive reasoning is the process of making educated guesses to provideexplanations for observations. Although many applications require the use ofknowledge for explanations, the utilization of abductive reasoning inconjunction with structured knowledge, such as a knowledge graph, remainslargely unexplored. To fill this gap, this paper introduces the task of complexlogical hypothesis generation, as an initial step towards abductive logicalreasoning with KG. In this task, we aim to generate a complex logicalhypothesis so that it can explain a set of observations. We find that thesupervised trained generative model can generate logical hypotheses that arestructurally closer to the reference hypothesis. However, when generalized tounseen observations, this training objective does not guarantee betterhypothesis generation. To address this, we introduce the Reinforcement Learningfrom Knowledge Graph (RLF-KG) method, which minimizes differences betweenobservations and conclusions drawn from generated hypotheses according to theKG. Experiments show that, with RLF-KG's assistance, the generated hypothesesprovide better explanations, and achieve state-of-the-art results on threewidely used KGs.",Jiaxin Bai,2023/12/25,2024/6/20,,,"['cs.AI', 'cs.CL']"
2312.13866v2,Understanding Inter-Session Intentions via Complex Logical Reasoning,http://arxiv.org/abs/2312.13866v2,"Understanding user intentions is essential for improving productrecommendations, navigation suggestions, and query reformulations. However,user intentions can be intricate, involving multiple sessions and attributerequirements connected by logical operators such as And, Or, and Not. Forinstance, a user may search for Nike or Adidas running shoes across varioussessions, with a preference for purple. In another example, a user may havepurchased a mattress in a previous session and is now looking for a matchingbed frame without intending to buy another mattress. Existing research onsession understanding has not adequately addressed making product or attributerecommendations for such complex intentions. In this paper, we present the taskof logical session complex query answering (LS-CQA), where sessions are treatedas hyperedges of items, and we frame the problem of complex intentionunderstanding as an LS-CQA task on an aggregated hypergraph of sessions, items,and attributes. This is a unique complex query answering task with sessions asordered hyperedges. We also introduce a new model, the Logical Session GraphTransformer (LSGT), which captures interactions among items across differentsessions and their logical connections using a transformer structure. Weanalyze the expressiveness of LSGT and prove the permutation invariance of theinputs for the logical operators. By evaluating LSGT on three datasets, wedemonstrate that it achieves state-of-the-art results.",Jiaxin Bai,2023/12/21,2024/6/14,,,"['cs.AI', 'cs.CL']"
2312.11720v2,Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models,http://arxiv.org/abs/2312.11720v2,"Logical reasoning is central to complex human activities, such as thinking,debating, and planning; it is also a central component of many AI systems aswell. In this paper, we investigate the extent to which encoder-onlytransformer language models (LMs) can reason according to logical rules. We askwhether those LMs can deduce theorems in propositional calculus and first-orderlogic; if their relative success in these problems reflects general logicalcapabilities; and which layers contribute the most to the task. First, we showfor several encoder-only LMs that they can be trained, to a reasonable degree,to determine logical validity on various datasets. Next, by cross-probingfine-tuned models on these datasets, we show that LMs have difficulty intransferring their putative logical reasoning ability, which suggests that theymay have learned dataset-specific features, instead of a general capability.Finally, we conduct a layerwise probing experiment, which shows that thehypothesis classification task is mostly solved through higher layers.",Paulo Pirozelli,2023/12/18,2024/7/1,,,"['cs.CL', 'cs.AI']"
2312.11522v1,Assessing SATNet's Ability to Solve the Symbol Grounding Problem,http://arxiv.org/abs/2312.11522v1,"SATNet is an award-winning MAXSAT solver that can be used to infer logicalrules and integrated as a differentiable layer in a deep neural network. It hadbeen shown to solve Sudoku puzzles visually from examples of puzzle digitimages, and was heralded as an impressive achievement towards the longstandingAI goal of combining pattern recognition with logical reasoning. In this paper,we clarify SATNet's capabilities by showing that in the absence of intermediatelabels that identify individual Sudoku digit images with their logicalrepresentations, SATNet completely fails at visual Sudoku (0% test accuracy).More generally, the failure can be pinpointed to its inability to learn toassign symbols to perceptual phenomena, also known as the symbol groundingproblem, which has long been thought to be a prerequisite for intelligentagents to perform real-world logical reasoning. We propose an MNIST based testas an easy instance of the symbol grounding problem that can serve as a sanitycheck for differentiable symbolic solvers in general. Naive applications ofSATNet on this test lead to performance worse than that of models withoutlogical reasoning capabilities. We report on the causes of SATNet's failure andhow to prevent them.",Oscar Chang,2023/12/13,2023/12/13,,,"['cs.AI', 'cs.LG']"
2312.07840v1,A Physics Lab Inside Your Head: Quantum Thought Experiments as an Educational Tool,http://arxiv.org/abs/2312.07840v1,"Thought experiments are where logical reasoning meets storytelling,catalysing progress in quantum science and technology. Schr\""odinger's famouscat brought quantum science to the public consciousness, while Deutsch'sthought experiment to test the many-worlds and Copenhagen interpretationsinvolved the first conception of a quantum computer. I will show how presentingthought experiments using quantum circuits can demystify apparent quantumparadoxes, and provide fun, conceptually important activities for learners toimplement themselves on near-term quantum devices. Additionally, I will explainhow thought experiments can be used as a first introduction to quantum, andoutline a workshop based on the ""quantum bomb tester"" for school students asyoung as 11. This paper draws upon my experience in developing and deliveringquantum computing workshops in Oxford, and in creating a quantum paradoxescontent series with IBM Quantum of videos, blogs and code tutorials.",Maria Violaris,2023/12/13,2023/12/13,,,"['physics.ed-ph', 'quant-ph']"
2311.13455v1,Generation of Explanations for Logic Reasoning,http://arxiv.org/abs/2311.13455v1,"This thesis delves into a fortiori arguments in deductive reasoning,underscoring their relevance in various domains such as law, philosophy, andartificial intelligence. The research is centred on employing GPT-3.5-turbo toautomate the analysis of these arguments, with a focus on understandingintricate reasoning processes, generating clear and coherent explanations, andcreating novel arguments. The methodology encompasses a series of tasksincluding detailed reasoning, interpretation, and the augmentation of afortiori arguments. It involves meticulously identifying these arguments indiverse contexts, differentiating comparative elements, and categorizing thembased on their logical structure.  Extensive experiments reveals the challenges encountered by GPT-3.5-turbo inaccurately detecting and classifying a fortiori arguments. Nevertheless, themodel demonstrates a performance that rivals specialized models, particularlyin extracting key components and interpreting underlying properties. Theintegration of external information into the model's processing significantlyelevates the quality of the generated explanations. Additionally, the modelexhibits a noteworthy capability in augmenting arguments, thus contributing tothe enrichment of the data set.  Despite facing certain limitations, this thesis makes significantcontributions to the fields of artificial intelligence and logical reasoning.It introduces novel methodologies, establishes a rigorous evaluation framework,and provides deep insights that set the stage for future advancements inautomated logical reasoning. The findings and methodologies presented hereinnot only underscore the potential of AI in complex reasoning tasks but alsohighlight areas for future research and development.",Yanyi Pu,2023/11/22,2023/11/22,,,"['cs.AI', 'cs.CL']"
2311.12890v3,De-fine: Decomposing and Refining Visual Programs with Auto-Feedback,http://arxiv.org/abs/2311.12890v3,"Visual programming, a modular and generalizable paradigm, integratesdifferent modules and Python operators to solve various vision-language tasks.Unlike end-to-end models that need task-specific data, it advances inperforming visual processing and reasoning in an unsupervised manner. Currentvisual programming methods generate programs in a single pass for each taskwhere the ability to evaluate and optimize based on feedback, unfortunately, islacking, which consequentially limits their effectiveness for complex,multi-step problems. Drawing inspiration from benders decomposition, weintroduce De-fine, a training-free framework that automatically decomposescomplex tasks into simpler subtasks and refines programs through auto-feedback.This model-agnostic approach can improve logical reasoning performance byintegrating the strengths of multiple models. Our experiments across variousvisual tasks show that De-fine creates more robust programs. Moreover, viewingeach feedback module as an independent agent will yield fresh prospects for thefield of agent research.",Minghe Gao,2023/11/21,2024/8/5,,,['cs.CV']
2311.09817v1,Neural-Logic Human-Object Interaction Detection,http://arxiv.org/abs/2311.09817v1,"The interaction decoder utilized in prevalent Transformer-based HOI detectorstypically accepts pre-composed human-object pairs as inputs. Though achievingremarkable performance, such paradigm lacks feasibility and cannot explorenovel combinations over entities during decoding. We present L OGIC HOI, a newHOI detector that leverages neural-logic reasoning and Transformer to inferfeasible interactions between entities. Specifically, we modify theself-attention mechanism in vanilla Transformer, enabling it to reason over the<human, action, object> triplet and constitute novel interactions. Meanwhile,such reasoning process is guided by two crucial properties for understandingHOI: affordances (the potential actions an object can facilitate) and proxemics(the spatial relations between humans and objects). We formulate these twoproperties in first-order logic and ground them into continuous space toconstrain the learning process of our approach, leading to improved performanceand zero-shot generalization capabilities. We evaluate L OGIC HOI on V-COCO andHICO-DET under both normal and zero-shot setups, achieving significantimprovements over existing methods.",Liulei Li,2023/11/16,2023/11/16,,,['cs.CV']
2311.06754v1,From Complex to Simple: Unraveling the Cognitive Tree for Reasoning with Small Language Models,http://arxiv.org/abs/2311.06754v1,"Reasoning is a distinctive human capacity, enabling us to address complexproblems by breaking them down into a series of manageable cognitive steps.Yet, complex logical reasoning is still cumbersome for language models. Basedon the dual process theory in cognitive science, we are the first to unravelthe cognitive reasoning abilities of language models. Our framework employs aniterative methodology to construct a Cognitive Tree (CogTree). The root node ofthis tree represents the initial query, while the leaf nodes consist ofstraightforward questions that can be answered directly. This constructioninvolves two main components: the implicit extraction module (referred to asthe intuitive system) and the explicit reasoning module (referred to as thereflective system). The intuitive system rapidly generates multiple responsesby utilizing in-context examples, while the reflective system scores theseresponses using comparative learning. The scores guide the intuitive system inits subsequent generation step. Our experimental results on two popular andchallenging reasoning tasks indicate that it is possible to achieve aperformance level comparable to that of GPT-3.5 (with 175B parameters), using asignificantly smaller language model that contains fewer parameters (<=7B) than5% of GPT-3.5.",Junbing Yan,2023/11/12,2023/11/12,,,['cs.CL']
2311.05821v1,Let's Reinforce Step by Step,http://arxiv.org/abs/2311.05821v1,"While recent advances have boosted LM proficiency in linguistic benchmarks,LMs consistently struggle to reason correctly on complex tasks likemathematics. We turn to Reinforcement Learning from Human Feedback (RLHF) as amethod with which to shape model reasoning processes. In particular, we exploretwo reward schemes, outcome-supervised reward models (ORMs) andprocess-supervised reward models (PRMs), to optimize for logical reasoning. Ourresults show that the fine-grained reward provided by PRM-based methodsenhances accuracy on simple mathematical reasoning (GSM8K) while, unexpectedly,reducing performance in complex tasks (MATH). Furthermore, we show the criticalrole reward aggregation functions play in model performance. Providingpromising avenues for future research, our study underscores the need forfurther exploration into fine-grained reward modeling for more reliablelanguage models.",Sarah Pan,2023/11/10,2023/11/10,,,['cs.CL']
2311.03753v1,COOL: A Constraint Object-Oriented Logic Programming Language and its Neural-Symbolic Compilation System,http://arxiv.org/abs/2311.03753v1,"This paper explores the integration of neural networks with logicprogramming, addressing the longstanding challenges of combining thegeneralization and learning capabilities of neural networks with the precisionof symbolic logic. Traditional attempts at this integration have been hamperedby difficulties in initial data acquisition, the reliability of undertrainednetworks, and the complexity of reusing and augmenting trained models. Toovercome these issues, we introduce the COOL (Constraint Object-Oriented Logic)programming language, an innovative approach that seamlessly combines logicalreasoning with neural network technologies. COOL is engineered to autonomouslyhandle data collection, mitigating the need for user-supplied initial data. Itincorporates user prompts into the coding process to reduce the risks ofundertraining and enhances the interaction among models throughout theirlifecycle to promote the reuse and augmentation of networks. Furthermore, thefoundational principles and algorithms in COOL's design and its compilationsystem could provide valuable insights for future developments in programminglanguages and neural network architectures.",Jipeng Han,2023/11/7,2023/11/7,,,"['cs.AI', 'cs.CL', 'cs.DC', 'cs.FL', 'cs.HC']"
2311.03094v1,Equivariance Is Not All You Need: Characterizing the Utility of Equivariant Graph Neural Networks for Particle Physics Tasks,http://arxiv.org/abs/2311.03094v1,"Incorporating inductive biases into ML models is an active area of MLresearch, especially when ML models are applied to data about the physicalworld. Equivariant Graph Neural Networks (GNNs) have recently become a popularmethod for learning from physics data because they directly incorporate thesymmetries of the underlying physical system. Drawing from the relevantliterature around group equivariant networks, this paper presents acomprehensive evaluation of the proposed benefits of equivariant GNNs by usingreal-world particle physics reconstruction tasks as an evaluation test-bed. Wedemonstrate that many of the theoretical benefits generally associated withequivariant networks may not hold for realistic systems and introducecompelling directions for future research that will benefit both the scientifictheory of ML and physics applications.",Savannah Thais,2023/11/6,2023/11/6,,,"['cs.LG', 'hep-ex']"
2311.02765v1,Rule Learning as Machine Translation using the Atomic Knowledge Bank,http://arxiv.org/abs/2311.02765v1,"Machine learning models, and in particular language models, are being appliedto various tasks that require reasoning. While such models are good atcapturing patterns their ability to reason in a trustable and controlled manneris frequently questioned. On the other hand, logic-based rule systems allow forcontrolled inspection and already established verification methods. However itis well-known that creating such systems manually is time-consuming and proneto errors. We explore the capability of transformers to translate sentencesexpressing rules in natural language into logical rules. We see reasoners asthe most reliable tools for performing logical reasoning and focus ontranslating language into the format expected by such tools. We performexperiments using the DKET dataset from the literature and create a dataset forlanguage to logic translation based on the Atomic knowledge bank.",Kristoffer sy,2023/11/5,2023/11/5,,,"['cs.CL', 'cs.AI']"
2310.18224v1,Borhan: A Novel System for Prioritized Default Logic,http://arxiv.org/abs/2310.18224v1,"Prioritized Default Logic presents an optimal solution for addressingreal-world problems characterized by incomplete information and the need toestablish preferences among diverse scenarios. Although it has reached greatsuccess in the theoretical aspect, its practical implementation has receivedless attention. In this article, we introduce Borhan, a system designed andcreated for prioritized default logic reasoning. To create an effective system,we have refined existing default logic definitions, including the extensionconcept, and introduced novel concepts. In addition to its theoretical merits,Borhan proves its practical utility by efficiently addressing a range ofprioritized default logic problems. In addition, one of the advantages of oursystem is its ability to both store and report the explanation path for anyinferred triple, enhancing transparency and interpretability. Borhan is offeredas an open-source system, implemented in Python, and even offers a simplifiedJava version as a plugin for the Protege ontology editor. Borhan thusrepresents a significant step forward in bridging the gap between thetheoretical foundations of default logic and its real-world applications.",Alireza Shahbazi,2023/10/27,2023/10/27,,,['cs.LO']
2310.15258v1,Breaking the Language Barrier: Improving Cross-Lingual Reasoning with Structured Self-Attention,http://arxiv.org/abs/2310.15258v1,"In this work, we study whether multilingual language models (MultiLMs) cantransfer logical reasoning abilities to other languages when they arefine-tuned for reasoning in a different language. We evaluate the cross-lingualreasoning abilities of MultiLMs in two schemes: (1) where the language of thecontext and the question remain the same in the new languages that are tested(i.e., the reasoning is still monolingual, but the model must transfer thelearned reasoning ability across languages), and (2) where the language of thecontext and the question is different (which we term code-switched reasoning).On two logical reasoning datasets, RuleTaker and LeapOfThought, we demonstratethat although MultiLMs can transfer reasoning ability across languages in amonolingual setting, they struggle to transfer reasoning abilities in acode-switched setting. Following this observation, we propose a novel attentionmechanism that uses a dedicated set of parameters to encourage cross-lingualattention in code-switched sequences, which improves the reasoning performanceby up to 14% and 4% on the RuleTaker and LeapOfThought datasets, respectively.",Negar Foroutan,2023/10/23,2023/10/23,,,['cs.CL']
2310.13566v1,Retrieval-Augmented Neural Response Generation Using Logical Reasoning and Relevance Scoring,http://arxiv.org/abs/2310.13566v1,"Constructing responses in task-oriented dialogue systems typically relies oninformation sources such the current dialogue state or external databases. Thispaper presents a novel approach to knowledge-grounded response generation thatcombines retrieval-augmented language models with logical reasoning. Theapproach revolves around a knowledge graph representing the current dialoguestate and background information, and proceeds in three steps. The knowledgegraph is first enriched with logically derived facts inferred usingprobabilistic logical programming. A neural model is then employed at each turnto score the conversational relevance of each node and edge of this extendedgraph. Finally, the elements with highest relevance scores are converted to anatural language form, and are integrated into the prompt for the neuralconversational model employed to generate the system response.  We investigate the benefits of the proposed approach on two datasets (KVRETand GraphWOZ) along with a human evaluation. Experimental results show that thecombination of (probabilistic) logical reasoning with conversational relevancescoring does increase both the factuality and fluency of the responses.",Nicholas Thomas Walker,2023/10/20,2023/10/20,,,"['cs.CL', 'cs.AI']"
2310.03686v2,DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers,http://arxiv.org/abs/2310.03686v2,"In recent years, many interpretability methods have been proposed to helpinterpret the internal states of Transformer-models, at different levels ofprecision and complexity. Here, to analyze encoder-decoder Transformers, wepropose a simple, new method: DecoderLens. Inspired by the LogitLens (fordecoder-only Transformers), this method involves allowing the decoder tocross-attend representations of intermediate encoder layers instead of usingthe final encoder output, as is normally done in encoder-decoder models. Themethod thus maps previously uninterpretable vector representations tohuman-interpretable sequences of words or symbols. We report results from theDecoderLens applied to models trained on question answering, logical reasoning,speech recognition and machine translation. The DecoderLens reveals severalspecific subtasks that are solved at low or intermediate layers, shedding newlight on the information flow inside the encoder component of this importantclass of models.",Anna Langedijk,2023/10/5,2024/4/3,,,['cs.CL']
2310.02133v1,Learning Reliable Logical Rules with SATNet,http://arxiv.org/abs/2310.02133v1,"Bridging logical reasoning and deep learning is crucial for advanced AIsystems. In this work, we present a new framework that addresses this goal bygenerating interpretable and verifiable logical rules through differentiablelearning, without relying on pre-specified logical structures. Our approachbuilds upon SATNet, a differentiable MaxSAT solver that learns the underlyingrules from input-output examples. Despite its efficacy, the learned weights inSATNet are not straightforwardly interpretable, failing to producehuman-readable rules. To address this, we propose a novel specification methodcalled ""maximum equality"", which enables the interchangeability between thelearned weights of SATNet and a set of propositional logical rules in weightedMaxSAT form. With the decoded weighted MaxSAT formula, we further introduceseveral effective verification techniques to validate it against the groundtruth rules. Experiments on stream transformations and Sudoku problems showthat our decoded rules are highly reliable: using exact solvers on them couldachieve 100% accuracy, whereas the original SATNet fails to give correctsolutions in many cases. Furthermore, we formally verify that our decodedlogical rules are functionally equivalent to the ground truth ones.",Zhaoyu Li,2023/10/3,2023/10/3,,,"['cs.AI', 'cs.LG']"
2309.13556v2,LOGICSEG: Parsing Visual Semantics with Neural Logic Learning and Reasoning,http://arxiv.org/abs/2309.13556v2,"Current high-performance semantic segmentation models are purely data-drivensub-symbolic approaches and blind to the structured nature of the visual world.This is in stark contrast to human cognition which abstracts visual perceptionsat multiple levels and conducts symbolic reasoning with such structuredabstraction. To fill these fundamental gaps, we devise LOGICSEG, a holisticvisual semantic parser that integrates neural inductive learning and logicreasoning with both rich data and symbolic knowledge. In particular, thesemantic concepts of interest are structured as a hierarchy, from which a setof constraints are derived for describing the symbolic relations and formalizedas first-order logic rules. After fuzzy logic-based continuous relaxation,logical formulae are grounded onto data and neural computational graphs, henceenabling logic-induced network training. During inference, logical constraintsare packaged into an iterative process and injected into the network in a formof several matrix multiplications, so as to achieve hierarchy-coherentprediction with logic reasoning. These designs together make LOGICSEG a generaland compact neural-logic machine that is readily integrated into existingsegmentation models. Extensive experiments over four datasets with varioussegmentation models and backbones verify the effectiveness and generality ofLOGICSEG. We believe this study opens a new avenue for visual semantic parsing.",Liulei Li,2023/9/24,2023/9/28,,,['cs.CV']
2309.13172v1,Walking-by-Logic: Signal Temporal Logic-Guided Model Predictive Control for Bipedal Locomotion Resilient to External Perturbations,http://arxiv.org/abs/2309.13172v1,"This study proposes a novel planning framework based on a model predictivecontrol formulation that incorporates signal temporal logic (STL)specifications for task completion guarantees and robustness quantification.This marks the first-ever study to apply STL-guided trajectory optimization forbipedal locomotion push recovery, where the robot experiences unexpecteddisturbances. Existing recovery strategies often struggle with complex tasklogic reasoning and locomotion robustness evaluation, making them susceptibleto failures caused by inappropriate recovery strategies or insufficientrobustness. To address this issue, the STL-guided framework generates optimaland safe recovery trajectories that simultaneously satisfy the taskspecification and maximize the locomotion robustness. Our framework outperformsa state-of-the-art locomotion controller in a high-fidelity dynamic simulation,especially in scenarios involving crossed-leg maneuvers. Furthermore, itdemonstrates versatility in tasks such as locomotion on stepping stones, wherethe robot must select from a set of disjointed footholds to maneuversuccessfully.",Zhaoyuan Gu,2023/9/22,2023/9/22,,,['cs.RO']
2309.05936v1,Do PLMs Know and Understand Ontological Knowledge?,http://arxiv.org/abs/2309.05936v1,"Ontological knowledge, which comprises classes and properties and theirrelationships, is integral to world knowledge. It is significant to explorewhether Pretrained Language Models (PLMs) know and understand such knowledge.However, existing PLM-probing studies focus mainly on factual knowledge,lacking a systematic probing of ontological knowledge. In this paper, we focuson probing whether PLMs store ontological knowledge and have a semanticunderstanding of the knowledge rather than rote memorization of the surfaceform. To probe whether PLMs know ontological knowledge, we investigate how wellPLMs memorize: (1) types of entities; (2) hierarchical relationships amongclasses and properties, e.g., Person is a subclass of Animal and Member ofSports Team is a subproperty of Member of ; (3) domain and range constraints ofproperties, e.g., the subject of Member of Sports Team should be a Person andthe object should be a Sports Team. To further probe whether PLMs trulyunderstand ontological knowledge beyond memorization, we comprehensively studywhether they can reliably perform logical reasoning with given knowledgeaccording to ontological entailment rules. Our probing results show that PLMscan memorize certain ontological knowledge and utilize implicit knowledge inreasoning. However, both the memorizing and reasoning performances are lessthan perfect, indicating incomplete knowledge and understanding.",Weiqi Wu,2023/9/12,2023/9/12,,,['cs.CL']
2308.15887v1,On the Potential of CLIP for Compositional Logical Reasoning,http://arxiv.org/abs/2308.15887v1,"In this paper we explore the possibility of using OpenAI's CLIP to performlogically coherent grounded visual reasoning. To that end, we formalize ourterms and give a geometric analysis of how embeddings in CLIP's latent spacewould need to be configured in order for the system to be logically coherent.Our main conclusion is that, as usually configured, CLIP cannot perform suchreasoning.",Justin Brody,2023/8/30,2023/8/30,,,"['cs.AI', 'cs.CV', 'cs.LG', 'cs.LO']"
2308.12740v2,Human Comprehensible Active Learning of Genome-Scale Metabolic Networks,http://arxiv.org/abs/2308.12740v2,"An important application of Synthetic Biology is the engineering of the hostcell system to yield useful products. However, an increase in the scale of thehost system leads to huge design space and requires a large number ofvalidation trials with high experimental costs. A comprehensible machinelearning approach that efficiently explores the hypothesis space and guidesexperimental design is urgently needed for the Design-Build-Test-Learn (DBTL)cycle of the host cell system. We introduce a novel machine learning frameworkILP-iML1515 based on Inductive Logic Programming (ILP) that performs abductivelogical reasoning and actively learns from training examples. In contrast tonumerical models, ILP-iML1515 is built on comprehensible logicalrepresentations of a genome-scale metabolic model and can update the model bylearning new logical structures from auxotrophic mutant trials. The ILP-iML1515framework 1) allows high-throughput simulations and 2) actively selectsexperiments that reduce the experimental cost of learning gene functions incomparison to randomly selected experiments.",Lun Ai,2023/8/24,2023/8/31,,,"['cs.AI', 'cs.LG', 'q-bio.MN', 'q-bio.QM']"
2308.07294v2,Why Not? Explaining Missing Entailments with Evee (Technical Report),http://arxiv.org/abs/2308.07294v2,"Understanding logical entailments derived by a description logic reasoner isnot always straight-forward for ontology users. For this reason, variousmethods for explaining entailments using justifications and proofs have beendeveloped and implemented as plug-ins for the ontology editor Prot\'eg\'e.However, when the user expects a missing consequence to hold, it is equallyimportant to explain why it does not follow from the ontology. In this paper,we describe a new version of $\rm E{\scriptsize VEE}$, a Prot\'eg\'e pluginthat now also provides explanations for missing consequences, via existing andnew techniques based on abduction and counterexamples.",Christian Alrabbaa,2023/8/14,2023/8/15,,,"['cs.AI', 'cs.LO']"
2308.06207v1,Thinking Like an Expert:Multimodal Hypergraph-of-Thought (HoT) Reasoning to boost Foundation Modals,http://arxiv.org/abs/2308.06207v1,"Reasoning ability is one of the most crucial capabilities of a foundationmodel, signifying its capacity to address complex reasoning tasks.Chain-of-Thought (CoT) technique is widely regarded as one of the effectivemethods for enhancing the reasoning ability of foundation models and hasgarnered significant attention. However, the reasoning process of CoT islinear, step-by-step, similar to personal logical reasoning, suitable forsolving general and slightly complicated problems. On the contrary, thethinking pattern of an expert owns two prominent characteristics that cannot behandled appropriately in CoT, i.e., high-order multi-hop reasoning andmultimodal comparative judgement. Therefore, the core motivation of this paperis transcending CoT to construct a reasoning paradigm that can think like anexpert. The hyperedge of a hypergraph could connect various vertices, making itnaturally suitable for modelling high-order relationships. Inspired by this,this paper innovatively proposes a multimodal Hypergraph-of-Thought (HoT)reasoning paradigm, which enables the foundation models to possess theexpert-level ability of high-order multi-hop reasoning and multimodalcomparative judgement. Specifically, a textual hypergraph-of-thought isconstructed utilizing triple as the primary thought to model higher-orderrelationships, and a hyperedge-of-thought is generated through multi-hopwalking paths to achieve multi-hop inference. Furthermore, we devise a visualhypergraph-of-thought to interact with the textual hypergraph-of-thought viaCross-modal Co-Attention Graph Learning for multimodal comparativeverification. Experimentations on the ScienceQA benchmark demonstrate theproposed HoT-based T5 outperforms CoT-based GPT3.5 and chatGPT, which is on parwith CoT-based GPT4 with a lower model size.",Fanglong Yao,2023/8/11,2023/8/11,,,['cs.CL']
2308.04814v1,Neuro-Symbolic RDF and Description Logic Reasoners: The State-Of-The-Art and Challenges,http://arxiv.org/abs/2308.04814v1,"Ontologies are used in various domains, with RDF and OWL being prominentstandards for ontology development. RDF is favored for its simplicity andflexibility, while OWL enables detailed domain knowledge representation.However, as ontologies grow larger and more expressive, reasoning complexityincreases, and traditional reasoners struggle to perform efficiently. Despiteoptimization efforts, scalability remains an issue. Additionally, advancementsin automated knowledge base construction have created large and expressiveontologies that are often noisy and inconsistent, posing further challenges forconventional reasoners. To address these challenges, researchers have exploredneuro-symbolic approaches that combine neural networks' learning capabilitieswith symbolic systems' reasoning abilities. In this chapter,we provide anoverview of the existing literature in the field of neuro-symbolic deductivereasoning supported by RDF(S), the description logics EL and ALC, and OWL 2 RL,discussing the techniques employed, the tasks they address, and other relevantefforts in this area.",Gunjan Singh,2023/8/9,2023/8/9,,,['cs.AI']
2307.12704v1,A system of inference based on proof search: an extended abstract,http://arxiv.org/abs/2307.12704v1,"Gentzen designed his natural deduction proof system to ``come as close aspossible to actual reasoning.'' Indeed, natural deduction proofs closelyresemble the static structure of logical reasoning in mathematical arguments.However, different features of inference are compelling to capture when onewants to support the process of searching for proofs. PSF (Proof SearchFramework) attempts to capture these features naturally and directly. Thedesign and metatheory of PSF are presented, and its ability to specify a rangeof proof systems for classical, intuitionistic, and linear logic isillustrated.",Dale Miller,2023/7/24,2023/7/24,,,['cs.LO']
2307.10573v2,"Invalid Logic, Equivalent Gains: The Bizarreness of Reasoning in Language Model Prompting",http://arxiv.org/abs/2307.10573v2,"Language models can be prompted to reason through problems in a manner thatsignificantly improves performance. However, \textit{why} such promptingimproves performance is unclear. Recent work showed that using logically\textit{invalid} Chain-of-Thought (CoT) prompting improves performance almostas much as logically \textit{valid} CoT prompting, and that editing CoT promptsto replace problem-specific information with abstract information orout-of-distribution information typically doesn't harm performance. Criticshave responded that these findings are based on too few and too easily solvedtasks to draw meaningful conclusions. To resolve this dispute, we test whetherlogically invalid CoT prompts offer the same level of performance gains aslogically valid prompts on the hardest tasks in the BIG-Bench benchmark, termedBIG-Bench Hard (BBH). We find that the logically \textit{invalid} reasoningprompts do indeed achieve similar performance gains on BBH tasks as logicallyvalid reasoning prompts. We also discover that some CoT prompts used byprevious works contain logical errors. This suggests that covariates beyondlogically valid reasoning are responsible for performance improvements.",Rylan Schaeffer,2023/7/20,2023/7/23,,,['cs.AI']
2307.08411v1,Neurosymbolic AI for Reasoning on Biomedical Knowledge Graphs,http://arxiv.org/abs/2307.08411v1,"Biomedical datasets are often modeled as knowledge graphs (KGs) because theycapture the multi-relational, heterogeneous, and dynamic natures of biomedicalsystems. KG completion (KGC), can, therefore, help researchers make predictionsto inform tasks like drug repositioning. While previous approaches for KGC wereeither rule-based or embedding-based, hybrid approaches based on neurosymbolicartificial intelligence are becoming more popular. Many of these methodspossess unique characteristics which make them even better suited towardbiomedical challenges. Here, we survey such approaches with an emphasis ontheir utilities and prospective benefits for biomedicine.",Lauren Nicole DeLong,2023/7/17,2023/7/17,,,"['cs.AI', 'cs.LG', 'cs.LO']"
2307.13701v1,$\text{EFO}_{k}$-CQA: Towards Knowledge Graph Complex Query Answering beyond Set Operation,http://arxiv.org/abs/2307.13701v1,"To answer complex queries on knowledge graphs, logical reasoning overincomplete knowledge is required due to the open-world assumption.Learning-based methods are essential because they are capable of generalizingover unobserved knowledge. Therefore, an appropriate dataset is fundamental toboth obtaining and evaluating such methods under this paradigm. In this paper,we propose a comprehensive framework for data generation, model training, andmethod evaluation that covers the combinatorial space of ExistentialFirst-order Queries with multiple variables ($\text{EFO}_{k}$). Thecombinatorial query space in our framework significantly extends those definedby set operations in the existing literature. Additionally, we construct adataset, $\text{EFO}_{k}$-CQA, with 741 types of query for empiricalevaluation, and our benchmark results provide new insights into how queryhardness affects the results. Furthermore, we demonstrate that the existingdataset construction process is systematically biased that hinders theappropriate development of query-answering methods, highlighting the importanceof our work. Our code and data are providedin~\url{https://github.com/HKUST-KnowComp/EFOK-CQA}.",Hang Yin,2023/7/15,2023/7/15,,,"['cs.AI', 'cs.DB', 'cs.LG', 'cs.LO']"
2307.05036v1,Neural-Symbolic Recommendation with Graph-Enhanced Information,http://arxiv.org/abs/2307.05036v1,"The recommendation system is not only a problem of inductive statistics fromdata but also a cognitive task that requires reasoning ability. The mostadvanced graph neural networks have been widely used in recommendation systemsbecause they can capture implicit structured information from graph-structureddata. However, like most neural network algorithms, they only learn matchingpatterns from a perception perspective. Some researchers use user behavior forlogic reasoning to achieve recommendation prediction from the perspective ofcognitive reasoning, but this kind of reasoning is a local one and ignoresimplicit information on a global scale. In this work, we combine the advantagesof graph neural networks and propositional logic operations to construct aneuro-symbolic recommendation model with both global implicit reasoning abilityand local explicit logic reasoning ability. We first build an item-item graphbased on the principle of adjacent interaction and use graph neural networks tocapture implicit information in global data. Then we transform user behaviorinto propositional logic expressions to achieve recommendations from theperspective of cognitive reasoning. Extensive experiments on five publicdatasets show that our proposed model outperforms several state-of-the-artmethods, source code is avaliable at [https://github.com/hanzo2020/GNNLR].",Bang Chen,2023/7/11,2023/7/11,,,"['cs.AI', 'cs.IR']"
2307.00165v1,Counterfactual Collaborative Reasoning,http://arxiv.org/abs/2307.00165v1,"Causal reasoning and logical reasoning are two important types of reasoningabilities for human intelligence. However, their relationship has not beenextensively explored under machine intelligence context. In this paper, weexplore how the two reasoning abilities can be jointly modeled to enhance bothaccuracy and explainability of machine learning models. More specifically, byintegrating two important types of reasoning ability -- counterfactualreasoning and (neural) logical reasoning -- we propose CounterfactualCollaborative Reasoning (CCR), which conducts counterfactual logic reasoning toimprove the performance. In particular, we use recommender system as an exampleto show how CCR alleviate data scarcity, improve accuracy and enhancetransparency. Technically, we leverage counterfactual reasoning to generate""difficult"" counterfactual training examples for data augmentation, which --together with the original training examples -- can enhance the modelperformance. Since the augmented data is model irrelevant, they can be used toenhance any model, enabling the wide applicability of the technique. Besides,most of the existing data augmentation methods focus on ""implicit dataaugmentation"" over users' implicit feedback, while our framework conducts""explicit data augmentation"" over users explicit feedback based oncounterfactual logic reasoning. Experiments on three real-world datasets showthat CCR achieves better performance than non-augmented models and implicitlyaugmented models, and also improves model transparency by generatingcounterfactual explanations.",Jianchao Ji,2023/6/30,2023/6/30,,,"['cs.IR', 'cs.AI', 'cs.CL', 'cs.LG']"
2309.13044v1,What is the Title of this Paper? Solving logic puzzles using algorithms,http://arxiv.org/abs/2309.13044v1,"This work delves into the realm of logic puzzles by focusing on the Knightand Knave problems popularized by Raymond Smullyan in his book series ""What isthe Name of This Book?"". The puzzles revolve around characters known as Knights(truth-tellers) and Knaves (liars), challenging solvers to determine the trueidentity of each person based on their statements. This paper explores theutilization of Python algorithms to automate the process of solving thesepuzzles, offering a computational approach that enhances efficiency andaccessibility. In this work, we aim to develop a Python algorithm capable ofparsing and analyzing the statements provided in the Knight and Knave puzzles.A logical reasoning framework is integrated within the algorithm to deduce theidentities of the characters based on their statements. The algorithm processesthe input statements, create a knowledge base, and make deductions followingthe rules of Knight and Knave logic. The developed algorithm is thoroughlytested on various instances of Knight and Knave puzzles, comparing its resultsto known solutions and manual approaches. We further expand the scope of theproblem by introducing a Normal (who can sometimes lie and sometimes say thetruth).",Ujaan Rakshit,2023/6/30,2023/6/30,,,"['cs.LO', 'cs.AI']"
2306.17034v1,Exploring & Exploiting High-Order Graph Structure for Sparse Knowledge Graph Completion,http://arxiv.org/abs/2306.17034v1,"Sparse knowledge graph (KG) scenarios pose a challenge for previous KnowledgeGraph Completion (KGC) methods, that is, the completion performance decreasesrapidly with the increase of graph sparsity. This problem is also exacerbatedbecause of the widespread existence of sparse KGs in practical applications. Toalleviate this challenge, we present a novel framework, LR-GCN, that is able toautomatically capture valuable long-range dependency among entities tosupplement insufficient structure features and distill logical reasoningknowledge for sparse KGC. The proposed approach comprises two main components:a GNN-based predictor and a reasoning path distiller. The reasoning pathdistiller explores high-order graph structures such as reasoning paths andencodes them as rich-semantic edges, explicitly compositing long-rangedependencies into the predictor. This step also plays an essential role indensifying KGs, effectively alleviating the sparse issue. Furthermore, the pathdistiller further distills logical reasoning knowledge from these minedreasoning paths into the predictor. These two components are jointly optimizedusing a well-designed variational EM algorithm. Extensive experiments andanalyses on four sparse benchmarks demonstrate the effectiveness of ourproposed method.",Tao He,2023/6/29,2023/6/29,,,"['cs.AI', 'cs.CL']"
2306.12069v1,Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension,http://arxiv.org/abs/2306.12069v1,"Machine reading comprehension (MRC) poses new challenges over logicalreasoning, which aims to understand the implicit logical relations entailed inthe given contexts and perform inference over them. Due to the complexity oflogic, logical relations exist at different granularity levels. However, mostexisting methods of logical reasoning individually focus on either entity-awareor discourse-based information but ignore the hierarchical relations that mayeven have mutual effects. In this paper, we propose a holistic graph network(HGN) which deals with context at both discourse level and word level, as thebasis for logical reasoning, to provide a more fine-grained relationextraction. Specifically, node-level and type-level relations, which can beinterpreted as bridges in the reasoning process, are modeled by a hierarchicalinteraction mechanism to improve the interpretation of MRC systems.Experimental results on logical reasoning QA datasets (ReClor and LogiQA) andnatural language inference datasets (SNLI and ANLI) show the effectiveness andgeneralization of our method, and in-depth analysis verifies its capability tounderstand complex logical relations.",Jialin Chen,2023/6/21,2023/6/21,,,"['cs.CL', 'cs.LG']"
2306.07932v2,Human-in-the-Loop through Chain-of-Thought,http://arxiv.org/abs/2306.07932v2,"While the emergence of powerful language models along with Chain-of-thoughtprompting has made automation more and more omnipresent, it sometimesdemonstrates its weakness in long-term or multi-step logical reasoning. Forexample, users don't always get desirable answers for complex mathematicalproblems without human involvement. Against this background, we present theManual Correction System (MCS) -- a human-in-the-loop system enhanced byChain-of-Thought prompting, which explores how manual correction of sub-logicsin rationales can improve LLM's reasoning performance. Moving one step forward,considering a system with human-in-the-loop involves more than having humansimprove performance but also controlling the cost. Therefore, we post aCost-utility Analysis Model for Human-in-the-Loop systems (CAMLOP) based onclassical economics theory to analyze, quantify and balance the utility and thecorresponding cost. We conduct experiments of MCS and CAMLOP with twelvedatasets. A significant advantage w.r.t cost and utility proves its superiorityover strong baselines.",Zefan Cai,2023/6/10,2023/6/23,,,"['cs.CL', 'cs.AI']"
2306.03872v3,Deductive Verification of Chain-of-Thought Reasoning,http://arxiv.org/abs/2306.03872v3,"Large Language Models (LLMs) significantly benefit from Chain-of-Thought(CoT) prompting in performing various reasoning tasks. While CoT allows modelsto produce more comprehensive reasoning processes, its emphasis on intermediatereasoning steps can inadvertently introduce hallucinations and accumulatederrors, thereby limiting models' ability to solve complex reasoning tasks.Inspired by how humans engage in careful and meticulous deductive logicalreasoning processes to solve tasks, we seek to enable language models toperform explicit and rigorous deductive reasoning, and also ensure thetrustworthiness of their reasoning process through self-verification. However,directly verifying the validity of an entire deductive reasoning process ischallenging, even with advanced models like ChatGPT. In light of this, wepropose to decompose a reasoning verification process into a series ofstep-by-step subprocesses, each only receiving their necessary context andpremises. To facilitate this procedure, we propose Natural Program, a naturallanguage-based deductive reasoning format. Our approach enables models togenerate precise reasoning steps where subsequent steps are more rigorouslygrounded on prior steps. It also empowers language models to carry outreasoning self-verification in a step-by-step manner. By integrating thisverification process into each deductive reasoning stage, we significantlyenhance the rigor and trustfulness of generated reasoning steps. Along thisprocess, we also improve the answer correctness on complex reasoning tasks.Code will be released at https://github.com/lz1oceani/verify_cot.",Zhan Ling,2023/6/6,2023/10/3,,,"['cs.CL', 'cs.AI', 'cs.LG']"
2306.03102v1,ChatGPT is a Remarkable Tool -- For Experts,http://arxiv.org/abs/2306.03102v1,"This paper investigates the capabilities of ChatGPT as an automated assistantin diverse domains, including scientific writing, mathematics, education,programming, and healthcare. We explore the potential of ChatGPT to enhanceproductivity, streamline problem-solving processes, and improve writing style.Furthermore, we highlight the potential risks associated with excessivereliance on ChatGPT in these fields. These limitations encompass factors likeincorrect and fictitious responses, inaccuracies in code, limited logicalreasoning abilities, overconfidence, and critical ethical concerns ofcopyrights and privacy violation. We outline areas and objectives where ChatGPTproves beneficial, applications where it should be used judiciously, andscenarios where its reliability may be limited. In light of observedlimitations, and given that the tool's fundamental errors may pose a specialchallenge for non-experts, ChatGPT should be used with a strategic methodology.By drawing from comprehensive experimental studies, we offer methods and flowcharts for effectively using ChatGPT. Our recommendations emphasize iterativeinteraction with ChatGPT and independent verification of its outputs.Considering the importance of utilizing ChatGPT judiciously and with expertise,we recommend its usage for experts who are well-versed in the respectivedomains.",Amos Azaria,2023/6/2,2023/6/2,,,"['cs.HC', 'cs.AI', 'cs.CL', 'cs.CY']"
2306.00790v1,Knowledge-based Reasoning and Learning under Partial Observability in Ad Hoc Teamwork,http://arxiv.org/abs/2306.00790v1,"Ad hoc teamwork refers to the problem of enabling an agent to collaboratewith teammates without prior coordination. Data-driven methods represent thestate of the art in ad hoc teamwork. They use a large labeled dataset of priorobservations to model the behavior of other agent types and to determine the adhoc agent's behavior. These methods are computationally expensive, lacktransparency, and make it difficult to adapt to previously unseen changes,e.g., in team composition. Our recent work introduced an architecture thatdetermined an ad hoc agent's behavior based on non-monotonic logical reasoningwith prior commonsense domain knowledge and predictive models of other agents'behavior that were learned from limited examples. In this paper, wesubstantially expand the architecture's capabilities to support: (a) onlineselection, adaptation, and learning of the models that predict the otheragents' behavior; and (b) collaboration with teammates in the presence ofpartial observability and limited communication. We illustrate andexperimentally evaluate the capabilities of our architecture in two simulatedmultiagent benchmark domains for ad hoc teamwork: Fort Attack and Half FieldOffense. We show that the performance of our architecture is comparable orbetter than state of the art data-driven baselines in both simple and complexscenarios, particularly in the presence of limited training data, partialobservability, and changes in team composition.",Hasra Dodampegama,2023/6/1,2023/6/1,,,"['cs.AI', 'cs.LO', 'cs.MA']"
2305.17716v4,InDL: A New Dataset and Benchmark for In-Diagram Logic Interpretation based on Visual Illusion,http://arxiv.org/abs/2305.17716v4,"This paper introduces a novel approach to evaluating deep learning models'capacity for in-diagram logic interpretation. Leveraging the intriguing realmof visual illusions, we establish a unique dataset, InDL, designed torigorously test and benchmark these models. Deep learning has witnessedremarkable progress in domains such as computer vision and natural languageprocessing. However, models often stumble in tasks requiring logical reasoningdue to their inherent 'black box' characteristics, which obscure thedecision-making process. Our work presents a new lens to understand thesemodels better by focusing on their handling of visual illusions -- a complexinterplay of perception and logic. We utilize six classic geometric opticalillusions to create a comparative framework between human and machine visualperception. This methodology offers a quantifiable measure to rank models,elucidating potential weaknesses and providing actionable insights for modelimprovements. Our experimental results affirm the efficacy of our benchmarkingstrategy, demonstrating its ability to effectively rank models based on theirlogic interpretation ability. As part of our commitment to reproducibleresearch, the source code and datasets will be made publicly available athttps://github.com/rabbit-magic-wh/InDL",Haobo Yang,2023/5/28,2023/6/5,,,"['cs.CV', 'cs.AI']"
2305.17518v1,Synthesizing a Progression of Subtasks for Block-Based Visual Programming Tasks,http://arxiv.org/abs/2305.17518v1,"Block-based visual programming environments play an increasingly importantrole in introducing computing concepts to K-12 students. In recent years, theyhave also gained popularity in neuro-symbolic AI, serving as a benchmark toevaluate general problem-solving and logical reasoning skills. The open-endedand conceptual nature of these visual programming tasks make them challenging,both for state-of-the-art AI agents as well as for novice programmers. Anatural approach to providing assistance for problem-solving is breaking down acomplex task into a progression of simpler subtasks; however, this is nottrivial given that the solution codes are typically nested and have non-linearexecution behavior. In this paper, we formalize the problem of synthesizingsuch a progression for a given reference block-based visual programming task.We propose a novel synthesis algorithm that generates a progression of subtasksthat are high-quality, well-spaced in terms of their complexity, and solvingthis progression leads to solving the reference task. We show the utility ofour synthesis algorithm in improving the efficacy of AI agents (in this case,neural program synthesizers) for solving tasks in the Karel programmingenvironment. Then, we conduct a user study to demonstrate that our synthesizedprogression of subtasks can assist a novice programmer in solving tasks in theHour of Code: Maze Challenge by Code-dot-org.",Alperen Tercan,2023/5/27,2023/5/27,,,"['cs.AI', 'cs.CY']"
2305.16572v1,Counterfactual reasoning: Testing language models' understanding of hypothetical scenarios,http://arxiv.org/abs/2305.16572v1,"Current pre-trained language models have enabled remarkable improvements indownstream tasks, but it remains difficult to distinguish effects ofstatistical correlation from more systematic logical reasoning grounded on theunderstanding of real world. We tease these factors apart by leveragingcounterfactual conditionals, which force language models to predict unusualconsequences based on hypothetical propositions. We introduce a set of testsfrom psycholinguistic experiments, as well as larger-scale controlled datasets,to probe counterfactual predictions from five pre-trained language models. Wefind that models are consistently able to override real-world knowledge incounterfactual scenarios, and that this effect is more robust in case ofstronger baseline world knowledge -- however, we also find that for most modelsthis effect appears largely to be driven by simple lexical cues. When wemitigate effects of both world knowledge and lexical cues to test knowledge oflinguistic nuances of counterfactuals, we find that only GPT-3 showssensitivity to these nuances, though this sensitivity is also non-triviallyimpacted by lexical associative factors.",Jiaxuan Li,2023/5/26,2023/5/26,,,['cs.CL']
2305.16426v2,Not wacky vs. definitely wacky: A study of scalar adverbs in pretrained language models,http://arxiv.org/abs/2305.16426v2,"Vector space models of word meaning all share the assumption that wordsoccurring in similar contexts have similar meanings. In such models, words thatare similar in their topical associations but differ in their logical forcetend to emerge as semantically close, creating well-known challenges for NLPapplications that involve logical reasoning. Modern pretrained language models,such as BERT, RoBERTa and GPT-3 hold the promise of performing better onlogical tasks than classic static word embeddings. However, reports are mixedabout their success. In the current paper, we advance this discussion through asystematic study of scalar adverbs, an under-explored class of words withstrong logical force. Using three different tasks, involving both naturalisticsocial media data and constructed examples, we investigate the extent to whichBERT, RoBERTa, GPT-2 and GPT-3 exhibit general, human-like, knowledge of thesecommon words. We ask: 1) Do the models distinguish amongst the three semanticcategories of MODALITY, FREQUENCY and DEGREE? 2) Do they have implicitrepresentations of full scales from maximally negative to maximally positive?3) How do word frequency and contextual factors impact model performance? Wefind that despite capturing some aspects of logical meaning, the models fallfar short of human performance.",Isabelle Lorge,2023/5/25,2023/10/22,,,['cs.CL']
2305.16373v1,DeepGate2: Functionality-Aware Circuit Representation Learning,http://arxiv.org/abs/2305.16373v1,"Circuit representation learning aims to obtain neural representations ofcircuit elements and has emerged as a promising research direction that can beapplied to various EDA and logic reasoning tasks. Existing solutions, such asDeepGate, have the potential to embed both circuit structural information andfunctional behavior. However, their capabilities are limited due to weaksupervision or flawed model design, resulting in unsatisfactory performance indownstream tasks. In this paper, we introduce DeepGate2, a novelfunctionality-aware learning framework that significantly improves upon theoriginal DeepGate solution in terms of both learning effectiveness andefficiency. Our approach involves using pairwise truth table differencesbetween sampled logic gates as training supervision, along with a well-designedand scalable loss function that explicitly considers circuit functionality.Additionally, we consider inherent circuit characteristics and design anefficient one-round graph neural network (GNN), resulting in an order ofmagnitude faster learning speed than the original DeepGate solution.Experimental results demonstrate significant improvements in two practicaldownstream tasks: logic synthesis and Boolean satisfiability solving. The codeis available at https://github.com/cure-lab/DeepGate2",Zhengyuan Shi,2023/5/25,2023/5/25,,,"['cs.LG', 'cs.AI', 'cs.AR']"
2305.13585v1,Query Structure Modeling for Inductive Logical Reasoning Over Knowledge Graphs,http://arxiv.org/abs/2305.13585v1,"Logical reasoning over incomplete knowledge graphs to answer complex logicalqueries is a challenging task. With the emergence of new entities and relationsin constantly evolving KGs, inductive logical reasoning over KGs has become acrucial problem. However, previous PLMs-based methods struggle to model thelogical structures of complex queries, which limits their ability to generalizewithin the same structure. In this paper, we propose a structure-modeledtextual encoding framework for inductive logical reasoning over KGs. It encodeslinearized query structures and entities using pre-trained language models tofind answers. For structure modeling of complex queries, we design stepwiseinstructions that implicitly prompt PLMs on the execution order of geometricoperations in each query. We further separately model different geometricoperations (i.e., projection, intersection, and union) on the representationspace using a pre-trained encoder with additional attention and maxout layersto enhance structured modeling. We conduct experiments on two inductive logicalreasoning datasets and three transductive datasets. The results demonstrate theeffectiveness of our method on logical reasoning over KGs in both inductive andtransductive settings.",Siyuan Wang,2023/5/23,2023/5/23,,,['cs.CL']
2305.12295v2,Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning,http://arxiv.org/abs/2305.12295v2,"Large Language Models (LLMs) have shown human-like reasoning abilities butstill struggle with complex logical problems. This paper introduces a novelframework, Logic-LM, which integrates LLMs with symbolic solvers to improvelogical problem-solving. Our method first utilizes LLMs to translate a naturallanguage problem into a symbolic formulation. Afterward, a deterministicsymbolic solver performs inference on the formulated problem. We also introducea self-refinement module, which utilizes the symbolic solver's error messagesto revise symbolic formalizations. We demonstrate Logic-LM's effectiveness onfive logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO,LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significantperformance boost of 39.2% over using LLM alone with standard prompting and18.4% over LLM with chain-of-thought prompting. Our findings suggest thatLogic-LM, by combining LLMs with symbolic logic, offers a promising avenue forfaithful logical reasoning. Code and data are publicly available athttps://github.com/teacherpeterpan/Logic-LLM.",Liangming Pan,2023/5/20,2023/10/19,,,"['cs.CL', 'cs.AI']"
2305.11098v1,A Simple Generative Model of Logical Reasoning and Statistical Learning,http://arxiv.org/abs/2305.11098v1,"Statistical learning and logical reasoning are two major fields of AIexpected to be unified for human-like machine intelligence. Most existing workconsiders how to combine existing logical and statistical systems. However,there is no theory of inference so far explaining how basic approaches tostatistical learning and logical reasoning stem from a common principle.Inspired by the fact that much empirical work in neuroscience suggests Bayesian(or probabilistic generative) approaches to brain function including learningand reasoning, we here propose a simple Bayesian model of logical reasoning andstatistical learning. The theory is statistically correct as it satisfiesKolmogorov's axioms, is consistent with both Fenstad's representation theoremand maximum likelihood estimation and performs exact Bayesian inference with alinear-time complexity. The theory is logically correct as it is a data-drivengeneralisation of uncertain reasoning from consistency, possibility,inconsistency and impossibility. The theory is correct in terms of machinelearning as its solution to generation and prediction tasks on the MNISTdataset is not only empirically reasonable but also theoretically correctagainst the K nearest neighbour method. We simply model how data causessymbolic knowledge in terms of its satisfiability in formal logic. Symbolicreasoning emerges as a result of the process of going the causality forwardsand backwards. The forward and backward processes correspond to aninterpretation and inverse interpretation in formal logic, respectively. Theinverse interpretation differentiates our work from the mainstream oftenreferred to as inverse entailment, inverse deduction or inverse resolution. Theperspective gives new insights into learning and reasoning towards human-likemachine intelligence.",Hiroyuki Kido,2023/5/18,2023/5/18,,,['cs.AI']
2305.07763v1,Knowledge Authoring for Rules and Actions,http://arxiv.org/abs/2305.07763v1,"Knowledge representation and reasoning (KRR) systems describe and reason withcomplex concepts and relations in the form of facts and rules. Unfortunately,wide deployment of KRR systems runs into the problem that domain experts havegreat difficulty constructing correct logical representations of their domainknowledge. Knowledge engineers can help with this construction process, butthere is a deficit of such specialists. The earlier Knowledge Authoring LogicMachine (KALM) based on Controlled Natural Language (CNL) was shown to havevery high accuracy for authoring facts and questions. More recently, KALMFL, asuccessor of KALM, replaced CNL with factual English, which is much lessrestrictive and requires very little training from users. However, KALMFL haslimitations in representing certain types of knowledge, such as authoring rulesfor multi-step reasoning or understanding actions with timestamps. To addressthese limitations, we propose KALMRA to enable authoring of rules and actions.Our evaluation using the UTI guidelines benchmark shows that KALMRA achieves ahigh level of correctness (100%) on rule authoring. When used for authoring andreasoning with actions, KALMRA achieves more than 99.3% correctness on the bAbIbenchmark, demonstrating its effectiveness in more sophisticated KRR jobs.Finally, we illustrate the logical reasoning capabilities of KALMRA by drawingattention to the problems faced by the recently made famous AI, ChatGPT.",Yuheng Wang,2023/5/12,2023/5/12,,,"['cs.CL', 'cs.AI']"
2305.07617v2,Scalable Coupling of Deep Learning with Logical Reasoning,http://arxiv.org/abs/2305.07617v2,"In the ongoing quest for hybridizing discrete reasoning with neural nets,there is an increasing interest in neural architectures that can learn how tosolve discrete reasoning or optimization problems from natural inputs. In thispaper, we introduce a scalable neural architecture and loss function dedicatedto learning the constraints and criteria of NP-hard reasoning problemsexpressed as discrete Graphical Models. Our loss function solves one of themain limitations of Besag's pseudo-loglikelihood, enabling learning of highenergies. We empirically show it is able to efficiently learn how to solveNP-hard reasoning problems from natural inputs as the symbolic, visual ormany-solutions Sudoku problems as well as the energy optimization formulationof the protein design problem, providing data efficiency, interpretability, and\textit{a posteriori} control over predictions.",Marianne Defresne,2023/5/12,2023/7/18,,,"['cs.AI', 'cs.LG', 'stat.ML']"
2305.07004v2,Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting,http://arxiv.org/abs/2305.07004v2,"Large language models (LLMs) demonstrate impressive multilingual capability,but their performance varies substantially across different languages. In thiswork, we introduce a simple yet effective method, called cross-lingual-thoughtprompting (XLT), to systematically improve the multilingual capability of LLMs.Specifically, XLT is a generic template prompt that stimulates cross-lingualand logical reasoning skills to enhance task performance across languages. Weconduct comprehensive evaluations on 7 typical benchmarks related to reasoning,understanding, and generation tasks, covering both high-resource andlow-resource languages. Experimental results show that XLT not only remarkablyenhances the performance of various multilingual tasks but also significantlyreduces the gap between the average performance and the best performance ofeach task in different languages. Notably, XLT brings over 10 points of averageimprovement in arithmetic reasoning and open-domain question-answering tasks.",Haoyang Huang,2023/5/11,2023/10/22,,,['cs.CL']
2305.04034v1,Wasserstein-Fisher-Rao Embedding: Logical Query Embeddings with Local Comparison and Global Transport,http://arxiv.org/abs/2305.04034v1,"Answering complex queries on knowledge graphs is important but particularlychallenging because of the data incompleteness. Query embedding methods addressthis issue by learning-based models and simulating logical reasoning with setoperators. Previous works focus on specific forms of embeddings, but scoringfunctions between embeddings are underexplored. In contrast to existing scoringfunctions motivated by local comparison or global transport, this workinvestigates the local and global trade-off with unbalanced optimal transporttheory. Specifically, we embed sets as bounded measures in $\real$ endowed witha scoring function motivated by the Wasserstein-Fisher-Rao metric. Such adesign also facilitates closed-form set operators in the embedding space.Moreover, we introduce a convolution-based algorithm for linear timecomputation and a block-diagonal kernel to enforce the trade-off. Results showthat WFRE can outperform existing query embedding methods on standard datasets,evaluation sets with combinatorially complex queries, and hierarchicalknowledge graphs. Ablation study shows that finding a better local and globaltrade-off is essential for performance improvement.",Zihao Wang,2023/5/6,2023/5/6,,,"['cs.AI', 'cs.DB', 'cs.LG']"
2305.02442v2,Tackling Universal Properties of Minimal Trap Spaces of Boolean Networks,http://arxiv.org/abs/2305.02442v2,"Minimal trap spaces (MTSs) capture subspaces in which the Boolean dynamics istrapped, whatever the update mode. They correspond to the attractors of themost permissive mode. Due to their versatility, the computation of MTSs hasrecently gained traction, essentially by focusing on their enumeration. In thispaper, we address the logical reasoning on universal properties of MTSs in thescope of two problems: the reprogramming of Boolean networks for identifyingthe permanent freeze of Boolean variables that enforce a given property on allthe MTSs, and the synthesis of Boolean networks from universal properties ontheir MTSs. Both problems reduce to solving the satisfiability of quantifiedpropositional logic formula with 3 levels of quantifiers($\exists\forall\exists$). In this paper, we introduce a Counter-Example GuidedRefinement Abstraction (CEGAR) to efficiently solve these problems by couplingthe resolution of two simpler formulas. We provide a prototype relying onAnswer-Set Programming for each formula and show its tractability on a widerange of Boolean models of biological networks.",Sara Riva,2023/5/3,2023/7/20,,,"['cs.LO', 'cs.AI', 'cs.DM', 'cs.SY', 'eess.SY', 'q-bio.MN']"
2305.02265v2,A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text,http://arxiv.org/abs/2305.02265v2,"Pretrained Vision-Language Models (VLMs) have achieved remarkable performancein image retrieval from text. However, their performance drops drastically whenconfronted with linguistically complex texts that they struggle to comprehend.Inspired by the Divide-and-Conquer algorithm and dual-process theory, in thispaper, we regard linguistically complex texts as compound proposition textscomposed of multiple simple proposition sentences and propose an end-to-endNeural Divide-and-Conquer Reasoning framework, dubbed NDCR. It contains threemain components: 1) Divide: a proposition generator divides the compoundproposition text into simple proposition sentences and produces theircorresponding representations, 2) Conquer: a pretrained VLMs-basedvisual-linguistic interactor achieves the interaction between decomposedproposition sentences and images, 3) Combine: a neural-symbolic reasonercombines the above reasoning states to obtain the final solution via a neurallogic reasoning approach. According to the dual-process theory, thevisual-linguistic interactor and neural-symbolic reasoner could be regarded asanalogical reasoning System 1 and logical reasoning System 2. We conductextensive experiments on a challenging image retrieval from contextualdescriptions data set. Experimental results and analyses indicate NDCRsignificantly improves performance in the complex image-text reasoning problem.Code link: https://github.com/YunxinLi/NDCR.",Yunxin Li,2023/5/3,2023/5/5,,,['cs.CL']
2305.00574v1,The Dark Side of Explanations: Poisoning Recommender Systems with Counterfactual Examples,http://arxiv.org/abs/2305.00574v1,"Deep learning-based recommender systems have become an integral part ofseveral online platforms. However, their black-box nature emphasizes the needfor explainable artificial intelligence (XAI) approaches to providehuman-understandable reasons why a specific item gets recommended to a givenuser. One such method is counterfactual explanation (CF). While CFs can behighly beneficial for users and system designers, malicious actors may alsoexploit these explanations to undermine the system's security. In this work, wepropose H-CARS, a novel strategy to poison recommender systems via CFs.Specifically, we first train a logical-reasoning-based surrogate model ontraining data derived from counterfactual explanations. By reversing thelearning process of the recommendation model, we thus develop a proficientgreedy algorithm to generate fabricated user profiles and their associatedinteraction records for the aforementioned surrogate model. Our experiments,which employ a well-known CF generation method and are conducted on twodistinct datasets, show that H-CARS yields significant and successful attackperformance.",Ziheng Chen,2023/4/30,2023/4/30,,,['cs.IR']
2304.11383v2,Sequential Recommendation with Probabilistic Logical Reasoning,http://arxiv.org/abs/2304.11383v2,"Deep learning and symbolic learning are two frequently employed methods inSequential Recommendation (SR). Recent neural-symbolic SR models demonstratetheir potential to enable SR to be equipped with concurrent perception andcognition capacities. However, neural-symbolic SR remains a challenging problemdue to open issues like representing users and items in logical reasoning. Inthis paper, we combine the Deep Neural Network (DNN) SR models with logicalreasoning and propose a general framework named Sequential Recommendation withProbabilistic Logical Reasoning (short for SR-PLR). This framework allowsSR-PLR to benefit from both similarity matching and logical reasoning bydisentangling feature embedding and logic embedding in the DNN andprobabilistic logic network. To better capture the uncertainty and evolution ofuser tastes, SR-PLR embeds users and items with a probabilistic method andconducts probabilistic logical reasoning on users' interaction patterns. Thenthe feature and logic representations learned from the DNN and logic networkare concatenated to make the prediction. Finally, experiments on varioussequential recommendation models demonstrate the effectiveness of the SR-PLR.",Huanhuan Yuan,2023/4/22,2023/5/15,,,['cs.AI']
2304.06203v2,LeafAI: query generator for clinical cohort discovery rivaling a human programmer,http://arxiv.org/abs/2304.06203v2,"Objective: Identifying study-eligible patients within clinical databases is acritical step in clinical research. However, accurate query design typicallyrequires extensive technical and biomedical expertise. We sought to create asystem capable of generating data model-agnostic queries while also providingnovel logical reasoning capabilities for complex clinical trial eligibilitycriteria.  Materials and Methods: The task of query creation from eligibility criteriarequires solving several text-processing problems, including named entityrecognition and relation extraction, sequence-to-sequence transformation,normalization, and reasoning. We incorporated hybrid deep learning andrule-based modules for these, as well as a knowledge base of the UnifiedMedical Language System (UMLS) and linked ontologies. To enable data-modelagnostic query creation, we introduce a novel method for tagging databaseschema elements using UMLS concepts. To evaluate our system, called LeafAI, wecompared the capability of LeafAI to a human database programmer to identifypatients who had been enrolled in 8 clinical trials conducted at ourinstitution. We measured performance by the number of actual enrolled patientsmatched by generated queries.  Results: LeafAI matched a mean 43% of enrolled patients with 27,225 eligibleacross 8 clinical trials, compared to 27% matched and 14,587 eligible inqueries by a human database programmer. The human programmer spent 26 totalhours crafting queries compared to several minutes by LeafAI.  Conclusions: Our work contributes a state-of-the-art data model-agnosticquery generation system capable of conditional reasoning using a knowledgebase. We demonstrate that LeafAI can rival an experienced human programmer infinding patients eligible for clinical trials.",Nicholas J Dobbins,2023/4/13,2023/8/14,,,['cs.CL']
2304.04886v1,Make flows small again: revisiting the flow framework,http://arxiv.org/abs/2304.04886v1,"We present a new flow framework for separation logic reasoning about programsthat manipulate general graphs. The framework overcomes problems in earlierdevelopments: it is based on standard fixed point theory, guarantees leastflows, rules out vanishing flows, and has an easy to understand notion offootprint as needed for soundness of the frame rule. In addition, we presentalgorithms for automating the frame rule, which we evaluate on graph updatesextracted from linearizability proofs for concurrent data structures. Theevaluation demonstrates that our algorithms help to automate key aspects ofthese proofs that have previously relied on user guidance or heuristics.",Roland Meyer,2023/4/10,2023/4/10,,,['cs.PL']
2304.04812v1,Scallop: A Language for Neurosymbolic Programming,http://arxiv.org/abs/2304.04812v1,"We present Scallop, a language which combines the benefits of deep learningand logical reasoning. Scallop enables users to write a wide range ofneurosymbolic applications and train them in a data- and compute-efficientmanner. It achieves these goals through three key features: 1) a flexiblesymbolic representation that is based on the relational data model; 2) adeclarative logic programming language that is based on Datalog and supportsrecursion, aggregation, and negation; and 3) a framework for automatic andefficient differentiable reasoning that is based on the theory of provenancesemirings. We evaluate Scallop on a suite of eight neurosymbolic applicationsfrom the literature. Our evaluation demonstrates that Scallop is capable ofexpressing algorithmic reasoning in diverse and challenging AI tasks, providesa succinct interface for machine learning programmers to integrate logicaldomain knowledge, and yields solutions that are comparable or superior tostate-of-the-art models in terms of accuracy. Furthermore, Scallop's solutionsoutperform these models in aspects such as runtime and data efficiency,interpretability, and generalizability.",Ziyang Li,2023/4/10,2023/4/10,,,"['cs.PL', 'cs.AI', 'cs.LG']"
2304.03439v3,Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4,http://arxiv.org/abs/2304.03439v3,"Harnessing logical reasoning ability is a comprehensive natural languageunderstanding endeavor. With the release of Generative Pretrained Transformer 4(GPT-4), highlighted as ""advanced"" at reasoning tasks, we are eager to learnthe GPT-4 performance on various logical reasoning tasks. This report analysesmultiple logical reasoning datasets, with popular benchmarks like LogiQA andReClor, and newly-released datasets like AR-LSAT. We test the multi-choicereading comprehension and natural language inference tasks with benchmarksrequiring logical reasoning. We further construct a logical reasoningout-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4.We also make a performance comparison between ChatGPT and GPT-4. Experimentresults show that ChatGPT performs significantly better than the RoBERTafine-tuning method on most logical reasoning benchmarks. With early access tothe GPT-4 API we are able to conduct intense experiments on the GPT-4 model.The results show GPT-4 yields even higher performance on most logical reasoningdatasets. Among benchmarks, ChatGPT and GPT-4 do relatively well on well-knowndatasets like LogiQA and ReClor. However, the performance drops significantlywhen handling newly released and out-of-distribution datasets. Logicalreasoning remains challenging for ChatGPT and GPT-4, especially onout-of-distribution and natural language inference datasets. We release theprompt-style logical reasoning datasets as a benchmark suite and name itLogiEval.",Hanmeng Liu,2023/4/7,2023/5/5,,,"['cs.CL', 'cs.AI']"
2303.14725v2,"Natural Language Reasoning, A Survey",http://arxiv.org/abs/2303.14725v2,"This survey paper proposes a clearer view of natural language reasoning inthe field of Natural Language Processing (NLP), both conceptually andpractically. Conceptually, we provide a distinct definition for naturallanguage reasoning in NLP, based on both philosophy and NLP scenarios, discusswhat types of tasks require reasoning, and introduce a taxonomy of reasoning.Practically, we conduct a comprehensive literature review on natural languagereasoning in NLP, mainly covering classical logical reasoning, natural languageinference, multi-hop question answering, and commonsense reasoning. The paperalso identifies and views backward reasoning, a powerful paradigm formulti-step reasoning, and introduces defeasible reasoning as one of the mostimportant future directions in natural language reasoning research. We focus onsingle-modality unstructured natural language text, excluding neuro-symbolictechniques and mathematical reasoning.",Fei Yu,2023/3/26,2023/5/13,,,['cs.CL']
2303.14617v1,Neural Graph Reasoning: Complex Logical Query Answering Meets Graph Databases,http://arxiv.org/abs/2303.14617v1,"Complex logical query answering (CLQA) is a recently emerged task of graphmachine learning that goes beyond simple one-hop link prediction and solves afar more complex task of multi-hop logical reasoning over massive, potentiallyincomplete graphs in a latent space. The task received a significant tractionin the community; numerous works expanded the field along theoretical andpractical axes to tackle different types of complex queries and graphmodalities with efficient systems. In this paper, we provide a holistic surveyof CLQA with a detailed taxonomy studying the field from multiple angles,including graph types (modality, reasoning domain, background semantics),modeling aspects (encoder, processor, decoder), supported queries (operators,patterns, projected variables), datasets, evaluation metrics, and applications.  Refining the CLQA task, we introduce the concept of Neural Graph Databases(NGDBs). Extending the idea of graph databases (graph DBs), NGDB consists of aNeural Graph Storage and a Neural Graph Engine. Inside Neural Graph Storage, wedesign a graph store, a feature store, and further embed information in alatent embedding store using an encoder. Given a query, Neural Query Enginelearns how to perform query planning and execution in order to efficientlyretrieve the correct results by interacting with the Neural Graph Storage.Compared with traditional graph DBs, NGDBs allow for a flexible and unifiedmodeling of features in diverse modalities using the embedding store. Moreover,when the graph is incomplete, they can provide robust retrieval of answerswhich a normal graph DB cannot recover. Finally, we point out promisingdirections, unsolved problems and applications of NGDB for future research.",Hongyu Ren,2023/3/26,2023/3/26,,,"['cs.DB', 'cs.AI', 'cs.LG']"
2303.13697v2,Soy: An Efficient MILP Solver for Piecewise-Affine Systems,http://arxiv.org/abs/2303.13697v2,"Piecewise-affine (PWA) systems are widely used for modeling and control ofrobotics problems including modeling contact dynamics. A common approach is toencode the control problem of the PWA system as a Mixed-Integer Convex Program(MICP), which can be solved by general-purpose off-the-shelf MICP solvers. Tomitigate the scalability challenge of solving these MICP problems, existingwork focuses on devising efficient and strong formulations of the problems,while less effort has been spent on exploiting their specific structure todevelop specialized solvers. The latter is the theme of our work. We focus onefficiently handling one-hot constraints, which are particularly relevant whenencoding PWA dynamics. We have implemented our techniques in a tool, Soy, whichorganically integrates logical reasoning, arithmetic reasoning, and stochasticlocal search. For a set of PWA control benchmarks, Soy solves more problems,faster, than two state-of-the-art MICP solvers.",Haoze Wu,2023/3/23,2023/8/15,,,"['cs.RO', 'cs.LO', 'cs.SY', 'eess.SY', 'math.OC']"
2303.05148v1,Weakly Supervised Knowledge Transfer with Probabilistic Logical Reasoning for Object Detection,http://arxiv.org/abs/2303.05148v1,"Training object detection models usually requires instance-level annotations,such as the positions and labels of all objects present in each image. Suchsupervision is unfortunately not always available and, more often, onlyimage-level information is provided, also known as weak supervision. Recentworks have addressed this limitation by leveraging knowledge from a richlyannotated domain. However, the scope of weak supervision supported by theseapproaches has been very restrictive, preventing them to use all availableinformation. In this work, we propose ProbKT, a framework based onprobabilistic logical reasoning that allows to train object detection modelswith arbitrary types of weak supervision. We empirically show on differentdatasets that using all available information is beneficial as our ProbKT leadsto significant improvement on target domain and better generalization comparedto existing baselines. We also showcase the ability of our approach to handlecomplex logic statements as supervision signal.",Martijn Oldenhof,2023/3/9,2023/3/9,,,"['cs.CV', 'stat.ML']"
2303.02829v2,Attribution-Scores and Causal Counterfactuals as Explanations in Artificial Intelligence,http://arxiv.org/abs/2303.02829v2,"In this expository article we highlight the relevance of explanations forartificial intelligence, in general, and for the newer developments in {\emexplainable AI}, referring to origins and connections of and among differentapproaches. We describe in simple terms, explanations in data management andmachine learning that are based on attribution-scores, and counterfactuals asfound in the area of causality. We elaborate on the importance of logicalreasoning when dealing with counterfactuals, and their use for scorecomputation.",Leopoldo Bertossi,2023/3/6,2023/3/22,,,"['cs.AI', 'cs.DB', 'cs.LG']"
2302.09458v1,Learning Language Representations with Logical Inductive Bias,http://arxiv.org/abs/2302.09458v1,"Transformer architectures have achieved great success in solving naturallanguage tasks, which learn strong language representations from large-scaleunlabeled texts. In this paper, we seek to go further beyond and explore a newlogical inductive bias for better language representation learning. Logicreasoning is known as a formal methodology to reach answers from givenknowledge and facts. Inspired by such a view, we develop a novel neuralarchitecture named FOLNet (First-Order Logic Network), to encode this newinductive bias. We construct a set of neural logic operators as learnable Hornclauses, which are further forward-chained into a fully differentiable neuralarchitecture (FOLNet). Interestingly, we find that the self-attention module intransformers can be composed by two of our neural logic operators, whichprobably explains their strong reasoning performance. Our proposed FOLNet hasthe same input and output interfaces as other pretrained models and thus couldbe pretrained/finetuned by using similar losses. It also allows FOLNet to beused in a plug-and-play manner when replacing other pretrained models. With ourlogical inductive bias, the same set of ``logic deduction skills'' learnedthrough pretraining are expected to be equally capable of solving diversedownstream tasks. For this reason, FOLNet learns language representations thathave much stronger transfer capabilities. Experimental results on severallanguage understanding tasks show that our pretrained FOLNet model outperformsthe existing strong transformer-based approaches.",Jianshu Chen,2023/2/19,2023/2/19,,,"['cs.CL', 'cs.LG', 'cs.LO']"
2301.08913v2,Unifying Structure Reasoning and Language Model Pre-training for Complex Reasoning,http://arxiv.org/abs/2301.08913v2,"Recent pre-trained language models (PLMs) equipped with foundation reasoningskills have shown remarkable performance on downstream complex tasks. However,the significant structure reasoning skill has been rarely studied, whichinvolves modeling implicit structure information within the text and performingexplicit logical reasoning over them to deduce the conclusion. This paperproposes a unified learning framework that combines explicit structurereasoning and language pre-training to endow PLMs with the structure reasoningskill. It first identifies several elementary structures within contexts toconstruct structured queries and performs step-by-step reasoning along thequeries to identify the answer entity. The fusion of textual semantics andstructure reasoning is achieved by using contextual representations learned byPLMs to initialize the representation space of structures, and performingstepwise reasoning on this semantic representation space. Experimental resultson four datasets demonstrate that the proposed model achieves significantimprovements in complex reasoning tasks involving diverse structures, and showstransferability to downstream tasks with limited training data andeffectiveness for complex reasoning of KGs modality.",Siyuan Wang,2023/1/21,2023/7/15,,,['cs.CL']
2301.08859v4,Logical Message Passing Networks with One-hop Inference on Atomic Formulas,http://arxiv.org/abs/2301.08859v4,"Complex Query Answering (CQA) over Knowledge Graphs (KGs) has attracted a lotof attention to potentially support many applications. Given that KGs areusually incomplete, neural models are proposed to answer the logical queries byparameterizing set operators with complex neural networks. However, suchmethods usually train neural set operators with a large number of entity andrelation embeddings from the zero, where whether and how the embeddings or theneural set operators contribute to the performance remains not clear. In thispaper, we propose a simple framework for complex query answering thatdecomposes the KG embeddings from neural set operators. We propose to representthe complex queries into the query graph. On top of the query graph, we proposethe Logical Message Passing Neural Network (LMPNN) that connects the localone-hop inferences on atomic formulas to the global logical reasoning forcomplex query answering. We leverage existing effective KG embeddings toconduct one-hop inferences on atomic formulas, the results of which areregarded as the messages passed in LMPNN. The reasoning process over theoverall logical formulas is turned into the forward pass of LMPNN thatincrementally aggregates local information to finally predict the answers'embeddings. The complex logical inference across different types of querieswill then be learned from training examples based on the LMPNN architecture.Theoretically, our query-graph represenation is more general than theprevailing operator-tree formulation, so our approach applies to a broaderrange of complex KG queries. Empirically, our approach yields the newstate-of-the-art neural CQA model. Our research bridges the gap between complexKG query answering tasks and the long-standing achievements of knowledge graphrepresentation learning.",Zihao Wang,2023/1/21,2023/8/26,,,"['cs.LG', 'cs.LO']"
2301.06237v1,A separation logic for sequences in pointer programs and its decidability,http://arxiv.org/abs/2301.06237v1,"Separation logic and its variants can describe various properties on pointerprograms. However, when it comes to properties on sequences, one may find ithard to formalize. To deal with properties on variable-length sequences andmultilevel data structures, we propose sequence-heap separation logic whichintegrates sequences into logical reasoning on heap-manipulated programs.Quantifiers over sequence variables and singleton heap storing sequence(sequence singleton heap) are new members in our logic. Further, we study thesatisfiability problem of two fragments. The propositional fragment ofsequence-heap separation logic is decidable, and the fragment with 2alternations on program variables and 1 alternation on sequence variables isundecidable. In addition, we explore boundaries between decidable andundecidable fragments of the logic with prenex normal form.",Tianyue Cao,2023/1/16,2023/1/16,,,"['cs.LO', 'cs.CL']"
2301.02983v1,Mind Reasoning Manners: Enhancing Type Perception for Generalized Zero-shot Logical Reasoning over Text,http://arxiv.org/abs/2301.02983v1,"Logical reasoning task involves diverse types of complex reasoning over text,based on the form of multiple-choice question answering. Given the context,question and a set of options as the input, previous methods achieve superiorperformances on the full-data setting. However, the current benchmark datasethas the ideal assumption that the reasoning type distribution on the trainsplit is close to the test split, which is inconsistent with many realapplication scenarios. To address it, there remain two problems to be studied:(1) How is the zero-shot capability of the models (train on seen types and teston unseen types)? (2) How to enhance the perception of reasoning types for themodels? For problem 1, we propose a new benchmark for generalized zero-shotlogical reasoning, named ZsLR. It includes six splits based on the three typesampling strategies. For problem 2, a type-aware model TaCo is proposed. Itutilizes both the heuristic input reconstruction and the contrastive learningto improve the type perception in the global representation. Extensiveexperiments on both the zero-shot and full-data settings prove the superiorityof TaCo over the state-of-the-art methods. Also, we experiment and verify thegeneralization capability of TaCo on other logical reasoning dataset.",Fangzhi Xu,2023/1/8,2023/1/8,,,['cs.AI']
2212.13894v2,LAMBADA: Backward Chaining for Automated Reasoning in Natural Language,http://arxiv.org/abs/2212.13894v2,"Remarkable progress has been made on automated reasoning with natural text,by using Language Models (LMs) and methods such as Chain-of-Thought andSelection-Inference. These techniques search for proofs in the forwarddirection from axioms to the conclusion, which suffers from a combinatorialexplosion of the search space, and thus high failure rates for problemsrequiring longer chains of reasoning. The classical automated reasoningliterature has shown that reasoning in the backward direction (i.e. from theintended conclusion to supporting axioms) is significantly more efficient atproof-finding. Importing this intuition into the LM setting, we develop aBackward Chaining algorithm, called LAMBADA, that decomposes reasoning intofour sub-modules. These sub-modules are simply implemented by few-shot promptedLM inference. We show that LAMBADA achieves sizable accuracy boosts overstate-of-the-art forward reasoning methods on challenging logical reasoningdatasets, particularly when deep and accurate proof chains are required.",Mehran Kazemi,2022/12/20,2023/5/29,,,"['cs.AI', 'cs.LG']"
2212.09561v5,Large Language Models are Better Reasoners with Self-Verification,http://arxiv.org/abs/2212.09561v5,"Recently, with the chain of thought (CoT) prompting, large language models(LLMs), e.g., GPT-3, have shown strong reasoning ability in several naturallanguage processing tasks such as arithmetic, commonsense, and logicalreasoning. However, LLMs with CoT require multi-step prompting and multi-tokenprediction, which is highly sensitive to individual mistakes and vulnerable toerror accumulation. The above issues make the LLMs need the ability to verifythe answers. In fact, after inferring conclusions in some thinking decisiontasks, people often check them by re-verifying steps to avoid some mistakes. Inthis paper, we propose and prove that LLMs also have similar self-verificationabilities. We take the conclusion obtained by CoT as one of the conditions forsolving the original problem. By performing a backward verification of theanswers that LLM deduced for itself, we can obtain interpretable answervalidation scores to select the candidate answer with the highest score.Experimental results demonstrate that the proposed method can improve thereasoning performance on various arithmetic, commonsense, and logical reasoningdatasets. Our code is publicly available at:https://github.com/WENGSYX/Self-Verification.",Yixuan Weng,2022/12/19,2023/10/19,,,"['cs.AI', 'cs.CL']"
2212.09282v2,APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning,http://arxiv.org/abs/2212.09282v2,"Logical reasoning of text is an important ability that requires understandingthe information present in the text, their interconnections, and then reasoningthrough them to infer new conclusions. Prior works on improving the logicalreasoning ability of language models require complex processing of trainingdata (e.g., aligning symbolic knowledge to text), yielding task-specific dataaugmentation solutions that restrict the learning of general logical reasoningskills. In this work, we propose APOLLO, an adaptively pretrained languagemodel that has improved logical reasoning abilities. We select a subset ofWikipedia, based on a set of logical inference keywords, for continuedpretraining of a language model. We use two self-supervised loss functions: amodified masked language modeling loss where only specific parts-of-speechwords, that would likely require more reasoning than basic languageunderstanding, are masked, and a sentence-level classification loss thatteaches the model to distinguish between entailment and contradiction types ofsentences. The proposed training paradigm is both simple and independent oftask formats. We demonstrate the effectiveness of APOLLO by comparing it withprior baselines on two logical reasoning datasets. APOLLO performs comparablyon ReClor and outperforms baselines on LogiQA. The code base has been madepublicly available.",Soumya Sanyal,2022/12/19,2023/6/5,,,"['cs.CL', 'cs.AI', 'cs.LG']"
2212.04966v1,Towards High-Order Complementary Recommendation via Logical Reasoning Network,http://arxiv.org/abs/2212.04966v1,"Complementary recommendation gains increasing attention in e-commerce sinceit expedites the process of finding frequently-bought-with products for usersin their shopping journey. Therefore, learning the product representation thatcan reflect this complementary relationship plays a central role in modernrecommender systems. In this work, we propose a logical reasoning network,LOGIREC, to effectively learn embeddings of products as well as varioustransformations (projection, intersection, negation) between them. LOGIREC iscapable of capturing the asymmetric complementary relationship between productsand seamlessly extending to high-order recommendations where more comprehensiveand meaningful complementary relationship is learned for a query set ofproducts. Finally, we further propose a hybrid network that is jointlyoptimized for learning a more generic product representation. We demonstratethe effectiveness of our LOGIREC on multiple public real-world datasets interms of various ranking-based metrics under both low-order and high-orderrecommendation scenarios.",Longfeng Wu,2022/12/9,2022/12/9,,,['cs.LG']
2212.03278v1,Counterfactual reasoning: Do language models need world knowledge for causal understanding?,http://arxiv.org/abs/2212.03278v1,"Current pre-trained language models have enabled remarkable improvements indownstream tasks, but it remains difficult to distinguish effects ofstatistical correlation from more systematic logical reasoning grounded onunderstanding of the real world. In this paper we tease these factors apart byleveraging counterfactual conditionals, which force language models to predictunusual consequences based on hypothetical propositions. We introduce a set oftests drawn from psycholinguistic experiments, as well as larger-scalecontrolled datasets, to probe counterfactual predictions from a variety ofpopular pre-trained language models. We find that models are consistently ableto override real-world knowledge in counterfactual scenarios, and that thiseffect is more robust in case of stronger baseline world knowledge -- however,we also find that for most models this effect appears largely to be driven bysimple lexical cues. When we mitigate effects of both world knowledge andlexical cues to test knowledge of linguistic nuances of counterfactuals, wefind that only GPT-3 shows sensitivity to these nuances, though thissensitivity is also non-trivially impacted by lexical associative factors.",Jiaxuan Li,2022/12/6,2022/12/6,,,['cs.CL']
2212.02746v1,UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression,http://arxiv.org/abs/2212.02746v1,"Geometry problem solving is a well-recognized testbed for evaluating thehigh-level multi-modal reasoning capability of deep models. In most existingworks, two main geometry problems: calculation and proving, are usually treatedas two specific tasks, hindering a deep model to unify its reasoning capabilityon multiple math tasks. However, in essence, these two tasks have similarproblem representations and overlapped math knowledge which can improve theunderstanding and reasoning ability of a deep model on both two tasks.Therefore, we construct a large-scale Unified Geometry problem benchmark,UniGeo, which contains 4,998 calculation problems and 9,543 proving problems.Each proving problem is annotated with a multi-step proof with reasons andmathematical expressions. The proof can be easily reformulated as a provingsequence that shares the same formats with the annotated program sequence forcalculation problems. Naturally, we also present a unified multi-task GeometricTransformer framework, Geoformer, to tackle calculation and proving problemssimultaneously in the form of sequence generation, which finally shows thereasoning ability can be improved on both two tasks by unifying formulation.Furthermore, we propose a Mathematical Expression Pretraining (MEP) method thataims to predict the mathematical expressions in the problem solution, thusimproving the Geoformer model. Experiments on the UniGeo demonstrate that ourproposed Geoformer obtains state-of-the-art performance by outperformingtask-specific model NGS with over 5.6% and 3.2% accuracies on calculation andproving problems, respectively.",Jiaqi Chen,2022/12/6,2022/12/6,,,"['cs.AI', 'cs.LG']"
2211.17113v1,Weisfeiler and Leman Go Relational,http://arxiv.org/abs/2211.17113v1,"Knowledge graphs, modeling multi-relational data, improve numerousapplications such as question answering or graph logical reasoning. Many graphneural networks for such data emerged recently, often outperforming shallowarchitectures. However, the design of such multi-relational graph neuralnetworks is ad-hoc, driven mainly by intuition and empirical insights. Up tonow, their expressivity, their relation to each other, and their (practical)learning performance is poorly understood. Here, we initiate the study ofderiving a more principled understanding of multi-relational graph neuralnetworks. Namely, we investigate the limitations in the expressive power of thewell-known Relational GCN and Compositional GCN architectures and shed somelight on their practical learning performance. By aligning both architectureswith a suitable version of the Weisfeiler-Leman test, we establish under whichconditions both models have the same expressive power in distinguishingnon-isomorphic (multi-relational) graphs or vertices with different structuralroles. Further, by leveraging recent progress in designing expressive graphneural networks, we introduce the $k$-RN architecture that provably overcomesthe expressiveness limitations of the above two architectures. Empirically, weconfirm our theoretical findings in a vertex classification setting over smalland large multi-relational graphs.",Pablo Barcelo,2022/11/30,2022/11/30,,,"['cs.LG', 'cs.NE', 'stat.ML']"
2211.15566v2,Neuro-Symbolic Spatio-Temporal Reasoning,http://arxiv.org/abs/2211.15566v2,"Knowledge about space and time is necessary to solve problems in the physicalworld: An AI agent situated in the physical world and interacting with objectsoften needs to reason about positions of and relations between objects; and assoon as the agent plans its actions to solve a task, it needs to consider thetemporal aspect (e.g., what actions to perform over time). Spatio-temporalknowledge, however, is required beyond interacting with the physical world, andis also often transferred to the abstract world of concepts through analogiesand metaphors (e.g., ""a threat that is hanging over our heads""). As spatial andtemporal reasoning is ubiquitous, different attempts have been made tointegrate this into AI systems. In the area of knowledge representation,spatial and temporal reasoning has been largely limited to modeling objects andrelations and developing reasoning methods to verify statements about objectsand relations. On the other hand, neural network researchers have tried toteach models to learn spatial relations from data with limited reasoningcapabilities. Bridging the gap between these two approaches in a mutuallybeneficial way could allow us to tackle many complex real-world problems, suchas natural language processing, visual question answering, and semantic imagesegmentation. In this chapter, we view this integration problem from theperspective of Neuro-Symbolic AI. Specifically, we propose a synergy betweenlogical reasoning and machine learning that will be grounded on spatial andtemporal knowledge. Describing some successful applications, remainingchallenges, and evaluation datasets pertaining to this direction is the maintopic of this contribution.",Jae Hee Lee,2022/11/28,2023/1/13,,,['cs.AI']
2211.13469v3,NQE: N-ary Query Embedding for Complex Query Answering over Hyper-Relational Knowledge Graphs,http://arxiv.org/abs/2211.13469v3,"Complex query answering (CQA) is an essential task for multi-hop and logicalreasoning on knowledge graphs (KGs). Currently, most approaches are limited toqueries among binary relational facts and pay less attention to n-ary facts(n>=2) containing more than two entities, which are more prevalent in the realworld. Moreover, previous CQA methods can only make predictions for a few giventypes of queries and cannot be flexibly extended to more complex logicalqueries, which significantly limits their applications. To overcome thesechallenges, in this work, we propose a novel N-ary Query Embedding (NQE) modelfor CQA over hyper-relational knowledge graphs (HKGs), which include massiven-ary facts. The NQE utilizes a dual-heterogeneous Transformer encoder andfuzzy logic theory to satisfy all n-ary FOL queries, including existentialquantifiers, conjunction, disjunction, and negation. We also propose a parallelprocessing algorithm that can train or predict arbitrary n-ary FOL queries in asingle batch, regardless of the kind of each query, with good flexibility andextensibility. In addition, we generate a new CQA dataset WD50K-NFOL, includingdiverse n-ary FOL queries over WD50K. Experimental results on WD50K-NFOL andother standard CQA datasets show that NQE is the state-of-the-art CQA methodover HKGs with good generalization capability. Our code and dataset arepublicly available.",Haoran Luo,2022/11/24,2023/3/31,,,['cs.AI']
2211.07727v1,Logical Tasks for Measuring Extrapolation and Rule Comprehension,http://arxiv.org/abs/2211.07727v1,"Logical reasoning is essential in a variety of human activities. Arepresentative example of a logical task is mathematics. Recent large-scalemodels trained on large datasets have been successful in various fields, buttheir reasoning ability in arithmetic tasks is limited, which we reproduceexperimentally. Here, we recast this limitation as not unique to mathematicsbut common to tasks that require logical operations. We then propose a new setof tasks, termed logical tasks, which will be the next challenge to address.This higher point of view helps the development of inductive biases that havebroad impact beyond the solution of individual tasks. We define andcharacterize logical tasks and discuss system requirements for their solution.Furthermore, we discuss the relevance of logical tasks to concepts such asextrapolation, explainability, and inductive bias. Finally, we providedirections for solving logical tasks.",Ippei Fujisawa,2022/11/14,2022/11/14,,,"['cs.AI', 'cs.LG']"
2211.10291v1,"Evident: a Development Methodology and a Knowledge Base Topology for Data Mining, Machine Learning and General Knowledge Management",http://arxiv.org/abs/2211.10291v1,"Software has been developed for knowledge discovery, prediction andmanagement for over 30 years. However, there are still unresolved pain pointswhen using existing project development and artifact management methodologies.Historically, there has been a lack of applicable methodologies. Further,methodologies that have been applied, such as Agile, have several limitationsincluding scientific unfalsifiability that reduce their applicability. Evident,a development methodology rooted in the philosophy of logical reasoning andEKB, a knowledge base topology, are proposed. Many pain points in data mining,machine learning and general knowledge management are alleviated conceptually.Evident can be extended potentially to accelerate philosophical exploration,science discovery, education as well as knowledge sharing & retention acrossthe globe. EKB offers one solution of storing information as knowledge, agranular level above data. Related topics in computer history, softwareengineering, database, sensor, philosophy, and project & organization &military managements are also discussed.",Mingwu,2022/11/9,2022/11/9,,,"['cs.AI', 'cs.DB', 'cs.LG', 'cs.SE']"
2211.03252v2,Zero-Shot Classification by Logical Reasoning on Natural Language Explanations,http://arxiv.org/abs/2211.03252v2,"Humans can classify data of an unseen category by reasoning on its languageexplanations. This ability is owing to the compositional nature of language: wecan combine previously seen attributes to describe the new category. Forexample, we might describe a sage thrasher as ""it has a slim straightrelatively short bill, yellow eyes and a long tail"", so that others can usetheir knowledge of attributes ""slim straight relatively short bill"", ""yelloweyes"" and ""long tail"" to recognize a sage thrasher. Inspired by thisobservation, in this work we tackle zero-shot classification task by logicallyparsing and reasoning on natural language expla-nations. To this end, wepropose the framework CLORE (Classification by LOgical Reasoning onExplanations). While previous methods usually regard textual information asimplicit features, CLORE parses explanations into logical structures and thenexplicitly reasons along thess structures on the input to produce aclassification score. Experimental results on explanation-based zero-shotclassification benchmarks demonstrate that CLORE is superior to baselines,which we further show mainly comes from higher scores on tasks requiring morelogical reasoning. We also demonstrate that our framework can be extended tozero-shot classification on visual modality. Alongside classificationdecisions, CLORE can provide the logical parsing and reasoning process as aclear form of rationale. Through empirical analysis we demonstrate that CLOREis also less affected by linguistic biases than baselines.",Chi Han,2022/11/7,2023/5/25,,,['cs.CL']
2211.02481v2,Commentary: Is the moon there if nobody looks -- Bell inequalities and physical reality,http://arxiv.org/abs/2211.02481v2,"Marian Kupczynski(MK)is the author of a controversial paper published (2020)in the journal Frontiers in Physics. The work is built around a mathematicalclaim by MK which is actually false, and MK's logical reasoning around hisclaim is also incorrect. The same claim was made by him in several other recentpapers published in other journals. A proof that the claimed result is false isthe main content of our present ""Comment"". It is purely a mathematicalcounter-example to a mathematical claim in a number of MK's papers.",Richard D. Gill,2022/11/4,2023/1/16,,,['quant-ph']
2210.12487v1,MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure,http://arxiv.org/abs/2210.12487v1,"In this paper, we propose a comprehensive benchmark to investigate models'logical reasoning capabilities in complex real-life scenarios. Currentexplanation datasets often employ synthetic data with simple reasoningstructures. Therefore, it cannot express more complex reasoning processes, suchas the rebuttal to a reasoning step and the degree of certainty of theevidence. To this end, we propose a comprehensive logical reasoning explanationform. Based on the multi-hop chain of reasoning, the explanation form includesthree main components: (1) The condition of rebuttal that the reasoning nodecan be challenged; (2) Logical formulae that uncover the internal texture ofreasoning nodes; (3) Reasoning strength indicated by degrees of certainty. Thefine-grained structure conforms to the real logical reasoning scenario, betterfitting the human cognitive process but, simultaneously, is more challengingfor the current models. We evaluate the current best models' performance onthis new explanation form. The experimental results show that generatingreasoning graphs remains a challenging task for current models, even with thehelp of giant pre-trained language models.",Yinya Huang,2022/10/22,2022/10/22,,,"['cs.AI', 'cs.CL', 'cs.LO']"
2210.08548v1,Investigating the Robustness of Natural Language Generation from Logical Forms via Counterfactual Samples,http://arxiv.org/abs/2210.08548v1,"The aim of Logic2Text is to generate controllable and faithful textsconditioned on tables and logical forms, which not only requires a deepunderstanding of the tables and logical forms, but also warrants symbolicreasoning over the tables. State-of-the-art methods based on pre-trained modelshave achieved remarkable performance on the standard test dataset. However, wequestion whether these methods really learn how to perform logical reasoning,rather than just relying on the spurious correlations between the headers ofthe tables and operators of the logical form. To verify this hypothesis, wemanually construct a set of counterfactual samples, which modify the originallogical forms to generate counterfactual logical forms with rarely co-occurredtable headers and logical operators. SOTA methods give much worse results onthese counterfactual samples compared with the results on the original testdataset, which verifies our hypothesis. To deal with this problem, we firstlyanalyze this bias from a causal perspective, based on which we propose twoapproaches to reduce the model's reliance on the shortcut. The first oneincorporates the hierarchical structure of the logical forms into the model.The second one exploits automatically generated counterfactual data fortraining. Automatic and manual experimental results on the original testdataset and the counterfactual dataset show that our method is effective toalleviate the spurious correlation. Our work points out the weakness ofprevious methods and takes a further step toward developing Logic2Text modelswith real logical reasoning ability.",Chengyuan Liu,2022/10/16,2022/10/16,,,"['cs.CL', 'cs.AI']"
2210.08008v2,Inductive Logical Query Answering in Knowledge Graphs,http://arxiv.org/abs/2210.08008v2,"Formulating and answering logical queries is a standard communicationinterface for knowledge graphs (KGs). Alleviating the notorious incompletenessof real-world KGs, neural methods achieved impressive results in linkprediction and complex query answering tasks by learning representations ofentities, relations, and queries. Still, most existing query answering methodsrely on transductive entity embeddings and cannot generalize to KGs containingnew entities without retraining the entity embeddings. In this work, we studythe inductive query answering task where inference is performed on a graphcontaining new entities with queries over both seen and unseen entities. Tothis end, we devise two mechanisms leveraging inductive node and relationalstructure representations powered by graph neural networks (GNNs).Experimentally, we show that inductive models are able to perform logicalreasoning at inference time over unseen nodes generalizing to graphs up to 500%larger than training ones. Exploring the efficiency--effectiveness trade-off,we find the inductive relational structure representation method generallyachieves higher performance, while the inductive node representation method isable to answer complex queries in the inference-only regime without anytraining on queries and scales to graphs of millions of nodes. Code isavailable at https://github.com/DeepGraphLearning/InductiveQE.",Mikhail Galkin,2022/10/13,2022/11/8,,,"['cs.AI', 'cs.LG']"
2210.02729v3,Join-Chain Network: A Logical Reasoning View of the Multi-head Attention in Transformer,http://arxiv.org/abs/2210.02729v3,"Developing neural architectures that are capable of logical reasoning hasbecome increasingly important for a wide range of applications (e.g., naturallanguage processing). Towards this grand objective, we propose a symbolicreasoning architecture that chains many join operators together to model outputlogical expressions. In particular, we demonstrate that such an ensemble ofjoin-chains can express a broad subset of ''tree-structured'' first-orderlogical expressions, named FOET, which is particularly useful for modelingnatural languages. To endow it with differentiable learning capability, weclosely examine various neural operators for approximating the symbolicjoin-chains. Interestingly, we find that the widely used multi-headself-attention module in transformer can be understood as a special neuraloperator that implements the union bound of the join operator in probabilisticpredicate space. Our analysis not only provides a new perspective on themechanism of the pretrained models such as BERT for natural languageunderstanding but also suggests several important future improvementdirections.",Jianyi Zhang,2022/10/6,2022/10/23,,,"['cs.CL', 'cs.LG']"
2209.14464v1,Neural Methods for Logical Reasoning Over Knowledge Graphs,http://arxiv.org/abs/2209.14464v1,"Reasoning is a fundamental problem for computers and deeply studied inArtificial Intelligence. In this paper, we specifically focus on answeringmulti-hop logical queries on Knowledge Graphs (KGs). This is a complicated taskbecause, in real-world scenarios, the graphs tend to be large and incomplete.Most previous works have been unable to create models that accept fullFirst-Order Logical (FOL) queries, which include negative queries, and haveonly been able to process a limited set of query structures. Additionally, mostmethods present logic operators that can only perform the logical operationthey are made for. We introduce a set of models that use Neural Networks tocreate one-point vector embeddings to answer the queries. The versatility ofneural networks allows the framework to handle FOL queries with Conjunction($\wedge$), Disjunction ($\vee$) and Negation ($\neg$) operators. Wedemonstrate experimentally the performance of our model through extensiveexperimentation on well-known benchmarking datasets. Besides having moreversatile operators, the models achieve a 10\% relative increase over the bestperforming state of the art and more than 30\% over the original method basedon single-point vector embeddings.",Alfonso Amayuelas,2022/9/28,2022/9/28,,,"['cs.AI', 'cs.LG']"
2209.13710v1,Towards Human-Compatible XAI: Explaining Data Differentials with Concept Induction over Background Knowledge,http://arxiv.org/abs/2209.13710v1,"Concept induction, which is based on formal logical reasoning overdescription logics, has been used in ontology engineering in order to createontology (TBox) axioms from the base data (ABox) graph. In this paper, we showthat it can also be used to explain data differentials, for example in thecontext of Explainable AI (XAI), and we show that it can in fact be done in away that is meaningful to a human observer. Our approach utilizes a large classhierarchy, curated from the Wikipedia category hierarchy, as backgroundknowledge.",Cara Widmer,2022/9/27,2022/9/27,,,['cs.AI']
2209.13358v1,Phases of methodological research in biostatistics - building the evidence base for new methods,http://arxiv.org/abs/2209.13358v1,"Although the biostatistical scientific literature publishes new methods at avery high rate, many of these developments are not trustworthy enough to beadopted by the scientific community. We propose a framework to think about howa piece of methodological work contributes to the evidence base for a method.Similarly to the well-known phases of clinical research in drug development, wedefine four phases of methodological research. These four phases cover (I)providing logical reasoning and proofs, (II) providing empirical evidence,first in a narrow target setting, then (III) in an extended range of settingsand for various outcomes, accompanied by appropriate application examples, and(IV) investigations that establish a method as sufficiently well-understood toknow when it is preferred over others and when it is not. We provide basicdefinitions of the four phases but acknowledge that more work is needed tofacilitate unambiguous classification of studies into phases. Methodologicaldevelopments that have undergone all four proposed phases are still rare, butwe give two examples with references. Our concept rebalances the emphasis tostudies in phase III and IV, i.e., carefully planned methods comparison studiesand studies that explore the empirical properties of existing methods in awider range of problems.",Georg Heinze,2022/9/27,2022/9/27,,,"['stat.ME', '62A01 (Primary)']"
2208.13330v1,Time-aware Self-Attention Meets Logic Reasoning in Recommender Systems,http://arxiv.org/abs/2208.13330v1,"At the age of big data, recommender systems have shown remarkable success asa key means of information filtering in our daily life. Recent years havewitnessed the technical development of recommender systems, from perceptionlearning to cognition reasoning which intuitively build the task ofrecommendation as the procedure of logical reasoning and have achievesignificant improvement. However, the logical statement in reasoning implicitlyadmits irrelevance of ordering, even does not consider time information whichplays an important role in many recommendation tasks. Furthermore,recommendation model incorporated with temporal context would tend to beself-attentive, i.e., automatically focus more (less) on the relevance(irrelevance), respectively.  To address these issues, in this paper, we propose a Time-awareSelf-Attention with Neural Collaborative Reasoning (TiSANCR) basedrecommendation model, which integrates temporal patterns and self-attentionmechanism into reasoning-based recommendation. Specially, temporal patternsrepresented by relative time, provide context and auxiliary information tocharacterize the user's preference in recommendation, while self-attention isleveraged to distill informative patterns and suppress irrelevances. Therefore,the fusion of self-attentive temporal information provides deeperrepresentation of user's preference. Extensive experiments on benchmarkdatasets demonstrate that the proposed TiSANCR achieves significant improvementand consistently outperforms the state-of-the-art recommendation methods.",Zhijian Luo,2022/8/29,2022/8/29,,,"['cs.IR', 'cs.AI', 'cs.LG']"
2208.11556v2,Knowledge-based and Data-driven Reasoning and Learning for Ad Hoc Teamwork,http://arxiv.org/abs/2208.11556v2,"We present an architecture for ad hoc teamwork, which refers to collaborationin a team of agents without prior coordination. State of the art methods forthis problem often include a data-driven component that uses a long history ofprior observations to model the behaviour of other agents (or agent types) andto determine the ad hoc agent's behaviour. In many practical domains, it ischallenging to find large training datasets, and necessary to understand andincrementally extend the existing models to account for changes in teamcomposition or domain attributes. Our architecture combines the principles ofknowledge-based and data-driven reasoning and learning. Specifically, we enablean ad hoc agent to perform non-monotonic logical reasoning with priorcommonsense domain knowledge and incrementally-updated simple predictive modelsof other agents' behaviour. We use the benchmark simulated multi-agentcollaboration domain Fort Attack to demonstrate that our architecture supportsadaptation to unforeseen changes, incremental learning and revision of modelsof other agents' behaviour from limited samples, transparency in the ad hocagent's decision making, and better performance than a data-driven baseline.",Hasra Dodampegama,2022/8/24,2022/10/19,,,"['cs.AI', 'cs.MA']"
2207.07238v1,Emotion Recognition in Conversation using Probabilistic Soft Logic,http://arxiv.org/abs/2207.07238v1,"Creating agents that can both appropriately respond to conversations andunderstand complex human linguistic tendencies and social cues has been a longstanding challenge in the NLP community. A recent pillar of research revolvesaround emotion recognition in conversation (ERC); a sub-field of emotionrecognition that focuses on conversations or dialogues that contain two or moreutterances. In this work, we explore an approach to ERC that exploits the useof neural embeddings along with complex structures in dialogues. We implementour approach in a framework called Probabilistic Soft Logic (PSL), adeclarative templating language that uses first-order like logical rules, thatwhen combined with data, define a particular class of graphical model.Additionally, PSL provides functionality for the incorporation of results fromneural models into PSL models. This allows our model to take advantage ofadvanced neural methods, such as sentence embeddings, and logical reasoningover the structure of a dialogue. We compare our method with state-of-the-artpurely neural ERC systems, and see almost a 20% improvement. With theseresults, we provide an extensive qualitative and quantitative analysis over theDailyDialog conversation dataset.",Eriq Augustine,2022/7/14,2022/7/14,,,"['cs.LG', 'cs.CL']"
2206.13998v2,Learning Symmetric Rules with SATNet,http://arxiv.org/abs/2206.13998v2,"SATNet is a differentiable constraint solver with a custom backpropagationalgorithm, which can be used as a layer in a deep-learning system. It is apromising proposal for bridging deep learning and logical reasoning. In fact,SATNet has been successfully applied to learn, among others, the rules of acomplex logical puzzle, such as Sudoku, just from input and output pairs whereinputs are given as images. In this paper, we show how to improve the learningof SATNet by exploiting symmetries in the target rules of a given but unknownlogical puzzle or more generally a logical formula. We present SymSATNet, avariant of SATNet that translates the given symmetries of the target rules to acondition on the parameters of SATNet and requires that the parameters shouldhave a particular parametric form that guarantees the condition. Therequirement dramatically reduces the number of parameters to learn for therules with enough symmetries, and makes the parameter learning of SymSATNetmuch easier than that of SATNet. We also describe a technique for automaticallydiscovering symmetries of the target rules from examples. Our experiments withSudoku and Rubik's cube show the substantial improvement of SymSATNet over thebaseline SATNet.",Sangho Lim,2022/6/28,2022/11/25,,,"['cs.AI', 'cs.LG', 'cs.LO', 'math.GR']"
2206.13174v1,Towards Unifying Perceptual Reasoning and Logical Reasoning,http://arxiv.org/abs/2206.13174v1,"An increasing number of scientific experiments support the view of perceptionas Bayesian inference, which is rooted in Helmholtz's view of perception asunconscious inference. Recent study of logic presents a view of logicalreasoning as Bayesian inference. In this paper, we give a simple probabilisticmodel that is applicable to both perceptual reasoning and logical reasoning. Weshow that the model unifies the two essential processes common in perceptualand logical systems: on the one hand, the process by which perceptual andlogical knowledge is derived from another knowledge, and on the other hand, theprocess by which such knowledge is derived from data. We fully characterise themodel in terms of logical consequence relations.",Hiroyuki Kido,2022/6/27,2022/6/27,,,['cs.AI']
2206.07711v1,On the Eve of True Explainability for OWL Ontologies: Description Logic Proofs with Evee and Evonne (Extended Version),http://arxiv.org/abs/2206.07711v1,"When working with description logic ontologies, understanding entailmentsderived by a description logic reasoner is not always straightforward. So far,the standard ontology editor Prot\'eg\'e offers two services to help:(black-box) justifications for OWL 2 DL ontologies, and (glass-box) proofs forlightweight OWL EL ontologies, where the latter exploits the proof facilitiesof reasoner ELK. Since justifications are often insufficient in explaininginferences, there is thus only little tool support for explaining inferences inmore expressive DLs. In this paper, we introduce EVEE-LIBS, a Java library forcomputing proofs for DLs up to ALCH, and EVEE-PROTEGE, a collection ofProt\'eg\'e plugins for displaying those proofs in Prot\'eg\'e. We also give ashort glimpse of the latest version of EVONNE, a more advanced standaloneapplication for displaying and interacting with proofs computed with EVEE-LIBS.",Christian Alrabbaa,2022/6/15,2022/6/15,,,"['cs.LO', 'cs.AI']"
2206.00426v1,Semantic Probabilistic Layers for Neuro-Symbolic Learning,http://arxiv.org/abs/2206.00426v1,"We design a predictive layer for structured-output prediction (SOP) that canbe plugged into any neural network guaranteeing its predictions are consistentwith a set of predefined symbolic constraints. Our Semantic Probabilistic Layer(SPL) can model intricate correlations, and hard constraints, over a structuredoutput space all while being amenable to end-to-end learning via maximumlikelihood. SPLs combine exact probabilistic inference with logical reasoningin a clean and modular way, learning complex distributions and restrictingtheir support to solutions of the constraint. As such, they can faithfully, andefficiently, model complex SOP tasks beyond the reach of alternativeneuro-symbolic approaches. We empirically demonstrate that SPLs outperformthese competitors in terms of accuracy on challenging SOP tasks includinghierarchical multi-label classification, pathfinding and preference learning,while retaining perfect constraint satisfaction.",Kareem Ahmed,2022/6/1,2022/6/1,,,"['cs.LG', 'cs.AI']"
2205.14591v2,TAR: Neural Logical Reasoning across TBox and ABox,http://arxiv.org/abs/2205.14591v2,"Many ontologies, i.e., Description Logic (DL) knowledge bases, have beendeveloped to provide rich knowledge about various domains. An ontology consistsof an ABox, i.e., assertion axioms between two entities or between a conceptand an entity, and a TBox, i.e., terminology axioms between two concepts.Neural logical reasoning (NLR) is a fundamental task to explore such knowledgebases, which aims at answering multi-hop queries with logical operations basedon distributed representations of queries and answers. While previous NLRmethods can give specific entity-level answers, i.e., ABox answers, they arenot able to provide descriptive concept-level answers, i.e., TBox answers,where each concept is a description of a set of entities. In other words,previous NLR methods only reason over the ABox of an ontology while ignoringthe TBox. In particular, providing TBox answers enables inferring theexplanations of each query with descriptive concepts, which make answerscomprehensible to users and are of great usefulness in the field of appliedontology. In this work, we formulate the problem of neural logical reasoningacross TBox and ABox (TA-NLR), solving which needs to address challenges inincorporating, representing, and operating on concepts. We propose an originalsolution named TAR for TA-NLR. Firstly, we incorporate description logic basedontological axioms to provide the source of concepts. Then, we representconcepts and queries as fuzzy sets, i.e., sets whose elements have degrees ofmembership, to bridge concepts and queries with entities. Moreover, we designoperators involving concepts on top of fuzzy set representation of concepts andqueries for optimization and inference. Extensive experimental results on tworeal-world datasets demonstrate the effectiveness of TAR for TA-NLR.",Zhenwei Tang,2022/5/29,2022/8/16,,,"['cs.AI', 'cs.LG', 'cs.LO']"
2205.12898v1,Reasoning over Logically Interacted Conditions for Question Answering,http://arxiv.org/abs/2205.12898v1,"Some questions have multiple answers that are not equally correct, i.e.answers are different under different conditions. Conditions are used todistinguish answers as well as to provide additional information to supportthem. In this paper, we study a more challenging task where answers areconstrained by a list of conditions that logically interact, which requiresperforming logical reasoning over the conditions to determine the correctnessof the answers. Even more challenging, we only provide evidences for a subsetof the conditions, so some questions may not have deterministic answers. Insuch cases, models are asked to find probable answers and identify conditionsthat need to be satisfied to make the answers correct. We propose a new model,TReasoner, for this challenging reasoning task. TReasoner consists of anentailment module, a reasoning module, and a generation module (if the answersare free-form text spans). TReasoner achieves state-of-the-art performance ontwo benchmark conditional QA datasets, outperforming the previousstate-of-the-art by 3-10 points.",Haitian Sun,2022/5/25,2022/5/25,,,"['cs.CL', 'cs.AI']"
2205.12598v2,RobustLR: Evaluating Robustness to Logical Perturbation in Deductive Reasoning,http://arxiv.org/abs/2205.12598v2,"Transformers have been shown to be able to perform deductive reasoning on alogical rulebase containing rules and statements written in English naturallanguage. While the progress is promising, it is currently unclear if thesemodels indeed perform logical reasoning by understanding the underlying logicalsemantics in the language. To this end, we propose RobustLR, a suite ofevaluation datasets that evaluate the robustness of these models to minimallogical edits in rulebases and some standard logical equivalence conditions. Inour experiments with RoBERTa and T5, we find that the models trained in priorworks do not perform consistently on the different perturbations in RobustLR,thus showing that the models are not robust to the proposed logicalperturbations. Further, we find that the models find it especially hard tolearn logical negation and disjunction operators. Overall, using our evaluationsets, we demonstrate some shortcomings of the deductive reasoning-basedlanguage models, which can eventually help towards designing better models forlogical reasoning over natural language. All the datasets and code base havebeen made publicly available.",Soumya Sanyal,2022/5/25,2022/11/8,,,"['cs.CL', 'cs.LG', 'cs.LO']"
2205.11502v2,On the Paradox of Learning to Reason from Data,http://arxiv.org/abs/2205.11502v2,"Logical reasoning is needed in a wide range of NLP tasks. Can a BERT model betrained end-to-end to solve logical reasoning problems presented in naturallanguage? We attempt to answer this question in a confined problem space wherethere exists a set of parameters that perfectly simulates logical reasoning. Wemake observations that seem to contradict each other: BERT attains near-perfectaccuracy on in-distribution test examples while failing to generalize to otherdata distributions over the exact same problem space. Our study provides anexplanation for this paradox: instead of learning to emulate the correctreasoning function, BERT has in fact learned statistical features thatinherently exist in logical reasoning problems. We also show that it isinfeasible to jointly remove statistical features from data, illustrating thedifficulty of learning to reason in general. Our result naturally extends toother neural models and unveils the fundamental difference between learning toreason and learning to achieve high performance on NLP benchmarks usingstatistical features.",Honghua Zhang,2022/5/23,2022/5/24,,,"['cs.CL', 'cs.AI']"
2205.11432v3,Logical Reasoning with Span-Level Predictions for Interpretable and Robust NLI Models,http://arxiv.org/abs/2205.11432v3,"Current Natural Language Inference (NLI) models achieve impressive results,sometimes outperforming humans when evaluating on in-distribution test sets.However, as these models are known to learn from annotation artefacts anddataset biases, it is unclear to what extent the models are learning the taskof NLI instead of learning from shallow heuristics in their training data. Weaddress this issue by introducing a logical reasoning framework for NLI,creating highly transparent model decisions that are based on logical rules.Unlike prior work, we show that improved interpretability can be achievedwithout decreasing the predictive accuracy. We almost fully retain performanceon SNLI, while also identifying the exact hypothesis spans that are responsiblefor each model prediction. Using the e-SNLI human explanations, we verify thatour model makes sensible decisions at a span level, despite not using any spanlabels during training. We can further improve model performance and span-leveldecisions by using the e-SNLI explanations during training. Finally, our modelis more robust in a reduced data setting. When training with only 1,000examples, out-of-distribution performance improves on the MNLI matched andmismatched validation sets by 13% and 16% relative to the baseline. Trainingwith fewer observations yields further improvements, both in-distribution andout-of-distribution.",Joe Stacey,2022/5/23,2022/10/21,,,"['cs.CL', 'cs.LG']"
2205.11177v1,"Consistency of UML class, object and statechart diagrams using ontology reasoners",http://arxiv.org/abs/2205.11177v1,"We propose an automatic approach to analyze the consistency andsatisfiability of Unified Modeling Language UML models containing multipleclass, object and statechart diagrams using logic reasoners for the WebOntology Language OWL 2. We describe how to translate UML models in OWL 2 andwe present a tool chain implementing this translation that can be used with anystandard compliant UML modeling tool. The proposed approach is limited inscope, but is fully automatic and does not require any expertise about OWL 2and its reasoners from the designer.",Ali Hanzala Khan,2022/5/23,2022/5/23,,,"['cs.AI', 'cs.CL', 'cs.SE']"
2205.08794v2,LogiGAN: Learning Logical Reasoning via Adversarial Pre-training,http://arxiv.org/abs/2205.08794v2,"We present LogiGAN, an unsupervised adversarial pre-training framework forimproving logical reasoning abilities of language models. Upon automaticidentifying logical reasoning phenomena in massive text corpus via detectionheuristics, we train language models to predict the masked-out logicalstatements. Inspired by the facilitation effect of reflective thinking in humanlearning, we analogically simulate the learning-thinking process with anadversarial Generator-Verifier architecture to assist logic learning. LogiGANimplements a novel sequential GAN approach that (a) circumvents thenon-differentiable challenge of the sequential GAN by leveraging the Generatoras a sentence-level generative likelihood scorer with a learning objective ofreaching scoring consensus with the Verifier; (b) is computationally feasiblefor large-scale pre-training with arbitrary target length. Both base and largesize language models pre-trained with LogiGAN demonstrate obvious performanceimprovement on 12 datasets requiring general reasoning abilities, revealing thefundamental role of logic in broad reasoning, as well as the effectiveness ofLogiGAN. Ablation studies on LogiGAN components reveal the relativeorthogonality between linguistic and logic abilities and suggest thatreflective thinking's facilitation effect might also generalize to machinelearning.",Xinyu Pi,2022/5/18,2022/12/9,,,['cs.CL']
2205.04423v1,Graph Neural Networks for Propositional Model Counting,http://arxiv.org/abs/2205.04423v1,"Graph Neural Networks (GNNs) have been recently leveraged to solve severallogical reasoning tasks. Nevertheless, counting problems such as propositionalmodel counting (#SAT) are still mostly approached with traditional solvers.Here we tackle this gap by presenting an architecture based on the GNNframework for belief propagation (BP) of Kuch et al., extended withself-attentive GNN and trained to approximately solve the #SAT problem. We rana thorough experimental investigation, showing that our model, trained on asmall set of random Boolean formulae, is able to scale effectively to muchlarger problem sizes, with comparable or better performances of state of theart approximate solvers. Moreover, we show that it can be efficientlyfine-tuned to provide good generalization results on different formulaedistributions, such as those coming from SAT-encoded combinatorial problems.",Gaia Saveri,2022/5/9,2022/5/9,,,"['cs.AI', 'cs.LG']"
2205.00731v2,Logiformer: A Two-Branch Graph Transformer Network for Interpretable Logical Reasoning,http://arxiv.org/abs/2205.00731v2,"Machine reading comprehension has aroused wide concerns, since it exploresthe potential of model for text understanding. To further equip the machinewith the reasoning capability, the challenging task of logical reasoning isproposed. Previous works on logical reasoning have proposed some strategies toextract the logical units from different aspects. However, there still remainsa challenge to model the long distance dependency among the logical units.Also, it is demanding to uncover the logical structures of the text and furtherfuse the discrete logic to the continuous text embedding. To tackle the aboveissues, we propose an end-to-end model Logiformer which utilizes a two-branchgraph transformer network for logical reasoning of text. Firstly, we introducedifferent extraction strategies to split the text into two sets of logicalunits, and construct the logical graph and the syntax graph respectively. Thelogical graph models the causal relations for the logical branch while thesyntax graph captures the co-occurrence relations for the syntax branch.Secondly, to model the long distance dependency, the node sequence from eachgraph is fed into the fully connected graph transformer structures. The twoadjacent matrices are viewed as the attention biases for the graph transformerlayers, which map the discrete logical structures to the continuous textembedding space. Thirdly, a dynamic gate mechanism and a question-awareself-attention module are introduced before the answer prediction to update thefeatures. The reasoning process provides the interpretability by employing thelogical units, which are consistent with human cognition. The experimentalresults show the superiority of our model, which outperforms thestate-of-the-art single model on two logical reasoning benchmarks.",Fangzhi Xu,2022/5/2,2022/7/8,,,['cs.CL']
2204.10382v1,"Facilitating automated conversion of scientific knowledge into scientific simulation models with the Machine Assisted Generation, Calibration, and Comparison (MAGCC) Framework",http://arxiv.org/abs/2204.10382v1,"The Machine Assisted Generation, Comparison, and Calibration (MAGCC)framework provides machine assistance and automation of recurrent crucial stepsand processes in the development, implementation, testing, and use ofscientific simulation models. MAGCC bridges systems for knowledge extractionvia natural language processing or extracted from existing mathematical modelsand provides a comprehensive workflow encompassing the composition ofscientific models and artificial intelligence (AI) assisted code generation.MAGCC accomplishes this through: 1) the development of a comprehensivelyexpressive formal knowledge representation knowledgebase, the StructuredScientific Knowledge Representation (SSKR) that encompasses all the types ofinformation needed to make any simulation model, 2) the use of an artificiallyintelligent logic reasoning system, the Computational Modeling Assistant (CMA),that takes information from the SSKR and generates, in a traceable fashion,model specifications across a range of simulation modeling methods, and 3) theuse of the CMA to generate executable code for a simulation model from thosemodel specifications. The MAGCC framework can be customized any scientificdomain, and future work will integrate newly developed code-generating AIsystems.",Chase Cockrell,2022/4/21,2022/4/21,,,"['cs.AI', 'cs.LG']"
2204.08753v1,Table-based Fact Verification with Self-adaptive Mixture of Experts,http://arxiv.org/abs/2204.08753v1,"The table-based fact verification task has recently gained widespreadattention and yet remains to be a very challenging problem. It inherentlyrequires informative reasoning over natural language together with differentnumerical and logical reasoning on tables (e.g., count, superlative,comparative). Considering that, we exploit mixture-of-experts and present inthis paper a new method: Self-adaptive Mixture-of-Experts Network (SaMoE).Specifically, we have developed a mixture-of-experts neural network torecognize and execute different types of reasoning -- the network is composedof multiple experts, each handling a specific part of the semantics forreasoning, whereas a management module is applied to decide the contribution ofeach expert network to the verification result. A self-adaptive method isdeveloped to teach the management module combining results of different expertsmore efficiently without external knowledge. The experimental resultsillustrate that our framework achieves 85.1% accuracy on the benchmark datasetTabFact, comparable with the previous state-of-the-art models. We hope ourframework can serve as a new baseline for table-based verification. Our code isavailable at https://github.com/THUMLP/SaMoE.",Yuxuan Zhou,2022/4/19,2022/4/19,,,"['cs.AI', 'cs.CL', 'cs.LG']"
2204.04680v1,Reasoning with Multi-Structure Commonsense Knowledge in Visual Dialog,http://arxiv.org/abs/2204.04680v1,"Visual Dialog requires an agent to engage in a conversation with humansgrounded in an image. Many studies on Visual Dialog focus on the understandingof the dialog history or the content of an image, while a considerable amountof commonsense-required questions are ignored. Handling these scenarios dependson logical reasoning that requires commonsense priors. How to capture relevantcommonsense knowledge complementary to the history and the image remains a keychallenge. In this paper, we propose a novel model by Reasoning withMulti-structure Commonsense Knowledge (RMK). In our model, the externalknowledge is represented with sentence-level facts and graph-level facts, toproperly suit the scenario of the composite of dialog history and image. On topof these multi-structure representations, our model can capture relevantknowledge and incorporate them into the vision and semantic features, viagraph-based interaction and transformer-based fusion. Experimental results andanalysis on VisDial v1.0 and VisDialCK datasets show that our proposed modeleffectively outperforms comparative methods.",Shunyu Zhang,2022/4/10,2022/4/10,,,"['cs.CV', 'cs.MM']"
2203.14487v1,Enhancing Neural Mathematical Reasoning by Abductive Combination with Symbolic Library,http://arxiv.org/abs/2203.14487v1,"Mathematical reasoning recently has been shown as a hard challenge for neuralsystems. Abilities including expression translation, logical reasoning, andmathematics knowledge acquiring appear to be essential to overcome thechallenge. This paper demonstrates that some abilities can be achieved throughabductive combination with discrete systems that have been programmed withhuman knowledge. On a mathematical reasoning dataset, we adopt the recentlyproposed abductive learning framework, and propose the ABL-Sym algorithm thatcombines the Transformer neural models with a symbolic mathematics library.ABL-Sym shows 9.73% accuracy improvement on the interpolation tasks and 47.22%accuracy improvement on the extrapolation tasks, over the state-of-the-artapproaches. Online demonstration: http://math.polixir.ai",Yangyang Hu,2022/3/28,2022/3/28,,,"['cs.LG', 'cs.AI']"
2203.13953v1,A Densely Connected Criss-Cross Attention Network for Document-level Relation Extraction,http://arxiv.org/abs/2203.13953v1,"Document-level relation extraction (RE) aims to identify relations betweentwo entities in a given document. Compared with its sentence-level counterpart,document-level RE requires complex reasoning. Previous research normallycompleted reasoning through information propagation on the mention-level orentity-level document-graph, but rarely considered reasoning at theentity-pair-level.In this paper, we propose a novel model, called DenselyConnected Criss-Cross Attention Network (Dense-CCNet), for document-level RE,which can complete logical reasoning at the entity-pair-level. Specifically,the Dense-CCNet performs entity-pair-level logical reasoning through theCriss-Cross Attention (CCA), which can collect contextual information inhorizontal and vertical directions on the entity-pair matrix to enhance thecorresponding entity-pair representation. In addition, we densely connectmultiple layers of the CCA to simultaneously capture the features of single-hopand multi-hop logical reasoning.We evaluate our Dense-CCNet model on threepublic document-level RE datasets, DocRED, CDR, and GDA. Experimental resultsdemonstrate that our model achieves state-of-the-art performance on these threedatasets.",Liang Zhang,2022/3/26,2022/3/26,,,['cs.CL']
2203.12186v1,AbductionRules: Training Transformers to Explain Unexpected Inputs,http://arxiv.org/abs/2203.12186v1,"Transformers have recently been shown to be capable of reliably performinglogical reasoning over facts and rules expressed in natural language, butabductive reasoning - inference to the best explanation of an unexpectedobservation - has been underexplored despite significant applications toscientific discovery, common-sense reasoning, and model interpretability.  We present AbductionRules, a group of natural language datasets designed totrain and test generalisable abduction over natural-language knowledge bases.We use these datasets to finetune pretrained Transformers and discuss theirperformance, finding that our models learned generalisable abductive techniquesbut also learned to exploit the structure of our data. Finally, we discuss theviability of this approach to abductive reasoning and ways in which it may beimproved in future work.",Nathan Young,2022/3/23,2022/3/23,,,['cs.CL']
2203.10557v2,A Neural-Symbolic Approach to Natural Language Understanding,http://arxiv.org/abs/2203.10557v2,"Deep neural networks, empowered by pre-trained language models, have achievedremarkable results in natural language understanding (NLU) tasks. However,their performances can drastically deteriorate when logical reasoning isneeded. This is because NLU in principle depends on not only analogicalreasoning, which deep neural networks are good at, but also logical reasoning.According to the dual-process theory, analogical reasoning and logicalreasoning are respectively carried out by System 1 and System 2 in the humanbrain. Inspired by the theory, we present a novel framework for NLU calledNeural-Symbolic Processor (NSP), which performs analogical reasoning based onneural processing and logical reasoning based on both neural and symbolicprocessing. As a case study, we conduct experiments on two NLU tasks, questionanswering (QA) and natural language inference (NLI), when numerical reasoning(a type of logical reasoning) is necessary. The experimental results show thatour method significantly outperforms state-of-the-art methods in both tasks.",Zhixuan Liu,2022/3/20,2022/10/21,,,['cs.CL']
2203.10261v1,FaiRR: Faithful and Robust Deductive Reasoning over Natural Language,http://arxiv.org/abs/2203.10261v1,"Transformers have been shown to be able to perform deductive reasoning on alogical rulebase containing rules and statements written in natural language.Recent works show that such models can also produce the reasoning steps (i.e.,the proof graph) that emulate the model's logical reasoning process. Currently,these black-box models generate both the proof graph and intermediateinferences within the same model and thus may be unfaithful. In this work, weframe the deductive logical reasoning task by defining three modularcomponents: rule selection, fact selection, and knowledge composition. The ruleand fact selection steps select the candidate rule and facts to be used andthen the knowledge composition combines them to generate new inferences. Thisensures model faithfulness by assured causal relation from the proof step tothe inference reasoning. To test our framework, we propose FaiRR (Faithful andRobust Reasoner) where the above three components are independently modeled bytransformers. We observe that FaiRR is robust to novel language perturbations,and is faster at inference than previous works on existing reasoning datasets.Additionally, in contrast to black-box generative models, the errors made byFaiRR are more interpretable due to the modular approach.",Soumya Sanyal,2022/3/19,2022/3/19,,,"['cs.CL', 'cs.AI', 'cs.LG']"
2203.10244v1,ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning,http://arxiv.org/abs/2203.10244v1,"Charts are very popular for analyzing data. When exploring charts, peopleoften ask a variety of complex reasoning questions that involve several logicaland arithmetic operations. They also commonly refer to visual features of achart in their questions. However, most existing datasets do not focus on suchcomplex reasoning questions as their questions are template-based and answerscome from a fixed-vocabulary. In this work, we present a large-scale benchmarkcovering 9.6K human-written questions as well as 23.1K questions generated fromhuman-written chart summaries. To address the unique challenges in ourbenchmark involving visual and logical reasoning over charts, we present twotransformer-based models that combine visual features and the data table of thechart in a unified way to answer questions. While our models achieve thestate-of-the-art results on the previous datasets as well as on our benchmark,the evaluation also reveals several challenges in answering complex reasoningquestions.",Ahmed Masry,2022/3/19,2022/3/19,,,['cs.CL']
2203.08992v1,AdaLoGN: Adaptive Logic Graph Network for Reasoning-Based Machine Reading Comprehension,http://arxiv.org/abs/2203.08992v1,"Recent machine reading comprehension datasets such as ReClor and LogiQArequire performing logical reasoning over text. Conventional neural models areinsufficient for logical reasoning, while symbolic reasoners cannot directlyapply to text. To meet the challenge, we present a neural-symbolic approachwhich, to predict an answer, passes messages over a graph representing logicalrelations between text units. It incorporates an adaptive logic graph network(AdaLoGN) which adaptively infers logical relations to extend the graph and,essentially, realizes mutual and iterative reinforcement between neural andsymbolic reasoning. We also implement a novel subgraph-to-node message passingmechanism to enhance context-option interaction for answering multiple-choicequestions. Our approach shows promising results on ReClor and LogiQA.",Xiao Li,2022/3/16,2022/3/16,,,"['cs.CL', 'cs.AI', 'cs.NE', 'cs.SC']"
2203.06342v1,What Makes Reading Comprehension Questions Difficult?,http://arxiv.org/abs/2203.06342v1,"For a natural language understanding benchmark to be useful in research, ithas to consist of examples that are diverse and difficult enough todiscriminate among current and near-future state-of-the-art systems. However,we do not yet know how best to select text sources to collect a variety ofchallenging examples. In this study, we crowdsource multiple-choice readingcomprehension questions for passages taken from seven qualitatively distinctsources, analyzing what attributes of passages contribute to the difficulty andquestion types of the collected examples. To our surprise, we find that passagesource, length, and readability measures do not significantly affect questiondifficulty. Through our manual annotation of seven reasoning types, we observeseveral trends between passage sources and reasoning types, e.g., logicalreasoning is more often required in questions written for technical passages.These results suggest that when creating a new benchmark dataset, selecting adiverse set of passages can help ensure a diverse range of question types, butthat passage difficulty need not be a priority.",Saku Sugawara,2022/3/12,2022/3/12,,,"['cs.CL', 'cs.AI']"
2203.05032v1,A Neural Programming Language for the Reservoir Computer,http://arxiv.org/abs/2203.05032v1,"From logical reasoning to mental simulation, biological and artificial neuralsystems possess an incredible capacity for computation. Such neural computersoffer a fundamentally novel computing paradigm by representing datacontinuously and processing information in a natively parallel and distributedmanner. To harness this computation, prior work has developed extensivetraining techniques to understand existing neural networks. However, the lackof a concrete and low-level programming language for neural networks precludesus from taking full advantage of a neural computing framework. Here, we providesuch a programming language using reservoir computing -- a simple recurrentneural network -- and close the gap between how we conceptualize and implementneural computers and silicon computers. By decomposing the reservoir's internalrepresentation and dynamics into a symbolic basis of its inputs, we define alow-level neural machine code that we use to program the reservoir to solvecomplex equations and store chaotic dynamical systems as random access memory(dRAM). Using this representation, we provide a fully distributed neuralimplementation of software virtualization and logical circuits, and evenprogram a playable game of pong inside of a reservoir computer. Taken together,we define a concrete, practical, and fully generalizable implementation ofneural computation.",Jason Z. Kim,2022/3/9,2022/3/9,,,"['cond-mat.dis-nn', 'math.DS', 'nlin.CD']"
2203.00357v1,MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning,http://arxiv.org/abs/2203.00357v1,"Logical reasoning is of vital importance to natural language understanding.Previous studies either employ graph-based models to incorporate priorknowledge about logical relations, or introduce symbolic logic into neuralmodels through data augmentation. These methods, however, heavily depend onannotated training data, and thus suffer from over-fitting and poorgeneralization problems due to the dataset sparsity. To address these twoproblems, in this paper, we propose MERIt, a MEta-path guided contrastivelearning method for logical ReasonIng of text, to perform self-supervisedpre-training on abundant unlabeled text data. Two novel strategies serve asindispensable components of our method. In particular, a strategy based onmeta-path is devised to discover the logical structure in natural texts,followed by a counterfactual data augmentation strategy to eliminate theinformation shortcut induced by pre-training. The experimental results on twochallenging logical reasoning benchmarks, i.e., ReClor and LogiQA, demonstratethat our method outperforms the SOTA baselines with significant improvements.",Fangkai Jiao,2022/3/1,2022/3/1,,,"['cs.CL', 'cs.AI']"
2202.13406v1,Towards Unifying Logical Entailment and Statistical Estimation,http://arxiv.org/abs/2202.13406v1,"This paper gives a generative model of the interpretation of formal logic fordata-driven logical reasoning. The key idea is to represent the interpretationas likelihood of a formula being true given a model of formal logic. Using thelikelihood, Bayes' theorem gives the posterior of the model being the casegiven the formula. The posterior represents an inverse interpretation of formallogic that seeks models making the formula true. The likelihood and posteriorcause Bayesian learning that gives the probability of the conclusion being truein the models where all the premises are true. This paper looks at statisticaland logical properties of the Bayesian learning. It is shown that thegenerative model is a unified theory of several different types of reasoning inlogic and statistics.",Hiroyuki Kido,2022/2/27,2022/2/27,,,['cs.AI']
2202.12512v1,MUC-driven Feature Importance Measurement and Adversarial Analysis for Random Forest,http://arxiv.org/abs/2202.12512v1,"The broad adoption of Machine Learning (ML) in security-critical fieldsdemands the explainability of the approach. However, the research onunderstanding ML models, such as Random Forest (RF), is still in its infantstage. In this work, we leverage formal methods and logical reasoning todevelop a novel model-specific method for explaining the prediction of RF. Ourapproach is centered around Minimal Unsatisfiable Cores (MUC) and provides acomprehensive solution for feature importance, covering local and globalaspects, and adversarial sample analysis. Experimental results on severaldatasets illustrate the high quality of our feature importance measurement. Wealso demonstrate that our adversarial analysis outperforms the state-of-the-artmethod. Moreover, our method can produce a user-centered report, which helpsprovide recommendations in real-life applications.",Shucen Ma,2022/2/25,2022/2/25,,,['cs.LG']
2202.09868v1,ExAIS: Executable AI Semantics,http://arxiv.org/abs/2202.09868v1,"Neural networks can be regarded as a new programming paradigm, i.e., insteadof building ever-more complex programs through (often informal) logicalreasoning in the programmers' mind, complex 'AI' systems are built byoptimising generic neural network models with big data. In this new paradigm,AI frameworks such as TensorFlow and PyTorch play a key role, which is asessential as the compiler for traditional programs. It is known that the lackof a proper semantics for programming languages (such as C), i.e., acorrectness specification for compilers, has contributed to many problematicprogram behaviours and security issues. While it is in general hard to have acorrectness specification for compilers due to the high complexity ofprogramming languages and their rapid evolution, we have a unique opportunityto do it right this time for neural networks (which have a limited set offunctions, and most of them have stable semantics). In this work, we report oureffort on providing a correctness specification of neural network frameworkssuch as TensorFlow. We specify the semantics of almost all TensorFlow layers inthe logical programming language Prolog. We demonstrate the usefulness of thesemantics through two applications. One is a fuzzing engine for TensorFlow,which features a strong oracle and a systematic way of generating valid neuralnetworks. The other is a model validation approach which enables consistent bugreporting for TensorFlow models.",Richard Schumi,2022/2/20,2022/2/20,,,"['cs.AI', 'cs.LG']"
2202.09836v1,Automated Reasoning in Non-classical Logics in the TPTP World,http://arxiv.org/abs/2202.09836v1,"Non-classical logics are used in a wide spectrum of disciplines, includingartificial intelligence, computer science, mathematics, and philosophy. Thede-facto standard infrastructure for automated theorem proving, the TPTP World,currently supports only classical logics. Similar standards for non-classicallogic reasoning do not exist (yet). This hampers practical development ofreasoning systems, and limits their interoperability and application. Thispaper describes the latest extension of the TPTP World, which provideslanguages and infrastructure for reasoning in non-classical logics. Theextensions integrate seamlessly with the existing TPTP World.",Alexander Steen,2022/2/20,2022/2/20,,,"['cs.AI', '68T30 (Primary) 68T27, 03B45, 03B60 (Secondary)']"
2202.05826v3,End-to-end Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking,http://arxiv.org/abs/2202.05826v3,"Machine learning systems perform well on pattern matching tasks, but theirability to perform algorithmic or logical reasoning is not well understood. Oneimportant reasoning capability is algorithmic extrapolation, in which modelstrained only on small/simple reasoning problems can synthesize complexstrategies for large/complex problems at test time. Algorithmic extrapolationcan be achieved through recurrent systems, which can be iterated many times tosolve difficult reasoning problems. We observe that this approach fails toscale to highly complex problems because behavior degenerates when manyiterations are applied -- an issue we refer to as ""overthinking."" We propose arecall architecture that keeps an explicit copy of the problem instance inmemory so that it cannot be forgotten. We also employ a progressive trainingroutine that prevents the model from learning behaviors that are specific toiteration number and instead pushes it to learn behaviors that can be repeatedindefinitely. These innovations prevent the overthinking problem, and enablerecurrent systems to solve extremely hard extrapolation tasks.",Arpit Bansal,2022/2/11,2022/10/14,,,"['cs.LG', 'cs.AI']"
2202.04161v1,Logical Reasoning for Task Oriented Dialogue Systems,http://arxiv.org/abs/2202.04161v1,"In recent years, large pretrained models have been used in dialogue systemsto improve successful task completion rates. However, lack of reasoningcapabilities of dialogue platforms make it difficult to provide relevant andfluent responses, unless the designers of a conversational experience spend aconsiderable amount of time implementing these capabilities in external rulebased modules. In this work, we propose a novel method to fine-tune pretrainedtransformer models such as Roberta and T5. to reason over a set of facts in agiven dialogue context. Our method includes a synthetic data generationmechanism which helps the model learn logical relations, such as comparisonbetween list of numerical values, inverse relations (and negation), inclusionand exclusion for categorical attributes, and application of a combination ofattributes over both numerical and categorical values, and spoken form fornumerical values, without need for additional training dataset. We show thatthe transformer based model can perform logical reasoning to answer questionswhen the dialogue context contains all the required information, otherwise itis able to extract appropriate constraints to pass to downstream components(e.g. a knowledge base) when partial information is available. We observe thattransformer based models such as UnifiedQA-T5 can be fine-tuned to performlogical reasoning (such as numerical and categorical attributes' comparison)over attributes that been seen in training time (e.g., accuracy of 90\%+ forcomparison of smaller than $k_{\max}$=5 values over heldout test dataset).",Sajjad Beygi,2022/2/8,2022/2/8,,,"['cs.CL', 'cs.AI']"
2202.04178v2,VAEL: Bridging Variational Autoencoders and Probabilistic Logic Programming,http://arxiv.org/abs/2202.04178v2,"We present VAEL, a neuro-symbolic generative model integrating variationalautoencoders (VAE) with the reasoning capabilities of probabilistic logic (L)programming. Besides standard latent subsymbolic variables, our model exploitsa probabilistic logic program to define a further structured representation,which is used for logical reasoning. The entire process is end-to-enddifferentiable. Once trained, VAEL can solve new unseen generation tasks by (i)leveraging the previously acquired knowledge encoded in the neural componentand (ii) exploiting new logical programs on the structured latent space. Ourexperiments provide support on the benefits of this neuro-symbolic integrationboth in terms of task generalization and data efficiency. To the best of ourknowledge, this work is the first to propose a general-purpose end-to-endframework integrating probabilistic logic programming into a deep generativemodel.",Eleonora Misino,2022/2/7,2022/5/25,,,"['cs.PL', 'cs.LG']"
2202.02436v2,Neural Logic Analogy Learning,http://arxiv.org/abs/2202.02436v2,"Letter-string analogy is an important analogy learning task which seems to beeasy for humans but very challenging for machines. The main idea behind currentapproaches to solving letter-string analogies is to design heuristic rules forextracting analogy structures and constructing analogy mappings. However, onekey problem is that it is difficult to build a comprehensive and exhaustive setof analogy structures which can fully describe the subtlety of analogies. Thisproblem makes current approaches unable to handle complicated letter-stringanalogy problems. In this paper, we propose Neural logic analogy learning(Noan), which is a dynamic neural architecture driven by differentiable logicreasoning to solve analogy problems. Each analogy problem is converted intological expressions consisting of logical variables and basic logicaloperations (AND, OR, and NOT). More specifically, Noan learns the logicalvariables as vector embeddings and learns each logical operation as a neuralmodule. In this way, the model builds computational graph integrating neuralnetwork with logical reasoning to capture the internal logical structure of theinput letter strings. The analogy learning problem then becomes a True/Falseevaluation problem of the logical expressions. Experiments show that ourmachine learning-based Noan approach outperforms state-of-the-art approaches onstandard letter-string analogy benchmark datasets.",Yujia Fan,2022/2/4,2022/4/8,,,"['cs.CL', 'cs.AI', 'cs.IR', 'cs.LG', 'cs.LO']"
2202.00531v2,PRIMA: Planner-Reasoner Inside a Multi-task Reasoning Agent,http://arxiv.org/abs/2202.00531v2,"We consider the problem of multi-task reasoning (MTR), where an agent cansolve multiple tasks via (first-order) logic reasoning. This capability isessential for human-like intelligence due to its strong generalizability andsimplicity for handling multiple tasks. However, a major challenge indeveloping effective MTR is the intrinsic conflict between reasoning capabilityand efficiency. An MTR-capable agent must master a large set of ""skills"" totackle diverse tasks, but executing a particular task at the inference stagerequires only a small subset of immediately relevant skills. How can wemaintain broad reasoning capability and also efficient specific-taskperformance? To address this problem, we propose a Planner-Reasoner frameworkcapable of state-of-the-art MTR capability and high efficiency. The Reasonermodels shareable (first-order) logic deduction rules, from which the Plannerselects a subset to compose into efficient reasoning paths. The entire model istrained in an end-to-end manner using deep reinforcement learning, andexperimental studies over a variety of domains validate its effectiveness.",Daoming Lyu,2022/2/1,2022/2/13,,,['cs.AI']
2201.11473v2,Reasoning Like Program Executors,http://arxiv.org/abs/2201.11473v2,"Reasoning over natural language is a long-standing goal for the researchcommunity. However, studies have shown that existing language models areinadequate in reasoning. To address the issue, we present POET, a novelreasoning pre-training paradigm. Through pre-training language models withprograms and their execution results, POET empowers language models to harvestthe reasoning knowledge possessed by program executors via a data-drivenapproach. POET is conceptually simple and can be instantiated by differentkinds of program executors. In this paper, we showcase two simple instancesPOET-Math and POET-Logic, in addition to a complex instance, POET-SQL.Experimental results on six benchmarks demonstrate that POET can significantlyboost model performance in natural language reasoning, such as numericalreasoning, logical reasoning, and multi-hop reasoning. POET opens a new gate onreasoning-enhancement pre-training, and we hope our analysis would shed lighton the future research of reasoning like program executors.",Xinyu Pi,2022/1/27,2022/10/22,,,"['cs.CL', 'cs.AI', 'cs.SC']"
2201.10266v1,Combining Commonsense Reasoning and Knowledge Acquisition to Guide Deep Learning in Robotics,http://arxiv.org/abs/2201.10266v1,"Algorithms based on deep network models are being used for many patternrecognition and decision-making tasks in robotics and AI. Training these modelsrequires a large labeled dataset and considerable computational resources,which are not readily available in many domains. Also, it is difficult toexplore the internal representations and reasoning mechanisms of these models.As a step towards addressing the underlying knowledge representation,reasoning, and learning challenges, the architecture described in this paperdraws inspiration from research in cognitive systems. As a motivating example,we consider an assistive robot trying to reduce clutter in any given scene byreasoning about the occlusion of objects and stability of object configurationsin an image of the scene. In this context, our architecture incrementallylearns and revises a grounding of the spatial relations between objects anduses this grounding to extract spatial information from input images.Non-monotonic logical reasoning with this information and incompletecommonsense domain knowledge is used to make decisions about stability andocclusion. For images that cannot be processed by such reasoning, regionsrelevant to the tasks at hand are automatically identified and used to traindeep network models to make the desired decisions. Image regions used to trainthe deep networks are also used to incrementally acquire previously unknownstate constraints that are merged with the existing knowledge for subsequentreasoning. Experimental evaluation performed using simulated and real-worldimages indicates that in comparison with baselines based just on deep networks,our architecture improves reliability of decision making and reduces the effortinvolved in training data-driven deep network models.",Mohan Sridharan,2022/1/25,2022/1/25,,,"['cs.AI', 'cs.CV', 'cs.LG', 'cs.LO', 'cs.RO']"
2201.08677v1,Scales and Hedges in a Logic with Analogous Semantics,http://arxiv.org/abs/2201.08677v1,"Logics with analogous semantics, such as Fuzzy Logic, have a number ofexplanatory and application advantages, the most well-known being the abilityto help experts develop control systems. From a cognitive systems perspective,such languages also have the advantage of being grounded in perception. Forsocial decision making in humans, it is vital that logical conclusions aboutothers (cognitive empathy) are grounded in empathic emotion (affectiveempathy). Classical Fuzzy Logic, however, has several disadvantages: it is notobvious how complex formulae, e.g., the description of events in a text, can be(a) formed, (b) grounded, and (c) used in logical reasoning. The two-layeredContext Logic (CL) was designed to address these issue. Formally based on alattice semantics, like classical Fuzzy Logic, CL also features an analogoussemantics for complex fomulae. With the Activation Bit Vector Machine (ABVM),it has a simple and classical logical reasoning mechanism with an inherentimagery process based on the Vector Symbolic Architecture (VSA) model ofdistributed neuronal processing. This paper adds to the existing theory howscales, as necessary for adjective and verb semantics can be handled by thesystem.",Hedda R. Schmidtke,2022/1/21,2022/1/21,,,"['cs.AI', 'cs.CY', 'I.2.4; F.4.1']"
2201.05716v4,Mechanizing Matching Logic In Coq,http://arxiv.org/abs/2201.05716v4,"Matching logic is a formalism for specifying, and reasoning about,mathematical structures, using patterns and pattern matching. Growing inpopularity, it has been used to define many logical systems such as separationlogic with recursive definitions and linear temporal logic. In addition, itserves as the logical foundation of the K semantic framework, which was used tobuild practical verifiers for a number of real-world languages. Despite being afundamental formal system accommodating substantial theories, matching logiclacks a general-purpose, machine-checked formalization. Hence, we formalizematching logic using the Coq proof assistant. Specifically, we create a newrepresentation of matching logic that uses a locally nameless encoding, and weformalize the syntax, semantics, and proof system of this representation in theCoq proof assistant. Crucially, we prove the soundness of the formalized proofsystem and provide a means to carry out interactive matching logic reasoning inCoq. We believe this work provides a previously unexplored avenue for reasoningabout matching logic, its models, and the proof system.",Pter Bereczky,2022/1/15,2022/9/21,,,['cs.LO']
2201.01787v2,Does Entity Abstraction Help Generative Transformers Reason?,http://arxiv.org/abs/2201.01787v2,"We study the utility of incorporating entity type abstractions intopre-trained Transformers and test these methods on four NLP tasks requiringdifferent forms of logical reasoning: (1) compositional language understandingwith text-based relational reasoning (CLUTRR), (2) abductive reasoning(ProofWriter), (3) multi-hop question answering (HotpotQA), and (4)conversational question answering (CoQA). We propose and empirically explorethree ways to add such abstraction: (i) as additional input embeddings, (ii) asa separate sequence to encode, and (iii) as an auxiliary prediction task forthe model. Overall, our analysis demonstrates that models with abstract entityknowledge performs better than without it. The best abstraction aware modelsachieved an overall accuracy of 88.8% and 91.8% compared to the baseline modelachieving 62.9% and 89.8% on CLUTRR and ProofWriter respectively. However, forHotpotQA and CoQA, we find that F1 scores improve by only 0.5% on average. Ourresults suggest that the benefit of explicit abstraction is significant informally defined logical reasoning settings requiring many reasoning hops, butpoint to the notion that it is less beneficial for NLP tasks having less formallogical structure.",Nicolas Gontier,2022/1/5,2022/11/21,,,"['cs.CL', 'cs.AI', 'cs.LG']"
2201.00716v2,Modeling Associative Reasoning Processes,http://arxiv.org/abs/2201.00716v2,"The human capability to reason about one domain by using knowledge of otherdomains has been researched for more than 50 years, but models that areformally sound and predict cognitive process are sparse. We propose a formallysound method that models associative reasoning by adapting logical reasoningmechanisms. In particular it is shown that the combination with largecommensense knowledge within a single reasoning system demands for an efficientand powerful association technique. This approach is also used for modellingmind-wandering and the Remote Associates Test (RAT) for testing creativity. Ina general discussion we show implications of the model for a broad variety ofcognitive phenomena including consciousness.",Claudia Schon,2022/1/3,2022/1/7,,,"['cs.AI', 'q-bio.NC']"
2112.15028v1,Reasoning in circles,http://arxiv.org/abs/2112.15028v1,"Circular proofs, introduced by Daniyar Shamkanov, are proofs in whichassumptions are allowed that are not axioms but do appear at least twice alonga branch. Shamkanov has shown that a formula belongs to the provability logicGL exactly if it has a circular proof in the modal logic K4. Shamkanov usesTait style proof systems and infinitary proofs. In this paper we prove the sameresult but then for sequent calculi and without the detour via infinitarysystems. We also obtain a mild generalisation of the result, implying that itsintuitionistic analogue holds as well.",Rosalie Iemhoff,2021/12/30,2021/12/30,,,"['math.LO', '03B45, 03F45, 03F07']"
2112.13705v2,Graph Collaborative Reasoning,http://arxiv.org/abs/2112.13705v2,"Graphs can represent relational information among entities and graphstructures are widely used in many intelligent tasks such as search,recommendation, and question answering. However, most of the graph-structureddata in practice suffers from incompleteness, and thus link prediction becomesan important research problem. Though many models are proposed for linkprediction, the following two problems are still less explored: (1) Mostmethods model each link independently without making use of the richinformation from relevant links, and (2) existing models are mostly designedbased on associative learning and do not take reasoning into consideration.With these concerns, in this paper, we propose Graph Collaborative Reasoning(GCR), which can use the neighbor link information for relational reasoning ongraphs from logical reasoning perspectives. We provide a simple approach totranslate a graph structure into logical expressions, so that the linkprediction task can be converted into a neural logic reasoning problem. Weapply logical constrained neural modules to build the network architectureaccording to the logical expression and use back propagation to efficientlylearn the model parameters, which bridges differentiable learning and symbolicreasoning in a unified architecture. To show the effectiveness of our work, weconduct experiments on graph-related tasks such as link prediction andrecommendation based on commonly used benchmark datasets, and our graphcollaborative reasoning approach achieves state-of-the-art performance.",Hanxiong Chen,2021/12/27,2021/12/28,,,"['cs.IR', 'cs.AI', 'cs.LG', 'cs.LO', 'cs.SI']"
2111.12301v2,Two-stage Rule-induction Visual Reasoning on RPMs with an Application to Video Prediction,http://arxiv.org/abs/2111.12301v2,"Raven's Progressive Matrices (RPMs) are frequently used in evaluating human'svisual reasoning ability. Researchers have made considerable efforts indeveloping systems to automatically solve the RPM problem, often through ablack-box end-to-end convolutional neural network for both visual recognitionand logical reasoning tasks. Based on the two intrinsic natures of RPM problem,visual recognition and logical reasoning, we propose a Two-stage Rule-InductionVisual Reasoner (TRIVR), which consists of a perception module and a reasoningmodule, to tackle the challenges of real-world visual recognition andsubsequent logical reasoning tasks, respectively. For the reasoning module, wefurther propose a ""2+1"" formulation that models human's thinking in solvingRPMs and significantly reduces the model complexity. It derives a reasoningrule from each RPM sample, which is not feasible for existing methods. As aresult, the proposed reasoning module is capable of yielding a set of reasoningrules modeling human in solving the RPM problems. To validate the proposedmethod on real-world applications, an RPM-like Video Prediction (RVP) datasetis constructed, where visual reasoning is conducted on RPMs constructed usingreal-world video frames. Experimental results on various RPM-like datasetsdemonstrate that the proposed TRIVR achieves a significant and consistentperformance gain compared with the state-of-the-art models.",Wentao He,2021/11/24,2022/1/5,,,['cs.CV']
2110.14266v1,SQALER: Scaling Question Answering by Decoupling Multi-Hop and Logical Reasoning,http://arxiv.org/abs/2110.14266v1,"State-of-the-art approaches to reasoning and question answering overknowledge graphs (KGs) usually scale with the number of edges and can only beapplied effectively on small instance-dependent subgraphs. In this paper, weaddress this issue by showing that multi-hop and more complex logical reasoningcan be accomplished separately without losing expressive power. Motivated bythis insight, we propose an approach to multi-hop reasoning that scaleslinearly with the number of relation types in the graph, which is usuallysignificantly smaller than the number of edges or nodes. This produces a set ofcandidate solutions that can be provably refined to recover the solution to theoriginal problem. Our experiments on knowledge-based question answering showthat our approach solves the multi-hop MetaQA dataset, achieves a newstate-of-the-art on the more challenging WebQuestionsSP, is orders of magnitudemore scalable than competitive approaches, and can achieve compositionalgeneralization out of the training distribution.",Mattia Atzeni,2021/10/27,2021/10/27,,,"['cs.LG', 'cs.AI', 'cs.CL', 'cs.IR']"
2110.13522v2,Probabilistic Entity Representation Model for Reasoning over Knowledge Graphs,http://arxiv.org/abs/2110.13522v2,"Logical reasoning over Knowledge Graphs (KGs) is a fundamental technique thatcan provide efficient querying mechanism over large and incomplete databases.Current approaches employ spatial geometries such as boxes to learn queryrepresentations that encompass the answer entities and model the logicaloperations of projection and intersection. However, their geometry isrestrictive and leads to non-smooth strict boundaries, which further results inambiguous answer entities. Furthermore, previous works propose transformationtricks to handle unions which results in non-closure and, thus, cannot bechained in a stream. In this paper, we propose a Probabilistic EntityRepresentation Model (PERM) to encode entities as a Multivariate Gaussiandensity with mean and covariance parameters to capture its semantic positionand smooth decision boundary, respectively. Additionally, we also define theclosed logical operations of projection, intersection, and union that can beaggregated using an end-to-end objective function. On the logical queryreasoning problem, we demonstrate that the proposed PERM significantlyoutperforms the state-of-the-art methods on various public benchmark KGdatasets on standard evaluation metrics. We also evaluate PERM's competence ona COVID-19 drug-repurposing case study and show that our proposed work is ableto recommend drugs with substantially better F1 than current methods. Finally,we demonstrate the working of our PERM's query answering process through alow-dimensional visualization of the Gaussian representations.",Nurendra Choudhary,2021/10/26,2021/10/30,,,"['cs.LG', 'cs.CL', 'cs.IR']"
2110.08012v2,A Survey on State-of-the-art Techniques for Knowledge Graphs Construction and Challenges ahead,http://arxiv.org/abs/2110.08012v2,"Global datasphere is increasing fast, and it is expected to reach 175Zettabytes by 20251 . However, most of the content is unstructured and is notunderstandable by machines. Structuring this data into a knowledge graphenables multitudes of intelligent applications such as deep question answering,recommendation systems, semantic search, etc. The knowledge graph is anemerging technology that allows logical reasoning and uncovers new insightsusing content along with the context. Thereby, it provides necessary syntax andreasoning semantics that enable machines to solve complex healthcare, security,financial institutions, economics, and business problems. As an outcome,enterprises are putting their effort into constructing and maintainingknowledge graphs to support various downstream applications. Manual approachesare too expensive. Automated schemes can reduce the cost of building knowledgegraphs up to 15-250 times. This paper critiques state-of-the-art automatedtechniques to produce knowledge graphs of near-human quality autonomously.Additionally, it highlights different research issues that need to be addressedto deliver high-quality knowledge graphs",Ali Hur,2021/10/15,2021/12/31,,,"['cs.AI', 'cs.DB']"
2110.06884v1,ConditionalQA: A Complex Reading Comprehension Dataset with Conditional Answers,http://arxiv.org/abs/2110.06884v1,"We describe a Question Answering (QA) dataset that contains complex questionswith conditional answers, i.e. the answers are only applicable when certainconditions apply. We call this dataset ConditionalQA. In addition toconditional answers, the dataset also features: (1) long context documents withinformation that is related in logically complex ways; (2) multi-hop questionsthat require compositional logical reasoning; (3) a combination of extractivequestions, yes/no questions, questions with multiple answers, andnot-answerable questions; (4) questions asked without knowing the answers. Weshow that ConditionalQA is challenging for many of the existing QA models,especially in selecting answer conditions. We believe that this dataset willmotivate further research in answering complex questions over long documents.Data and leaderboard are publicly available at\url{https://github.com/haitian-sun/ConditionalQA}.",Haitian Sun,2021/10/13,2021/10/13,,,"['cs.CL', 'cs.AI']"
2109.13006v1,RuleBert: Teaching Soft Rules to Pre-trained Language Models,http://arxiv.org/abs/2109.13006v1,"While pre-trained language models (PLMs) are the go-to solution to tacklemany natural language processing problems, they are still very limited in theirability to capture and to use common-sense knowledge. In fact, even ifinformation is available in the form of approximate (soft) logical rules, it isnot clear how to transfer it to a PLM in order to improve its performance fordeductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs howto reason with soft Horn rules. We introduce a classification task where, givenfacts and soft rules, the PLM should return a prediction with a probability fora given hypothesis. We release the first dataset for this task, and we proposea revised loss function that enables the PLM to learn how to predict preciseprobabilities for the task. Our evaluation results show that the resultingfine-tuned models achieve very high performance, even on logical rules thatwere unseen at training. Moreover, we demonstrate that logical notionsexpressed by the rules are transferred to the fine-tuned model, yieldingstate-of-the-art results on external datasets.",Mohammed Saeed,2021/9/24,2021/9/24,,,"['cs.AI', 'cs.CL', 'cs.LG', 'cs.LO', 'cs.NE', '68T50', 'F.2.2; I.2.7']"
2109.08460v1,Neural Unification for Logic Reasoning over Natural Language,http://arxiv.org/abs/2109.08460v1,"Automated Theorem Proving (ATP) deals with the development of computerprograms being able to show that some conjectures (queries) are a logicalconsequence of a set of axioms (facts and rules). There exists severalsuccessful ATPs where conjectures and axioms are formally provided (e.g.formalised as First Order Logic formulas). Recent approaches, such as (Clark etal., 2020), have proposed transformer-based architectures for derivingconjectures given axioms expressed in natural language (English). Theconjecture is verified through a binary text classifier, where the transformersmodel is trained to predict the truth value of a conjecture given the axioms.The RuleTaker approach of (Clark et al., 2020) achieves appealing results bothin terms of accuracy and in the ability to generalize, showing that when themodel is trained with deep enough queries (at least 3 inference steps), thetransformers are able to correctly answer the majority of queries (97.6%) thatrequire up to 5 inference steps. In this work we propose a new architecture,namely the Neural Unifier, and a relative training procedure, which achievesstate-of-the-art results in term of generalisation, showing that mimicking awell-known inference procedure, the backward chaining, it is possible to answerdeep queries even when the model is trained only on shallow ones. The approachis demonstrated in experiments using a diverse set of benchmark data.",Gabriele Picco,2021/9/17,2021/9/17,,,['cs.CL']
2109.08295v1,Geolog: Scalable Logic Programming on Spatial Data,http://arxiv.org/abs/2109.08295v1,"Spatial data is ubiquitous in our data-driven society. The Logic Programmingcommunity has been investigating the use of spatial data in different settings.Despite the success of this research, the Geographic Information System (GIS)community has rarely made use of these new approaches. This has mainly tworeasons. First, there is a lack of tools that tightly integrate logicalreasoning into state-of-the-art GIS software. Second, the scalability ofsolutions has often not been tested and hence, some solutions might work on toyexamples but do not scale well to real-world settings. The two maincontributions of this paper are (1) the Relation Based Programming paradigm,expressing rules on relations instead of individual entities, and (2) Geolog, atool for spatio-logical reasoning that can be installed on top of ArcMap, whichis an industry standard GIS. We evaluate our new Relation Based Programmingparadigm in four real-world scenarios and show that up to two orders ofmagnitude in performance gain can be achieved compared to the prevalent EntityBased Programming paradigm.",Tobias Grubenmann,2021/9/17,2021/9/17,,,['cs.LO']
2109.04803v1,Combining Event Calculus and Description Logic Reasoning via Logic Programming,http://arxiv.org/abs/2109.04803v1,"The paper introduces a knowledge representation language that combines theevent calculus with description logic in a logic programming framework. Thepurpose is to provide the user with an expressive language for modelling andanalysing systems that evolve over time. The approach is exemplified with thelogic programming language as implemented in the Fusemate system. The paperextends Fusemate's rule language with a weakly DL-safe interface to thedescription logic $\cal ALCIF$ and adapts the event calculus to this extendedlanguage. This way, time-stamped ABoxes can be manipulated as fluents in theevent calculus. All that is done in the frame of Fusemate's concept ofstratification by time. The paper provides conditions for soundness andcompleteness where appropriate. Using an elaborated example it demonstrates theinterplay of the event calculus, description logic and logic programming rulesfor computing possible models as plausible explanations of the current state ofthe modelled system.",Peter Baumgartner,2021/9/10,2021/9/10,,,['cs.LO']
2109.04746v1,Counterfactual Adversarial Learning with Representation Interpolation,http://arxiv.org/abs/2109.04746v1,"Deep learning models exhibit a preference for statistical fitting overlogical reasoning. Spurious correlations might be memorized when there existsstatistical bias in training data, which severely limits the model performanceespecially in small data scenarios. In this work, we introduce CounterfactualAdversarial Training framework (CAT) to tackle the problem from a causalityperspective. Particularly, for a specific sample, CAT first generates acounterfactual representation through latent space interpolation in anadversarial manner, and then performs Counterfactual Risk Minimization (CRM) oneach original-counterfactual pair to adjust sample-wise loss weightdynamically, which encourages the model to explore the true causal effect.Extensive experiments demonstrate that CAT achieves substantial performanceimprovement over SOTA across different downstream tasks, including sentenceclassification, natural language inference and question answering.",Wei Wang,2021/9/10,2021/9/10,,,['cs.LG']
2109.08307v1,Sinoledge: A Knowledge Engine based on Logical Reasoning and Distributed Micro Services,http://arxiv.org/abs/2109.08307v1,"We propose a knowledge engine called Sinoledge mainly for doctors,physicians, and researchers in medical field to organize thoughts, managereasoning process, test and deploy to production environments effortlessly. Ourproposal can be related to rule engine usually used in business or medicalfields. More importantly, our proposal provides a user-friendly interface, aneasy-maintain way of organizing knowledge, an understandable testingfunctionality and a highly available and efficient back-end architecture.",Yining Huang,2021/8/29,2021/8/29,,,"['cs.AI', 'cs.SE']"
2108.06743v2,Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning,http://arxiv.org/abs/2108.06743v2,"To quantitatively and intuitively explore the generalization ability ofpre-trained language models (PLMs), we have designed several tasks ofarithmetic and logical reasoning. We both analyse how well PLMs generalize whenthe test data is in the same distribution as the train data and when it isdifferent, for the latter analysis, we have also designed a cross-distributiontest set other than the in-distribution test set. We conduct experiments on oneof the most advanced and publicly released generative PLM - BART. Our researchfinds that the PLMs can easily generalize when the distribution is the same,however, it is still difficult for them to generalize out of the distribution.",Cunxiang Wang,2021/8/15,2021/10/19,,,"['cs.CL', 'cs.AI']"
2108.00648v1,From LSAT: The Progress and Challenges of Complex Reasoning,http://arxiv.org/abs/2108.00648v1,"Complex reasoning aims to draw a correct inference based on complex rules. Asa hallmark of human intelligence, it involves a degree of explicit readingcomprehension, interpretation of logical knowledge and complex ruleapplication. In this paper, we take a step forward in complex reasoning bysystematically studying the three challenging and domain-general tasks of theLaw School Admission Test (LSAT), including analytical reasoning, logicalreasoning and reading comprehension. We propose a hybrid reasoning system tointegrate these three tasks and achieve impressive overall performance on theLSAT tests. The experimental results demonstrate that our system endows itselfa certain complex reasoning ability, especially the fundamental readingcomprehension and challenging logical reasoning capacities. Further analysisalso shows the effectiveness of combining the pre-trained models with thetask-specific reasoning module, and integrating symbolic knowledge intodiscrete interpretable reasoning steps in complex reasoning. We further shed alight on the potential future directions, like unsupervised symbolic knowledgeextraction, model interpretability, few-shot learning and comprehensivebenchmark for complex reasoning.",Siyuan Wang,2021/8/2,2021/8/2,,,['cs.CL']
2107.02794v2,"Improving Coherence and Consistency in Neural Sequence Models with Dual-System, Neuro-Symbolic Reasoning",http://arxiv.org/abs/2107.02794v2,"Human reasoning can often be understood as an interplay between two systems:the intuitive and associative (""System 1"") and the deliberative and logical(""System 2""). Neural sequence models -- which have been increasingly successfulat performing complex, structured tasks -- exhibit the advantages and failuremodes of System 1: they are fast and learn patterns from data, but are ofteninconsistent and incoherent. In this work, we seek a lightweight, training-freemeans of improving existing System 1-like sequence models by adding System2-inspired logical reasoning. We explore several variations on this theme inwhich candidate generations from a neural sequence model are examined forlogical consistency by a symbolic reasoning module, which can either accept orreject the generations. Our approach uses neural inference to mediate betweenthe neural System 1 and the logical System 2. Results in robust storygeneration and grounded instruction-following show that this approach canincrease the coherence and accuracy of neurally-based generations.",Maxwell Nye,2021/7/6,2021/12/15,,,"['cs.AI', 'cs.CL', 'cs.LG']"
2106.09237v1,Towards Assurance-Driven Architectural Decomposition of Software Systems,http://arxiv.org/abs/2106.09237v1,"Computer systems are so complex, so they are usually designed and analyzed interms of layers of abstraction. Complexity is still a challenge facing logicalreasoning tools that are used to find software design flaws and implementationbugs. Abstraction is also a common technique for scaling those tools to morecomplex systems. However, the abstractions used in the design phase of systemsare in many cases different from those used for assurance. In this paper weargue that different software quality assurance techniques operate on differentaspects of software systems. To facilitate assurance, and for a smoothintegration of assurance tools into the Software Development Lifecycle (SDLC),we present a 4-dimensional meta-architecture that separates computational,coordination, and stateful software artifacts early on in the design stage. Weenumerate some of the design and assurance challenges that can be addressed bythis meta-architecture, and demonstrate it on the high-level design of a simplefile system.",Ramy Shahin,2021/6/17,2021/6/17,,,['cs.SE']
2106.11072v1,Techniques for Symbol Grounding with SATNet,http://arxiv.org/abs/2106.11072v1,"Many experts argue that the future of artificial intelligence is limited bythe field's ability to integrate symbolic logical reasoning into deep learningarchitectures. The recently proposed differentiable MAXSAT solver, SATNet, wasa breakthrough in its capacity to integrate with a traditional neural networkand solve visual reasoning problems. For instance, it can learn the rules ofSudoku purely from image examples. Despite its success, SATNet was shown tosuccumb to a key challenge in neurosymbolic systems known as the SymbolGrounding Problem: the inability to map visual inputs to symbolic variableswithout explicit supervision (""label leakage""). In this work, we present aself-supervised pre-training pipeline that enables SATNet to overcome thislimitation, thus broadening the class of problems that SATNet architectures cansolve to include datasets where no intermediary labels are available at all. Wedemonstrate that our method allows SATNet to attain full accuracy even with aharder problem setup that prevents any label leakage. We additionally introducea proofreading method that further improves the performance of SATNetarchitectures, beating the state-of-the-art on Visual Sudoku.",Sever Topan,2021/6/16,2021/6/16,,,"['cs.AI', 'cs.LG', 'stat.ML']"
2106.01709v1,SIRE: Separate Intra- and Inter-sentential Reasoning for Document-level Relation Extraction,http://arxiv.org/abs/2106.01709v1,"Document-level relation extraction has attracted much attention in recentyears. It is usually formulated as a classification problem that predictsrelations for all entity pairs in the document. However, previous worksindiscriminately represent intra- and inter-sentential relations in the sameway, confounding the different patterns for predicting them. Besides, theycreate a document graph and use paths between entities on the graph as cluesfor logical reasoning. However, not all entity pairs can be connected with apath and have the correct logical reasoning paths in their graph. Thus manycases of logical reasoning cannot be covered. This paper proposes an effectivearchitecture, SIRE, to represent intra- and inter-sentential relations indifferent ways. We design a new and straightforward form of logical reasoningmodule that can cover more logical reasoning chains. Experiments on the publicdatasets show SIRE outperforms the previous state-of-the-art methods. Furtheranalysis shows that our predictions are reliable and explainable. Our code isavailable at https://github.com/DreamInvoker/SIRE.",Shuang Zeng,2021/6/3,2021/6/3,,,"['cs.CL', 'cs.AI', 'cs.LG']"
2106.01562v1,Discriminative Reasoning for Document-level Relation Extraction,http://arxiv.org/abs/2106.01562v1,"Document-level relation extraction (DocRE) models generally use graphnetworks to implicitly model the reasoning skill (i.e., pattern recognition,logical reasoning, coreference reasoning, etc.) related to the relation betweenone entity pair in a document. In this paper, we propose a novel discriminativereasoning framework to explicitly model the paths of these reasoning skillsbetween each entity pair in this document. Thus, a discriminative reasoningnetwork is designed to estimate the relation probability distribution ofdifferent reasoning paths based on the constructed graph and vectorizeddocument contexts for each entity pair, thereby recognizing their relation.Experimental results show that our method outperforms the previousstate-of-the-art performance on the large-scale DocRE dataset. The code ispublicly available at https://github.com/xwjim/DRN.",Wang Xu,2021/6/3,2021/6/3,,,['cs.CL']
2106.00248v2,Volta at SemEval-2021 Task 9: Statement Verification and Evidence Finding with Tables using TAPAS and Transfer Learning,http://arxiv.org/abs/2106.00248v2,"Tables are widely used in various kinds of documents to present informationconcisely. Understanding tables is a challenging problem that requires anunderstanding of language and table structure, along with numerical and logicalreasoning. In this paper, we present our systems to solve Task 9 ofSemEval-2021: Statement Verification and Evidence Finding with Tables(SEM-TAB-FACTS). The task consists of two subtasks: (A) Given a table and astatement, predicting whether the table supports the statement and (B)Predicting which cells in the table provide evidence for/against the statement.We fine-tune TAPAS (a model which extends BERT's architecture to capturetabular structure) for both the subtasks as it has shown state-of-the-artperformance in various table understanding tasks. In subtask A, we evaluate howtransfer learning and standardizing tables to have a single header row improvesTAPAS' performance. In subtask B, we evaluate how different fine-tuningstrategies can improve TAPAS' performance. Our systems achieve an F1 score of67.34 in subtask A three-way classification, 72.89 in subtask A two-wayclassification, and 62.95 in subtask B.",Devansh Gautam,2021/6/1,2021/6/17,,,['cs.CL']
2105.14167v3,NeuralLog: Natural Language Inference with Joint Neural and Logical Reasoning,http://arxiv.org/abs/2105.14167v3,"Deep learning (DL) based language models achieve high performance on variousbenchmarks for Natural Language Inference (NLI). And at this time, symbolicapproaches to NLI are receiving less attention. Both approaches (symbolic andDL) have their advantages and weaknesses. However, currently, no methodcombines them in a system to solve the task of NLI. To merge symbolic and deeplearning methods, we propose an inference framework called NeuralLog, whichutilizes both a monotonicity-based logical inference engine and a neuralnetwork language model for phrase alignment. Our framework models the NLI taskas a classic search problem and uses the beam search algorithm to search foroptimal inference paths. Experiments show that our joint logic and neuralinference system improves accuracy on the NLI task and can achieve state-of-artaccuracy on the SICK and MED datasets.",Zeming Chen,2021/5/29,2021/6/10,,,"['cs.CL', 'cs.AI']"
2105.10118v1,Probabilistic Sufficient Explanations,http://arxiv.org/abs/2105.10118v1,"Understanding the behavior of learned classifiers is an important task, andvarious black-box explanations, logical reasoning approaches, andmodel-specific methods have been proposed. In this paper, we introduceprobabilistic sufficient explanations, which formulate explaining an instanceof classification as choosing the ""simplest"" subset of features such that onlyobserving those features is ""sufficient"" to explain the classification. Thatis, sufficient to give us strong probabilistic guarantees that the model willbehave similarly when all features are observed under the data distribution. Inaddition, we leverage tractable probabilistic reasoning tools such asprobabilistic circuits and expected predictions to design a scalable algorithmfor finding the desired explanations while keeping the guarantees intact. Ourexperiments demonstrate the effectiveness of our algorithm in findingsufficient explanations, and showcase its advantages compared to Anchors andlogical explanations.",Eric Wang,2021/5/21,2021/5/21,,,"['cs.LG', 'cs.AI']"
2105.03659v1,Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text,http://arxiv.org/abs/2105.03659v1,"Logical reasoning of text requires understanding critical logical informationin the text and performing inference over them. Large-scale pre-trained modelsfor logical reasoning mainly focus on word-level semantics of text whilestruggling to capture symbolic logic. In this paper, we propose to understandlogical symbols and expressions in the text to arrive at the answer. Based onsuch logical information, we not only put forward a context extension frameworkbut also propose a data augmentation algorithm. The former extends the contextto cover implicit logical expressions following logical equivalence laws. Thelatter augments literally similar but logically different instances to bettercapture logical information, especially logical negative and conditionalrelationships. We conduct experiments on ReClor dataset. The results show thatour method achieves the state-of-the-art performance, and both logic-drivencontext extension framework and data augmentation algorithm can help improvethe accuracy. And our multi-model ensemble system is the first to surpass humanperformance on both EASY set and HARD set of ReClor.",Siyuan Wang,2021/5/8,2021/5/8,,,['cs.CL']
2104.07869v1,Faithfully Explainable Recommendation via Neural Logic Reasoning,http://arxiv.org/abs/2104.07869v1,"Knowledge graphs (KG) have become increasingly important to endow modernrecommender systems with the ability to generate traceable reasoning paths toexplain the recommendation process. However, prior research rarely considersthe faithfulness of the derived explanations to justify the decision makingprocess. To the best of our knowledge, this is the first work that models andevaluates faithfully explainable recommendation under the framework of KGreasoning. Specifically, we propose neural logic reasoning for explainablerecommendation (LOGER) by drawing on interpretable logical rules to guide thepath reasoning process for explanation generation. We experiment on threelarge-scale datasets in the e-commerce domain, demonstrating the effectivenessof our method in delivering high-quality recommendations as well asascertaining the faithfulness of the derived explanation.",Yaxin Zhu,2021/4/16,2021/4/16,,,['cs.IR']
2103.15100v3,The General Theory of General Intelligence: A Pragmatic Patternist Perspective,http://arxiv.org/abs/2103.15100v3,"A multi-decade exploration into the theoretical foundations of artificial andnatural general intelligence, which has been expressed in a series of books andpapers and used to guide a series of practical and research-prototype softwaresystems, is reviewed at a moderate level of detail. The review coversunderlying philosophies (patternist philosophy of mind, foundationalphenomenological and logical ontology), formalizations of the concept ofintelligence, and a proposed high level architecture for AGI systems partlydriven by these formalizations and philosophies. The implementation of specificcognitive processes such as logical reasoning, program learning, clustering andattention allocation in the context and language of this high levelarchitecture is considered, as is the importance of a common (e.g. typedmetagraph based) knowledge representation for enabling ""cognitive synergy""between the various processes. The specifics of human-like cognitivearchitecture are presented as manifestations of these general principles, andkey aspects of machine consciousness and machine ethics are also treated inthis context. Lessons for practical implementation of advanced AGI inframeworks such as OpenCog Hyperon are briefly considered.",Ben Goertzel,2021/3/28,2021/4/4,,,['cs.AI']
2103.14349v2,DAGN: Discourse-Aware Graph Network for Logical Reasoning,http://arxiv.org/abs/2103.14349v2,"Recent QA with logical reasoning questions requires passage-level relationsamong the sentences. However, current approaches still focus on sentence-levelrelations interacting among tokens. In this work, we explore aggregatingpassage-level clues for solving logical reasoning QA by using discourse-basedinformation. We propose a discourse-aware graph network (DAGN) that reasonsrelying on the discourse structure of the texts. The model encodes discourseinformation as a graph with elementary discourse units (EDUs) and discourserelations, and learns the discourse-aware features via a graph network fordownstream QA tasks. Experiments are conducted on two logical reasoning QAdatasets, ReClor and LogiQA, and our proposed DAGN achieves competitiveresults. The source code is available at https://github.com/Eleanor-H/DAGN.",Yinya Huang,2021/3/26,2021/4/8,,,['cs.CL']
2103.14230v2,Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and Execution,http://arxiv.org/abs/2103.14230v2,"Spatial-temporal reasoning is a challenging task in Artificial Intelligence(AI) due to its demanding but unique nature: a theoretic requirement onrepresenting and reasoning based on spatial-temporal knowledge in mind, and anapplied requirement on a high-level cognitive system capable of navigating andacting in space and time. Recent works have focused on an abstract reasoningtask of this kind -- Raven's Progressive Matrices (RPM). Despite theencouraging progress on RPM that achieves human-level performance in terms ofaccuracy, modern approaches have neither a treatment of human-like reasoning ongeneralization, nor a potential to generate answers. To fill in this gap, wepropose a neuro-symbolic Probabilistic Abduction and Execution (PrAE) learner;central to the PrAE learner is the process of probabilistic abduction andexecution on a probabilistic scene representation, akin to the mentalmanipulation of objects. Specifically, we disentangle perception and reasoningfrom a monolithic model. The neural visual perception frontend predictsobjects' attributes, later aggregated by a scene inference engine to produce aprobabilistic scene representation. In the symbolic logical reasoning backend,the PrAE learner uses the representation to abduce the hidden rules. An answeris predicted by executing the rules on the probabilistic representation. Theentire system is trained end-to-end in an analysis-by-synthesis manner withoutany visual attribute annotations. Extensive experiments demonstrate that thePrAE learner improves cross-configuration generalization and is capable ofrendering an answer, in contrast to prior works that merely make a categoricalchoice from candidates.",Chi Zhang,2021/3/26,2021/5/14,,,"['cs.AI', 'cs.CV', 'cs.LG']"
2103.07766v2,Context Transformer with Stacked Pointer Networks for Conversational Question Answering over Knowledge Graphs,http://arxiv.org/abs/2103.07766v2,"Neural semantic parsing approaches have been widely used for QuestionAnswering (QA) systems over knowledge graphs. Such methods provide theflexibility to handle QA datasets with complex queries and a large number ofentities. In this work, we propose a novel framework named CARTON, whichperforms multi-task semantic parsing for handling the problem of conversationalquestion answering over a large-scale knowledge graph. Our framework consistsof a stack of pointer networks as an extension of a context transformer modelfor parsing the input question and the dialog history. The framework generatesa sequence of actions that can be executed on the knowledge graph. We evaluateCARTON on a standard dataset for complex sequential question answering on whichCARTON outperforms all baselines. Specifically, we observe performanceimprovements in F1-score on eight out of ten question types compared to theprevious state of the art. For logical reasoning questions, an improvement of11 absolute points is reached.",Joan Plepi,2021/3/13,2021/6/24,,,['cs.CL']
2103.05222v2,Data augmentation by morphological mixup for solving Raven's Progressive Matrices,http://arxiv.org/abs/2103.05222v2,"Raven's Progressive Matrices (RPMs) are frequently used in testing human'svisual reasoning ability. Recent advances of RPM-like datasets and solutionmodels partially address the challenges of visually understanding the RPMquestions and logically reasoning the missing answers. In view of the poorgeneralization performance due to insufficient samples in RPM datasets, wepropose an effective scheme, namely Candidate Answer Morphological Mixup(CAM-Mix). CAM-Mix serves as a data augmentation strategy by gray-scale imagemorphological mixup, which regularizes various solution methods and overcomesthe model overfitting problem. By creating new negative candidate answerssemantically similar to the correct answers, a more accurate decision boundarycould be defined. By applying the proposed data augmentation method, asignificant and consistent performance improvement is achieved on variousRPM-like datasets compared with the state-of-the-art models.",Wentao He,2021/3/9,2021/11/19,,,['cs.CV']
2103.02191v1,Extracting Optimal Explanations for Ensemble Trees via Logical Reasoning,http://arxiv.org/abs/2103.02191v1,"Ensemble trees are a popular machine learning model which often yields highprediction performance when analysing structured data. Although individualsmall decision trees are deemed explainable by nature, an ensemble of largetrees is often difficult to understand. In this work, we propose an approachcalled optimised explanation (OptExplain) that faithfully extracts globalexplanations of ensemble trees using a combination of logical reasoning,sampling and optimisation. Building on top of this, we propose a method calledthe profile of equivalent classes (ProClass), which uses MAX-SAT to simplifythe explanation even further. Our experimental study on several datasets showsthat our approach can provide high-quality explanations to large ensemble treesmodels, and it betters recent top-performers.",Gelin Zhang,2021/3/3,2021/3/3,,,['cs.LO']
2102.05171v1,The Cognition of Counterexample in Mathematics Students,http://arxiv.org/abs/2102.05171v1,"Studying Mathematics requires a synthesis of skills from a multitude ofacademic disciplines; logical reasoning being chief among them. This paperexplores mathematical logical preparedness of students entering first yearuniversity mathematics courses and also the effectiveness of using logicalfacility to predict successful course outcomes. We analyze data collected fromstudents enrolled at the University of Winnipeg in a pre-service course forhigh school teachers. We do find that, being able to successfully answerlogical questions, both before and after intervention, are significant inrelation to improved student outcomes.",Shannon Ezzat,2021/2/9,2021/2/9,,,"['math.HO', '97E30']"
2101.04921v2,Neural Sequence-to-grid Module for Learning Symbolic Rules,http://arxiv.org/abs/2101.04921v2,"Logical reasoning tasks over symbols, such as learning arithmetic operationsand computer program evaluations, have become challenges to deep learning. Inparticular, even state-of-the-art neural networks fail to achieve\textit{out-of-distribution} (OOD) generalization of symbolic reasoning tasks,whereas humans can easily extend learned symbolic rules. To resolve thisdifficulty, we propose a neural sequence-to-grid (seq2grid) module, an inputpreprocessor that automatically segments and aligns an input sequence into agrid. As our module outputs a grid via a novel differentiable mapping, anyneural network structure taking a grid input, such as ResNet or TextCNN, can bejointly trained with our module in an end-to-end fashion. Extensive experimentsshow that neural networks having our module as an input preprocessor achieveOOD generalization on various arithmetic and algorithmic problems includingnumber sequence prediction problems, algebraic word problems, and computerprogram evaluation problems while other state-of-the-art sequence transductionmodels cannot. Moreover, we verify that our module enhances TextCNN to solvethe bAbI QA tasks without external memory.",Segwang Kim,2021/1/13,2021/4/26,,,"['cs.LG', 'cs.AI', 'cs.CL']"
2101.04742v1,Programming and Reasoning with Partial Observability,http://arxiv.org/abs/2101.04742v1,"Computer programs are increasingly being deployed in partially-observableenvironments. A partially observable environment is an environment whose stateis not completely visible to the program, but from which the program receivespartial observations. Developers typically deal with partial observability bywriting a state estimator that, given observations, attempts to deduce thehidden state of the environment. In safety-critical domains, to formally verifysafety properties developers may write an environment model. The model capturesthe relationship between observations and hidden states and is used to provethe software correct.  In this paper, we present a new methodology for writing and verifyingprograms in partially observable environments. We present belief programming, aprogramming methodology where developers write an environment model that theprogram runtime automatically uses to perform state estimation. A beliefprogram dynamically updates and queries a belief state that captures thepossible states the environment could be in. To enable verification, we presentEpistemic Hoare Logic that reasons about the possible belief states of a beliefprogram the same way that classical Hoare logic reasons about the possiblestates of a program. We develop these concepts by defining a semantics and aprogram logic for a simple core language called BLIMP. In a case study, we showhow belief programming could be used to write and verify a controller for theMars Polar Lander in BLIMP. We present an implementation of BLIMP called CBLIMPand evaluate it to determine the feasibility of belief programming.",Eric Atkinson,2021/1/12,2021/1/12,,,['cs.PL']
2012.08673v2,A Closer Look at the Robustness of Vision-and-Language Pre-trained Models,http://arxiv.org/abs/2012.08673v2,"Large-scale pre-trained multimodal transformers, such as ViLBERT and UNITER,have propelled the state of the art in vision-and-language (V+L) research to anew level. Although achieving impressive performance on standard tasks, todate, it still remains unclear how robust these pre-trained models are. Toinvestigate, we conduct a host of thorough evaluations on existing pre-trainedmodels over 4 different types of V+L specific model robustness: (i) LinguisticVariation; (ii) Logical Reasoning; (iii) Visual Content Manipulation; and (iv)Answer Distribution Shift. Interestingly, by standard model finetuning,pre-trained V+L models already exhibit better robustness than manytask-specific state-of-the-art methods. To further enhance model robustness, wepropose Mango, a generic and efficient approach that learns a MultimodalAdversarial Noise GeneratOr in the embedding space to fool pre-trained V+Lmodels. Differing from previous studies focused on one specific type ofrobustness, Mango is task-agnostic, and enables universal performance lift forpre-trained models over diverse tasks designed to evaluate broad aspects ofrobustness. Comprehensive experiments demonstrate that Mango achieves new stateof the art on 7 out of 9 robustness benchmarks, surpassing existing methods bya significant margin. As the first comprehensive study on V+L robustness, thiswork puts robustness of pre-trained models into sharper focus, pointing newdirections for future study.",Linjie Li,2020/12/15,2021/3/30,,,"['cs.CV', 'cs.CL']"
2012.08479v3,"Bayes Meets Entailment and Prediction: Commonsense Reasoning with Non-monotonicity, Paraconsistency and Predictive Accuracy",http://arxiv.org/abs/2012.08479v3,"The recent success of Bayesian methods in neuroscience and artificialintelligence gives rise to the hypothesis that the brain is a Bayesian machine.Since logic and learning are both practices of the human brain, it leads toanother hypothesis that there is a Bayesian interpretation underlying bothlogical reasoning and machine learning. In this paper, we introduce agenerative model of logical consequence relations. It formalises the process ofhow the truth value of a sentence is probabilistically generated from theprobability distribution over states of the world. We show that the generativemodel characterises a classical consequence relation, paraconsistentconsequence relation and nonmonotonic consequence relation. In particular, thegenerative model gives a new consequence relation that outperforms them inreasoning with inconsistent knowledge. We also show that the generative modelgives a new classification algorithm that outperforms several representativealgorithms in predictive accuracy and complexity on the Kaggle Titanic dataset.",Hiroyuki Kido,2020/12/15,2021/1/27,,,['cs.AI']
2012.05876v2,Neurosymbolic AI: The 3rd Wave,http://arxiv.org/abs/2012.05876v2,"Current advances in Artificial Intelligence (AI) and Machine Learning (ML)have achieved unprecedented impact across research communities and industry.Nevertheless, concerns about trust, safety, interpretability and accountabilityof AI were raised by influential thinkers. Many have identified the need forwell-founded knowledge representation and reasoning to be integrated with deeplearning and for sound explainability. Neural-symbolic computing has been anactive area of research for many years seeking to bring together robustlearning in neural networks with reasoning and explainability via symbolicrepresentations for network models. In this paper, we relate recent and earlyresearch results in neurosymbolic AI with the objective of identifying the keyingredients of the next wave of AI systems. We focus on research thatintegrates in a principled way neural network-based learning with symbolicknowledge representation and logical reasoning. The insights provided by 20years of neural-symbolic computing are shown to shed new light onto theincreasingly prominent role of trust, safety, interpretability andaccountability of AI. We also identify promising directions and challenges forthe next decade of AI research from the perspective of neural-symbolic systems.",Artur d'Avila Garcez,2020/12/10,2020/12/16,,,"['cs.AI', 'cs.LG', 'I.2.4; I.2.6']"
2011.13354v4,Braid: Weaving Symbolic and Neural Knowledge into Coherent Logical Explanations,http://arxiv.org/abs/2011.13354v4,"Traditional symbolic reasoning engines, while attractive for their precisionand explicability, have a few major drawbacks: the use of brittle inferenceprocedures that rely on exact matching (unification) of logical terms, aninability to deal with uncertainty, and the need for a precompiled rule-base ofknowledge (the ""knowledge acquisition"" problem). To address these issues, wedevise a novel logical reasoner called Braid, that supports probabilisticrules, and uses the notion of custom unification functions and dynamic rulegeneration to overcome the brittle matching and knowledge-gap problem prevalentin traditional reasoners. In this paper, we describe the reasoning algorithmsused in Braid, and their implementation in a distributed task-based frameworkthat builds proof/explanation graphs for an input query. We use a simple QAexample from a children's story to motivate Braid's design and explain how thevarious components work together to produce a coherent logical explanation.Finally, we evaluate Braid on the ROC Story Cloze test and achieve close tostate-of-the-art results while providing frame-based explanations.",Aditya Kalyanpur,2020/11/26,2021/12/5,,,['cs.CL']
2011.07986v2,Neural Software Analysis,http://arxiv.org/abs/2011.07986v2,"Many software development problems can be addressed by program analysistools, which traditionally are based on precise, logical reasoning andheuristics to ensure that the tools are practical. Recent work has showntremendous success through an alternative way of creating developer tools,which we call neural software analysis. The key idea is to train a neuralmachine learning model on numerous code examples, which, once trained, makespredictions about previously unseen code. In contrast to traditional programanalysis, neural software analysis naturally handles fuzzy information, such ascoding conventions and natural language embedded in code, without relying onmanually encoded heuristics. This article gives an overview of neural softwareanalysis, discusses when to (not) use it, and presents three example analyses.The analyses address challenging software development problems: bug detection,type prediction, and code completion. The resulting tools complement andoutperform traditional program analyses, and are used in industrial practice.",Michael Pradel,2020/11/16,2021/4/8,,,"['cs.SE', 'cs.LG', 'cs.PL']"
2011.04864v1,Natural Language Inference in Context -- Investigating Contextual Reasoning over Long Texts,http://arxiv.org/abs/2011.04864v1,"Natural language inference (NLI) is a fundamental NLP task, investigating theentailment relationship between two texts. Popular NLI datasets present thetask at sentence-level. While adequate for testing semantic representations,they fall short for testing contextual reasoning over long texts, which is anatural part of the human inference process. We introduce ConTRoL, a newdataset for ConTextual Reasoning over Long texts. Consisting of 8,325expert-designed ""context-hypothesis"" pairs with gold labels, ConTRoL is apassage-level NLI dataset with a focus on complex contextual reasoning typessuch as logical reasoning. It is derived from competitive selection andrecruitment test (verbal reasoning test) for police recruitment, with expertlevel quality. Compared with previous NLI benchmarks, the materials in ConTRoLare much more challenging, involving a range of reasoning types. Empiricalresults show that state-of-the-art language models perform by far worse thaneducated humans. Our dataset can also serve as a testing-set for downstreamtasks like Checking Factual Correctness of Summaries.",Hanmeng Liu,2020/11/10,2020/11/10,,,['cs.CL']
2011.00992v1,"The P-T Probability Framework for Semantic Communication, Falsification, Confirmation, and Bayesian Reasoning",http://arxiv.org/abs/2011.00992v1,"Many researchers want to unify probability and logic by defining logicalprobability or probabilistic logic reasonably. This paper tries to unifystatistics and logic so that we can use both statistical probability andlogical probability at the same time. For this purpose, this paper proposes theP-T probability framework, which is assembled with Shannon's statisticalprobability framework for communication, Kolmogorov's probability axioms forlogical probability, and Zadeh's membership functions used as truth functions.Two kinds of probabilities are connected by an extended Bayes' theorem, withwhich we can convert a likelihood function and a truth function from one toanother. Hence, we can train truth functions (in logic) by samplingdistributions (in statistics). This probability framework was developed in theauthor's long-term studies on semantic information, statistical learning, andcolor vision. This paper first proposes the P-T probability framework andexplains different probabilities in it by its applications to semanticinformation theory. Then, this framework and the semantic information methodsare applied to statistical learning, statistical mechanics, hypothesisevaluation (including falsification), confirmation, and Bayesian reasoning.Theoretical applications illustrate the reasonability and practicability ofthis framework. This framework is helpful for interpretable AI. To interpretneural networks, we need further study.",Chenguang Lu,2020/10/29,2020/10/29,,,"['stat.OT', '68T27, 68T30, 60B05, 68P30, 94A17, 94A34, 30B42, 30B48, 03B50, 03B65', 'E.4; I.2.0; I.2.3; I.2.6; I.5.3; H.1.1']"
2010.11465v1,Beta Embeddings for Multi-Hop Logical Reasoning in Knowledge Graphs,http://arxiv.org/abs/2010.11465v1,"One of the fundamental problems in Artificial Intelligence is to performcomplex multi-hop logical reasoning over the facts captured by a knowledgegraph (KG). This problem is challenging, because KGs can be massive andincomplete. Recent approaches embed KG entities in a low dimensional space andthen use these embeddings to find the answer entities. However, it has been anoutstanding challenge of how to handle arbitrary first-order logic (FOL)queries as present methods are limited to only a subset of FOL operators. Inparticular, the negation operator is not supported. An additional limitation ofpresent methods is also that they cannot naturally model uncertainty. Here, wepresent BetaE, a probabilistic embedding framework for answering arbitrary FOLqueries over KGs. BetaE is the first method that can handle a complete set offirst-order logical operations: conjunction ($\wedge$), disjunction ($\vee$),and negation ($\neg$). A key insight of BetaE is to use probabilisticdistributions with bounded support, specifically the Beta distribution, andembed queries/entities as distributions, which as a consequence allows us toalso faithfully model uncertainty. Logical operations are performed in theembedding space by neural operators over the probabilistic embeddings. Wedemonstrate the performance of BetaE on answering arbitrary FOL queries onthree large, incomplete KGs. While being more general, BetaE also increasesrelative performance by up to 25.4% over the current state-of-the-art KGreasoning methods that can only handle conjunctive queries without negation.",Hongyu Ren,2020/10/22,2020/10/22,,,"['cs.AI', 'cs.DB', 'cs.LG']"
2010.10645v1,Axiom Learning and Belief Tracing for Transparent Decision Making in Robotics,http://arxiv.org/abs/2010.10645v1,"A robot's ability to provide descriptions of its decisions and beliefspromotes effective collaboration with humans. Providing such transparency isparticularly challenging in integrated robot systems that includeknowledge-based reasoning methods and data-driven learning algorithms. Towardsaddressing this challenge, our architecture couples the complementary strengthsof non-monotonic logical reasoning, deep learning, and decision-tree induction.During reasoning and learning, the architecture enables a robot to provideon-demand relational descriptions of its decisions, beliefs, and the outcomesof hypothetical actions. These capabilities are grounded and evaluated in thecontext of scene understanding tasks and planning tasks performed usingsimulated images and images from a physical robot manipulating tabletopobjects.",Tiago Mota,2020/10/20,2020/10/20,,,"['cs.AI', 'cs.RO']"
2009.14786v2,Measuring Systematic Generalization in Neural Proof Generation with Transformers,http://arxiv.org/abs/2009.14786v2,"We are interested in understanding how well Transformer language models(TLMs) can perform reasoning tasks when trained on knowledge encoded in theform of natural language. We investigate their systematic generalizationabilities on a logical reasoning task in natural language, which involvesreasoning over relationships between entities grounded in first-order logicalproofs. Specifically, we perform soft theorem-proving by leveraging TLMs togenerate natural language proofs. We test the generated proofs for logicalconsistency, along with the accuracy of the final inference. We observelength-generalization issues when evaluated on longer-than-trained sequences.However, we observe TLMs improve their generalization performance after beingexposed to longer, exhaustive proofs. In addition, we discover that TLMs areable to generalize better using backward-chaining proofs compared to theirforward-chaining counterparts, while they find it easier to generate forwardchaining proofs. We observe that models that are not trained to generate proofsare better at generalizing to problems based on longer proofs. This suggeststhat Transformers have efficient internal reasoning strategies that are harderto interpret. These results highlight the systematic generalization behavior ofTLMs in the context of logical reasoning, and we believe this work motivatesdeeper inspection of their underlying reasoning strategies.",Nicolas Gontier,2020/9/30,2020/10/20,,,"['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']"
2008.13603v2,Deciding SHACL Shape Containment through Description Logics Reasoning (Extended Version),http://arxiv.org/abs/2008.13603v2,"The Shapes Constraint Language (SHACL) allows for formalizing constraintsover RDF data graphs. A shape groups a set of constraints that may be fulfilledby nodes in the RDF graph. We investigate the problem of containment betweenSHACL shapes. One shape is contained in a second shape if every graph nodemeeting the constraints of the first shape also meets the constraints of thesecond. To decide shape containment, we map SHACL shape graphs into descriptionlogic axioms such that shape containment can be answered by description logicreasoning. We identify several, increasingly tight syntactic restrictions ofSHACL for which this approach becomes sound and complete.",Martin Leinberger,2020/8/31,2021/4/22,,,['cs.LO']
2008.09514v1,Neural Logic Reasoning,http://arxiv.org/abs/2008.09514v1,"Recent years have witnessed the success of deep neural networks in manyresearch areas. The fundamental idea behind the design of most neural networksis to learn similarity patterns from data for prediction and inference, whichlacks the ability of cognitive reasoning. However, the concrete ability ofreasoning is critical to many theoretical and practical problems. On the otherhand, traditional symbolic reasoning methods do well in making logicalinference, but they are mostly hard rule-based reasoning, which limits theirgeneralization ability to different tasks since difference tasks may requiredifferent rules. Both reasoning and generalization ability are important forprediction tasks such as recommender systems, where reasoning provides strongconnection between user history and target items for accurate prediction, andgeneralization helps the model to draw a robust user portrait over noisyinputs.  In this paper, we propose Logic-Integrated Neural Network (LINN) to integratethe power of deep learning and logic reasoning. LINN is a dynamic neuralarchitecture that builds the computational graph according to input logicalexpressions. It learns basic logical operations such as AND, OR, NOT as neuralmodules, and conducts propositional logical reasoning through the network forinference. Experiments on theoretical task show that LINN achieves significantperformance on solving logical equations and variables. Furthermore, we testour approach on the practical task of recommendation by formulating the taskinto a logical inference problem. Experiments show that LINN significantlyoutperforms state-of-the-art recommendation models in Top-K recommendation,which verifies the potential of LINN in practice.",Shaoyun Shi,2020/8/20,2020/8/20,,,"['cs.LG', 'cs.AI', 'cs.IR', 'cs.LO', 'stat.ML']"
2008.01623v1,Semantic based model of Conceptual Work Products for formal verification of complex interactive systems,http://arxiv.org/abs/2008.01623v1,"Many clinical workflows depend on interactive computer systems for highlytechnical, conceptual work products, such as diagnoses, treatment plans, carecoordination, and case management. We describe an automatic logic reasoner toverify objective specifications for these highly technical, but abstract, workproducts that are essential to care. The conceptual work productsspecifications serve as a fundamental output requirement, which must be clearlystated, correct and solvable. There is strategic importance for suchspecifications because, in turn, they enable system model checking to verifythat machine functions taken with user procedures are actually able to achievethese abstract products. We chose case management of Multiple Sclerosis (MS)outpatients as our use case for its challenging complexity. As a first step, weillustrate how graphical class and state diagrams from UML can be developed andcritiqued with subject matter experts to serve as specifications of theconceptual work product of case management. A key feature is that thespecification must be declarative and thus independent of any process ortechnology. Our Work Domain Ontology with tools from Semantic Web is needed totranslate UML class and state diagrams for verification of solvability withautomatic reasoning. The solvable model will then be ready for subsequent usewith model checking on the system of human procedures and machine functions. Weused the expressive rule language SPARQL Inferencing Notation (SPIN) to developformal representations of the UML class diagram, the state machine, and theirinteractions. Using SPIN, we proved the consistency of the interactions ofstatic and dynamic concepts. We discussed how the new SPIN rule engine could beincorporated in the Object Management Group (OMG) Ontology Definition Metamodel(ODM)",Mohcine Madkour,2020/8/4,2020/8/4,,,"['cs.SE', 'stat.ML']"
2007.12020v1,Few-shot Visual Reasoning with Meta-analogical Contrastive Learning,http://arxiv.org/abs/2007.12020v1,"While humans can solve a visual puzzle that requires logical reasoning byobserving only few samples, it would require training over large amount of datafor state-of-the-art deep reasoning models to obtain similar performance on thesame task. In this work, we propose to solve such a few-shot (or low-shot)visual reasoning problem, by resorting to analogical reasoning, which is aunique human ability to identify structural or relational similarity betweentwo sets. Specifically, given training and test sets that contain the same typeof visual reasoning problems, we extract the structural relationships betweenelements in both domains, and enforce them to be as similar as possible withanalogical learning. We repeatedly apply this process with slightly modifiedqueries of the same problem under the assumption that it does not affect therelationship between a training and a test sample. This allows to learn therelational similarity between the two samples in an effective manner even witha single pair of samples. We validate our method on RAVEN dataset, on which itoutperforms state-of-the-art method, with larger gains when the training datais scarce. We further meta-learn our analogical contrastive learning model overthe same tasks with diverse attributes, and show that it generalizes to thesame visual reasoning problem with unseen attributes.",Youngsung Kim,2020/7/23,2020/7/23,,,"['cs.LG', 'cs.AI', 'stat.ML']"
2007.09402v1,Mapping computational thinking mindsets between educational levels with cognitive network science,http://arxiv.org/abs/2007.09402v1,"Computational thinking is a way of reasoning about the world in terms ofdata. This mindset channels number crunching toward an ambition to discoverknowledge through logic, models and simulations. Here we show how computationalcognitive science can be used to reconstruct and analyse the structure ofcomputational thinking mindsets (forma mentis in Latin) through complexnetworks. As a case study, we investigate cognitive networks tied to keyconcepts of computational thinking provided by: (i) 159 high school studentsenrolled in a science curriculum and (ii) 59 researchers in complex systems andsimulations. Researchers' reconstructed forma mentis highlighted a positivemindset about scientific modelling, semantically framing data and simulationsas ways of discovering nature. Students correctly identified different aspectsof logic reasoning but perceived ""computation"" as a distressing,anxiety-eliciting task, framed with math jargon and lacking links to real-worlddiscovery. Students' mindsets around ""data"", ""model"" and ""simulations""critically revealed no awareness of numerical modelling as a way forunderstanding the world. Our findings provide evidence of a crippledcomputational thinking mindset in students, who acquire mathematical skillsthat are not channelled toward real-world discovery through coding. Thisunlinked knowledge ends up being perceived as distressing number-crunchingexpertise with no relevant outcome. The virtuous mindset of researchersreported here indicates that computational thinking can be restored by trainingstudents specifically in coding, modelling and simulations in relation todiscovering nature. Our approach opens innovative ways for quantifyingcomputational thinking and enhancing its development through mindsetreconstruction.",Massimo Stella,2020/7/18,2020/7/18,,,"['physics.soc-ph', 'cs.CY', 'cs.SI', 'physics.ed-ph']"
2007.08124v1,LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning,http://arxiv.org/abs/2007.08124v1,"Machine reading is a fundamental task for testing the capability of naturallanguage understanding, which is closely related to human cognition in manyaspects. With the rising of deep learning techniques, algorithmic models rivalhuman performances on simple QA, and thus increasingly challenging machinereading datasets have been proposed. Though various challenges such as evidenceintegration and commonsense knowledge have been integrated, one of thefundamental capabilities in human reading, namely logical reasoning, is notfully investigated. We build a comprehensive dataset, named LogiQA, which issourced from expert-written questions for testing human Logical reasoning. Itconsists of 8,678 QA instances, covering multiple types of deductive reasoning.Results show that state-of-the-art neural models perform by far worse thanhuman ceiling. Our dataset can also serve as a benchmark for reinvestigatinglogical AI under the deep learning NLP setting. The dataset is freely availableat https://github.com/lgw863/LogiQA-dataset",Jian Liu,2020/7/16,2020/7/16,,,['cs.CL']
2007.07320v2,Learning Syllogism with Euler Neural-Networks,http://arxiv.org/abs/2007.07320v2,"Traditional neural networks represent everything as a vector, and are able toapproximate a subset of logical reasoning to a certain degree. As basic logicrelations are better represented by topological relations between regions, wepropose a novel neural network that represents everything as a ball and is ableto learn topological configuration as an Euler diagram. So comes the name EulerNeural-Network (ENN). The central vector of a ball is a vector that can inheritrepresentation power of traditional neural network. ENN distinguishes fourspatial statuses between balls, namely, being disconnected, being partiallyoverlapped, being part of, being inverse part of. Within each status, idealvalues are defined for efficient reasoning. A novel back-propagation algorithmwith six Rectified Spatial Units (ReSU) can optimize an Euler diagramrepresenting logical premises, from which logical conclusion can be deduced. Incontrast to traditional neural network, ENN can precisely represent all 24different structures of Syllogism. Two large datasets are created: oneextracted from WordNet-3.0 covers all types of Syllogism reasoning, the otherextracted all family relations from DBpedia. Experiment results approve thesuperior power of ENN in logical representation and reasoning. Datasets andsource code are available upon request.",Tiansi Dong,2020/7/14,2020/7/20,,,"['cs.LG', 'cs.AI', 'stat.ML']"
2007.03704v1,Computer-Aided Personalized Education,http://arxiv.org/abs/2007.03704v1,"The shortage of people trained in STEM fields is becoming acute, anduniversities and colleges are straining to satisfy this demand. In the case ofcomputer science, for instance, the number of US students taking introductorycourses has grown three-fold in the past decade. Recently, massive open onlinecourses (MOOCs) have been promoted as a way to ease this strain. This at bestprovides access to education. The bigger challenge though is coping withheterogeneous backgrounds of different students, retention, providing feedback,and assessment. Personalized education relying on computational tools canaddress this challenge.  While automated tutoring has been studied at different times in differentcommunities, recent advances in computing and education technology offerexciting opportunities to transform the manner in which students learn. Inparticular, at least three trends are significant. First, progress in logicalreasoning, data analytics, and natural language processing has led to tutoringtools for automatic assessment, personalized instruction including targetedfeedback, and adaptive content generation for a variety of subjects. Second,research in the science of learning and human-computer interaction is leadingto a better understanding of how different students learn, when and what typesof interventions are effective for different instructional goals, and how tomeasure the success of educational tools. Finally, the recent emergence ofonline education platforms, both in academia and industry, is leading to newopportunities for the development of a shared infrastructure. This CCC workshopbrought together researchers developing educational tools based on technologiessuch as logical reasoning and machine learning with researchers in education,human-computer interaction, and cognitive psychology.",Rajeev Alur,2020/7/7,2020/7/7,,,"['cs.CY', 'cs.HC']"
2007.00364v2,Medical idioms for clinical Bayesian network development,http://arxiv.org/abs/2007.00364v2,"Bayesian Networks (BNs) are graphical probabilistic models that have provenpopular in medical applications. While numerous medical BNs have beenpublished, most are presented fait accompli without explanation of how thenetwork structure was developed or justification of why it represents thecorrect structure for the given medical application. This means that theprocess of building medical BNs from experts is typically ad hoc and offerslittle opportunity for methodological improvement. This paper proposesgenerally applicable and reusable medical reasoning patterns to aid thosedeveloping medical BNs. The proposed method complements and extends theidiom-based approach introduced by Neil, Fenton, and Nielsen in 2000. Wepropose instances of their generic idioms that are specific to medical BNs. Werefer to the proposed medical reasoning patterns as medical idioms. Inaddition, we extend the use of idioms to represent interventional andcounterfactual reasoning. We believe that the proposed medical idioms arelogical reasoning patterns that can be combined, reused and applied genericallyto help develop medical BNs. All proposed medical idioms have been illustratedusing medical examples on coronary artery disease. The method has also beenapplied to other ongoing BNs being developed with medical experts. Finally, weshow that applying the proposed medical idioms to published BN models resultsin models with a clearer structure.",Evangelia Kyrimi,2020/7/1,2020/7/2,,,['cs.AI']
2006.15892v2,Matrix Shuffle-Exchange Networks for Hard 2D Tasks,http://arxiv.org/abs/2006.15892v2,"Convolutional neural networks have become the main tools for processingtwo-dimensional data. They work well for images, yet convolutions have alimited receptive field that prevents its applications to more complex 2Dtasks. We propose a new neural model, called Matrix Shuffle-Exchange network,that can efficiently exploit long-range dependencies in 2D data and hascomparable speed to a convolutional neural network. It is derived from NeuralShuffle-Exchange network and has $\mathcal{O}( \log{n})$ layers and$\mathcal{O}( n^2 \log{n})$ total time and space complexity for processing a $n\times n$ data matrix. We show that the Matrix Shuffle-Exchange network iswell-suited for algorithmic and logical reasoning tasks on matrices and densegraphs, exceeding convolutional and graph neural network baselines. Itsdistinct advantage is the capability of retaining full long-range dependencymodelling when generalizing to larger instances - much larger than could beprocessed with models equipped with a dense attention mechanism.",Emls Ozoli,2020/6/29,2020/10/5,,,"['cs.LG', 'stat.ML']"
2006.13155v1,Logical Neural Networks,http://arxiv.org/abs/2006.13155v1,"We propose a novel framework seamlessly providing key properties of bothneural nets (learning) and symbolic logic (knowledge and reasoning). Everyneuron has a meaning as a component of a formula in a weighted real-valuedlogic, yielding a highly intepretable disentangled representation. Inference isomnidirectional rather than focused on predefined target variables, andcorresponds to logical reasoning, including classical first-order logic theoremproving as a special case. The model is end-to-end differentiable, and learningminimizes a novel loss function capturing logical contradiction, yieldingresilience to inconsistent knowledge. It also enables the open-world assumptionby maintaining bounds on truth values which can have probabilistic semantics,yielding resilience to incomplete knowledge.",Ryan Riegel,2020/6/23,2020/6/23,,,"['cs.AI', 'cs.LG', 'cs.LO']"
2006.12146v1,ReCO: A Large Scale Chinese Reading Comprehension Dataset on Opinion,http://arxiv.org/abs/2006.12146v1,"This paper presents the ReCO, a human-curated ChineseReading Comprehensiondataset on Opinion. The questions in ReCO are opinion based queries issued tothe commercial search engine. The passages are provided by the crowdworkers whoextract the support snippet from the retrieved documents. Finally, anabstractive yes/no/uncertain answer was given by the crowdworkers. The releaseof ReCO consists of 300k questions that to our knowledge is the largest inChinese reading comprehension. A prominent characteristic of ReCO is that inaddition to the original context paragraph, we also provided the supportevidence that could be directly used to answer the question. Quality analysisdemonstrates the challenge of ReCO that requires various types of reasoningskills, such as causal inference, logical reasoning, etc. Current QA modelsthat perform very well on many question answering problems, such as BERT, onlyachieve 77% accuracy on this dataset, a large margin behind humans nearly 92%performance, indicating ReCO presents a good challenge for machine readingcomprehension. The codes, datasets are freely available athttps://github.com/benywon/ReCO.",BingningWang,2020/6/22,2020/6/22,,,['cs.CL']
2006.05896v4,A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning,http://arxiv.org/abs/2006.05896v4,"Much progress has been made in semi-supervised learning (SSL) by combiningmethods that exploit different aspects of the data distribution, e.g.consistency regularisation relies on properties of $p(x)$, whereas entropyminimisation pertains to the label distribution $p(y|x)$. Focusing on thelatter, we present a probabilistic model for discriminative SSL, that mirrorsits classical generative counterpart. Under the assumption $y|x$ isdeterministic, the prior over latent variables becomes discrete. We show thatseveral well-known SSL methods can be interpreted as approximating this prior,and can be improved upon. We extend the discriminative model to neuro-symbolicSSL, where label features satisfy logical rules, by showing such rules relatedirectly to the above prior, thus justifying a family of methods that linkstatistical learning and logical reasoning, and unifying them with regular SSL.",Carl Allen,2020/6/10,2021/5/31,,,"['cs.LG', 'cs.LO', 'stat.ML']"
2006.04757v3,Mathematical Reasoning via Self-supervised Skip-tree Training,http://arxiv.org/abs/2006.04757v3,"We examine whether self-supervised language modeling applied to mathematicalformulas enables logical reasoning. We suggest several logical reasoning tasksthat can be used to evaluate language models trained on formal mathematicalstatements, such as type inference, suggesting missing assumptions andcompleting equalities. To train language models for formal mathematics, wepropose a novel skip-tree task. We find that models trained on the skip-treetask show surprisingly strong mathematical reasoning abilities, and outperformmodels trained on standard skip-sequence tasks. We also analyze the models'ability to formulate new conjectures by measuring how often the predictions areprovable and useful in other proofs.",Markus N. Rabe,2020/6/8,2020/8/12,,,"['cs.LG', 'cs.AI', 'cs.PL', 'stat.ML']"
2006.04403v1,Global Robustness Verification Networks,http://arxiv.org/abs/2006.04403v1,"The wide deployment of deep neural networks, though achieving great successin many domains, has severe safety and reliability concerns. Existingadversarial attack generation and automatic verification techniques cannotformally verify whether a network is globally robust, i.e., the absence or notof adversarial examples in the input space. To address this problem, we developa global robustness verification framework with three components: 1) a novelrule-based ``back-propagation'' finding which input region is responsible forthe class assignment by logic reasoning; 2) a new network architecture SlidingDoor Network (SDN) enabling feasible rule-based ``back-propagation''; 3) aregion-based global robustness verification (RGRV) approach. Moreover, wedemonstrate the effectiveness of our approach on both synthetic and realdatasets.",Weidi Sun,2020/6/8,2020/6/8,,,['cs.LG']
2005.09089v4,Scaling Exact Inference for Discrete Probabilistic Programs,http://arxiv.org/abs/2005.09089v4,"Probabilistic programming languages (PPLs) are an expressive means ofrepresenting and reasoning about probabilistic models. The computationalchallenge of probabilistic inference remains the primary roadblock for applyingPPLs in practice. Inference is fundamentally hard, so there is no one-size-fitsall solution. In this work, we target scalable inference for an important classof probabilistic programs: those whose probability distributions are discrete.Discrete distributions are common in many fields, including text analysis,network verification, artificial intelligence, and graph analysis, but theyprove to be challenging for existing PPLs.  We develop a domain-specific probabilistic programming language called Dicethat features a new approach to exact discrete probabilistic program inference.Dice exploits program structure in order to factorize inference, enabling us toperform exact inference on probabilistic programs with hundreds of thousands ofrandom variables. Our key technical contribution is a new reduction fromdiscrete probabilistic programs to weighted model counting (WMC). Thisreduction separates the structure of the distribution from its parameters,enabling logical reasoning tools to exploit that structure for probabilisticinference. We (1) show how to compositionally reduce Dice inference to WMC, (2)prove this compilation correct with respect to a denotational semantics, (3)empirically demonstrate the performance benefits over prior approaches, and (4)analyze the types of structure that allow Dice to scale to large probabilisticprograms.",Steven Holtzen,2020/5/18,2020/10/16,,,['cs.PL']
2005.08129v5,Neural Collaborative Reasoning,http://arxiv.org/abs/2005.08129v5,"Existing Collaborative Filtering (CF) methods are mostly designed based onthe idea of matching, i.e., by learning user and item embeddings from datausing shallow or deep models, they try to capture the associative relevancepatterns in data, so that a user embedding can be matched with relevant itemembeddings using designed or learned similarity functions. However, as acognition rather than a perception intelligent task, recommendation requiresnot only the ability of pattern recognition and matching from data, but alsothe ability of cognitive reasoning in data. In this paper, we propose toadvance Collaborative Filtering (CF) to Collaborative Reasoning (CR), whichmeans that each user knows part of the reasoning space, and they collaboratefor reasoning in the space to estimate preferences for each other. Technically,we propose a Neural Collaborative Reasoning (NCR) framework to bridge learningand reasoning. Specifically, we integrate the power of representation learningand logical reasoning, where representations capture similarity patterns indata from perceptual perspectives, and logic facilitates cognitive reasoningfor informed decision making. An important challenge, however, is to bridgedifferentiable neural networks and symbolic reasoning in a shared architecturefor optimization and inference. To solve the problem, we propose a modularizedreasoning architecture, which learns logical operations such as AND ($\wedge$),OR ($\vee$) and NOT ($\neg$) as neural modules for implication reasoning($\rightarrow$). In this way, logical expressions can be equivalently organizedas neural networks, so that logical reasoning and prediction can be conductedin a continuous space. Experiments on real-world datasets verified theadvantages of our framework compared with both shallow, deep and reasoningmodels.",Hanxiong Chen,2020/5/16,2021/5/3,,,"['cs.IR', 'cs.AI', 'cs.LG', 'cs.SC']"
2005.00961v3,Bayesian Entailment Hypothesis: How Brains Implement Monotonic and Non-monotonic Reasoning,http://arxiv.org/abs/2005.00961v3,"Recent success of Bayesian methods in neuroscience and artificialintelligence gives rise to the hypothesis that the brain is a Bayesian machine.Since logic, as the laws of thought, is a product and practice of the humanbrain, it leads to another hypothesis that there is a Bayesian algorithm anddata-structure for logical reasoning. In this paper, we give a Bayesian accountof entailment and characterize its abstract inferential properties. TheBayesian entailment is shown to be a monotonic consequence relation in anextreme case. In general, it is a sort of non-monotonic consequence relationwithout Cautious monotony or Cut. The preferential entailment, which is arepresentative non-monotonic consequence relation, is shown to be maximum aposteriori entailment, which is an approximation of the Bayesian entailment. Wefinally discuss merits of our proposals in terms of encoding preferences ondefaults, handling change and contradiction, and modeling human entailment.",Hiroyuki Kido,2020/5/3,2021/1/27,,,"['cs.AI', 'cs.LG', 'cs.LO']"
2004.13577v1,Unifying Neural Learning and Symbolic Reasoning for Spinal Medical Report Generation,http://arxiv.org/abs/2004.13577v1,"Automated medical report generation in spine radiology, i.e., given spinalmedical images and directly create radiologist-level diagnosis reports tosupport clinical decision making, is a novel yet fundamental study in thedomain of artificial intelligence in healthcare. However, it is incrediblychallenging because it is an extremely complicated task that involves visualperception and high-level reasoning processes. In this paper, we propose theneural-symbolic learning (NSL) framework that performs human-like learning byunifying deep neural learning and symbolic logical reasoning for the spinalmedical report generation. Generally speaking, the NSL framework firstlyemploys deep neural learning to imitate human visual perception for detectingabnormalities of target spinal structures. Concretely, we design an adversarialgraph network that interpolates a symbolic graph reasoning module into agenerative adversarial network through embedding prior domain knowledge,achieving semantic segmentation of spinal structures with high complexity andvariability. NSL secondly conducts human-like symbolic logical reasoning thatrealizes unsupervised causal effect analysis of detected entities ofabnormalities through meta-interpretive learning. NSL finally fills thesediscoveries of target diseases into a unified template, successfully achievinga comprehensive medical report generation. When it employed in a real-worldclinical dataset, a series of empirical studies demonstrate its capacity onspinal medical report generation as well as show that our algorithm remarkablyexceeds existing methods in the detection of spinal structures. These indicateits potential as a clinical tool that contributes to computer-aided diagnosis.",Zhongyi Han,2020/4/28,2020/4/28,,,"['cs.CV', 'cs.AI', 'cs.LG', 'eess.IV']"
2004.02995v2,Multi-Step Inference for Reasoning Over Paragraphs,http://arxiv.org/abs/2004.02995v2,"Complex reasoning over text requires understanding and chaining togetherfree-form predicates and logical connectives. Prior work has largely tried todo this either symbolically or with black-box transformers. We present a middleground between these two extremes: a compositional model reminiscent of neuralmodule networks that can perform chained logical reasoning. This model firstfinds relevant sentences in the context and then chains them together usingneural modules. Our model gives significant performance improvements (up to29\% relative error reduction when comfibined with a reranker) on ROPES, arecently introduced complex reasoning dataset.",Jiangming Liu,2020/4/6,2021/6/7,,,['cs.CL']
2003.13159v1,Extending Automated Deduction for Commonsense Reasoning,http://arxiv.org/abs/2003.13159v1,"Commonsense reasoning has long been considered as one of the holy grails ofartificial intelligence. Most of the recent progress in the field has beenachieved by novel machine learning algorithms for natural language processing.However, without incorporating logical reasoning, these algorithms remainarguably shallow. With some notable exceptions, developers of practicalautomated logic-based reasoners have mostly avoided focusing on the problem.The paper argues that the methods and algorithms used by existing automatedreasoners for classical first-order logic can be extended towards commonsensereasoning. Instead of devising new specialized logics we propose a framework ofextensions to the mainstream resolution-based search methods to make thesecapable of performing search tasks for practical commonsense reasoning withreasonable efficiency. The proposed extensions mostly rely on operating onordinary proof trees and are devised to handle commonsense knowledge basescontaining inconsistencies, default rules, taxonomies, topics, relevance,confidence and similarity measures. We claim that machine learning is bestsuited for the construction of commonsense knowledge bases while the extendedlogic-based methods would be well-suited for actually answering queries fromthese knowledge bases.",Tanel Tammet,2020/3/29,2020/3/29,,,"['cs.AI', 'I.2.3; I.2.4']"
2003.08316v2,From Statistical Relational to Neuro-Symbolic Artificial Intelligence,http://arxiv.org/abs/2003.08316v2,Neuro-symbolic and statistical relational artificial intelligence bothintegrate frameworks for learning with logical reasoning. This surveyidentifies several parallels across seven different dimensions between thesetwo fields. These cannot only be used to characterize and positionneuro-symbolic artificial intelligence approaches but also to identify a numberof directions for further research.,Luc De Raedt,2020/3/18,2020/3/24,,,['cs.AI']
math/0508272v1,Reasonably complete forcing notions,http://arxiv.org/abs/math/0508272v1,"We introduce more properties of forcing notions which imply that theirlambda-support iterations are lambda-proper, where lambda is an inaccessiblecardinal. This paper is a direct continuation of section A.2 ofmath.LO/0210205. As an application of our iteration result we show that it isconsistent that dominating numbers associated with two normal filters on lambdaare distinct.",Andrzej Roslanowski,2005/8/16,2005/8/16,,,['math.LO']
math/0205325v1,On Successive Approximations To The Choice Problem and Logic,http://arxiv.org/abs/math/0205325v1,"This paper studies the formation of logical operations from pre-logicalprocesses. We are concerned with the reasons for certain mental processestaking form of logical reasoning and the underlying drives for consolidation oflogical operations in human mind. Starting from Piaget's approach to Logic(Piaget, 1956) we discuss whether the evolutionary adaptation can be such adriving force and whether the limits of human mind can result in the standardsystem of logical operations. The paper demonstrates that the classicaltwo-valued propositional logic can begin from a method of successiveapproximations applied to a decision-making problem within a framework ofSubject-in-an-environment survival. The presented results shed a new light onthe known model of human choice by Lefebvre (Lefebvre, 1991, 1995).",Valeriy Bulitko,2002/5/31,2002/5/31,,,"['math.LO', '03B05; 91E10']"
cond-mat/9808041v1,Neural networks and logical reasoning systems. A translation table,http://arxiv.org/abs/cond-mat/9808041v1,"A correspondence is established between the elements of logic reasoningsystems (knowledge bases, rules, inference and queries) and the hardware anddynamical operations of neural networks. The correspondence is framed as ageneral translation dictionary which, hopefully, will allow to go back andforth between symbolic and network formulations, a desirable step inlearning-oriented systems and multicomputer networks. In the framework of Hornclause logics it is found that atomic propositions with n arguments correspondto nodes with n-th order synapses, rules to synaptic intensity constraints,forward chaining to synaptic dynamics and queries either to simple nodeactivation or to a query tensor dynamics.",Joao Martins,1998/8/4,1998/8/4,,,['cond-mat.dis-nn']
cmp-lg/9609002v1,Inferring Acceptance and Rejection in Dialogue by Default Rules of Inference,http://arxiv.org/abs/cmp-lg/9609002v1,"This paper discusses the processes by which conversants in a dialogue caninfer whether their assertions and proposals have been accepted or rejected bytheir conversational partners. It expands on previous work by showing thatlogical consistency is a necessary indicator of acceptance, but that it is notsufficient, and that logical inconsistency is sufficient as an indicator ofrejection, but it is not necessary. I show how conversants can use informationstructure and prosody as well as logical reasoning in distinguishing betweenacceptances and logically consistent rejections, and relate this work toprevious work on implicature and default reasoning by introducing three newclasses of rejection: {\sc implicature rejections}, {\sc epistemic rejections}and {\sc deliberation rejections}. I show how these rejections are inferred asa result of default inferences, which, by other analyses, would have beenblocked by the context. In order to account for these facts, I propose a modelof the common ground that allows these default inferences to go through, andshow how the model, originally proposed to account for the various forms ofacceptance, can also model all types of rejection.",Marilyn A. Walker,1996/9/7,1996/9/7,,,"['cmp-lg', 'cs.CL']"
