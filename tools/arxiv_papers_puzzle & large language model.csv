Paper ID,Title,URL,Summary,First Author,Publish Date,Update Date,Code URL,Stars,Categories
2508.10358v1,What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles,http://arxiv.org/abs/2508.10358v1,"We investigate the capacity of Large Language Models (LLMs) for imaginativereasoning--the proactive construction, testing, and revision of hypotheses ininformation-sparse environments. Existing benchmarks, often static or focusedon social deduction, fail to capture the dynamic, exploratory nature of thisreasoning process. To address this gap, we introduce a comprehensive researchframework based on the classic ""Turtle Soup"" game, integrating a benchmark, anagent, and an evaluation protocol. We present TurtleSoup-Bench, the firstlarge-scale, bilingual, interactive benchmark for imaginative reasoning,comprising 800 turtle soup puzzles sourced from both the Internet and expertauthors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'performance in this setting. To evaluate reasoning quality, we develop amulti-dimensional protocol measuring logical consistency, detail completion,and conclusion alignment. Experiments with leading LLMs reveal clear capabilitylimits, common failure patterns, and a significant performance gap compared tohumans. Our work offers new insights into LLMs' imaginative reasoning andestablishes a foundation for future research on exploratory agent behavior.",Mengtao Zhou,2025/8/14,2025/8/14,,N/A,['cs.AI']
2508.10142v1,Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs,http://arxiv.org/abs/2508.10142v1,"Large language models (LLMs) excel at solving problems with clear andcomplete statements, but often struggle with nuanced environments orinteractive tasks which are common in most real-world scenarios. Thishighlights the critical need for developing LLMs that can effectively engage inlogically consistent multi-turn dialogue, seek information and reason withincomplete data. To this end, we introduce a novel benchmark comprising a suiteof multi-turn tasks each designed to test specific reasoning, interactivedialogue, and information-seeking abilities. These tasks have deterministicscoring mechanisms, thus eliminating the need for human intervention.Evaluating frontier models on our benchmark reveals significant headroom. Ouranalysis shows that most errors emerge from poor instruction following,reasoning failures, and poor planning. This benchmark provides valuableinsights into the strengths and weaknesses of current LLMs in handling complex,interactive scenarios and offers a robust platform for future research aimed atimproving these critical capabilities.",Kartikeya Badola,2025/8/13,2025/8/13,,N/A,['cs.CL']
2508.01306v1,PUZZLED: Jailbreaking LLMs through Word-Based Puzzles,http://arxiv.org/abs/2508.01306v1,"As large language models (LLMs) are increasingly deployed across diversedomains, ensuring their safety has become a critical concern. In response,studies on jailbreak attacks have been actively growing. Existing approachestypically rely on iterative prompt engineering or semantic transformations ofharmful instructions to evade detection. In this work, we introduce PUZZLED, anovel jailbreak method that leverages the LLM's reasoning capabilities. Itmasks keywords in a harmful instruction and presents them as word puzzles forthe LLM to solve. We design three puzzle types-word search, anagram, andcrossword-that are familiar to humans but cognitively demanding for LLMs. Themodel must solve the puzzle to uncover the masked words and then proceed togenerate responses to the reconstructed harmful instruction. We evaluatePUZZLED on five state-of-the-art LLMs and observe a high average attack successrate (ASR) of 88.8%, specifically 96.5% on GPT-4.1 and 92.3% on Claude 3.7Sonnet. PUZZLED is a simple yet powerful attack that transforms familiarpuzzles into an effective jailbreak strategy by harnessing LLMs' reasoningcapabilities.",Yelim Ahn,2025/8/2,2025/8/2,,N/A,"['cs.AI', 'cs.CR']"
2507.17699v1,Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning Models via Tool Augmentations,http://arxiv.org/abs/2507.17699v1,"Large Reasoning Models (LRMs) have become a central focus in today's largelanguage model (LLM) research, where models are designed to output astep-by-step thinking process before arriving at a final answer to handlecomplex reasoning tasks. Despite their promise, recent empirical studies (e.g.,[Shojaee et al., 2025] from Apple) suggest that this thinking process may notactually enhance reasoning ability, where LLMs without explicit reasoningactually outperform LRMs on tasks with low or high complexity. In this work, werevisit these findings and investigate whether the limitations of LRMs persistwhen tool augmentations are introduced. We incorporate two types of tools,Python interpreters and scratchpads, and evaluate three representative LLMs andtheir LRM counterparts on Apple's benchmark reasoning puzzles. Our results showthat, with proper tool use, LRMs consistently outperform their non-reasoningcounterparts across all levels of task complexity. These findings challenge therecent narrative that reasoning is an illusion and highlight the potential oftool-augmented LRMs for solving complex problems.",Zhao Song,2025/7/23,2025/7/23,,N/A,['cs.AI']
2507.14267v1,DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation,http://arxiv.org/abs/2507.14267v1,"Materials discovery relies on high-throughput, high-fidelity simulationtechniques such as Density Functional Theory (DFT), which require years oftraining, extensive parameter fine-tuning and systematic error handling. Toaddress these challenges, we introduce the DFT-based Research Engine forAgentic Materials Screening (DREAMS), a hierarchical, multi-agent framework forDFT simulation that combines a central Large Language Model (LLM) planner agentwith domain-specific LLM agents for atomistic structure generation, systematicDFT convergence testing, High-Performance Computing (HPC) scheduling, and errorhandling. In addition, a shared canvas helps the LLM agents to structure theirdiscussions, preserve context and prevent hallucination. We validate DREAMScapabilities on the Sol27LC lattice-constant benchmark, achieving averageerrors below 1\% compared to the results of human DFT experts. Furthermore, weapply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstratingits long-term and complex problem-solving capabilities. The framework againreproduces expert-level literature adsorption-energy differences. Finally,DREAMS is employed to quantify functional-driven uncertainties with Bayesianensemble sampling, confirming the Face Centered Cubic (FCC)-site preference atthe Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMSapproaches L3-level automation - autonomous exploration of a defined designspace - and significantly reduces the reliance on human expertise andintervention, offering a scalable path toward democratized, high-throughput,high-fidelity computational materials discovery.",Ziqi Wang,2025/7/18,2025/7/18,,N/A,"['cs.AI', 'cond-mat.mtrl-sci']"
2507.11932v1,Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs,http://arxiv.org/abs/2507.11932v1,"Mental visualization, the ability to construct and manipulate visualrepresentations internally, is a core component of human cognition and plays avital role in tasks involving reasoning, prediction, and abstraction. Despitethe rapid progress of Multimodal Large Language Models (MLLMs), currentbenchmarks primarily assess passive visual perception, offering limited insightinto the more active capability of internally constructing visual patterns tosupport problem solving. Yet mental visualization is a critical cognitive skillin humans, supporting abilities such as spatial navigation, predicting physicaltrajectories, and solving complex visual problems through imaginativesimulation. To bridge this gap, we introduce Hyperphantasia, a syntheticbenchmark designed to evaluate the mental visualization abilities of MLLMsthrough four carefully constructed puzzles. Each task is procedurally generatedand presented at three difficulty levels, enabling controlled analysis of modelperformance across increasing complexity. Our comprehensive evaluation ofstate-of-the-art models reveals a substantial gap between the performance ofhumans and MLLMs. Additionally, we explore the potential of reinforcementlearning to improve visual simulation capabilities. Our findings suggest thatwhile some models exhibit partial competence in recognizing visual patterns,robust mental visualization remains an open challenge for current MLLMs.",Mohammad Shahab Sepehri,2025/7/16,2025/7/16,,N/A,['cs.CV']
2507.07313v1,Frontier LLMs Still Struggle with Simple Reasoning Tasks,http://arxiv.org/abs/2507.07313v1,"While state-of-the-art large language models (LLMs) demonstrate advancedreasoning capabilities-achieving remarkable performance on challengingcompetitive math and coding benchmarks-they also frequently fail on tasks thatare easy for humans. This work studies the performance of frontier LLMs on abroad set of such ""easy"" reasoning problems. By extending previous work in theliterature, we create a suite of procedurally generated simple reasoning tasks,including counting, first-order logic, proof trees, and travel planning, withchangeable parameters (such as document length. or the number of variables in amath problem) that can arbitrarily increase the amount of computation requiredto produce the answer while preserving the fundamental difficulty. Whileprevious work showed that traditional, non-thinking models can be made to failon such problems, we demonstrate that even state-of-the-art thinking modelsconsistently fail on such problems and for similar reasons (e.g. statisticalshortcuts, errors in intermediate steps, and difficulties in processing longcontexts). To further understand the behavior of the models, we introduce theunpuzzles dataset, a different ""easy"" benchmark consisting of trivializedversions of well-known math and logic puzzles. Interestingly, while modern LLMsexcel at solving the original puzzles, they tend to fail on the trivializedversions, exhibiting several systematic failure patterns related to memorizingthe originals. We show that this happens even if the models are otherwise ableto solve problems with different descriptions but requiring the same logic. Ourresults highlight that out-of-distribution generalization is still problematicfor frontier language models and the new generation of thinking models, evenfor simple reasoning tasks, and making tasks easier does not necessarily implyimproved performance.",Alan Malek,2025/7/9,2025/7/9,,N/A,"['cs.CL', 'cs.AI', 'cs.LG']"
2507.04699v1,A Visual Leap in CLIP Compositionality Reasoning through Generation of Counterfactual Sets,http://arxiv.org/abs/2507.04699v1,"Vision-language models (VLMs) often struggle with compositional reasoning dueto insufficient high-quality image-text data. To tackle this challenge, wepropose a novel block-based diffusion approach that automatically generatescounterfactual datasets without manual annotation. Our method utilizes largelanguage models to identify entities and their spatial relationships. It thenindependently generates image blocks as ""puzzle pieces"" coherently arrangedaccording to specified compositional rules. This process creates diverse,high-fidelity counterfactual image-text pairs with precisely controlledvariations. In addition, we introduce a specialized loss function thatdifferentiates inter-set from intra-set samples, enhancing training efficiencyand reducing the need for negative samples. Experiments demonstrate thatfine-tuning VLMs with our counterfactual datasets significantly improves visualreasoning performance. Our approach achieves state-of-the-art results acrossmultiple benchmarks while using substantially less training data than existingmethods.",Zexi Jia,2025/7/7,2025/7/7,,N/A,['cs.CV']
2507.00092v1,Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models,http://arxiv.org/abs/2507.00092v1,"Large Language Models (LLMs) have demonstrated remarkable capabilities atsolving complex reasoning tasks with Chain-of-Thought (CoT) prompting, buttheir decision-making processes remain somewhat blackbox. We introducetextbfinverse reasoning, a novel paradigm enabling LLMs to decompose andexplain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a4-billion-parameter reasoning model, employs a metacognitive structure thatreflects back via attention processes to identify major decision points andgenerate explanations of reasoning choices. While typical CoT approaches aredirected towards forward reasoning generation, inverse reasoning providesinsight into why specific reasoning chains were selected over others. Throughthorough testing of logical reasoning puzzles, math problems and ethicaldilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, wedemonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) forits task, and offers performance almost on par with models like Claude-3.5Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework forLLM self-reflection via inverse reasoning, (ii) a novel metalearning frameworkto reverse the attention flow, (iii) comprehensive evaluation frameworks forreasoning transparency, and (iv) evidence that increasing reasoning usinginverse reasoning improves interpretability along with reasoning performance.Our work creates new avenues for transparent AI systems and closes significantgaps in AI safety, education, and scientific discovery.",Basab Jha,2025/6/30,2025/6/30,,N/A,"['cs.AI', 'cs.CL', 'cs.LG']"
2506.23508v1,Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably,http://arxiv.org/abs/2506.23508v1,"Post-training algorithms such as Supervised Fine-Tuning (SFT) andReinforcement Fine-Tuning (RFT) are widely used to adapt multimodal largelanguage models to downstream tasks. While effective at task adaptation, theirimpact on prior knowledge remains unclear. In this paper, we introduce jigsawpuzzles as a novel task absent from existing pretraining corpora andsystematically study the behavior of SFT and RFT on an open-source multimodalmodel, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapidtask acquisition but leads to catastrophic forgetting, whereas RFT learns moreslowly on novel tasks but maintains prior knowledge. We analyze this phenomenonthrough the lens of learning dynamics, showing that RFT reinforces correctsamples that are naturally aligned with the base model's probability landscape,mitigating interference with prior knowledge. Moreover, supervised training oncorrect RFT-simulated rollouts allows SFT to preserve knowledge while rapidlylearning new tasks. These findings suggest that data distribution, rather thanalgorithmic differences, plays a central role in forgetting, and highlightRFT's potential for stable continual learning in multimodal large languagemodels.",Zhihao Zhang,2025/6/30,2025/6/30,,N/A,"['cs.CL', 'cs.AI']"
2506.21734v3,Hierarchical Reasoning Model,http://arxiv.org/abs/2506.21734v3,"Reasoning, the process of devising and executing complex goal-oriented actionsequences, remains a critical challenge in AI. Current large language models(LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer frombrittle task decomposition, extensive data requirements, and high latency.Inspired by the hierarchical and multi-timescale processing in the human brain,we propose the Hierarchical Reasoning Model (HRM), a novel recurrentarchitecture that attains significant computational depth while maintainingboth training stability and efficiency. HRM executes sequential reasoning tasksin a single forward pass without explicit supervision of the intermediateprocess, through two interdependent recurrent modules: a high-level moduleresponsible for slow, abstract planning, and a low-level module handling rapid,detailed computations. With only 27 million parameters, HRM achievesexceptional performance on complex reasoning tasks using only 1000 trainingsamples. The model operates without pre-training or CoT data, yet achievesnearly perfect performance on challenging tasks including complex Sudokupuzzles and optimal path finding in large mazes. Furthermore, HRM outperformsmuch larger models with significantly longer context windows on the Abstractionand Reasoning Corpus (ARC), a key benchmark for measuring artificial generalintelligence capabilities. These results underscore HRM's potential as atransformative advancement toward universal computation and general-purposereasoning systems.",Guan Wang,2025/6/26,2025/8/4,,N/A,"['cs.AI', 'cs.LG']"
2506.19095v1,Baba is LLM: Reasoning in a Game with Dynamic Rules,http://arxiv.org/abs/2506.19095v1,"Large language models (LLMs) are known to perform well on language tasks, butstruggle with reasoning tasks. This paper explores the ability of LLMs to playthe 2D puzzle game Baba is You, in which players manipulate rules byrearranging text blocks that define object properties. Given that thisrule-manipulation relies on language abilities and reasoning, it is acompelling challenge for LLMs. Six LLMs are evaluated using different prompttypes, including (1) simple, (2) rule-extended and (3) action-extended prompts.In addition, two models (Mistral, OLMo) are finetuned using textual andstructural data from the game. Results show that while larger models(particularly GPT-4o) perform better in reasoning and puzzle solving, smallerunadapted models struggle to recognize game mechanics or apply rule changes.Finetuning improves the ability to analyze the game levels, but does notsignificantly improve solution formulation. We conclude that even forstate-of-the-art and finetuned LLMs, reasoning about dynamic rule changes isdifficult (specifically, understanding the use-mention distinction). Theresults provide insights into the applicability of LLMs to complexproblem-solving tasks and highlight the suitability of games with dynamicallychanging rules for testing reasoning and reflection by LLMs.",Fien van Wetten,2025/6/23,2025/6/23,,N/A,['cs.AI']
2506.18485v1,MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models,http://arxiv.org/abs/2506.18485v1,"Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as apowerful learn-to-reason paradigm for Large Language Models (LLMs) to tacklecomplex reasoning tasks. However, existing RLVR methods overlook one of themost distinctive capabilities of LLMs, their in-context learning ability, asprominently demonstrated by the success of Chain-of-Thought (CoT) prompting.This motivates us to explore how reinforcement learning can be effectivelycombined with in-context learning to better improve the reasoning capabilitiesof LLMs. In this paper, we introduce Motivation-enhanced ReinforcementFinetuning} (MeRF), an intuitive yet effective method enhancing reinforcementlearning of LLMs by involving ``telling LLMs the rules of the game''.Specifically, MeRF directly injects the reward specification into the prompt,which serves as an in-context motivation for model to improve its responseswith awareness of the optimization objective. This simple modificationleverages the in-context learning ability of LLMs aligning generation withoptimization, thereby incentivizing the model to generate desired outputs fromboth inner motivation and external reward. Empirical evaluations on the Knightsand Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that\texttt{MeRF} achieves substantial performance gains over baselines. Moreover,ablation studies show that performance improves with greater consistencybetween the in-context motivation and the external reward function, while themodel also demonstrates an ability to adapt to misleading motivations throughreinforcement learning.",Junjie Zhang,2025/6/23,2025/6/23,,N/A,"['cs.CL', 'cs.AI']"
2506.13886v1,Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles,http://arxiv.org/abs/2506.13886v1,"Across languages, numeral systems vary widely in how they construct andcombine numbers. While humans consistently learn to navigate this diversity,large language models (LLMs) struggle with linguistic-mathematical puzzlesinvolving cross-linguistic numeral systems, which humans can learn to solvesuccessfully. We investigate why this task is difficult for LLMs through aseries of experiments that untangle the linguistic and mathematical aspects ofnumbers in language. Our experiments establish that models cannot consistentlysolve such problems unless the mathematical operations in the problems areexplicitly marked using known symbols ($+$, $\times$, etc, as in ""twenty +three""). In further ablation studies, we probe how individual parameters ofnumeral construction and combination affect performance. While humans use theirlinguistic understanding of numbers to make inferences about the implicitcompositional structure of numerals, LLMs seem to lack this notion of implicitnumeral structure. We conclude that the ability to flexibly infer compositionalrules from implicit patterns in human-scale data remains an open challenge forcurrent reasoning models.",Antara Raaghavi Bhattacharya,2025/6/16,2025/6/16,,N/A,"['cs.CL', 'cs.AI']"
2506.10887v2,Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers,http://arxiv.org/abs/2506.10887v2,"Large language models (LLMs) can acquire new knowledge through fine-tuning,but this process exhibits a puzzling duality: models can generalize remarkablyfrom new facts, yet are also prone to hallucinating incorrect information.However, the reasons for this phenomenon remain poorly understood. In thiswork, we argue that both behaviors stem from a single mechanism known asout-of-context reasoning (OCR): the ability to deduce implications byassociating concepts, even those without a causal link. Our experiments acrossfive prominent LLMs confirm that OCR indeed drives both generalization andhallucination, depending on whether the associated concepts are causallyrelated. To build a rigorous theoretical understanding of this phenomenon, wethen formalize OCR as a synthetic factual recall task. We empirically show thata one-layer single-head attention-only transformer with factorized output andvalue matrices can learn to solve this task, while a model with combinedweights cannot, highlighting the crucial role of matrix factorization. Ourtheoretical analysis shows that the OCR capability can be attributed to theimplicit bias of gradient descent, which favors solutions that minimize thenuclear norm of the combined output-value matrix. This mathematical structureexplains why the model learns to associate facts and implications with highsample efficiency, regardless of whether the correlation is causal or merelyspurious. Ultimately, our work provides a theoretical foundation forunderstanding the OCR phenomenon, offering a new lens for analyzing andmitigating undesirable behaviors from knowledge injection.",Yixiao Huang,2025/6/12,2025/7/4,,N/A,"['cs.CL', 'cs.LG']"
2506.08295v1,From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?,http://arxiv.org/abs/2506.08295v1,"While existing benchmarks probe the reasoning abilities of large languagemodels (LLMs) across diverse domains, they predominantly assess passivereasoning, providing models with all the information needed to reach asolution. By contrast, active reasoning-where an LLM must interact withexternal systems to acquire missing evidence or data-has received littlesystematic attention. To address this shortfall, we present AR-Bench, a novelbenchmark designed explicitly to evaluate an LLM's active reasoning skills.AR-Bench comprises three task families-detective cases, situation puzzles, andguessing numbers-that together simulate real-world, agentic scenarios andmeasure performance across commonsense, logical, and symbolic reasoningchallenges. Empirical evaluation on AR-Bench demonstrates that contemporaryLLMs exhibit pronounced difficulties with active reasoning: they frequentlyfail to acquire or leverage the information needed to solve tasks. This gaphighlights a stark divergence between their passive and active reasoningabilities. Moreover, ablation studies indicate that even advanced strategies,such as tree-based searching or post-training approaches, yield only modestgains and fall short of the levels required for real-world deployment.Collectively, these findings highlight the critical need to advance methodologyfor active reasoning, e.g., incorporating interactive learning, real-timefeedback loops, and environment-aware objectives for training. The benchmark ispublicly available at: https://github.com/tmlr-group/AR-Bench.",Zhanke Zhou,2025/6/9,2025/6/9,,N/A,"['cs.LG', 'cs.AI', 'cs.CL']"
2506.06524v1,ScriptDoctor: Automatic Generation of PuzzleScript Games via Large Language Models and Tree Search,http://arxiv.org/abs/2506.06524v1,"There is much interest in using large pre-trained models in Automatic GameDesign (AGD), whether via the generation of code, assets, or more abstractconceptualization of design ideas. But so far this interest largely stems fromthe ad hoc use of such generative models under persistent human supervision.Much work remains to show how these tools can be integrated intolonger-time-horizon AGD pipelines, in which systems interface with game enginesto test generated content autonomously. To this end, we introduce ScriptDoctor,a Large Language Model (LLM)-driven system for automatically generating andtesting games in PuzzleScript, an expressive but highly constrained descriptionlanguage for turn-based puzzle games over 2D gridworlds. ScriptDoctor generatesand tests game design ideas in an iterative loop, where human-authored examplesare used to ground the system's output, compilation errors from thePuzzleScript engine are used to elicit functional code, and search-based agentsplay-test generated games. ScriptDoctor serves as a concrete example of thepotential of automated, open-ended LLM-based workflows in generating novel gamecontent.",Sam Earle,2025/6/6,2025/6/6,,N/A,"['cs.AI', 'cs.HC']"
2506.04821v1,LogicPuzzleRL: Cultivating Robust Mathematical Reasoning in LLMs via Reinforcement Learning,http://arxiv.org/abs/2506.04821v1,"Large language models (LLMs) excel at many supervised tasks but oftenstruggle with structured reasoning in unfamiliar settings. This discrepancysuggests that standard fine-tuning pipelines may instill narrow,domain-specific heuristics rather than fostering general-purpose thinkingstrategies. In this work, we propose a ""play to learn"" framework thatfine-tunes LLMs through reinforcement learning on a suite of seven custom logicpuzzles, each designed to cultivate distinct reasoning skills such asconstraint propagation, spatial consistency, and symbolic deduction. Using areinforcement learning setup with verifiable rewards, models receive binaryfeedback based on puzzle correctness, encouraging iterative, hypothesis-drivenproblem solving. We demonstrate that this training approach significantlyimproves out-of-distribution performance on a range of mathematical benchmarks,especially for mid-difficulty problems that require multi-step reasoning.Analyses across problem categories and difficulty levels reveal that puzzletraining promotes transferable reasoning routines, strengthening algebraicmanipulation, geometric inference, and combinatorial logic, while offeringlimited gains on rote or highly specialized tasks. These findings show thatreinforcement learning over logic puzzles reshapes the internal reasoning ofLLMs, enabling more robust and compositional generalization without relying ontask-specific symbolic tools.",Zhen Hao Wong,2025/6/5,2025/6/5,,N/A,['cs.LG']
2506.04633v1,Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual Simulations,http://arxiv.org/abs/2506.04633v1,"Spatial cognition is essential for human intelligence, enablingproblem-solving through visual simulations rather than solely relying on verbalreasoning. However, existing AI benchmarks primarily assess verbal reasoning,neglecting the complexities of non-verbal, multi-step visual simulation. Weintroduce STARE(Spatial Transformations and Reasoning Evaluation), a benchmarkdesigned to rigorously evaluate multimodal large language models on tasksbetter solved through multi-step visual simulation. STARE features 4K tasksspanning foundational geometric transformations (2D and 3D), integrated spatialreasoning (cube net folding and tangram puzzles), and real-world spatialreasoning (perspective and temporal reasoning), reflecting practical cognitivechallenges like object assembly, mechanical diagram interpretation, andeveryday spatial navigation. Our evaluations show that models excel atreasoning over simpler 2D transformations, but perform close to random chanceon more complex tasks like 3D cube net folding and tangram puzzles that requiremulti-step visual simulations. Humans achieve near-perfect accuracy but takeconsiderable time (up to 28.9s) on complex tasks, significantly speeding up(down by 7.5 seconds on average) with intermediate visual simulations. Incontrast, models exhibit inconsistent performance gains from visualsimulations, improving on most tasks but declining in specific cases liketangram puzzles (GPT-4o, o1) and cube net folding (Claude-3.5, Gemini-2.0Flash), indicating that models may not know how to effectively leverageintermediate visual information.",Linjie Li,2025/6/5,2025/6/5,,N/A,['cs.CV']
2506.11080v1,MANBench: Is Your Multimodal Model Smarter than Human?,http://arxiv.org/abs/2506.11080v1,"The rapid advancement of Multimodal Large Language Models (MLLMs) has igniteddiscussions regarding their potential to surpass human performance inmultimodal tasks. In response, we introduce MANBench (Multimodal Ability NormsBenchmark), a bilingual benchmark (English and Chinese) comprising 1,314questions across nine tasks, spanning knowledge-based and non-knowledge-baseddomains. MANBench emphasizes intuitive reasoning, seamless cross-modalintegration, and real-world complexity, providing a rigorous evaluationframework.  Through extensive human experiments involving diverse participants, wecompared human performance against state-of-the-art MLLMs. The results indicatethat while MLLMs excel in tasks like Knowledge and Text-Image Understanding,they struggle with deeper cross-modal reasoning tasks such as TransmorphicUnderstanding, Image Consistency, and Multi-image Understanding. Moreover, bothhumans and MLLMs face challenges in highly complex tasks like Puzzles andSpatial Imagination.  MANBench highlights the strengths and limitations of MLLMs, revealing thateven advanced models fall short of achieving human-level performance acrossmany domains. We hope MANBench will inspire efforts to bridge the gap betweenMLLMs and human multimodal capabilities. The code and dataset are available athttps://github.com/micdz/MANBench.",Han Zhou,2025/6/4,2025/6/4,,N/A,['cs.CL']
2506.02911v1,Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning,http://arxiv.org/abs/2506.02911v1,"Cell type annotation is a key task in analyzing the heterogeneity ofsingle-cell RNA sequencing data. Although recent foundation models automatethis process, they typically annotate cells independently, without consideringbatch-level cellular context or providing explanatory reasoning. In contrast,human experts often annotate distinct cell types for different cell clustersbased on their domain knowledge. To mimic this workflow, we introduce theCellPuzzles task, where the objective is to assign unique cell types to a batchof cells. This benchmark spans diverse tissues, diseases, and donor conditions,and requires reasoning across the batch-level cellular context to ensure labeluniqueness. We find that off-the-shelf large language models (LLMs) struggle onCellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trainedvia supervised fine-tuning on distilled reasoning traces, followed byreinforcement learning with batch-level rewards. Cell-o1 achievesstate-of-the-art performance, outperforming o1 by over 73% and generalizingwell across contexts. Further analysis of training dynamics and reasoningbehaviors provides insights into batch-level annotation performance andemergent expert-like reasoning. Code and data are available athttps://github.com/ncbi-nlp/cell-o1.",Yin Fang,2025/6/3,2025/6/3,,N/A,"['cs.CL', 'cs.AI', 'cs.CE', 'cs.HC', 'cs.LG']"
2505.24273v1,How Much Backtracking is Enough? Exploring the Interplay of SFT and RL in Enhancing LLM Reasoning,http://arxiv.org/abs/2505.24273v1,"Recent breakthroughs in large language models (LLMs) have effectivelyimproved their reasoning abilities, particularly on mathematical and logicalproblems that have verifiable answers, through techniques such as supervisedfinetuning (SFT) and reinforcement learning (RL). Prior research indicates thatRL effectively internalizes search strategies, enabling long chain-of-thought(CoT) reasoning, with backtracking emerging naturally as a learned capability.However, the precise benefits of backtracking, specifically, how significantlyit contributes to reasoning improvements and the optimal extent of its use,remain poorly understood. In this work, we systematically investigate thedynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and SelfReference. Our findings highlight that short CoT sequences used in SFT as awarm-up do have moderate contribution to RL training, compared with cold-startRL; however such contribution diminishes when tasks become increasinglydifficult. Motivated by this observation, we construct synthetic datasetsvarying systematically in the number of backtracking steps and conductcontrolled experiments to isolate the influence of either the correctness(content) or the structure (i.e., backtrack frequency). We find that (1) longerCoT with backtracks generally induce better and more stable RL training, (2)more challenging problems with larger search space tend to need higher numbersof backtracks during the SFT stage. Additionally, we demonstrate throughexperiments on distilled data that RL training is largely unaffected by thecorrectness of long CoT sequences, suggesting that RL prioritizes structuralpatterns over content correctness. Collectively, our results offer practicalinsights into designing optimal training strategies to effectively scalereasoning in LLMs.",Hongyi James Cai,2025/5/30,2025/5/30,,N/A,['cs.AI']
2505.23590v2,Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles,http://arxiv.org/abs/2505.23590v2,"The application of rule-based reinforcement learning (RL) to multimodal largelanguage models (MLLMs) introduces unique challenges and potential deviationsfrom findings in text-only domains, particularly for perception-heavy tasks.This paper provides a comprehensive study of rule-based visual RL, using jigsawpuzzles as a structured experimental framework. Jigsaw puzzles offer inherentground truth, adjustable difficulty, and demand complex decision-making, makingthem ideal for this study. Our research reveals several key findings:\textit{Firstly,} we find that MLLMs, initially performing near to randomguessing on the simplest jigsaw puzzles, achieve near-perfect accuracy andgeneralize to complex, unseen configurations through fine-tuning.\textit{Secondly,} training on jigsaw puzzles can induce generalization toother visual tasks, with effectiveness tied to specific task configurations.\textit{Thirdly,} MLLMs can learn and generalize with or without explicitreasoning, though open-source models often favor direct answering.Consequently, even when trained for step-by-step reasoning, they can ignore thethinking process in deriving the final answer. \textit{Fourthly,} we observethat complex reasoning patterns appear to be pre-existing rather than emergent,with their frequency increasing alongside training and task difficulty.\textit{Finally,} our results demonstrate that RL exhibits more effectivegeneralization than Supervised Fine-Tuning (SFT), and an initial SFT cold startphase can hinder subsequent RL optimization. Although these observations arebased on jigsaw puzzles and may vary across other visual tasks, this researchcontributes a valuable piece of jigsaw to the larger puzzle of collectiveunderstanding rule-based visual RL and its potential in multimodal learning.The code is available at: https://github.com/zifuwanggg/Jigsaw-R1.",Zifu Wang,2025/5/29,2025/6/2,,N/A,"['cs.CV', 'cs.AI', 'cs.CL']"
2505.19914v2,Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles,http://arxiv.org/abs/2505.19914v2,"Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel atadvanced reasoning tasks like math and coding via Reinforcement Learning withVerifiable Rewards (RLVR), but still struggle with puzzles solvable by humanswithout domain knowledge. We introduce Enigmata, the first comprehensive suitetailored for improving LLMs with puzzle reasoning skills. It includes 36 tasksacross seven categories, each with 1) a generator that produces unlimitedexamples with controllable difficulty and 2) a rule-based verifier forautomatic evaluation. This generator-verifier design supports scalable,multi-task RL training, fine-grained analysis, and seamless RLVR integration.We further propose Enigmata-Eval, a rigorous benchmark, and develop optimizedmulti-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarkslike Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizeswell to out-of-domain puzzle benchmarks and mathematical reasoning, with littlemulti-tasking trade-off. When trained on larger models like Seed1.5-Thinking(20B activated parameters and 200B total parameters), puzzle data from Enigmatafurther boosts SoTA performance on advanced math and STEM reasoning tasks suchas AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalizationbenefits of Enigmata. This work offers a unified, controllable framework foradvancing logical reasoning in LLMs. Resources of this work can be found athttps://seed-enigmata.github.io.",Jiangjie Chen,2025/5/26,2025/6/9,,N/A,"['cs.CL', 'cs.AI']"
2505.18901v1,PromptWise: Online Learning for Cost-Aware Prompt Assignment in Generative Models,http://arxiv.org/abs/2505.18901v1,"The rapid advancement of generative AI models has provided users withnumerous options to address their prompts. When selecting a generative AI modelfor a given prompt, users should consider not only the performance of thechosen model but also its associated service cost. The principle guiding suchconsideration is to select the least expensive model among the availablesatisfactory options. However, existing model-selection approaches typicallyprioritize performance, overlooking pricing differences between models. In thispaper, we introduce PromptWise, an online learning framework designed to assigna sequence of prompts to a group of large language models (LLMs) in acost-effective manner. PromptWise strategically queries cheaper models first,progressing to more expensive options only if the lower-cost models fail toadequately address a given prompt. Through numerical experiments, wedemonstrate PromptWise's effectiveness across various tasks, including puzzlesof varying complexity and code generation/translation tasks. The resultshighlight that PromptWise consistently outperforms cost-unaware baselinemethods, emphasizing that directly assigning prompts to the most expensivemodels can lead to higher costs and potentially lower average performance.",Xiaoyan Hu,2025/5/24,2025/5/24,,N/A,"['cs.LG', 'cs.AI']"
2505.17225v1,Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models,http://arxiv.org/abs/2505.17225v1,"Large language models have demonstrated remarkable proficiency in long andcomplex reasoning tasks. However, they frequently exhibit a problematicreliance on familiar reasoning patterns, a phenomenon we term \textit{reasoningrigidity}. Despite explicit instructions from users, these models oftenoverride clearly stated conditions and default to habitual reasoningtrajectories, leading to incorrect conclusions. This behavior presentssignificant challenges, particularly in domains such as mathematics and logicpuzzle, where precise adherence to specified constraints is critical. Tosystematically investigate reasoning rigidity, a behavior largely unexplored inprior work, we introduce a expert-curated diagnostic set, \dataset{}. Ourdataset includes specially modified variants of existing mathematicalbenchmarks, namely AIME and MATH500, as well as well-known puzzles deliberatelyredesigned to require deviation from familiar reasoning strategies. Using thisdataset, we identify recurring contamination patterns that occur when modelsdefault to ingrained reasoning. Specifically, we categorize this contaminationinto three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust,and (iii) Partial Instruction Attention, each causing models to ignore ordistort provided instructions. We publicly release our diagnostic set tofacilitate future research on mitigating reasoning rigidity in language models.",Doohyuk Jang,2025/5/22,2025/5/22,,N/A,['cs.AI']
2505.16135v1,Sudoku-Bench: Evaluating creative reasoning with Sudoku variants,http://arxiv.org/abs/2505.16135v1,"Existing reasoning benchmarks for large language models (LLMs) frequentlyfail to capture authentic creativity, often rewarding memorization ofpreviously observed patterns. We address this shortcoming with Sudoku-Bench, acurated benchmark of challenging and unconventional Sudoku variantsspecifically selected to evaluate creative, multi-step logical reasoning.Sudoku variants form an unusually effective domain for reasoning research: eachpuzzle introduces unique or subtly interacting constraints, making memorizationinfeasible and requiring solvers to identify novel logical breakthroughs(``break-ins''). Despite their diversity, Sudoku variants maintain a common andcompact structure, enabling clear and consistent evaluation. Sudoku-Benchincludes a carefully chosen puzzle set, a standardized text-based puzzlerepresentation, and flexible tools compatible with thousands of publiclyavailable puzzles -- making it easy to extend into a general researchenvironment. Baseline experiments show that state-of-the-art LLMs solve fewerthan 15\% of puzzles unaided, highlighting significant opportunities to advancelong-horizon, strategic reasoning capabilities.",Jeffrey Seely,2025/5/22,2025/5/22,,N/A,"['cs.AI', 'cs.CL', 'cs.LG']"
2505.16114v1,Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language,http://arxiv.org/abs/2505.16114v1,"Solving puzzles in natural language poses a long-standing challenge in AI.While large language models (LLMs) have recently shown impressive capabilitiesin a variety of tasks, they continue to struggle with complex puzzles thatdemand precise reasoning and exhaustive search. In this paper, we proposeLogic-of-Thought (Logot), a novel framework that bridges LLMs with logicprogramming to address this problem. Our method leverages LLMs to translatepuzzle rules and states into answer set programs (ASPs), the solution of whichare then accurately and efficiently inferred by an ASP interpreter. This hybridapproach combines the natural language understanding of LLMs with the precisereasoning capabilities of logic programs. We evaluate our method on variousgrid puzzles and dynamic puzzles involving actions, demonstrating near-perfectaccuracy across all tasks. Our code and data are available at:https://github.com/naiqili/Logic-of-Thought.",Naiqi Li,2025/5/22,2025/5/22,,N/A,['cs.AI']
2505.16088v2,Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning,http://arxiv.org/abs/2505.16088v2,"Modern BPE tokenizers often split calendar dates into meaningless fragments,e.g., 20250312 $\rightarrow$ 202, 503, 12, inflating token counts and obscuringthe inherent structure needed for robust temporal reasoning. In this work, we(1) introduce a simple yet interpretable metric, termed date fragmentationratio, that measures how faithfully a tokenizer preserves multi-digit datecomponents; (2) release DateAugBench, a suite of 6500 examples spanning threetemporal reasoning tasks: context-based date resolution, format-invariancepuzzles, and date arithmetic across historical, contemporary, and future timeperiods; and (3) through layer-wise probing and causal attention-hop analyses,uncover an emergent date-abstraction mechanism whereby large language modelsstitch together the fragments of month, day, and year components for temporalreasoning. Our experiments show that excessive fragmentation correlates withaccuracy drops of up to 10 points on uncommon dates like historical andfuturistic dates. Further, we find that the larger the model, the faster theemergent date abstraction that heals date fragments is accomplished. Lastly, weobserve a reasoning path that LLMs follow to assemble date fragments, typicallydiffering from human interpretation (year $\rightarrow$ month $\rightarrow$day). Our datasets and code are made publicly available\href{https://github.com/gagan3012/date-fragments}{here}.",Gagan Bhatia,2025/5/22,2025/5/25,,N/A,"['cs.CL', 'cs.AI']"
2505.15993v1,Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku,http://arxiv.org/abs/2505.15993v1,"The success of Large Language Models (LLMs) in human-AI collaborativedecision-making hinges on their ability to provide trustworthy, gradual, andtailored explanations. Solving complex puzzles, such as Sudoku, offers acanonical example of this collaboration, where clear and customizedexplanations often hold greater importance than the final solution. In thisstudy, we evaluate the performance of five LLMs in solving and explaining\sixsix{} Sudoku puzzles. While one LLM demonstrates limited success in solvingpuzzles, none can explain the solution process in a manner that reflectsstrategic reasoning or intuitive problem-solving. These findings underscoresignificant challenges that must be addressed before LLMs can become effectivepartners in human-AI collaborative decision-making.",Anirudh Maiya,2025/5/21,2025/5/21,,N/A,['cs.CL']
2505.15146v2,lmgame-Bench: How Good are LLMs at Playing Games?,http://arxiv.org/abs/2505.15146v2,"Playing video games requires perception, memory, and planning, exactly thefaculties modern large language model (LLM) agents are expected to master. Westudy the major challenges in using popular video games to evaluate modern LLMsand find that directly dropping LLMs into games cannot make an effectiveevaluation, for three reasons -- brittle vision perception, prompt sensitivity,and potential data contamination. We introduce lmgame-Bench to turn games intoreliable evaluations. lmgame-Bench features a suite of platformer, puzzle, andnarrative games delivered through a unified Gym-style API and paired withlightweight perception and memory scaffolds, and is designed to stabilizeprompt variance and remove contamination. Across 13 leading models, we showlmgame-Bench is challenging while still separating models well. Correlationanalysis shows that every game probes a unique blend of capabilities oftentested in isolation elsewhere. More interestingly, performing reinforcementlearning on a single game from lmgame-Bench transfers both to unseen games andto external planning tasks. Our evaluation code is available athttps://github.com/lmgame-org/GamingAgent/lmgame-bench.",Lanxiang Hu,2025/5/21,2025/6/3,,N/A,['cs.AI']
2505.14615v1,SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas,http://arxiv.org/abs/2505.14615v1,"We introduce SATBench, a benchmark for evaluating the logical reasoningcapabilities of large language models (LLMs) through logical puzzles derivedfrom Boolean satisfiability (SAT) problems. Unlike prior work that focuses oninference rule-based reasoning, which often involves deducing conclusions froma set of premises, our approach leverages the search-based nature of SATproblems, where the objective is to find a solution that fulfills a specifiedset of logical constraints. Each instance in SATBench is generated from a SATformula, then translated into a story context and conditions using LLMs. Thegeneration process is fully automated and allows for adjustable difficulty byvarying the number of clauses. All 2100 puzzles are validated through bothLLM-assisted and solver-based consistency checks, with human validation on asubset. Experimental results show that even the strongest model, o4-mini,achieves only 65.0% accuracy on hard UNSAT problems, close to the randombaseline of 50%. SATBench exposes fundamental limitations in the search-basedlogical reasoning abilities of current LLMs and provides a scalable testbed forfuture research in logical reasoning.",Anjiang Wei,2025/5/20,2025/5/20,,N/A,"['cs.AI', 'cs.CL', 'cs.LG', 'cs.LO']"
2505.12929v1,Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs,http://arxiv.org/abs/2505.12929v1,"Reinforcement learning (RL) has become a cornerstone for enhancing thereasoning capabilities of large language models (LLMs), with recent innovationssuch as Group Relative Policy Optimization (GRPO) demonstrating exceptionaleffectiveness. In this study, we identify a critical yet underexplored issue inRL training: low-probability tokens disproportionately influence model updatesdue to their large gradient magnitudes. This dominance hinders the effectivelearning of high-probability tokens, whose gradients are essential for LLMs'performance but are substantially suppressed. To mitigate this interference, wepropose two novel methods: Advantage Reweighting and Low-Probability TokenIsolation (Lopti), both of which effectively attenuate gradients fromlow-probability tokens while emphasizing parameter updates driven byhigh-probability tokens. Our approaches promote balanced updates across tokenswith varying probabilities, thereby enhancing the efficiency of RL training.Experimental results demonstrate that they substantially improve theperformance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&KLogic Puzzle reasoning tasks. Our implementation is available athttps://github.com/zhyang2226/AR-Lopti.",Zhihe Yang,2025/5/19,2025/5/19,,N/A,"['cs.CL', 'cs.AI', 'cs.LG']"
2505.12717v1,ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving,http://arxiv.org/abs/2505.12717v1,"Large language models (LLMs) demonstrate significant reasoning capabilities,particularly through long chain-of-thought (CoT) processes, which can beelicited by reinforcement learning (RL). However, prolonged CoT reasoningpresents limitations, primarily verbose outputs due to excessive introspection.The reasoning process in these LLMs often appears to follow a trial-and-errormethodology rather than a systematic, logical deduction. In contrast,tree-of-thoughts (ToT) offers a conceptually more advanced approach by modelingreasoning as an exploration within a tree structure. This reasoning structurefacilitates the parallel generation and evaluation of multiple reasoningbranches, allowing for the active identification, assessment, and pruning ofunproductive paths. This process can potentially lead to improved performanceand reduced token costs. Building upon the long CoT capability of LLMs, weintroduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with arule-based reward. ToTRL is designed to guide LLMs in developing the parallelToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMsas players in a puzzle game during the ToTRL training process. Solving puzzlegames inherently necessitates exploring interdependent choices and managingmultiple constraints, which requires the construction and exploration of athought tree, providing challenging tasks for cultivating the ToT reasoningcapability. Our empirical evaluations demonstrate that our ToTQwen3-8B model,trained with our ToTRL, achieves significant improvement in performance andreasoning efficiency on complex reasoning tasks.",Haoyuan Wu,2025/5/19,2025/5/19,,N/A,['cs.CL']
2505.08996v1,A suite of LMs comprehend puzzle statements as well as humans,http://arxiv.org/abs/2505.08996v1,"Recent claims suggest that large language models (LMs) underperform humans incomprehending minimally complex English statements (Dentella et al., 2024).Here, we revisit those findings and argue that human performance wasoverestimated, while LLM abilities were underestimated. Using the same stimuli,we report a preregistered study comparing human responses in two conditions:one allowed rereading (replicating the original study), and one that restrictedrereading (a more naturalistic comprehension test). Human accuracy droppedsignificantly when rereading was restricted (73%), falling below that ofFalcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfectaccuracy. Results further show that both humans and models aredisproportionately challenged by queries involving potentially reciprocalactions (e.g., kissing), suggesting shared pragmatic sensitivities rather thanmodel-specific deficits. Additional analyses using Llama-2-70B logprobabilities, a recoding of open-ended model responses, and grammaticalityratings of other sentences reveal systematic underestimation of modelperformance. We find that GPT-4o can align with either naive or expertgrammaticality judgments, depending on prompt framing. These findingsunderscore the need for more careful experimental design and coding practicesin LLM evaluation, and they challenge the assumption that current models areinherently weaker than humans at language comprehension.",Adele E Goldberg,2025/5/13,2025/5/13,,N/A,['cs.CL']
2505.08827v2,RLSR: Reinforcement Learning from Self Reward,http://arxiv.org/abs/2505.08827v2,"Large language models can generate solutions to complex problems, buttraining them with reinforcement learning typically requires verifiable rewardsthat are expensive to create and not possible for all domains. We demonstratethat LLMs can effectively self-improve through self-judging without referencesolutions, leveraging the inherent asymmetry between generating and verifyingsolutions. Our experiments show that models can provide reliable reward signalswithout ground truth answers, enabling reinforcement learning in domains whereverifiable rewards are impractical. By implementing self-judging acrossCountdown puzzles and integration problems, we achieve performance comparableto formal verification without ground truth solutions. Most notably, Qwen 2.57B DeepSeek Distilled trained with self-rewards qualifies for the prestigiousMIT Integration Bee competition, performance through self-supervisedimprovement. When combined with synthetic question generation, we establish acomplete self-improvement loop where models generate practice problems, solvethem, and evaluate their own performance without any external validation. Ourfindings demonstrate that LLM judges can provide effective reward signals fortraining, unlocking reinforcement learning in countless domains previouslylimited by reward engineering challenges. This work represents a significantstep toward autonomous AI systems that continuously improve throughself-directed learning rather than human-guided training, potentiallyaccelerating progress across domains where training data is scarce orevaluation is complex.",Toby Simonds,2025/5/12,2025/8/6,,N/A,"['cs.LG', 'cs.AI']"
2505.06176v1,MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills,http://arxiv.org/abs/2505.06176v1,"Retouching is an essential task in post-manipulation of raw photographs.Generative editing, guided by text or strokes, provides a new tool accessibleto users but can easily change the identity of the original objects inunacceptable and unpredictable ways. In contrast, although traditionalprocedural edits, as commonly supported by photoediting tools (e.g., Gimp,Lightroom), are conservative, they are still preferred by professionals.Unfortunately, professional quality retouching involves many individualprocedural editing operations that is challenging to plan for most novices. Inthis paper, we ask if a multimodal large language model (MLLM) can be taught tocritique raw photographs, suggest suitable remedies, and finally realize themwith a given set of pre-authored procedural image operations. We demonstratethat MLLMs can be first made aware of the underlying image processingoperations, by training them to solve specially designed visual puzzles.Subsequently, such an operation-aware MLLM can both plan and propose editsequences. To facilitate training, given a set of expert-edited photos, wesynthesize a reasoning dataset by procedurally manipulating the expert editsand then grounding a pretrained LLM on the visual adjustments, to synthesizereasoning for finetuning. The proposed retouching operations are, byconstruction, understandable by the users, preserve object details andresolution, and can be optionally overridden. We evaluate our setup on avariety of test examples and show advantages, in terms of explainability andidentity preservation, over existing generative and other proceduralalternatives. Code, data, models, and supplementary results can be found viaour project website at https://monetgpt.github.io.",Niladri Shekhar Dutt,2025/5/9,2025/5/9,,N/A,"['cs.GR', 'cs.CV', 'cs.LG']"
2505.01539v1,Parameterized Argumentation-based Reasoning Tasks for Benchmarking Generative Language Models,http://arxiv.org/abs/2505.01539v1,"Generative large language models as tools in the legal domain have thepotential to improve the justice system. However, the reasoning behavior ofcurrent generative models is brittle and poorly understood, hence cannot beresponsibly applied in the domains of law and evidence. In this paper, weintroduce an approach for creating benchmarks that can be used to evaluate thereasoning capabilities of generative language models. These benchmarks aredynamically varied, scalable in their complexity, and have formally unambiguousinterpretations. In this study, we illustrate the approach on the basis ofwitness testimony, focusing on the underlying argument attack structure. Wedynamically generate both linear and non-linear argument attack graphs ofvarying complexity and translate these into reasoning puzzles about witnesstestimony expressed in natural language. We show that state-of-the-art largelanguage models often fail in these reasoning puzzles, already at lowcomplexity. Obvious mistakes are made by the models, and their inconsistentperformance indicates that their reasoning capabilities are brittle.Furthermore, at higher complexity, even state-of-the-art models specificallypresented for reasoning capabilities make mistakes. We show the viability ofusing a parametrized benchmark with varying complexity to evaluate thereasoning capabilities of generative language models. As such, the findingscontribute to a better understanding of the limitations of the reasoningcapabilities of generative models, which is essential when designingresponsible AI systems in the legal domain.",Cor Steging,2025/5/2,2025/5/2,,N/A,"['cs.AI', 'cs.LG']"
2504.13950v1,Open-Medical-R1: How to Choose Data for RLVR Training at Medicine Domain,http://arxiv.org/abs/2504.13950v1,"This paper explores optimal data selection strategies for ReinforcementLearning with Verified Rewards (RLVR) training in the medical domain. WhileRLVR has shown exceptional potential for enhancing reasoning capabilities inlarge language models, most prior implementations have focused on mathematicsand logical puzzles, with limited exploration of domain-specific applicationslike medicine. We investigate four distinct data sampling strategies fromMedQA-USMLE: random sampling (baseline), and filtering using Phi-4,Gemma-3-27b-it, and Gemma-3-12b-it models. Using Gemma-3-12b-it as our basemodel and implementing Group Relative Policy Optimization (GRPO), we evaluateperformance across multiple benchmarks including MMLU, GSM8K, MMLU-Pro, andCMMLU. Our findings demonstrate that models trained on filtered data generallyoutperform those trained on randomly selected samples. Notably, training onself-filtered samples (using Gemma-3-12b-it for filtering) achieved superiorperformance in medical domains but showed reduced robustness across differentbenchmarks, while filtering with larger models from the same series yieldedbetter overall robustness. These results provide valuable insights intoeffective data organization strategies for RLVR in specialized domains andhighlight the importance of thoughtful data selection in achieving optimalperformance. You can access our repository(https://github.com/Qsingle/open-medical-r1) to get the codes.",Zhongxi Qiu,2025/4/16,2025/4/16,,N/A,"['cs.LG', 'cs.AI']"
2504.04968v1,The Dream Within Huang Long Cave: AI-Driven Interactive Narrative for Family Storytelling and Emotional Reflection,http://arxiv.org/abs/2504.04968v1,"This paper introduces the art project The Dream Within Huang Long Cave, anAI-driven interactive and immersive narrative experience. The project offersnew insights into AI technology, artistic practice, and psychoanalysis.Inspired by actual geographical landscapes and familial archetypes, the workcombines psychoanalytic theory and computational technology, providing anartistic response to the concept of the non-existence of the Big Other. Thenarrative is driven by a combination of a large language model (LLM) and arealistic digital character, forming a virtual agent named YELL. Throughdialogue and exploration within a cave automatic virtual environment (CAVE),the audience is invited to unravel the language puzzles presented by YELL andhelp him overcome his life challenges. YELL is a fictional embodiment of theBig Other, modeled after the artist's real father. Through a cross-temporalinteraction with this digital father, the project seeks to deconstruct complexfamilial relationships. By demonstrating the non-existence of the Big Other, weaim to underscore the authenticity of interpersonal emotions, positioning artas a bridge for emotional connection and understanding within family dynamics.",Jiayang Huang,2025/4/7,2025/4/7,,N/A,"['cs.MM', 'cs.AI']"
2504.02254v1,LLMs as Deceptive Agents: How Role-Based Prompting Induces Semantic Ambiguity in Puzzle Tasks,http://arxiv.org/abs/2504.02254v1,"Recent advancements in Large Language Models (LLMs) have not only showcasedimpressive creative capabilities but also revealed emerging agentic behaviorsthat exploit linguistic ambiguity in adversarial settings. In this study, weinvestigate how an LLM, acting as an autonomous agent, leverages semanticambiguity to generate deceptive puzzles that mislead and challenge human users.Inspired by the popular puzzle game ""Connections"", we systematically comparepuzzles produced through zero-shot prompting, role-injected adversarialprompts, and human-crafted examples, with an emphasis on understanding theunderlying agent decision-making processes. Employing computational analyseswith HateBERT to quantify semantic ambiguity, alongside subjective humanevaluations, we demonstrate that explicit adversarial agent behaviorssignificantly heighten semantic ambiguity -- thereby increasing cognitive loadand reducing fairness in puzzle solving. These findings provide criticalinsights into the emergent agentic qualities of LLMs and underscore importantethical considerations for evaluating and safely deploying autonomous languagesystems in both educational technologies and entertainment.",Seunghyun Yoo,2025/4/3,2025/4/3,,N/A,"['cs.CL', 'cs.AI', '68T50, 68T05, 68U35']"
2504.00226v1,Large Language Models in Numberland: A Quick Test of Their Numerical Reasoning Abilities,http://arxiv.org/abs/2504.00226v1,"An essential element of human mathematical reasoning is our number sense --an abstract understanding of numbers and their relationships -- which allows usto solve problems involving vast number spaces using limited computationalresources. Mathematical reasoning of Large Language Models (LLMs) is oftentested on high-level problems (such as Olympiad challenges, geometry, wordproblems, and puzzles), but their low-level number sense remains less explored.We introduce ""Numberland,"" a 100-problem test to evaluate the numericalreasoning abilities of LLM-based agents. The tasks -- basic operations,advanced calculations (e.g., exponentiation, complex numbers), prime numberchecks, and the 24 game -- aim to test elementary skills and their integrationin solving complex and uncertain problems. We evaluated five LLM-based agents:OpenAI's o1 and o1-mini, Google Gemini, Microsoft Copilot, and AnthropicClaude. They scored 74-95% on the first three tasks that allow deterministicsteps to solutions. In the 24 game, which needs trial-and-error search,performance dropped to 10-73%. We tested the top 24 solver (o1 with 73%accuracy) on 25 harder problems, and its score fell to 27%, confirming searchas a bottleneck. These results, along with the types of mistakes, suggest afragile number of LLMs, which is a bit surprising given their prowess inchallenging benchmarks. The limits of LLM numerical reasoning highlight thescope of simple, targeted tests to evaluate and explain LLM math skills toensure safe use.",Roussel Rahman,2025/3/31,2025/3/31,,N/A,['cs.AI']
2504.00043v2,CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation,http://arxiv.org/abs/2504.00043v2,"Existing reasoning evaluation frameworks for Large Language Models (LLMs) andLarge Vision-Language Models (LVLMs) predominantly assess either text-basedreasoning or vision-language understanding capabilities, with limited dynamicinterplay between textual and visual constraints. To address this limitation,we introduce CrossWordBench, a benchmark designed to evaluate the reasoningcapabilities of both LLMs and LVLMs through the medium of crossword puzzles --a task requiring multimodal adherence to semantic constraints from text-basedclues and intersectional constraints from visual grid structures.CrossWordBench leverages a controllable puzzle generation framework thatproduces puzzles in two formats (text and image), supports adjustabledifficulty through prefill ratio control, and offers different evaluationstrategies, ranging from direct puzzle solving to interactive modes. Ourextensive evaluation of over 20 models reveals that reasoning LLMssubstantially outperform non-reasoning models by effectively leveragingcrossing-letter constraints. We further demonstrate that LVLMs struggle withthe task, showing a strong correlation between their puzzle-solving performanceand grid-parsing accuracy. Our findings highlight limitations of the reasoningcapabilities of current LLMs and LVLMs, and provide an effective approach forcreating multimodal constrained tasks for future evaluations.",Jixuan Leng,2025/3/30,2025/8/11,,N/A,"['cs.CL', 'cs.AI', 'cs.CV']"
2503.19990v3,LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?,http://arxiv.org/abs/2503.19990v3,"Multi-step spatial reasoning entails understanding and reasoning aboutspatial relationships across multiple sequential steps, which is crucial fortackling complex real-world applications, such as robotic manipulation,autonomous navigation, and automated assembly. To assess how well currentMultimodal Large Language Models (MLLMs) have acquired this fundamentalcapability, we introduce LEGO-Puzzles, a scalable benchmark designed toevaluate both spatial understanding and sequential reasoning in MLLMs throughLEGO-based tasks. LEGO-Puzzles consists of 1,100 carefully curated visualquestion-answering (VQA) samples spanning 11 distinct tasks, ranging from basicspatial understanding to complex multi-step reasoning. Based on LEGO-Puzzles,we conduct a comprehensive evaluation of 20 state-of-the-art MLLMs and uncoversignificant limitations in their spatial reasoning capabilities: even the mostpowerful MLLMs can answer only about half of the test cases, whereas humanparticipants achieve over 90% accuracy. Furthermore, based on LEGO-Puzzles, wedesign generation tasks to investigate whether MLLMs can transfer their spatialunderstanding and reasoning abilities to image generation. Our experiments showthat only GPT-4o and Gemini-2.0-Flash exhibit a limited ability to follow theseinstructions, while other MLLMs either replicate the input image or generatecompletely irrelevant outputs. Overall, LEGO-Puzzles exposes criticaldeficiencies in existing MLLMs' spatial understanding and sequential reasoningcapabilities, and underscores the need for further advancements in multimodalspatial reasoning.",Kexian Tang,2025/3/25,2025/6/20,,N/A,['cs.AI']
2503.18394v1,Solving Situation Puzzles with Large Language Model and External Reformulation,http://arxiv.org/abs/2503.18394v1,"In recent years, large language models (LLMs) have shown an impressiveability to perform arithmetic and symbolic reasoning tasks. However, we foundthat LLMs (e.g., ChatGPT) cannot perform well on reasoning that requiresmultiple rounds of dialogue, especially when solving situation puzzles.Specifically, LLMs intend to ask very detailed questions focusing on a specificaspect or same/similar questions after several rounds of Q&As. To help LLMs getout of the above dilemma, we propose a novel external reformulationmethodology, where the situation puzzle will be reformulated after severalrounds of Q&A or when the LLMs raise an incorrect guess. Experiments showsuperior performance (e.g., win rate, number of question/guess attempts) of ourmethod than directly using LLMs for solving situation puzzles, highlighting thepotential of strategic problem reformulation to enhance the reasoningcapabilities of LLMs in complex interactive scenarios.",Kun Li,2025/3/24,2025/3/24,,N/A,"['cs.LG', 'cs.CL']"
2503.17645v1,A Modular Dataset to Demonstrate LLM Abstraction Capability,http://arxiv.org/abs/2503.17645v1,"Large language models (LLMs) exhibit impressive capabilities but strugglewith reasoning errors due to hallucinations and flawed logic. To investigatetheir internal representations of reasoning, we introduce ArrangementPuzzle, anovel puzzle dataset with structured solutions and automated stepwisecorrectness verification. We trained a classifier model on LLM activations onthis dataset and found that it achieved over 80% accuracy in predictingreasoning correctness, implying that LLMs internally distinguish betweencorrect and incorrect reasoning steps, with the strongest representations inmiddle-late Transformer layers. Further analysis reveals that LLMs encodeabstract reasoning concepts within the middle activation layers of thetransformer architecture, distinguishing logical from semantic equivalence.These findings provide insights into LLM reasoning mechanisms and contribute toimproving AI reliability and interpretability, thereby offering the possibilityto manipulate and refine LLM reasoning.",Adam Atanas,2025/3/22,2025/3/22,,N/A,"['cs.AI', 'cs.LG']"
2503.15944v1,From Chaos to Order: The Atomic Reasoner Framework for Fine-grained Reasoning in Large Language Models,http://arxiv.org/abs/2503.15944v1,"Recent advances in large language models (LLMs) have shown remarkableprogress, yet their capacity for logical ``slow-thinking'' reasoning persistsas a critical research frontier. Current inference scaling paradigms sufferfrom two fundamental constraints: fragmented thought flows compromising logicalcoherence, and intensively computational complexity that escalates with searchspace dimensions. To overcome these limitations, we present \textbf{AtomicReasoner} (\textbf{AR}), a cognitive inference strategy that enablesfine-grained reasoning through systematic atomic-level operations. ARdecomposes the reasoning process into atomic cognitive units, employing acognitive routing mechanism to dynamically construct reasoning representationsand orchestrate inference pathways. This systematic methodology implementsstepwise, structured cognition, which ensures logical coherence whilesignificantly reducing cognitive load, effectively simulating the cognitivepatterns observed in human deep thinking processes. Extensive experimentalresults demonstrate AR's superior reasoning capabilities without thecomputational burden of exhaustive solution searches, particularly excelling inlinguistic logic puzzles. These findings substantiate AR's effectiveness inenhancing LLMs' capacity for robust, long-sequence logical reasoning anddeliberation.",Jinyi Liu,2025/3/20,2025/3/20,,N/A,['cs.CL']
2503.15113v1,Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs,http://arxiv.org/abs/2503.15113v1,"Large Language Models (LLMs) have demonstrated remarkable text generationcapabilities, and recent advances in training paradigms have led tobreakthroughs in their reasoning performance. In this work, we investigate howthe reasoning effort of such models scales with problem complexity. We use theinfinitely scalable Tents puzzle, which has a known linear-time solution, toanalyze this scaling behavior. Our results show that reasoning effort scaleswith problem size, but only up to a critical problem complexity. Beyond thisthreshold, the reasoning effort does not continue to increase, and may evendecrease. This observation highlights a critical limitation in the logicalcoherence of current LLMs as problem complexity increases, and underscores theneed for strategies to improve reasoning scalability. Furthermore, our resultsreveal significant performance differences between current state-of-the-artreasoning models when faced with increasingly complex logical puzzles.",Benjamin Estermann,2025/3/19,2025/3/19,,N/A,['cs.AI']
2503.11336v1,Rule-Guided Feedback: Enhancing Reasoning by Enforcing Rule Adherence in Large Language Models,http://arxiv.org/abs/2503.11336v1,"In this paper, we introduce Rule-Guided Feedback (RGF), a framework designedto enhance Large Language Model (LLM) performance through structured ruleadherence and strategic information seeking. RGF implements a teacher-studentparadigm where rule-following is forced through established guidelines. Ourframework employs a Teacher model that rigorously evaluates each student outputagainst task-specific rules, providing constructive guidance rather than directanswers when detecting deviations. This iterative feedback loop serves twocrucial purposes: maintaining solutions within defined constraints andencouraging proactive information seeking to resolve uncertainties. We evaluateRGF on diverse tasks including Checkmate-in-One puzzles, Sonnet Writing,Penguins-In-a-Table classification, GSM8k, and StrategyQA. Our findings suggestthat structured feedback mechanisms can significantly enhance LLMs' performanceacross various domains.",Aissatou Diallo,2025/3/14,2025/3/14,,N/A,['cs.CL']
2503.11207v2,Can Large Reasoning Models do Analogical Reasoning under Perceptual Uncertainty?,http://arxiv.org/abs/2503.11207v2,"This work presents a first evaluation of two state-of-the-art Large ReasoningModels (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning,focusing on well-established nonverbal human IQ tests based on Raven'sprogressive matrices. We benchmark with the I-RAVEN dataset and its extension,I-RAVEN-X, which tests the ability to generalize to longer reasoning rules andranges of the attribute values. To assess the influence of visual uncertaintieson these symbolic analogical reasoning tests, we extend the I-RAVEN-X dataset,which otherwise assumes an oracle perception. We adopt a two-fold strategy tosimulate this imperfect visual perception: 1) we introduce confoundingattributes which, being sampled at random, do not contribute to the predictionof the correct answer of the puzzles, and 2) we smoothen the distributions ofthe input attributes' values. We observe a sharp decline in OpenAI's o3-minitask accuracy, dropping from 86.6% on the original I-RAVEN to just 17.0% --approaching random chance -- on the more challenging I-RAVEN-X, which increasesinput length and range and emulates perceptual uncertainty. This drop occurreddespite spending 3.4x more reasoning tokens. A similar trend is also observedfor DeepSeek R1: from 80.6% to 23.2%. On the other hand, a neuro-symbolicprobabilistic abductive model, ARLC, that achieves state-of-the-artperformances on I-RAVEN, can robustly reason under all theseout-of-distribution tests, maintaining strong accuracy with only a modestaccuracy reduction from 98.6% to 88.0%. Our code is available athttps://github.com/IBM/raven-large-language-models.",Giacomo Camposampiero,2025/3/14,2025/6/4,,N/A,"['cs.AI', 'cs.LG']"
2503.10556v1,Short-term AI literacy intervention does not reduce over-reliance on incorrect ChatGPT recommendations,http://arxiv.org/abs/2503.10556v1,"In this study, we examined whether a short-form AI literacy interventioncould reduce the adoption of incorrect recommendations from large languagemodels. High school seniors were randomly assigned to either a control or anintervention group, which received an educational text explaining ChatGPT'sworking mechanism, limitations, and proper use. Participants solved mathpuzzles with the help of ChatGPT's recommendations, which were incorrect inhalf of the cases. Results showed that students adopted incorrect suggestions52.1% of the time, indicating widespread over-reliance. The educationalintervention did not significantly reduce over-reliance. Instead, it led to anincrease in ignoring ChatGPT's correct recommendations. We conclude that theusage of ChatGPT is associated with over-reliance and it is not trivial toincrease AI literacy to counter over-reliance.",Brett Puppart,2025/3/13,2025/3/13,,N/A,"['cs.CY', 'q-bio.NC']"
2503.04094v1,PokChamp: an Expert-level Minimax Language Agent,http://arxiv.org/abs/2503.04094v1,"We introduce Pok\'eChamp, a minimax agent powered by Large Language Models(LLMs) for Pok\'emon battles. Built on a general framework for two-playercompetitive games, Pok\'eChamp leverages the generalist capabilities of LLMs toenhance minimax tree search. Specifically, LLMs replace three key modules: (1)player action sampling, (2) opponent modeling, and (3) value functionestimation, enabling the agent to effectively utilize gameplay history andhuman knowledge to reduce the search space and address partial observability.Notably, our framework requires no additional LLM training. We evaluatePok\'eChamp in the popular Gen 9 OU format. When powered by GPT-4o, it achievesa win rate of 76% against the best existing LLM-based bot and 84% against thestrongest rule-based bot, demonstrating its superior performance. Even with anopen-source 8-billion-parameter Llama 3.1 model, Pok\'eChamp consistentlyoutperforms the previous best LLM-based bot, Pok\'ellmon powered by GPT-4o,with a 64% win rate. Pok\'eChamp attains a projected Elo of 1300-1500 on thePok\'emon Showdown online ladder, placing it among the top 30%-10% of humanplayers. In addition, this work compiles the largest real-player Pok\'emonbattle dataset, featuring over 3 million games, including more than 500khigh-Elo matches. Based on this dataset, we establish a series of battlebenchmarks and puzzles to evaluate specific battling skills. We further providekey updates to the local game engine. We hope this work fosters furtherresearch that leverage Pok\'emon battle as benchmark to integrate LLMtechnologies with game-theoretic algorithms addressing general multiagentproblems. Videos, code, and dataset available athttps://sites.google.com/view/pokechamp-llm.",Seth Karten,2025/3/6,2025/3/6,,N/A,"['cs.LG', 'cs.MA']"
2502.20238v2,FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving,http://arxiv.org/abs/2502.20238v2,"Many challenging reasoning tasks require not just rapid, intuitive responses,but a more deliberate, multi-step approach. Recent progress in large languagemodels (LLMs) highlights an important shift from the ""System 1"" way of quickreactions to the ""System 2"" style of reflection-and-correction problem solving.However, current benchmarks heavily rely on the final-answer accuracy, leavingmuch of a model's intermediate reasoning steps unexamined. This fails to assessthe model's ability to reflect and rectify mistakes within the reasoningprocess. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmarkfor fine-grained evaluation of LLMs' reasoning capabilities. Each puzzle can bedecomposed into atomic steps, making it ideal for rigorous validation ofintermediate correctness. Building on this, we introduce two tasks: statechecking, and state transition, for a comprehensive evaluation of how modelsassess the current situation and plan the next move. To support broaderresearch, we also provide a puzzle training set aimed at enhancing performanceon general mathematical tasks. We show that models trained on our statechecking and transition data demonstrate gains in math reasoning by up to 5.1%on GSM8K.",Guizhen Chen,2025/2/27,2025/6/1,,N/A,['cs.CL']
2502.19918v4,Meta-Reasoner: Dynamic Guidance for Optimized Inference-time Reasoning in Large Language Models,http://arxiv.org/abs/2502.19918v4,"Large Language Models (LLMs) struggle with high computational time and errorpropagation during inference time, especially for complex tasks like math,puzzles, or coding requiring multi-step thinking. While existing reasoningmodels with chain-of-thoughts (CoT) can enable LLMs to do step-wise analysisand reflection, they often face the issue of wasting computation on lessproductive solutions and fail to make progress during inference time. In thispaper, we propose Meta-Reasoner, a new framework to enable LLMs ``Think abouthow to think'', i.e., optimize the inference compute by adjusting strategies onhow to reason during inference time. Inspired by dual-process theory, ourmethod decouples the high-level strategy generation (e.g., backtracking,switching approaches, or restarting) from stepwise CoT generation via alightweight progress report. The strategy module only consider the summarizedversion from the previous CoTs to propose new strategies accordingly. We employthe contextual multi-armed bandits (CMABs) for this module to iterativelyevaluate the previous reasoning states and dynamically adjust the strategy toavoid reasoning get stuck in less productive paths during inference.Evaluations on math problems (e.g., Game-of-24, TheoremQA) and scientificproblems (e.g., SciBench) demonstrate that our method improves performance by9-12\% over previous SOTA methods while reducing inference time by 28-35\%.This approach also generalizes to other domains like creative writing,demonstrating its versatility for diverse reasoning-intensive problems usingLLMs.",Yuan Sui,2025/2/27,2025/8/8,,N/A,"['cs.AI', 'cs.LG']"
2502.19805v1,Implicit Search via Discrete Diffusion: A Study on Chess,http://arxiv.org/abs/2502.19805v1,"In the post-AlphaGo era, there has been a renewed interest in searchtechniques such as Monte Carlo Tree Search (MCTS), particularly in theirapplication to Large Language Models (LLMs). This renewed attention is drivenby the recognition that current next-token prediction models often lack theability for long-term planning. Is it possible to instill search-like abilitieswithin the models to enhance their planning abilities without relying onexplicit search? We propose DiffuSearch , a model that does \textit{implicitsearch} by looking into the future world via discrete diffusion modeling. Weinstantiate DiffuSearch on a classical board game, Chess, where explicit searchis known to be essential. Through extensive controlled experiments, we showDiffuSearch outperforms both the searchless and explicit search-enhancedpolicies. Specifically, DiffuSearch outperforms the one-step policy by 19.2%and the MCTS-enhanced policy by 14% on action accuracy. Furthermore,DiffuSearch demonstrates a notable 30% enhancement in puzzle-solving abilitiescompared to explicit search-based policies, along with a significant 540 Eloincrease in game-playing strength assessment. These results indicate thatimplicit search via discrete diffusion is a viable alternative to explicitsearch over a one-step policy. All codes are publicly available at\href{https://github.com/HKUNLP/DiffuSearch}{https://github.com/HKUNLP/DiffuSearch}.",Jiacheng Ye,2025/2/27,2025/2/27,,N/A,"['cs.LG', 'cs.AI']"
2502.18431v1,TextGames: Learning to Self-Play Text-Based Puzzle Games via Language Model Reasoning,http://arxiv.org/abs/2502.18431v1,"Reasoning is a fundamental capability of large language models (LLMs),enabling them to comprehend, analyze, and solve complex problems. In thispaper, we introduce TextGames, an innovative benchmark specifically crafted toassess LLMs through demanding text-based games that require advanced skills inpattern recognition, spatial awareness, arithmetic, and logical reasoning. Ouranalysis probes LLMs' performance in both single-turn and multi-turn reasoning,and their abilities in leveraging feedback to correct subsequent answersthrough self-reflection. Our findings reveal that, although LLMs exhibitproficiency in addressing most easy and medium-level problems, they facesignificant challenges with more difficult tasks. In contrast, humans arecapable of solving all tasks when given sufficient time. Moreover, we observethat LLMs show improved performance in multi-turn predictions throughself-reflection, yet they still struggle with sequencing, counting, andfollowing complex rules consistently. Additionally, models optimized forreasoning outperform pre-trained LLMs that prioritize instruction following,highlighting the crucial role of reasoning skills in addressing highly complexproblems.",Frederikus Hudi,2025/2/25,2025/2/25,,N/A,"['cs.CL', 'cs.AI']"
2502.16906v1,AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning Abilities of Large Language Models,http://arxiv.org/abs/2502.16906v1,"While logical reasoning evaluation of Large Language Models (LLMs) hasattracted significant attention, existing benchmarks predominantly rely onmultiple-choice formats that are vulnerable to random guessing, leading tooverestimated performance and substantial performance fluctuations. To obtainmore accurate assessments of models' reasoning capabilities, we propose anautomated method for synthesizing open-ended logic puzzles, and use it todevelop a bilingual benchmark, AutoLogi. Our approach features program-basedverification and controllable difficulty levels, enabling more reliableevaluation that better distinguishes models' reasoning abilities. Extensiveevaluation of eight modern LLMs shows that AutoLogi can better reflect truemodel capabilities, with performance scores spanning from 35% to 73% comparedto the narrower range of 21% to 37% on the source multiple-choice dataset.Beyond benchmark creation, this synthesis method can generate high-qualitytraining data by incorporating program verifiers into the rejection samplingprocess, enabling systematic enhancement of LLMs' reasoning capabilities acrossdiverse datasets.",Qin Zhu,2025/2/24,2025/2/24,,N/A,['cs.CL']
2502.15776v1,Logic.py: Bridging the Gap between LLMs and Constraint Solvers,http://arxiv.org/abs/2502.15776v1,"We present a novel approach to formalise and solve search-based problemsusing large language models, which significantly improves upon previousstate-of-the-art results. We demonstrate the efficacy of this approach on thelogic puzzles benchmark ZebraLogicBench. Instead of letting the LLM attempt todirectly solve the puzzles, our method prompts the model to formalise theproblem in a logic-focused domain-specific language (DSL) called Logic.py. Thisformalised representation is then solved using a constraint solver, leveragingthe strengths of both the language model and the solver. Our approach achievesa remarkable 65% absolute improvement over the baseline performance of Llama3.1 70B on ZebraLogicBench, setting a new state-of-the-art with an accuracy ofover 90%. This significant advancement demonstrates the potential of combininglanguage models with domain-specific languages and auxiliary tools ontraditionally challenging tasks for LLMs.",Pascal Kesseli,2025/2/17,2025/2/17,,N/A,"['cs.AI', 'cs.LO', '68T27', 'F.4.1; I.2.3; I.2.8']"
2502.01584v3,PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models,http://arxiv.org/abs/2502.01584v3,"Existing benchmarks for frontier models often test specialized, ""PhD-level""knowledge that is difficult for non-experts to grasp. In contrast, we present abenchmark with 594 problems based on the NPR Sunday Puzzle Challenge thatrequires only general knowledge. Our benchmark is challenging for both humansand models; however correct solutions are easy to verify, and models' mistakesare easy to spot. As LLMs are more widely deployed in society, we believe it isuseful to develop benchmarks for frontier models that humans can understandwithout the need for deep domain expertise.  Our work reveals capability gaps that are not evident in existing benchmarks:OpenAI o1 significantly outperforms other reasoning models on our benchmark,despite being on par with other models when tested on benchmarks that testspecialized knowledge. Furthermore, our analysis of reasoning outputs uncoversnew kinds of failures. DeepSeek R1, for instance, often concedes with ""I giveup"" before providing an answer that it knows is wrong. R1 can also beremarkably ""uncertain"" in its output and in rare cases, it does not ""finishthinking,"" which suggests the need for techniques to ""wrap up"" before thecontext window limit is reached. We also quantify the effectiveness ofreasoning longer to identify the point beyond which more reasoning is unlikelyto improve accuracy on our benchmark.",Zixuan Wu,2025/2/3,2025/3/31,,N/A,"['cs.AI', 'cs.LG']"
2502.01100v2,ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning,http://arxiv.org/abs/2502.01100v2,"We investigate the logical reasoning capabilities of large language models(LLMs) and their scalability in complex non-monotonic reasoning. To this end,we introduce ZebraLogic, a comprehensive evaluation framework for assessing LLMreasoning performance on logic grid puzzles derived from constraintsatisfaction problems (CSPs). ZebraLogic enables the generation of puzzles withcontrollable and quantifiable complexity, facilitating a systematic study ofthe scaling limits of models such as Llama, o1 models, and DeepSeek-R1. Byencompassing a broad range of search space complexities and diverse logicalconstraints, ZebraLogic provides a structured environment to evaluate reasoningunder increasing difficulty.  Our results reveal a significant decline in accuracy as problem complexitygrows -- a phenomenon we term the curse of complexity. This limitation persistseven with larger models and increased inference-time computation, suggestinginherent constraints in current LLM reasoning capabilities. Additionally, weexplore strategies to enhance logical reasoning, including Best-of-N sampling,backtracking mechanisms, and self-verification prompts. Our findings offercritical insights into the scalability of LLM reasoning, highlight fundamentallimitations, and outline potential directions for improvement.",Bill Yuchen Lin,2025/2/3,2025/7/15,,N/A,"['cs.AI', 'cs.CL', 'cs.LG']"
2502.01081v2,The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles,http://arxiv.org/abs/2502.01081v2,"The releases of OpenAI's o-[n] series, such as o1, o3, and o4-mini, mark asignificant paradigm shift in Large Language Models towards advanced reasoningcapabilities. Notably, models like o3 have demonstrated strong performance onbenchmarks like the Abstraction and Reasoning Corpus for Artificial GeneralIntelligence (ARC-AGI). However, this benchmark is limited to symbolicpatterns, whereas humans often perceive and reason about multimodal scenariosinvolving both vision and language data. Thus, there is an urgent need toinvestigate advanced reasoning capabilities in multimodal tasks. To this end,we track the evolution of the GPT-[n] and o-[n] series models (including o1,o3, and o4-mini) on challenging multimodal puzzles from PuzzleVQA andAlgoPuzzleVQA, which demand fine-grained visual perception. Our results revealthat o-[n] series, particularly later iterations like o3 and o4-mini,significantly outperform the GPT-[n] series and show strong scalability inmultimodal reasoning. Nonetheless, despite these substantial advancements andthe superior capabilities demonstrated by the o-[n] series, our findingshighlight that even these leading models face persistent challenges.Difficulties are particularly evident in tasks requiring precise visualperception, robust compositional reasoning across multiple visual attributes,and solving complex algorithmic or highly combinatorial puzzles, indicatingcritical areas for future AGI development. We plan to continuously track newmodels in the series and update our results in this paper accordingly. Allresources used in this evaluation are openly available athttps://github.com/declare-lab/LLM-PuzzleTest.",Vernon Y. H. Toh,2025/2/3,2025/5/21,,N/A,"['cs.CV', 'cs.AI', 'cs.CL']"
2502.00817v1,Probing Large Language Models in Reasoning and Translating Complex Linguistic Puzzles,http://arxiv.org/abs/2502.00817v1,"This paper investigates the utilization of Large Language Models (LLMs) forsolving complex linguistic puzzles, a domain requiring advanced reasoning andadept translation capabilities akin to human cognitive processes. We explorespecific prompting techniques designed to enhance ability of LLMs to reason andelucidate their decision-making pathways, with a focus on Input-OutputPrompting (IO), Chain-of-Thought Prompting (CoT), and Solo PerformancePrompting (SPP). Utilizing datasets from the Puzzling Machine Competition andvarious Linguistics Olympiads, we employ a comprehensive set of metrics toassess the performance of GPT-4 0603, a prominent LLM, across these promptingmethods. Our findings illuminate the potential of LLMs in linguistic reasoningand complex translation tasks, highlighting their capabilities and identifyinglimitations in the context of linguistic puzzles. This research contributessignificantly to the broader field of Natural Language Processing (NLP) byproviding insights into the optimization of LLM applications for improvedreasoning and translation accuracy, thereby enriching the ongoing dialogue inNLP advancements.",Zheng-Lin Lin,2025/2/2,2025/2/2,,N/A,['cs.CL']
2501.17665v1,Planning with Vision-Language Models and a Use Case in Robot-Assisted Teaching,http://arxiv.org/abs/2501.17665v1,"Automating the generation of Planning Domain Definition Language (PDDL) withLarge Language Model (LLM) opens new research topic in AI planning,particularly for complex real-world tasks. This paper introduces Image2PDDL, anovel framework that leverages Vision-Language Models (VLMs) to automaticallyconvert images of initial states and descriptions of goal states into PDDLproblems. By providing a PDDL domain alongside visual inputs, Imasge2PDDLaddresses key challenges in bridging perceptual understanding with symbolicplanning, reducing the expertise required to create structured probleminstances, and improving scalability across tasks of varying complexity. Weevaluate the framework on various domains, including standard planning domainslike blocksworld and sliding tile puzzles, using datasets with multipledifficulty levels. Performance is assessed on syntax correctness, ensuringgrammar and executability, and content correctness, verifying accurate staterepresentation in generated PDDL problems. The proposed approach demonstratespromising results across diverse task complexities, suggesting its potentialfor broader applications in AI planning. We will discuss a potential use casein robot-assisted teaching of students with Autism Spectrum Disorder.",Xuzhe Dang,2025/1/29,2025/1/29,,N/A,"['cs.RO', 'cs.AI']"
2501.16842v2,Adapting Network Information into Semantics for Generalizable and Plug-and-Play Multi-Scenario Network Diagnosis,http://arxiv.org/abs/2501.16842v2,"Leverage large language model (LLM) to refer the fault is considered to be apotential solution for intelligent network fault diagnosis. However, how torepresent network information in a paradigm that can be understood by LLMs hasalways been a core issue that has puzzled scholars in the field of networkintelligence. To address this issue, we propose LLM-based Network SemanticGeneration (LNSG) algorithm, which integrates semanticization and symbolizationmethods to uniformly describe the entire multi-modal network information. Basedon the LNSG and LLMs, we present NetSemantic, a plug-and-play,data-independent, network information semantic fault diagnosis framework. Itenables rapid adaptation to various network environments and provides efficientfault diagnosis capabilities. Experimental results demonstrate that NetSemanticexcels in network fault diagnosis across various complex scenarios in azero-shot manner.",Tiao Tan,2025/1/28,2025/3/22,,N/A,['cs.NI']
2501.04755v1,Improving Human-Robot Teaching by Quantifying and Reducing Mental Model Mismatch,http://arxiv.org/abs/2501.04755v1,"The rapid development of artificial intelligence and robotics has had asignificant impact on our lives, with intelligent systems increasinglyperforming tasks traditionally performed by humans. Efficient knowledgetransfer requires matching the mental model of the human teacher with thecapabilities of the robot learner. This paper introduces the Mental ModelMismatch (MMM) Score, a feedback mechanism designed to quantify and reducemismatches by aligning human teaching behavior with robot learning behavior.Using Large Language Models (LLMs), we analyze teacher intentions in naturallanguage to generate adaptive feedback. A study with 150 participants teachinga virtual robot to solve a puzzle game shows that intention-based feedbacksignificantly outperforms traditional performance-based feedback or nofeedback. The results suggest that intention-based feedback improvesinstructional outcomes, improves understanding of the robot's learning processand reduces misconceptions. This research addresses a critical gap inhuman-robot interaction (HRI) by providing a method to quantify and mitigatediscrepancies between human mental models and robot capabilities, with the goalof improving robot learning and human teaching effectiveness.",Phillip Richter,2025/1/8,2025/1/8,,N/A,"['cs.RO', 'cs.HC']"
2501.00226v2,Generative Emergent Communication: Large Language Model is a Collective World Model,http://arxiv.org/abs/2501.00226v2,"Large Language Models (LLMs) have demonstrated a remarkable ability tocapture extensive world knowledge, yet how this is achieved without directsensorimotor experience remains a fundamental puzzle. This study proposes anovel theoretical solution by introducing the Collective World Modelhypothesis. We argue that an LLM does not learn a world model from scratch;instead, it learns a statistical approximation of a collective world model thatis already implicitly encoded in human language through a society-wide processof embodied, interactive sense-making. To formalize this process, we introducegenerative emergent communication (Generative EmCom), a framework built on theCollective Predictive Coding (CPC). This framework models the emergence oflanguage as a process of decentralized Bayesian inference over the internalstates of multiple agents. We argue that this process effectively creates anencoder-decoder structure at a societal scale: human society collectivelyencodes its grounded, internal representations into language, and an LLMsubsequently decodes these symbols to reconstruct a latent space that mirrorsthe structure of the original collective representations. This perspectiveprovides a principled, mathematical explanation for how LLMs acquire theircapabilities. The main contributions of this paper are: 1) the formalization ofthe Generative EmCom framework, clarifying its connection to world models andmulti-agent reinforcement learning, and 2) its application to interpret LLMs,explaining phenomena such as distributional semantics as a natural consequenceof representation reconstruction. This work provides a unified theory thatbridges individual cognitive development, collective language evolution, andthe foundations of large-scale AI.",Tadahiro Taniguchi,2024/12/31,2025/7/16,,N/A,"['cs.AI', 'cs.CL']"
2412.15296v1,Confidence in the Reasoning of Large Language Models,http://arxiv.org/abs/2412.15296v1,"There is a growing literature on reasoning by large language models (LLMs),but the discussion on the uncertainty in their responses is still lacking. Ouraim is to assess the extent of confidence that LLMs have in their answers andhow it correlates with accuracy. Confidence is measured (i) qualitatively interms of persistence in keeping their answer when prompted to reconsider, and(ii) quantitatively in terms of self-reported confidence score. We investigatethe performance of three LLMs -- GPT4o, GPT4-turbo and Mistral -- on twobenchmark sets of questions on causal judgement and formal fallacies and a setof probability and statistical puzzles and paradoxes. Although the LLMs showsignificantly better performance than random guessing, there is a widevariability in their tendency to change their initial answers. There is apositive correlation between qualitative confidence and accuracy, but theoverall accuracy for the second answer is often worse than for the firstanswer. There is a strong tendency to overstate the self-reported confidencescore. Confidence is only partially explained by the underlying token-levelprobability. The material effects of prompting on qualitative confidence andthe strong tendency for overconfidence indicate that current LLMs do not haveany internally coherent sense of confidence.",Yudi Pawitan,2024/12/19,2024/12/19,,N/A,"['cs.CL', 'cs.LG']"
2412.11908v1,Can Language Models Rival Mathematics Students? Evaluating Mathematical Reasoning through Textual Manipulation and Human Experiments,http://arxiv.org/abs/2412.11908v1,"In this paper we look at the ability of recent large language models (LLMs)at solving mathematical problems in combinatorics. We compare models LLaMA-2,LLaMA-3.1, GPT-4, and Mixtral against each other and against human pupils andundergraduates with prior experience in mathematical olympiads. To facilitatethese comparisons we introduce the Combi-Puzzles dataset, which contains 125problem variants based on 25 combinatorial reasoning problems. Each problem ispresented in one of five distinct forms, created by systematically manipulatingthe problem statements through adversarial additions, numeric parameterchanges, and linguistic obfuscation. Our variations preserve the mathematicalcore and are designed to measure the generalisability of LLM problem-solvingabilities, while also increasing confidence that problems are submitted to LLMsin forms that have not been seen as training instances. We found that a modelbased on GPT-4 outperformed all other models in producing correct responses,and performed significantly better in the mathematical variation of theproblems than humans. We also found that modifications to problem statementssignificantly impact the LLM's performance, while human performance remainsunaffected.",Andrii Nikolaiev,2024/12/16,2024/12/16,,N/A,['cs.CL']
2412.11385v1,Why Does ChatGPT "Delve" So Much? Exploring the Sources of Lexical Overrepresentation in Large Language Models,http://arxiv.org/abs/2412.11385v1,"Scientific English is currently undergoing rapid change, with words like""delve,"" ""intricate,"" and ""underscore"" appearing far more frequently than justa few years ago. It is widely assumed that scientists' use of large languagemodels (LLMs) is responsible for such trends. We develop a formal, transferablemethod to characterize these linguistic changes. Application of our methodyields 21 focal words whose increased occurrence in scientific abstracts islikely the result of LLM usage. We then pose ""the puzzle of lexicaloverrepresentation"": WHY are such words overused by LLMs? We fail to findevidence that lexical overrepresentation is caused by model architecture,algorithm choices, or training data. To assess whether reinforcement learningfrom human feedback (RLHF) contributes to the overuse of focal words, weundertake comparative model testing and conduct an exploratory online study.While the model testing is consistent with RLHF playing a role, ourexperimental results suggest that participants may be reacting differently to""delve"" than to other focal words. With LLMs quickly becoming a driver ofglobal language change, investigating these potential sources of lexicaloverrepresentation is important. We note that while insights into the workingsof LLMs are within reach, a lack of transparency surrounding model developmentremains an obstacle to such research.",Tom S. Juzek,2024/12/16,2024/12/16,,N/A,"['cs.CL', 'cs.AI', 'cs.LG']"
2412.09012v2,What Makes Cryptic Crosswords Challenging for LLMs?,http://arxiv.org/abs/2412.09012v2,"Cryptic crosswords are puzzles that rely on general knowledge and thesolver's ability to manipulate language on different levels, dealing withvarious types of wordplay. Previous research suggests that solving such puzzlesis challenging even for modern NLP models, including Large Language Models(LLMs). However, there is little to no research on the reasons for their poorperformance on this task. In this paper, we establish the benchmark results forthree popular LLMs: Gemma2, LLaMA3 and ChatGPT, showing that their performanceon this task is still significantly below that of humans. We also investigatewhy these models struggle to achieve superior performance. We release our codeand introduced datasets athttps://github.com/bodasadallah/decrypting-crosswords.",Abdelrahman Sadallah,2024/12/12,2025/1/14,,N/A,"['cs.CL', 'cs.AI']"
2412.17819v1,Inductive Linguistic Reasoning with Large Language Models,http://arxiv.org/abs/2412.17819v1,"Evaluating large language models (LLMs) on their linguistic reasoningcapabilities is an important task to understand the gaps in their skills thatmay surface during large-scale adoption. In this work, we investigate theabilities of such models to perform abstract multilingual reasoning through thelens of linguistic puzzles on extremely low-resource languages. As thesetranslation tasks involve inductive and deductive reasoning from referenceinstances, we examine whether diverse auxiliary demonstrations can beautomatically induced from seed exemplars, through analogical prompting. Weemploy a two-stage procedure, first generating analogical exemplars with alanguage model, and then applying them in-context along with provided targetlanguage exemplars. Our results on the modeLing dataset show that analogicalprompting is effective in eliciting models' knowledge of language grammarsimilarities, boosting the performance of GPT-4o by as much as 8.1% andLlama-3.1-405B-Instruct by 5.9% over chain-of-thought approaches. These gainsare attributable to the analogical demonstrations, both when self-generated aswell as when produced by weaker multilingual models. Furthermore, wedemonstrate that our method generalizes to other tasks present in LinguisticsOlympiad competitions, achieving sizable improvements across all problem typesand difficulty levels included in the LINGOLY dataset with GPT-4o. We alsoreport several findings about interesting phenomena which drive linguisticreasoning performance, suggesting that such puzzles are a valuable benchmarkfor new reasoning methods.",Raghav Ramji,2024/12/9,2024/12/9,,N/A,"['cs.CL', 'cs.AI']"
2412.01621v3,NYT-Connections: A Deceptively Simple Text Classification Task that Stumps System-1 Thinkers,http://arxiv.org/abs/2412.01621v3,"Large Language Models (LLMs) have shown impressive performance on variousbenchmarks, yet their ability to engage in deliberate reasoning remainsquestionable. We present NYT-Connections, a collection of 358 simple wordclassification puzzles derived from the New York Times Connections game. Thisbenchmark is designed to penalize quick, intuitive ""System 1"" thinking,isolating fundamental reasoning skills. We evaluated six recent LLMs, a simplemachine learning heuristic, and humans across three configurations:single-attempt, multiple attempts without hints, and multiple attempts withcontextual hints. Our findings reveal a significant performance gap: eventop-performing LLMs like GPT-4 fall short of human performance by nearly 30%.Notably, advanced prompting techniques such as Chain-of-Thought andSelf-Consistency show diminishing returns as task difficulty increases.NYT-Connections uniquely combines linguistic isolation, resistance to intuitiveshortcuts, and regular updates to mitigate data leakage, offering a novel toolfor assessing LLM reasoning capabilities.",Angel Yahir Loredo Lopez,2024/12/2,2025/2/25,,N/A,"['cs.CL', 'cs.AI']"
2411.19146v5,Puzzle: Distillation-Based NAS for Inference-Optimized LLMs,http://arxiv.org/abs/2411.19146v5,"Large language models (LLMs) offer remarkable capabilities, yet their highinference costs restrict wider adoption. While increasing parameter countsimproves accuracy, it also broadens the gap between state-of-the-artcapabilities and practical deployability. We present Puzzle, a hardware-awareframework that accelerates the inference of LLMs while preserving theircapabilities. Using neural architecture search (NAS) at a large-scale, Puzzleoptimizes models with tens of billions of parameters. Our approach utilizesblockwise local knowledge distillation (BLD) for parallel architectureexploration and employs mixed-integer programming for precise constraintoptimization.  We showcase our framework's impact via Llama-3.1-Nemotron-51B-Instruct(Nemotron-51B) and Llama-3.3-Nemotron-49B, two publicly available modelsderived from Llama-70B-Instruct. Both models achieve a 2.17x inferencethroughput speedup, fitting on a single NVIDIA H100 GPU while retaining 98.4%of the original model's benchmark accuracies. These are the most accuratemodels supporting single H100 GPU inference with large batch sizes, despitetraining on 45B tokens at most, far fewer than the 15T used to train Llama-70B.Lastly, we show that lightweight alignment on these derived models allows themto surpass the parent model in specific capabilities. Our work establishes thatpowerful LLM models can be optimized for efficient deployment with onlynegligible loss in quality, underscoring that inference performance, notparameter count alone, should guide model selection.",Akhiad Bercovich,2024/11/28,2025/6/3,,N/A,['cs.LG']
2411.18142v3,Autonomous Imagination: Closed-Loop Decomposition of Visual-to-Textual Conversion in Visual Reasoning for Multimodal Large Language Models,http://arxiv.org/abs/2411.18142v3,"Under pure textual modality, Large Language Models (LLMs) have demonstratedremarkable success in complex reasoning tasks by decomposing them into simplersub-problems. However, Multimodal Large Language Models (MLLMs) still strugglewith some seemingly straightforward visual tasks, such as counting and solvingjigsaw puzzles. We argue that these tasks challenge the ability ofvisual-to-textual conversion, where MLLMs convert visual information perceivedfrom the input scene, to textual information for further reasoning andgenerating the answer. If the complexity of the visual input is beyond theperceptual capability of the MLLMs, without decomposing this conversionprocess, simply scaling inference-time reasoning cannot solve the task becauseit repeatedly encounters the same perceptual bottleneck. We propose anapproach, autonomous imagination, to enable MLLMs to iteratively modify visualinputs (e.g. isolating objects, rearranging puzzle pieces) into intermediatevisual states, decomposing visual-to-textual conversion into closed-loop visualmodification steps. We show that, without any retraining, MLLMs can now solvetasks initially beyond their perceptual capability, highlighting thatclosed-loop visual modification can be an effective way of decomposing thevisual reasoning task into solvable substeps. Project page:https://future-item.github.io/autoimagine-site/",Jingming Liu,2024/11/27,2025/6/11,,N/A,['cs.CV']
2410.23123v2,On Memorization of Large Language Models in Logical Reasoning,http://arxiv.org/abs/2410.23123v2,"Large language models (LLMs) achieve good performance on challengingreasoning benchmarks, yet could also make basic reasoning mistakes. Thiscontrasting behavior is puzzling when it comes to understanding the mechanismsbehind LLMs' reasoning capabilities. One hypothesis is that the increasinglyhigh and nearly saturated performance on common reasoning benchmarks could bedue to the memorization of similar problems. In this paper, we systematicallyinvestigate this hypothesis with a quantitative measurement of memorization inreasoning tasks, using a dynamically generated logical reasoning benchmarkbased on Knights and Knaves (K&K) puzzles. We find that LLMs could interpolateand memorize the training puzzles (achieving near-perfect accuracy) afterfine-tuning, yet they struggle with slight variations of these puzzles. On theother hand, we show that while fine-tuning leads to heavy memorization, it alsoconsistently improves generalization performance. Through in-depth analyseswith perturbation tests, cross difficulty-level transferability, probing modelinternals, and fine-tuning with wrong answers, we establish that LLMs developreasoning skills on K&K puzzles alongside memorization. Finally, our analysisbased on a per-sample memorization score sheds light on how LLMs switch betweenreasoning and memorization when solving logical puzzles. Our code and data areavailable at https://memkklogic.github.io.",Chulin Xie,2024/10/30,2025/3/4,,N/A,['cs.CL']
2410.20016v3,Vulnerability of LLMs to Vertically Aligned Text Manipulations,http://arxiv.org/abs/2410.20016v3,"Vertical text input is commonly encountered in various real-worldapplications, such as mathematical computations and word-based Sudoku puzzles.While current large language models (LLMs) have excelled in natural languagetasks, they remain vulnerable to variations in text formatting. Recent researchdemonstrates that modifying input formats, such as vertically aligning wordsfor encoder-based models, can substantially lower accuracy in textclassification tasks. While easily understood by humans, these inputs cansignificantly mislead models, posing a potential risk of bypassing detection inreal-world scenarios involving harmful or sensitive information. With theexpanding application of LLMs, a crucial question arises: \textit{Dodecoder-based LLMs exhibit similar vulnerabilities to vertically formatted textinput?} In this paper, we investigate the impact of vertical text input on theperformance of various LLMs across multiple text classification datasets andanalyze the underlying causes. Our findings are as follows: (i) Vertical textinput significantly degrades the accuracy of LLMs in text classification tasks.(ii) \textit{Chain of Thought (CoT)} reasoning does not help LLMs recognizevertical input or mitigate its vulnerability, but \textit{few-shot learning}with careful analysis does. (iii) We explore the underlying cause of thevulnerability by analyzing the inherent issues in tokenization and attentionmatrices.",Zhecheng Li,2024/10/26,2025/7/19,,N/A,['cs.CL']
2410.19733v1,The Potential and Value of AI Chatbot in Personalized Cognitive Training,http://arxiv.org/abs/2410.19733v1,"In recent years, the rapid aging of the global population has led to anincrease in cognitive disorders, such as Alzheimer's disease, presentingsignificant public health challenges. Although no effective treatmentscurrently exist to reverse Alzheimer's, prevention and early intervention,including cognitive training, are critical. This report explores the potentialof AI chatbots in enhancing personalized cognitive training. We introduce ReMe,a web-based framework designed to create AI chatbots that facilitate cognitivetraining research, specifically targeting episodic memory tasks derived frompersonal life logs. By leveraging large language models, ReMe provides enhanceduser-friendly, interactive, and personalized training experiences. Case studiesdemonstrate ReMe's effectiveness in engaging users through life recall andopen-ended language puzzles, highlighting its potential to improve cognitivetraining design. Despite promising results, further research is needed tovalidate training effectiveness through large-scale studies that includecognitive ability evaluations. Overall, ReMe offers a promising approach topersonalized cognitive training, utilizing AI capabilities to meet the growingdemand for non-pharmacological interventions in cognitive health, with futureresearch aiming to expand its applications and efficacy.",Zilong Wang,2024/10/25,2024/10/25,,N/A,['cs.AI']
2410.13835v2,Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs,http://arxiv.org/abs/2410.13835v2,"Practitioners have consistently observed three puzzling phenomena intransformer-based large language models (LLMs): attention sinks, value-statedrains, and residual-state peaks, collectively referred to as extreme-tokenphenomena. These phenomena are characterized by certain so-called ""sink tokens""receiving disproportionately high attention weights, exhibiting significantlysmaller value states, and having much larger residual-state norms than those ofother tokens. These extreme tokens give rise to various challenges in LLMinference, quantization, and interpretability.  We elucidate the mechanisms behind extreme-token phenomena. First, we showthat these phenomena arise in very simple architectures -- transformers withone to three layers -- trained on a toy model, the Bigram-Backcopy (BB) task.In this setting, we identify an active-dormant mechanism, where attention headsbecome sinks for specific input domains while remaining non-sinks for others.Our theoretical analysis of the training dynamics reveals that these phenomenaare driven by a mutual reinforcement mechanism. Building on these insights, wepropose strategies to mitigate extreme-token phenomena during pretraining,including replacing softmax with ReLU and Adam with SGD. Next, we extend ouranalysis to pretrained LLMs, including Llama and OLMo, showing that manyattention heads exhibit a similar active-dormant mechanism as in the BB task,and that the mutual reinforcement mechanism also governs the emergence ofextreme-token phenomena during LLM pretraining. Our results reveal that many ofthe static and dynamic properties of extreme-token phenomena predicted by theBB task align with observations in pretrained LLMs.",Tianyu Guo,2024/10/17,2024/11/7,,N/A,['cs.LG']
2410.11459v1,Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models,http://arxiv.org/abs/2410.11459v1,"Large language models (LLMs) have exhibited outstanding performance inengaging with humans and addressing complex questions by leveraging their vastimplicit knowledge and robust reasoning capabilities. However, such models arevulnerable to jailbreak attacks, leading to the generation of harmfulresponses. Despite recent research on single-turn jailbreak strategies tofacilitate the development of defence mechanisms, the challenge of revealingvulnerabilities under multi-turn setting remains relatively under-explored. Inthis work, we propose Jigsaw Puzzles (JSP), a straightforward yet effectivemulti-turn jailbreak strategy against the advanced LLMs. JSP splits questionsinto harmless fractions as the input of each turn, and requests LLMs toreconstruct and respond to questions under multi-turn interaction. Ourexperimental results demonstrate that the proposed JSP jailbreak bypassesoriginal safeguards against explicitly harmful content, achieving an averageattack success rate of 93.76% on 189 harmful queries across 5 advanced LLMs(Gemini-1.5-Pro, Llama-3.1-70B, GPT-4, GPT-4o, GPT-4o-mini). Moreover, JSPachieves a state-of-the-art attack success rate of 92% on GPT-4 on the harmfulquery benchmark, and exhibits strong resistant to defence strategies. Warning:this paper contains offensive examples.",Hao Yang,2024/10/15,2024/10/15,,N/A,['cs.CL']
2410.06733v1,Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles,http://arxiv.org/abs/2410.06733v1,"While advancements in NLP have significantly improved the performance ofLarge Language Models (LLMs) on tasks requiring vertical thinking, theirlateral thinking capabilities remain under-explored and challenging to measuredue to the complexity of assessing creative thought processes and the scarcityof relevant data. To address these challenges, we introduce SPLAT, a benchmarkleveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs.This benchmark, containing 975 graded situation puzzles across three difficultylevels, employs a new multi-turn player-judge framework instead of thetraditional model-based evaluation, which often necessitates a strongerevaluation model. This framework simulates an interactive game where the model(player) asks the evaluation model (judge) questions about an incomplete storyto infer the full scenario. The judge answers based on a detailed referencescenario or evaluates if the player's predictions align with the reference one.This approach lessens dependence on more robust evaluation models, enabling theassessment of state-of-the-art LLMs. The experiments demonstrate that a robustevaluation model, such as WizardLM-2, closely matches human judgements in bothintermediate question-answering and final scenario accuracy, achieving over 80%agreement-similar to the agreement levels among humans. Furthermore, applyingdata and reasoning processes from our benchmark to other lateralthinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads toperformance enhancements. This suggests that our benchmark effectivelyevaluates and elicits the lateral thinking abilities of LLMs. Code is availableat: https://github.com/chenqi008/LateralThinking.",Qi Chen,2024/10/9,2024/10/9,,N/A,"['cs.CL', 'cs.AI', 'cs.CV']"
2410.05262v1,TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles,http://arxiv.org/abs/2410.05262v1,"As the application of Large Language Models (LLMs) expands, the demand forreliable evaluations increases. Existing LLM evaluation benchmarks primarilyrely on static datasets, making it challenging to assess model performance indynamic interactions with users. Moreover, these benchmarks often depend onspecific background knowledge, complicating the measurement of a model'slogical reasoning capabilities. Other dynamic evaluation methods based onstrong models or manual efforts may introduce biases and incur high costs andtime demands, hindering large-scale application. To address these issues, wepropose TurtleBench. TurtleBench collects real user guesses from our onlineTurtle Soup Puzzle platform that we developed. This approach allows for therelatively dynamic generation of evaluation datasets, mitigating the risk ofmodel cheating while aligning assessments more closely with genuine user needsfor reasoning capabilities, thus enhancing the reliability of evaluations.TurtleBench includes 1,532 user guesses along with the correctness of guessesafter annotation. Using this dataset, we thoroughly evaluated nine of the mostadvanced LLMs available today. Notably, the OpenAI o1 series models did notachieve leading results in these evaluations. We propose several hypotheses forfurther research, such as ""the latent reasoning of o1 utilizes trivialChain-of-Thought (CoT) techniques"" and ""increasing CoT length not only providesreasoning benefits but also incurs noise costs.""",Qingchen Yu,2024/10/7,2024/10/7,,N/A,['cs.CL']
2410.02892v3,The Role of Deductive and Inductive Reasoning in Large Language Models,http://arxiv.org/abs/2410.02892v3,"Large Language Models (LLMs) have demonstrated impressive capabilities inreasoning tasks, yet their reliance on static prompt structures and limitedadaptability to complex scenarios remains a significant challenge. In thispaper, we propose the Deductive and InDuctive(DID) method, a novel frameworkthat enhances LLM reasoning by dynamically integrating both deductive andinductive reasoning approaches. Drawing from cognitive science principles, DIDimplements a dual-metric complexity evaluation system that combines Littlestonedimension and information entropy to precisely assess task difficulty and guidedecomposition strategies. DID enables the model to progressively adapt itsreasoning pathways based on problem complexity, mirroring human cognitiveprocesses. We evaluate DID's effectiveness across multiple benchmarks,including the AIW and MR-GSM8K, as well as our custom Holiday Puzzle datasetfor temporal reasoning. Our results demonstrate significant improvements inreasoning quality and solution accuracy - achieving 70.3% accuracy on AIW(compared to 62.2% for Tree of Thought) while maintaining lower computationalcosts. The success of DID in improving LLM performance while preservingcomputational efficiency suggests promising directions for developing morecognitively aligned and capable language models. Our work contributes atheoretically grounded, input-centric approach to enhancing LLM reasoningcapabilities, offering an efficient alternative to traditionaloutput-exploration methods.",Chengkun Cai,2024/10/3,2025/7/7,,N/A,"['cs.AI', 'cs.CL', 'cs.LG']"
2409.12618v2,Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning,http://arxiv.org/abs/2409.12618v2,"Iterative human engagement is a common and effective means of leveraging theadvanced language processing power of large language models (LLMs). Usingwell-structured prompts in a conversational manner, human users can effectivelyinfluence an LLM to develop more thoughtful and accurate responses. Motivatedby this insight, we propose the Iteration of Thought (IoT) framework forenhancing LLM responses by generating ""thought""-provoking prompts vis a vis aninput query and the current iteration of an LLM's response. Unlike static orsemi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT),IoT adapts its reasoning path dynamically, based on evolving context, andwithout generating alternate explorative thoughts which are ultimatelydiscarded. The three components of the IoT framework are (1) an Inner DialogueAgent (IDA) responsible for generating instructive, context-specific prompts;(2) an LLM Agent (LLMA) that processes these prompts to refine its responses;and (3) an iterative prompting loop that implements a conversation between theformer two components. We introduce two variants of our framework: AutonomousIteration of Thought (AIoT), where an LLM decides when to stop iterating, andGuided Iteration of Thought (GIoT), which always forces a fixed numberiterations. We investigate the performance of IoT across various datasets,spanning complex reasoning tasks from the GPQA dataset, explorativeproblem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hopquestion answering from the HotpotQA dataset. Our results show that IoTrepresents a viable paradigm for autonomous response refinement in LLMs,showcasing significant improvements over CoT and thereby enabling more adaptiveand efficient reasoning systems that minimize human intervention.",Santosh Kumar Radha,2024/9/19,2024/10/1,,N/A,"['cs.CL', 'cs.AI', 'cs.LG', 'cs.MA']"
2409.10502v1,Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles,http://arxiv.org/abs/2409.10502v1,"Causal language modeling using the Transformer architecture has yieldedremarkable capabilities in Large Language Models (LLMs) over the last fewyears. However, the extent to which fundamental search and reasoningcapabilities emerged within LLMs remains a topic of ongoing debate. In thiswork, we study if causal language modeling can learn a complex task such assolving Sudoku puzzles. To solve a Sudoku, the model is first required tosearch over all empty cells of the puzzle to decide on a cell to fill and thenapply an appropriate strategy to fill the decided cell. Sometimes, theapplication of a strategy only results in thinning down the possible values ina cell rather than concluding the exact value of the cell. In such cases,multiple strategies are applied one after the other to fill a single cell. Weobserve that Transformer models trained on this synthetic task can indeed learnto solve Sudokus (our model solves $94.21\%$ of the puzzles fully correctly)when trained on a logical sequence of steps taken by a solver. We find thattraining Transformers with the logical sequence of steps is necessary andwithout such training, they fail to learn Sudoku. We also extend our analysisto Zebra puzzles (known as Einstein puzzles) and show that the model solves$92.04 \%$ of the puzzles fully correctly. In addition, we study the internalrepresentations of the trained Transformer and find that through linearprobing, we can decode information about the set of possible values in anygiven cell from them, pointing to the presence of a strong reasoning engineimplicit in the Transformer weights.",Kulin Shah,2024/9/16,2024/9/16,,N/A,"['cs.LG', 'cs.CL']"
2409.08890v1,A Market for Lemons? Strategic Directions for a Vigilant Application of Artificial Intelligence in Entrepreneurship Research,http://arxiv.org/abs/2409.08890v1,"The rapid expansion of AI adoption (e.g., using machine learning, deeplearning, or large language models as research methods) and the increasingavailability of big data have the potential to bring about the most significanttransformation in entrepreneurship scholarship the field has ever witnessed.This article makes a pressing meta-contribution by highlighting a significantrisk of unproductive knowledge exchanges in entrepreneurship research amid theAI revolution. It offers strategies to mitigate this risk and provides guidancefor future AI-based studies to enhance their collective impact and relevance.Drawing on Akerlof's renowned market-for-lemons concept, we identify thepotential for significant knowledge asymmetries emerging from the field'sevolution into its current landscape (e.g., complexities around constructvalidity, theory building, and research relevance). Such asymmetries areparticularly deeply ingrained due to what we term the double-black-box puzzle,where the widely recognized black box nature of AI methods intersects with theblack box nature of the entrepreneurship phenomenon driven by inherentuncertainty. As a result, these asymmetries could lead to an increase insuboptimal research products that go undetected, collectively creating a marketfor lemons that undermines the field's well-being, reputation, and impact.However, importantly, if these risks can be mitigated, the AI revolution couldherald a new golden era for entrepreneurship research. We discuss the necessaryactions to elevate the field to a higher level of AI resilience whilesteadfastly maintaining its foundational principles and core values.",Martin Obschonka,2024/9/13,2024/9/13,,N/A,"['econ.GN', 'q-fin.EC']"
2409.00105v2,Negation Blindness in Large Language Models: Unveiling the NO Syndrome in Image Generation,http://arxiv.org/abs/2409.00105v2,"Foundational Large Language Models (LLMs) have changed the way we perceivetechnology. They have been shown to excel in tasks ranging from poem writingand coding to essay generation and puzzle solving. With the incorporation ofimage generation capability, they have become more comprehensive and versatileAI tools. At the same time, researchers are striving to identify thelimitations of these tools to improve them further. Currently identified flawsinclude hallucination, biases, and bypassing restricted commands to generateharmful content. In the present work, we have identified a fundamentallimitation related to the image generation ability of LLMs, and termed it TheNO Syndrome. This negation blindness refers to LLMs inability to correctlycomprehend NO related natural language prompts to generate the desired images.Interestingly, all tested LLMs including GPT-4, Gemini, and Copilot were foundto be suffering from this syndrome. To demonstrate the generalization of thislimitation, we carried out simulation experiments and conducted entropy-basedand benchmark statistical analysis tests on various LLMs in multiple languages,including English, Hindi, and French. We conclude that the NO syndrome is asignificant flaw in current LLMs that needs to be addressed. A related findingof this study showed a consistent discrepancy between image and textualresponses as a result of this NO syndrome. We posit that the introduction of anegation context-aware reinforcement learning based feedback loop between theLLMs textual response and generated image could help ensure the generated textis based on both the LLMs correct contextual understanding of the negationquery and the generated visual output.",Mohammad Nadeem,2024/8/27,2024/9/4,,N/A,"['cs.CL', 'cs.AI', 'cs.LG']"
2408.00584v1,"Non Verbis, Sed Rebus: Large Language Models are Weak Solvers of Italian Rebuses",http://arxiv.org/abs/2408.00584v1,"Rebuses are puzzles requiring constrained multi-step reasoning to identify ahidden phrase from a set of images and letters. In this work, we introduce alarge collection of verbalized rebuses for the Italian language and use it toassess the rebus-solving capabilities of state-of-the-art large languagemodels. While general-purpose systems such as LLaMA-3 and GPT-4o perform poorlyon this task, ad-hoc fine-tuning seems to improve models' performance. However,we find that performance gains from training are largely motivated bymemorization. Our results suggest that rebus solving remains a challenging testbed to evaluate large language models' linguistic proficiency and sequentialinstruction-following skills.",Gabriele Sarti,2024/8/1,2024/8/1,,N/A,"['cs.CL', 'cs.AI']"
2407.11240v1,Making New Connections: LLMs as Puzzle Generators for The New York Times' Connections Word Game,http://arxiv.org/abs/2407.11240v1,"The Connections puzzle is a word association game published daily by The NewYork Times (NYT). In this game, players are asked to find groups of four wordsthat are connected by a common theme. While solving a given Connections puzzlerequires both semantic knowledge and abstract reasoning, generating novelpuzzles additionally requires a form of metacognition: generators must be ableto accurately model the downstream reasoning of potential solvers. In thispaper, we investigate the ability of the GPT family of Large Language Models(LLMs) to generate challenging and creative word games for human players. Westart with an analysis of the word game Connections and the unique challengesit poses as a Procedural Content Generation (PCG) domain. We then propose amethod for generating Connections puzzles using LLMs by adapting a Tree ofThoughts (ToT) prompting approach. We evaluate this method by conducting a userstudy, asking human players to compare AI-generated puzzles against publishedConnections puzzles. Our findings show that LLMs are capable puzzle creators,and can generate diverse sets of enjoyable, challenging, and creativeConnections puzzles as judged by human users.",Tim Merino,2024/7/15,2024/7/15,,N/A,"['cs.AI', 'cs.CL']"
2407.09985v2,A Training Data Recipe to Accelerate A* Search with Language Models,http://arxiv.org/abs/2407.09985v2,"Combining Large Language Models (LLMs) with heuristic search algorithms likeA* holds the promise of enhanced LLM reasoning and scalable inference. Toaccelerate training and reduce computational demands, we investigate thecoreset selection problem for the training data of LLM heuristic learning. Fewmethods to learn the heuristic functions consider the interaction between thesearch algorithm and the machine learning model. In this work, we empiricallydisentangle the requirements of A* search algorithm from the requirements ofthe LLM to generalise on this task. Surprisingly, we find an overlap betweentheir requirements; A* requires more accurate predictions on search nodes nearthe goal, and LLMs need the same set of nodes for effective generalisation.With these insights, we derive a data-selection distribution for learningLLM-based heuristics. On three classical planning domains, maze navigation,Sokoban and sliding tile puzzles, our technique reduces the number ofiterations required to find the solutions by up to 15x, with a wall-clockspeed-up of search up to 5x. The codebase is athttps://github.com/devaansh100/a_star.",Devaansh Gupta,2024/7/13,2024/10/23,,N/A,"['cs.AI', 'cs.LG']"
2407.07053v5,Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model,http://arxiv.org/abs/2407.07053v5,"Although most current large multimodal models (LMMs) can already understandphotos of natural scenes and portraits, their understanding of abstract images,e.g., charts, maps, or layouts, and visual reasoning capabilities remains quiterudimentary. They often struggle with simple daily tasks, such as reading timefrom a clock, understanding a flowchart, or planning a route using a road map.In light of this, we design a multi-modal self-instruct, utilizing largelanguage models and their code capabilities to synthesize massive abstractimages and visual reasoning instructions across daily scenarios. Our strategyeffortlessly creates a multimodal benchmark with 11,193 instructions for eightvisual scenarios: charts, tables, simulated maps, dashboards, flowcharts,relation graphs, floor plans, and visual puzzles. \textbf{This benchmark,constructed with simple lines and geometric elements, exposes the shortcomingsof most advanced LMMs} like Claude-3.5-Sonnet and GPT-4o in abstract imageunderstanding, spatial relations reasoning, and visual element induction.Besides, to verify the quality of our synthetic data, we fine-tune an LMM using62,476 synthetic chart, table and road map instructions. The resultsdemonstrate improved chart understanding and map navigation performance, andalso demonstrate potential benefits for other visual reasoning tasks. Our codeis available at: \url{https://github.com/zwq2018/Multi-modal-Self-instruct}.",Wenqi Zhang,2024/7/9,2024/10/3,,N/A,['cs.CV']
2407.04973v1,LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts,http://arxiv.org/abs/2407.04973v1,"We propose LogicVista, an evaluation benchmark that assesses the integratedlogical reasoning capabilities of multimodal large language models (MLLMs) inVisual contexts. Recent advancements in MLLMs have demonstrated variousfascinating abilities, from crafting poetry based on an image to performingmathematical reasoning. However, there is still a lack of systematic evaluationof MLLMs' proficiency in logical reasoning tasks, which are essential foractivities like navigation and puzzle-solving. Thus we evaluate general logicalcognition abilities across 5 logical reasoning tasks encompassing 9 differentcapabilities, using a sample of 448 multiple-choice questions. Each question isannotated with the correct answer and the human-written reasoning behind theselection, enabling both open-ended and multiple-choice evaluation. A total of8 MLLMs are comprehensively evaluated using LogicVista. Code and Data Availableat https://github.com/Yijia-Xiao/LogicVista.",Yijia Xiao,2024/7/6,2024/7/6,,N/A,"['cs.AI', 'cs.CL', 'cs.CV', 'cs.LG']"
2407.03956v2,Solving Zebra Puzzles Using Constraint-Guided Multi-Agent Systems,http://arxiv.org/abs/2407.03956v2,"Prior research has enhanced the ability of Large Language Models (LLMs) tosolve logic puzzles using techniques such as chain-of-thought prompting orintroducing a symbolic representation. These frameworks are still usuallyinsufficient to solve complicated logical problems, such as Zebra puzzles, dueto the inherent complexity of translating natural language clues into logicalstatements. We introduce a multi-agent system, ZPS, that integrates LLMs withan off the shelf theorem prover. This system tackles the complex puzzle-solvingtask by breaking down the problem into smaller, manageable parts, generatingSMT (Satisfiability Modulo Theories) code to solve them with a theorem prover,and using feedback between the agents to repeatedly improve their answers. Wealso introduce an automated grid puzzle grader to assess the correctness of ourpuzzle solutions and show that the automated grader is reliable by evaluatingit in a user-study. Our approach shows improvement in all three LLMs we tested,with GPT-4 showing 166% improvement in the number of fully correct solutions.",Shmuel Berman,2024/7/4,2024/7/9,,N/A,"['cs.MA', 'cs.CL', '68T01, 68T20, 68T27,', 'I.2.3; I.2.6; I.2.7; I.2.11']"
2406.12546v2,"Liar, Liar, Logical Mire: A Benchmark for Suppositional Reasoning in Large Language Models",http://arxiv.org/abs/2406.12546v2,"Knights and knaves problems represent a classic genre of logical puzzleswhere characters either tell the truth or lie. The objective is to logicallydeduce each character's identity based on their statements. The challengearises from the truth-telling or lying behavior, which influences the logicalimplications of each statement. Solving these puzzles requires not only directdeductions from individual statements, but the ability to assess thetruthfulness of statements by reasoning through various hypothetical scenarios.As such, knights and knaves puzzles serve as compelling examples ofsuppositional reasoning. In this paper, we introduce $\textit{TruthQuest}$, abenchmark for suppositional reasoning based on the principles of knights andknaves puzzles. Our benchmark presents problems of varying complexity,considering both the number of characters and the types of logical statementsinvolved. Evaluations on $\textit{TruthQuest}$ show that large language modelslike Llama 3 and Mixtral-8x7B exhibit significant difficulties solving thesetasks. A detailed error analysis of the models' output reveals thatlower-performing models exhibit a diverse range of reasoning errors, frequentlyfailing to grasp the concept of truth and lies. In comparison, more proficientmodels primarily struggle with accurately inferring the logical implications ofpotentially false statements.",Philipp Mondorf,2024/6/18,2024/10/7,,N/A,['cs.CL']
2406.12172v1,Navigating the Labyrinth: Evaluating and Enhancing LLMs' Ability to Reason About Search Problems,http://arxiv.org/abs/2406.12172v1,"Recently, Large Language Models (LLMs) attained impressive performance inmath and reasoning benchmarks. However, they still often struggle with logicproblems and puzzles that are relatively easy for humans. To furtherinvestigate this, we introduce a new benchmark, SearchBench, containing 11unique search problem types, each equipped with automated pipelines to generatean arbitrary number of instances and analyze the feasibility, correctness, andoptimality of LLM-generated solutions. We show that even the most advanced LLMsfail to solve these problems end-to-end in text, e.g. GPT4 solves only 1.4%.SearchBench problems require considering multiple pathways to the solution aswell as backtracking, posing a significant challenge to auto-regressive models.Instructing LLMs to generate code that solves the problem helps, but onlyslightly, e.g., GPT4's performance rises to 11.7%. In this work, we show thatin-context learning with A* algorithm implementations enhances performance. Thefull potential of this promoting approach emerges when combined with ourproposed Multi-Stage-Multi-Try method, which breaks down the algorithmimplementation into two stages and verifies the first stage against unit tests,raising GPT-4's performance above 57%.",Nasim Borazjanizadeh,2024/6/18,2024/6/18,,N/A,['cs.AI']
2406.11555v1,Input Conditioned Graph Generation for Language Agents,http://arxiv.org/abs/2406.11555v1,"Recent progress in Large Language Models (LLMs) and language agents hasdemonstrated significant promise for various future applications acrossmultiple disciplines. While traditional approaches to language agents oftenrely on fixed, handcrafted designs, our research aims to develop both learnableand dynamic agents. Our method uses an existing framework that abstractslanguage agents as graphs. Within this graph framework, we aim to learn a modelthat can generate edges for every given input to the language agent. Thisallows us to generate edges that represent the flow of communication within thegraph based on the given input, thereby adjusting the internal communication ofa language agent. We learn to generate these edges using a pretrained LLM thatis fine-tuned with reinforcement learning. This LLM can be fine-tuned onseveral datasets simultaneously, and we hypothesize that the model learns toadapt to these different domains during training, achieving good overallperformance when encountering data from different domains during deployment. Wedemonstrate that our approach surpasses the previous static approach by nearly6% accuracy on a combined dataset of MMLU and CMMLU, and by more than 10% whentrained with a sparsity-inducing loss. It also performs superior in additionalexperiments conducted with the MMLU and Mini Crossword Puzzles datasets. Thecode is available at https://github.com/lukasVierling/DynamicGPTSwarm.",Lukas Vierling,2024/6/17,2024/6/17,,N/A,"['cs.CL', 'cs.AI']"
2406.11012v7,Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game,http://arxiv.org/abs/2406.11012v7,"The New York Times Connections game has emerged as a popular and challengingpursuit for word puzzle enthusiasts. We collect 438 Connections games toevaluate the performance of state-of-the-art large language models (LLMs)against expert and novice human players. Our results show that even the bestperforming LLM, Claude 3.5 Sonnet, which has otherwise shown impressivereasoning abilities on a wide variety of benchmarks, can only fully solve 18%of the games. Novice and expert players perform better than Claude 3.5 Sonnet,with expert human players significantly outperforming it. We create a taxonomyof the knowledge types required to successfully cluster and categorize words inthe Connections game. We find that while LLMs perform relatively well oncategorizing words based on semantic relations they struggle with other typesof knowledge such as Encyclopedic Knowledge, Multiword Expressions or knowledgethat combines both Word Form and Meaning. Our results establish the New YorkTimes Connections game as a challenging benchmark for evaluating abstractreasoning capabilities in AI systems.",Prisha Samadarshi,2024/6/16,2024/10/14,,N/A,"['cs.CL', 'cs.AI']"
2406.10842v1,Large Language Models for Automatic Milestone Detection in Group Discussions,http://arxiv.org/abs/2406.10842v1,"Large language models like GPT have proven widely successful on naturallanguage understanding tasks based on written text documents. In this paper, weinvestigate an LLM's performance on recordings of a group oral communicationtask in which utterances are often truncated or not well-formed. We propose anew group task experiment involving a puzzle with several milestones that canbe achieved in any order. We investigate methods for processing transcripts todetect if, when, and by whom a milestone has been completed. We demonstratethat iteratively prompting GPT with transcription chunks outperforms semanticsimilarity search methods using text embeddings, and further discuss thequality and randomness of GPT responses under different context window sizes.",Zhuoxu Duan,2024/6/16,2024/6/16,,N/A,"['cs.CL', 'cs.AI', 'cs.HC']"
2406.06196v3,LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low-Resource and Extinct Languages,http://arxiv.org/abs/2406.06196v3,"In this paper, we present the LingOly benchmark, a novel benchmark foradvanced reasoning abilities in large language models. Using challengingLinguistic Olympiad puzzles, we evaluate (i) capabilities for in-contextidentification and generalisation of linguistic patterns in very low-resourceor extinct languages, and (ii) abilities to follow complex task instructions.The LingOly benchmark covers more than 90 mostly low-resource languages,minimising issues of data contamination, and contains 1,133 problems across 6formats and 5 levels of human difficulty. We assess performance with bothdirect accuracy and comparison to a no-context baseline to penalisememorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark tobe challenging, and models perform poorly on the higher difficulty problems. Onharder problems, even the top model only achieved 38.7% accuracy, a 24.7%improvement over the no-context baseline. Large closed models typicallyoutperform open models, and in general, the higher resource the language, thebetter the scores. These results indicate, in absence of memorisation, truemulti-step out-of-domain reasoning remains a challenge for current languagemodels.",Andrew M. Bean,2024/6/10,2024/10/31,,N/A,['cs.CL']
2406.04947v1,BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense,http://arxiv.org/abs/2406.04947v1,"This paper outlines our approach to SemEval 2024 Task 9, BRAINTEASER: A NovelTask Defying Common Sense. The task aims to evaluate the ability of languagemodels to think creatively. The dataset comprises multi-choice questions thatchallenge models to think ""outside of the box"". We fine-tune 2 models, BERT andRoBERTa Large. Next, we employ a Chain of Thought (CoT) zero-shot promptingapproach with 6 large language models, such as GPT-3.5, Mixtral, and Llama2.Finally, we utilize ReConcile, a technique that employs a ""round tableconference"" approach with multiple agents for zero-shot learning, to generateconsensus answers among 3 selected language models. Our best method achieves anoverall accuracy of 85 percent on the sentence puzzles subtask.",Baktash Ansari,2024/6/7,2024/6/7,,N/A,['cs.CL']
2406.03689v3,Evaluating the World Model Implicit in a Generative Model,http://arxiv.org/abs/2406.03689v3,"Recent work suggests that large language models may implicitly learn worldmodels. How should we assess this possibility? We formalize this question forthe case where the underlying reality is governed by a deterministic finiteautomaton. This includes problems as diverse as simple logical reasoning,geographic navigation, game-playing, and chemistry. We propose new evaluationmetrics for world model recovery inspired by the classic Myhill-Nerode theoremfrom language theory. We illustrate their utility in three domains: gameplaying, logic puzzles, and navigation. In all domains, the generative modelswe consider do well on existing diagnostics for assessing world models, but ourevaluation metrics reveal their world models to be far less coherent than theyappear. Such incoherence creates fragility: using a generative model to solverelated but subtly different tasks can lead to failures. Building generativemodels that meaningfully capture the underlying logic of the domains they modelwould be immensely valuable; our results suggest new ways to assess how close agiven model is to that goal.",Keyon Vafa,2024/6/6,2024/11/10,,N/A,"['cs.CL', 'cs.AI']"
2406.02100v1,Exploring Mathematical Extrapolation of Large Language Models with Synthetic Data,http://arxiv.org/abs/2406.02100v1,"Large Language Models (LLMs) have shown excellent performance in languageunderstanding, text generation, code synthesis, and many other tasks, whilethey still struggle in complex multi-step reasoning problems, such asmathematical reasoning. In this paper, through a newly proposed arithmeticalpuzzle problem, we show that the model can perform well on multi-step reasoningtasks via fine-tuning on high-quality synthetic data. Experimental results withthe open-llama-3B model on three different test datasets show that not only themodel can reach a zero-shot pass@1 at 0.44 on the in-domain dataset, it alsodemonstrates certain generalization capabilities on the out-of-domain datasets.Specifically, this paper has designed two out-of-domain datasets in the form ofextending the numerical range and the composing components of the arithmeticalpuzzle problem separately. The fine-tuned models have shown encouragingperformance on these two far more difficult tasks with the zero-shot pass@1 at0.33 and 0.35, respectively.",Haolong Li,2024/6/4,2024/6/4,,N/A,['cs.CL']
2405.17503v3,Code Repair with LLMs gives an Exploration-Exploitation Tradeoff,http://arxiv.org/abs/2405.17503v3,"Iteratively improving and repairing source code with large language models(LLMs), known as refinement, has emerged as a popular way of generatingprograms that would be too complex to construct in one shot. Given a bank oftest cases, together with a candidate program, an LLM can improve that programby being prompted with failed test cases. But it remains an open question howto best iteratively refine code, with prior work employing simple greedy orbreadth-first strategies. We show here that refinement exposes anexplore-exploit tradeoff: exploit by refining the program that passes the mosttest cases, or explore by refining a lesser considered program. We frame thisas an arm-acquiring bandit problem, which we solve with Thompson Sampling. Theresulting LLM-based program synthesis algorithm is broadly applicable: Acrossloop invariant synthesis, visual reasoning puzzles, and competition programmingproblems, we find that our new method can solve more problems using fewerlanguage model calls.",Hao Tang,2024/5/26,2024/10/29,,N/A,"['cs.SE', 'cs.AI', 'cs.CL', 'cs.PL']"
2405.07035v2,A Turkish Educational Crossword Puzzle Generator,http://arxiv.org/abs/2405.07035v2,"This paper introduces the first Turkish crossword puzzle generator designedto leverage the capabilities of large language models (LLMs) for educationalpurposes. In this work, we introduced two specially created datasets: one withover 180,000 unique answer-clue pairs for generating relevant clues from thegiven answer, and another with over 35,000 samples containing text, answer,category, and clue data, aimed at producing clues for specific texts andkeywords within certain categories. Beyond entertainment, this generatoremerges as an interactive educational tool that enhances memory, vocabulary,and problem-solving skills. It's a notable step in AI-enhanced education,merging game-like engagement with learning for Turkish and setting newstandards for interactive, intelligent learning tools in Turkish.",Kamyar Zeinalipour,2024/5/11,2024/5/15,,N/A,['cs.CL']
2405.04086v1,Optimizing Language Model's Reasoning Abilities with Weak Supervision,http://arxiv.org/abs/2405.04086v1,"While Large Language Models (LLMs) have demonstrated proficiency in handlingcomplex queries, much of the past work has depended on extensively annotateddatasets by human experts. However, this reliance on fully-supervisedannotations poses scalability challenges, particularly as models and datarequirements grow. To mitigate this, we explore the potential of enhancingLLMs' reasoning abilities with minimal human supervision. In this work, weintroduce self-reinforcement, which begins with Supervised Fine-Tuning (SFT) ofthe model using a small collection of annotated questions. Then it iterativelyimproves LLMs by learning from the differences in responses from the SFT andunfinetuned models on unlabeled questions. Our approach provides an efficientapproach without relying heavily on extensive human-annotated explanations.However, current reasoning benchmarks typically only include golden-referenceanswers or rationales. Therefore, we present \textsc{PuzzleBen}, a weaklysupervised benchmark that comprises 25,147 complex questions, answers, andhuman-generated rationales across various domains, such as brainteasers,puzzles, riddles, parajumbles, and critical reasoning tasks. A unique aspect ofour dataset is the inclusion of 10,000 unannotated questions, enabling us toexplore utilizing fewer supersized data to boost LLMs' inference capabilities.Our experiments underscore the significance of \textsc{PuzzleBen}, as well asthe effectiveness of our methodology as a promising direction in futureendeavors. Our dataset and code will be published soon on \texttt{AnonymityLink}.",Yongqi Tong,2024/5/7,2024/5/7,,N/A,['cs.CL']
2404.13591v2,MARVEL: Multidimensional Abstraction and Reasoning through Visual Evaluation and Learning,http://arxiv.org/abs/2404.13591v2,"While multi-modal large language models (MLLMs) have shown significantprogress on many popular visual reasoning benchmarks, whether they possessabstract visual reasoning abilities remains an open question. Similar to theSudoku puzzles, abstract visual reasoning (AVR) problems require findinghigh-level patterns (e.g., repetition constraints) that control the inputshapes (e.g., digits) in a specific task configuration (e.g., matrix). However,existing AVR benchmarks only considered a limited set of patterns (addition,conjunction), input shapes (rectangle, square), and task configurations (3 by 3matrices). To evaluate MLLMs' reasoning abilities comprehensively, we introduceMARVEL, a multidimensional AVR benchmark with 770 puzzles composed of six coreknowledge patterns, geometric and abstract shapes, and five different taskconfigurations. To inspect whether the model accuracy is grounded in perceptionand reasoning, MARVEL complements the general AVR question with perceptionquestions in a hierarchical evaluation framework. We conduct comprehensiveexperiments on MARVEL with nine representative MLLMs in zero-shot and few-shotsettings. Our experiments reveal that all models show near-random performanceon the AVR question, with significant performance gaps (40%) compared to humansacross all patterns and task configurations. Further analysis of perceptionquestions reveals that MLLMs struggle to comprehend the visual features(near-random performance) and even count the panels in the puzzle ( <45%),hindering their ability for abstract reasoning. We release our entire code anddataset.",Yifan Jiang,2024/4/21,2024/4/24,,N/A,"['cs.CV', 'cs.LG']"
2404.11730v2,Missed Connections: Lateral Thinking Puzzles for Large Language Models,http://arxiv.org/abs/2404.11730v2,"The Connections puzzle published each day by the New York Times tasks playerswith dividing a bank of sixteen words into four groups of four words that eachrelate to a common theme. Solving the puzzle requires both common linguisticknowledge (i.e. definitions and typical usage) as well as, in many cases,lateral or abstract thinking. This is because the four categories ascend incomplexity, with the most challenging category often requiring thinking aboutwords in uncommon ways or as parts of larger phrases. We investigate thecapacity for automated AI systems to play Connections and explore the game'spotential as an automated benchmark for abstract reasoning and a way to measurethe semantic information encoded by data-driven linguistic systems. Inparticular, we study both a sentence-embedding baseline and modern largelanguage models (LLMs). We report their accuracy on the task, measure theimpacts of chain-of-thought prompting, and discuss their failure modes.Overall, we find that the Connections task is challenging yet feasible, and astrong test-bed for future work.",Graham Todd,2024/4/17,2024/4/21,,N/A,"['cs.CL', 'cs.AI']"
2404.06186v1,Clue-Instruct: Text-Based Clue Generation for Educational Crossword Puzzles,http://arxiv.org/abs/2404.06186v1,"Crossword puzzles are popular linguistic games often used as tools to engagestudents in learning. Educational crosswords are characterized by less crypticand more factual clues that distinguish them from traditional crosswordpuzzles. Despite there exist several publicly available clue-answer pairdatabases for traditional crosswords, educational clue-answer pairs datasetsare missing. In this article, we propose a methodology to build educationalclue generation datasets that can be used to instruct Large Language Models(LLMs). By gathering from Wikipedia pages informative content associated withrelevant keywords, we use Large Language Models to automatically generatepedagogical clues related to the given input keyword and its context. With suchan approach, we created clue-instruct, a dataset containing 44,075 uniqueexamples with text-keyword pairs associated with three distinct crosswordclues. We used clue-instruct to instruct different LLMs to generate educationalclues from a given input content and keyword. Both human and automaticevaluations confirmed the quality of the generated clues, thus validating theeffectiveness of our approach.",Andrea Zugarini,2024/4/9,2024/4/9,,N/A,"['cs.CL', 'cs.AI']"
2403.14982v2,MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts,http://arxiv.org/abs/2403.14982v2,"Our paper presents team MasonTigers submission to the SemEval-2024 Task 9 -which provides a dataset of puzzles for testing natural language understanding.We employ large language models (LLMs) to solve this task through severalprompting techniques. Zero-shot and few-shot prompting generate reasonably goodresults when tested with proprietary LLMs, compared to the open-source models.We obtain further improved results with chain-of-thought prompting, aniterative prompting method that breaks down the reasoning process step-by-step.We obtain our best results by utilizing an ensemble of chain-of-thoughtprompts, placing 2nd in the word puzzle subtask and 13th in the sentence puzzlesubtask. The strong performance of prompted LLMs demonstrates their capabilityfor complex reasoning when provided with a decomposition of the thoughtprocess. Our work sheds light on how step-wise explanatory prompts can unlockmore of the knowledge encoded in the parameters of large models.",Md Nishat Raihan,2024/3/22,2024/4/3,,N/A,['cs.CL']
2403.13315v3,PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns,http://arxiv.org/abs/2403.13315v3,"Large multimodal models extend the impressive capabilities of large languagemodels by integrating multimodal understanding abilities. However, it is notclear how they can emulate the general intelligence and reasoning ability ofhumans. As recognizing patterns and abstracting concepts are key to generalintelligence, we introduce PuzzleVQA, a collection of 2000 puzzle instancesbased on abstract patterns. With this dataset, we evaluate large multimodalmodels with abstract patterns based on fundamental concepts, including colors,numbers, sizes, and shapes. Through our experiments on state-of-the-art largemultimodal models, we find that they are not able to generalize well to simpleabstract patterns. Notably, GPT-4V achieves a score of 46.4% on single-conceptpuzzles, which shows that state-of-the-art models struggle on our dataset. Todiagnose the reasoning challenges in large multimodal models, we progressivelyguide the models with our ground truth reasoning explanations for visualperception, inductive reasoning, and deductive reasoning. Our systematicanalysis finds that the main bottlenecks of GPT-4V are weaker visual perceptionand inductive reasoning abilities. Through this work, we hope to shed light onthe limitations of large multimodal models and how they can better emulatehuman cognitive processes in the future. Our data and code are available athttps://puzzlevqa.github.io",Yew Ken Chia,2024/3/20,2024/8/17,,N/A,['cs.CV']
2403.03864v3,Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning,http://arxiv.org/abs/2403.03864v3,"This paper introduces the novel task of multimodal puzzle solving, framedwithin the context of visual question-answering. We present a new dataset,AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodallanguage models in solving algorithmic puzzles that necessitate both visualunderstanding, language understanding, and complex algorithmic reasoning. Wecreate the puzzles to encompass a diverse array of mathematical and algorithmictopics such as boolean logic, combinatorics, graph theory, optimization,search, etc., aiming to evaluate the gap between visual data interpretation andalgorithmic problem-solving skills. The dataset is generated automatically fromcode authored by humans. All our puzzles have exact solutions that can be foundfrom the algorithm without tedious human calculations. It ensures that ourdataset can be scaled up arbitrarily in terms of reasoning complexity anddataset size. Our investigation reveals that large language models (LLMs) suchas GPT4V and Gemini exhibit limited performance in puzzle-solving tasks. Wefind that their performance is near random in a multi-choice question-answeringsetup for a significant number of puzzles. The findings emphasize thechallenges of integrating visual, language, and algorithmic knowledge forsolving complex reasoning problems.",Deepanway Ghosal,2024/3/6,2024/3/13,,N/A,"['cs.CV', 'cs.AI']"
2402.11814v1,An Empirical Evaluation of LLMs for Solving Offensive Security Challenges,http://arxiv.org/abs/2402.11814v1,"Capture The Flag (CTF) challenges are puzzles related to computer securityscenarios. With the advent of large language models (LLMs), more and more CTFparticipants are using LLMs to understand and solve the challenges. However, sofar no work has evaluated the effectiveness of LLMs in solving CTF challengeswith a fully automated workflow. We develop two CTF-solving workflows,human-in-the-loop (HITL) and fully-automated, to examine the LLMs' ability tosolve a selected set of CTF challenges, prompted with information about thequestion. We collect human contestants' results on the same set of questions,and find that LLMs achieve higher success rate than an average humanparticipant. This work provides a comprehensive evaluation of the capability ofLLMs in solving real world CTF challenges, from real competition to fullyautomated workflow. Our results provide references for applying LLMs incybersecurity education and pave the way for systematic evaluation of offensivecybersecurity capabilities in LLMs.",Minghao Shao,2024/2/19,2024/2/19,,N/A,['cs.CR']
2402.11291v3,Puzzle Solving using Reasoning of Large Language Models: A Survey,http://arxiv.org/abs/2402.11291v3,"Exploring the capabilities of Large Language Models (LLMs) in puzzle solvingunveils critical insights into their potential and challenges in AI, marking asignificant step towards understanding their applicability in complex reasoningtasks. This survey leverages a unique taxonomy -- dividing puzzles intorule-based and rule-less categories -- to critically assess LLMs throughvarious methodologies, including prompting techniques, neuro-symbolicapproaches, and fine-tuning. Through a critical review of relevant datasets andbenchmarks, we assess LLMs' performance, identifying significant challenges incomplex puzzle scenarios. Our findings highlight the disparity between LLMcapabilities and human-like reasoning, particularly in those requiring advancedlogical inference. The survey underscores the necessity for novel strategiesand richer datasets to advance LLMs' puzzle-solving proficiency and contributeto AI's logical reasoning and creative problem-solving advancements.",Panagiotis Giadikiaroglou,2024/2/17,2024/9/14,,N/A,"['cs.CL', 'cs.AI']"
2401.12125v3,CodeTailor: LLM-Powered Personalized Parsons Puzzles for Engaging Support While Learning Programming,http://arxiv.org/abs/2401.12125v3,"Learning to program can be challenging, and providing high-quality and timelysupport at scale is hard. Generative AI and its products, like ChatGPT, cancreate a solution for most intro-level programming problems. However, studentsmight use these tools to just generate code for them, resulting in reducedengagement and limited learning. In this paper, we present CodeTailor, a systemthat leverages a large language model (LLM) to provide personalized help tostudents while still encouraging cognitive engagement. CodeTailor provides apersonalized Parsons puzzle to support struggling students. In a Parsonspuzzle, students place mixed-up code blocks in the correct order to solve aproblem. A technical evaluation with previous incorrect student code snippetsdemonstrated that CodeTailor could deliver high-quality (correct, personalized,and concise) Parsons puzzles based on their incorrect code. We conducted awithin-subjects study with 18 novice programmers. Participants perceivedCodeTailor as more engaging than just receiving an LLM-generated solution (thebaseline condition). In addition, participants applied more supported elementsfrom the scaffolded practice to the posttest when using CodeTailor thanbaseline. Overall, most participants preferred using CodeTailor versus justreceiving the LLM-generated code for learning. Qualitative observations andinterviews also provided evidence for the benefits of CodeTailor, includingthinking more about solution construction, fostering continuity in learning,promoting reflection, and boosting confidence. We suggest future design ideasto facilitate active learning opportunities with generative AI techniques.",Xinying Hou,2024/1/22,2024/5/30,,N/A,"['cs.CY', 'cs.HC']"
2401.05604v2,REBUS: A Robust Evaluation Benchmark of Understanding Symbols,http://arxiv.org/abs/2401.05604v2,"We propose a new benchmark evaluating the performance of multimodal largelanguage models on rebus puzzles. The dataset covers 333 original examples ofimage-based wordplay, cluing 13 categories such as movies, composers, majorcities, and food. To achieve good performance on the benchmark of identifyingthe clued word or phrase, models must combine image recognition and stringmanipulation with hypothesis testing, multi-step reasoning, and anunderstanding of human cognition, making for a complex, multimodal evaluationof capabilities. We find that GPT-4o significantly outperforms all othermodels, followed by proprietary models outperforming all other evaluatedmodels. However, even the best model has a final accuracy of only 42\%, whichgoes down to just 7\% on hard puzzles, highlighting the need for substantialimprovements in reasoning. Further, models rarely understand all parts of apuzzle, and are almost always incapable of retroactively explaining the correctanswer. Our benchmark can therefore be used to identify major shortcomings inthe knowledge and reasoning of multimodal large language models.",Andrew Gritsevskiy,2024/1/11,2024/6/3,,N/A,"['cs.CL', 'cs.AI', 'cs.CV', 'cs.CY']"
2401.00290v1,Red Teaming for Large Language Models At Scale: Tackling Hallucinations on Mathematics Tasks,http://arxiv.org/abs/2401.00290v1,"We consider the problem of red teaming LLMs on elementary calculations andalgebraic tasks to evaluate how various prompting techniques affect the qualityof outputs. We present a framework to procedurally generate numerical questionsand puzzles, and compare the results with and without the application ofseveral red teaming techniques. Our findings suggest that even thoughstructured reasoning and providing worked-out examples slow down thedeterioration of the quality of answers, the gpt-3.5-turbo and gpt-4 models arenot well suited for elementary calculations and reasoning tasks, also whenbeing red teamed.",Aleksander Buszydlik,2023/12/30,2023/12/30,,N/A,"['cs.CL', 'cs.AI', 'I.2.7']"
2312.01339v4,ArabIcros: AI-Powered Arabic Crossword Puzzle Generation for Educational Applications,http://arxiv.org/abs/2312.01339v4,"This paper presents the first Arabic crossword puzzle generator driven byadvanced AI technology. Leveraging cutting-edge large language models includingGPT4, GPT3-Davinci, GPT3-Curie, GPT3-Babbage, GPT3-Ada, and BERT, the systemgenerates distinctive and challenging clues. Based on a dataset comprising over50,000 clue-answer pairs, the generator employs fine-tuning, few/zero-shotlearning strategies, and rigorous quality-checking protocols to enforce thegeneration of high-quality clue-answer pairs. Importantly, educationalcrosswords contribute to enhancing memory, expanding vocabulary, and promotingproblem-solving skills, thereby augmenting the learning experience through afun and engaging approach, reshaping the landscape of traditional learningmethods. The overall system can be exploited as a powerful educational toolthat amalgamates AI and innovative learning techniques, heralding atransformative era for Arabic crossword puzzles and the intersection oftechnology and education.",Kamyar Zeinalipour,2023/12/3,2024/1/26,,N/A,"['cs.CL', 'cs.AI']"
2311.09105v2,MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation,http://arxiv.org/abs/2311.09105v2,"Understanding events in texts is a core objective of natural languageunderstanding, which requires detecting event occurrences, extracting eventarguments, and analyzing inter-event relationships. However, due to theannotation challenges brought by task complexity, a large-scale datasetcovering the full process of event understanding has long been absent. In thispaper, we introduce MAVEN-Arg, which augments MAVEN datasets with eventargument annotations, making the first all-in-one dataset supporting eventdetection, event argument extraction (EAE), and event relation extraction. Asan EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensiveschema covering 162 event types and 612 argument roles, all with expert-writtendefinitions and examples; (2) a large data scale, containing 98,591 events and290,613 arguments obtained with laborious human annotation; (3) the exhaustiveannotation supporting all task variants of EAE, which annotates both entity andnon-entity event arguments in document level. Experiments indicate thatMAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietarylarge language models (LLMs). Furthermore, to demonstrate the benefits of anall-in-one dataset, we preliminarily explore a potential application, futureevent prediction, with LLMs. MAVEN-Arg and codes can be obtained fromhttps://github.com/THU-KEG/MAVEN-Argument.",Xiaozhi Wang,2023/11/15,2024/6/18,,N/A,['cs.CL']
2311.08374v2,A Ship of Theseus: Curious Cases of Paraphrasing in LLM-Generated Texts,http://arxiv.org/abs/2311.08374v2,"In the realm of text manipulation and linguistic transformation, the questionof authorship has been a subject of fascination and philosophical inquiry. Muchlike the Ship of Theseus paradox, which ponders whether a ship remains the samewhen each of its original planks is replaced, our research delves into anintriguing question: Does a text retain its original authorship when itundergoes numerous paraphrasing iterations? Specifically, since Large LanguageModels (LLMs) have demonstrated remarkable proficiency in both the generationof original content and the modification of human-authored texts, a pivotalquestion emerges concerning the determination of authorship in instances whereLLMs or similar paraphrasing tools are employed to rephrase the text--i.e.,whether authorship should be attributed to the original human author or theAI-powered tool. Therefore, we embark on a philosophical voyage through theseas of language and authorship to unravel this intricate puzzle. Using acomputational approach, we discover that the diminishing performance in textclassification models, with each successive paraphrasing iteration, is closelyassociated with the extent of deviation from the original author's style, thusprovoking a reconsideration of the current notion of authorship.",Nafis Irtiza Tripto,2023/11/14,2024/6/6,,N/A,['cs.CL']
2311.07387v2,Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study,http://arxiv.org/abs/2311.07387v2,"Large Language Models (LLMs) have shown remarkable proficiency in languageunderstanding and have been successfully applied to a variety of real-worldtasks through task-specific fine-tuning or prompt engineering. Despite theseadvancements, it remains an open question whether LLMs are fundamentallycapable of reasoning and planning, or if they primarily rely on recalling andsynthesizing information from their training data. In our research, weintroduce a novel task -- Minesweeper -- specifically designed in a formatunfamiliar to LLMs and absent from their training datasets. This taskchallenges LLMs to identify the locations of mines based on numerical cluesprovided by adjacent opened cells. Successfully completing this task requiresan understanding of each cell's state, discerning spatial relationships betweenthe clues and mines, and strategizing actions based on logical deductions drawnfrom the arrangement of the cells. Our experiments, including trials with theadvanced GPT-4 model, indicate that while LLMs possess the foundationalabilities required for this task, they struggle to integrate these into acoherent, multi-step logical reasoning process needed to solve Minesweeper.These findings highlight the need for further research to understand the natureof reasoning capabilities in LLMs under similar circumstances, and to explorepathways towards more sophisticated AI reasoning and planning models.",Yinghao Li,2023/11/13,2024/4/16,,N/A,['cs.CL']
2310.10692v4,ACES: Generating Diverse Programming Puzzles with with Autotelic Generative Models,http://arxiv.org/abs/2310.10692v4,"The ability to invent novel and interesting problems is a remarkable featureof human intelligence that drives innovation, art, and science. We propose amethod that aims to automate this process by harnessing the power ofstate-of-the-art generative models to produce a diversity of challenging yetsolvable problems, here in the context of Python programming puzzles. Inspiredby the intrinsically motivated literature, Autotelic CodE Search (ACES) jointlyoptimizes for the diversity and difficulty of generated problems. We representproblems in a space of LLM-generated semantic descriptors describing theprogramming skills required to solve them (e.g. string manipulation, dynamicprogramming, etc.) and measure their difficulty empirically as a linearlydecreasing function of the success rate of Llama-3-70B, a state-of-the-art LLMproblem solver. ACES iteratively prompts a large language model to generatedifficult problems achieving a diversity of target semantic descriptors(goal-directed exploration) using previously generated problems as in-contextexamples. ACES generates problems that are more diverse and more challengingthan problems produced by baseline methods and three times more challengingthan problems found in existing Python programming benchmarks on average across11 state-of-the-art code LLMs.",Julien Pourcel,2023/10/15,2024/5/29,,N/A,"['cs.LG', 'cs.AI']"
2310.10686v1,Autonomous Tree-search Ability of Large Language Models,http://arxiv.org/abs/2310.10686v1,"Large Language Models have excelled in remarkable reasoning capabilities withadvanced prompting techniques, but they fall short on tasks that requireexploration, strategic foresight, and sequential decision-making. Recent workspropose to utilize external programs to define search logic, such that LLMs canperform passive tree search to solve more challenging reasoning tasks. Thoughimpressive results have been achieved, there are several fundamentallimitations of these approaches. First, passive tree searches are not efficientas they usually require multiple rounds of LLM API calls to solve one singleproblem. Moreover, passive search methods are not flexible since they needtask-specific program designs. Then a natural question arises: can we maintainthe tree-search capability of LLMs without the aid of external programs, andcan still generate responses that clearly demonstrate the process of atree-structure search? To this end, we propose a new concept called autonomoustree-search ability of LLM, which can automatically generate a responsecontaining search trajectories for the correct answer. Concretely, we performsearch trajectories using capable LLM API via a fixed system prompt, allowingthem to perform autonomous tree-search (ATS) right out of the box. Experimentson 4 puzzle games demonstrate our method can achieve huge improvements. TheATS-BFS method outperforms the Chain of Thought approach by achieving anaverage accuracy improvement of 33%. Compared to Tree of Thoughts, it requires65.6% or 47.7% less GPT-api cost to attain a comparable level of accuracy.Moreover, we have collected data using the ATS prompt method and fine-tunedLLaMA. This approach yield a greater improvement compared to the onesfine-tuned on CoT data. Specifically, it outperforms CoT-tuned LLaMAs by anaverage of 40.6% and 38.5% for LLaMA2-7B and LLaMA2-13B, respectively.",Zheyu Zhang,2023/10/14,2023/10/14,,N/A,"['cs.CL', 'cs.AI']"
2310.05993v1,Measuring reasoning capabilities of ChatGPT,http://arxiv.org/abs/2310.05993v1,"I shall quantify the logical faults generated by ChatGPT when applied toreasoning tasks. For experiments, I use the 144 puzzles from the library\url{https://users.utcluj.ro/~agroza/puzzles/maloga}~\cite{groza:fol}. Thelibrary contains puzzles of various types, including arithmetic puzzles,logical equations, Sudoku-like puzzles, zebra-like puzzles, truth-tellingpuzzles, grid puzzles, strange numbers, or self-reference puzzles. The correctsolutions for these puzzles were checked using the theorem proverProver9~\cite{mccune2005release} and the finite models finderMace4~\cite{mccune2003mace4} based on human-modelling in Equational First OrderLogic. A first output of this study is the benchmark of 100 logical puzzles.For this dataset ChatGPT provided both correct answer and justification for 7\%only. %, while BARD for 5\%. Since the dataset seems challenging, theresearchers are invited to test the dataset on more advanced or tuned modelsthan ChatGPT3.5 with more crafted prompts. A second output is theclassification of reasoning faults conveyed by ChatGPT. This classificationforms a basis for a taxonomy of reasoning faults generated by large languagemodels. I have identified 67 such logical faults, among which: inconsistencies,implication does not hold, unsupported claim, lack of commonsense, wrongjustification. The 100 solutions generated by ChatGPT contain 698 logicalfaults. That is on average, 7 fallacies for each reasoning task. A third ouputis the annotated answers of the ChatGPT with the corresponding logical faults.Each wrong statement within the ChatGPT answer was manually annotated, aimingto quantify the amount of faulty text generated by the language model. Onaverage, 26.03\% from the generated text was a logical fault.",Adrian Groza,2023/10/8,2023/10/8,,N/A,['cs.AI']
2310.05253v2,Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models,http://arxiv.org/abs/2310.05253v2,"Claim verification plays a crucial role in combating misinformation. Whileexisting works on claim verification have shown promising results, a crucialpiece of the puzzle that remains unsolved is to understand how to verify claimswithout relying on human-annotated data, which is expensive to create at alarge scale. Additionally, it is important for models to provide comprehensiveexplanations that can justify their decisions and assist human fact-checkers.This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK)Reasoning that can verify complex claims and generate explanations without theneed for annotated evidence using Large Language Models (LLMs). FOLK leveragesthe in-context learning ability of LLMs to translate the claim into aFirst-Order-Logic (FOL) clause consisting of predicates, each corresponding toa sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoningover a set of knowledge-grounded question-and-answer pairs to make veracitypredictions and generate explanations to justify its decision-making process.This process makes our model highly explanatory, providing clear explanationsof its reasoning process in human-readable form. Our experiment resultsindicate that FOLK outperforms strong baselines on three datasets encompassingvarious claim verification challenges. Our code and data are available.",Haoran Wang,2023/10/8,2023/10/20,,N/A,"['cs.CL', 'cs.AI', 'cs.LG']"
2310.05057v3,BRAINTEASER: Lateral Thinking Puzzles for Large Language Models,http://arxiv.org/abs/2310.05057v3,"The success of language models has inspired the NLP community to attend totasks that require implicit and complex reasoning, relying on human-likecommonsense mechanisms. While such vertical thinking tasks have been relativelypopular, lateral thinking puzzles have received little attention. To bridgethis gap, we devise BRAINTEASER: a multiple-choice Question Answering taskdesigned to test the model's ability to exhibit lateral thinking and defydefault commonsense associations. We design a three-step procedure for creatingthe first lateral thinking benchmark, consisting of data collection, distractorgeneration, and generation of adversarial examples, leading to 1,100 puzzleswith high-quality annotations. To assess the consistency of lateral reasoningby models, we enrich BRAINTEASER based on a semantic and contextualreconstruction of its questions. Our experiments with state-of-the-artinstruction- and commonsense language models reveal a significant gap betweenhuman and model performance, which is further widened when consistency acrossadversarial formats is considered. We make all of our code and data availableto stimulate work on developing and evaluating lateral thinking models.",Yifan Jiang,2023/10/8,2023/11/9,,N/A,['cs.CL']
2307.09042v2,Emotional Intelligence of Large Language Models,http://arxiv.org/abs/2307.09042v2,"Large Language Models (LLMs) have demonstrated remarkable abilities acrossnumerous disciplines, primarily assessed through tasks in language generation,knowledge utilization, and complex reasoning. However, their alignment withhuman emotions and values, which is critical for real-world applications, hasnot been systematically evaluated. Here, we assessed LLMs' EmotionalIntelligence (EI), encompassing emotion recognition, interpretation, andunderstanding, which is necessary for effective communication and socialinteractions. Specifically, we first developed a novel psychometric assessmentfocusing on Emotion Understanding (EU), a core component of EI, suitable forboth humans and LLMs. This test requires evaluating complex emotions (e.g.,surprised, joyful, puzzled, proud) in realistic scenarios (e.g., despitefeeling underperformed, John surprisingly achieved a top score). With areference frame constructed from over 500 adults, we tested a variety ofmainstream LLMs. Most achieved above-average EQ scores, with GPT-4 exceeding89% of human participants with an EQ of 117. Interestingly, a multivariatepattern analysis revealed that some LLMs apparently did not reply on thehuman-like mechanism to achieve human-level performance, as theirrepresentational patterns were qualitatively distinct from humans. In addition,we discussed the impact of factors such as model size, training method, andarchitecture on LLMs' EQ. In summary, our study presents one of the firstpsychometric evaluations of the human-like characteristics of LLMs, which mayshed light on the future development of LLMs aiming for both high intellectualand emotional intelligence. Project website:https://emotional-intelligence.github.io/",Xuena Wang,2023/7/18,2023/7/28,,N/A,['cs.AI']
2307.07699v1,Leveraging Large Language Models to Generate Answer Set Programs,http://arxiv.org/abs/2307.07699v1,"Large language models (LLMs), such as GPT-3 and GPT-4, have demonstratedexceptional performance in various natural language processing tasks and haveshown the ability to solve certain reasoning problems. However, their reasoningcapabilities are limited and relatively shallow, despite the application ofvarious prompting techniques. In contrast, formal logic is adept at handlingcomplex reasoning, but translating natural language descriptions into formallogic is a challenging task that non-experts struggle with. This paper proposesa neuro-symbolic method that combines the strengths of large language modelsand answer set programming. Specifically, we employ an LLM to transform naturallanguage descriptions of logic puzzles into answer set programs. We carefullydesign prompts for an LLM to convert natural language descriptions into answerset programs in a step by step manner. Surprisingly, with just a few in-contextlearning examples, LLMs can generate reasonably complex answer set programs.The majority of errors made are relatively simple and can be easily correctedby humans, thus enabling LLMs to effectively assist in the creation of answerset programs.",Adam Ishay,2023/7/15,2023/7/15,,N/A,"['cs.AI', 'cs.CL', 'cs.SC']"
2307.05300v4,Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration,http://arxiv.org/abs/2307.05300v4,"Human intelligence thrives on cognitive synergy, where collaboration amongdifferent minds yield superior outcomes compared to isolated individuals. Inthis work, we propose Solo Performance Prompting (SPP), which transforms asingle LLM into a cognitive synergist by engaging in multi-turnself-collaboration with multiple personas. A cognitive synergist is anintelligent agent that collaboratively combines multiple minds' strengths andknowledge to enhance problem-solving in complex tasks. By dynamicallyidentifying and simulating different personas based on task inputs, SPPunleashes the potential of cognitive synergy in LLMs. Our in-depth analysisshows that assigning multiple fine-grained personas in LLMs improvesproblem-solving abilities compared to using a single or fixed number ofpersonas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,Codenames Collaborative, and Logic Grid Puzzle, encompassing bothknowledge-intensive and reasoning-intensive types. Unlike previous works, suchas Chain-of-Thought, that solely enhance the reasoning abilities in LLMs,experimental results demonstrate that SPP effectively reduces factualhallucination, and maintains strong reasoning capabilities. Additionally,comparative experiments show that cognitive synergy only emerges in GPT-4 anddoes not appear in less capable models, such as GPT-3.5-turbo andLlama2-13b-chat, which draws an interesting analogy to human development. Code,data, and prompts can be found at:https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.",Zhenhailong Wang,2023/7/11,2024/3/26,,N/A,"['cs.AI', 'cs.CL']"
2307.04964v2,Secrets of RLHF in Large Language Models Part I: PPO,http://arxiv.org/abs/2307.04964v2,"Large language models (LLMs) have formulated a blueprint for the advancementof artificial general intelligence. Its primary objective is to function as ahuman-centric (helpful, honest, and harmless) assistant. Alignment with humansassumes paramount significance, and reinforcement learning with human feedback(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.Current technical routes usually include \textbf{reward models} to measurehuman preferences, \textbf{Proximal Policy Optimization} (PPO) to optimizepolicy model outputs, and \textbf{process supervision} to improve step-by-stepreasoning capabilities. However, due to the challenges of reward design,environment interaction, and agent training, coupled with huge trial and errorcost of large language models, there is a significant barrier for AIresearchers to motivate the development of technical alignment and safe landingof LLMs. The stable training of RLHF has still been a puzzle. In the firstreport, we dissect the framework of RLHF, re-evaluate the inner workings ofPPO, and explore how the parts comprising PPO algorithms impact policy agenttraining. We identify policy constraints being the key factor for the effectiveimplementation of the PPO algorithm. Therefore, we explore the PPO-max, anadvanced version of PPO algorithm, to efficiently improve the trainingstability of the policy model. Based on our main results, we perform acomprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.The absence of open-source implementations has posed significant challenges tothe investigation of LLMs alignment. Therefore, we are eager to releasetechnical reports, reward models and PPO codes, aiming to make modestcontributions to the advancement of LLMs.",Rui Zheng,2023/7/11,2023/7/18,,N/A,"['cs.CL', 'cs.AI', 'cs.LG']"
2307.05518v1,Procedurally generating rules to adapt difficulty for narrative puzzle games,http://arxiv.org/abs/2307.05518v1,"This paper focuses on procedurally generating rules and communicating them toplayers to adjust the difficulty. This is part of a larger project to collectand adapt games in educational games for young children using a digital puzzlegame designed for kindergarten. A genetic algorithm is used together with adifficulty measure to find a target number of solution sets and a largelanguage model is used to communicate the rules in a narrative context. Duringtesting the approach was able to find rules that approximate any given targetdifficulty within two dozen generations on average. The approach was combinedwith a large language model to create a narrative puzzle game where playershave to host a dinner for animals that can't get along. Future experiments willtry to improve evaluation, specialize the language model on children'sliterature, and collect multi-modal data from players to guide adaptation.",Thomas Volden,2023/7/7,2023/7/7,,N/A,"['cs.HC', 'cs.AI']"
2306.12255v1,Solving and Generating NPR Sunday Puzzles with Large Language Models,http://arxiv.org/abs/2306.12255v1,"We explore the ability of large language models to solve and generate puzzlesfrom the NPR Sunday Puzzle game show using PUZZLEQA, a dataset comprising 15years of on-air puzzles. We evaluate four large language models using PUZZLEQA,in both multiple choice and free response formats, and explore two promptengineering techniques to improve free response performance: chain-of-thoughtreasoning and prompt summarization. We find that state-of-the-art largelanguage models can solve many PUZZLEQA puzzles: the best model, GPT-3.5,achieves 50.2% loose accuracy. However, in our few-shot puzzle generationexperiment, we find no evidence that models can generate puzzles: GPT-3.5generates puzzles with answers that do not conform to the generated rules.Puzzle generation remains a challenging task for future work.",Jingmiao Zhao,2023/6/21,2023/6/21,,N/A,['cs.CL']
2305.18618v1,"Chatbots put to the test in math and logic problems: A preliminary comparison and assessment of ChatGPT-3.5, ChatGPT-4, and Google Bard",http://arxiv.org/abs/2305.18618v1,"A comparison between three chatbots which are based on large language models,namely ChatGPT-3.5, ChatGPT-4 and Google Bard is presented, focusing on theirability to give correct answers to mathematics and logic problems. Inparticular, we check their ability to Understand the problem at hand; Applyappropriate algorithms or methods for its solution; and Generate a coherentresponse and a correct answer. We use 30 questions that are clear, without anyambiguities, fully described with plain text only, and have a unique, welldefined correct answer. The questions are divided into two sets of 15 each. Thequestions of Set A are 15 ""Original"" problems that cannot be found online,while Set B contains 15 ""Published"" problems that one can find online, usuallywith their solution. Each question is posed three times to each chatbot. Theanswers are recorded and discussed, highlighting their strengths andweaknesses. It has been found that for straightforward arithmetic, algebraicexpressions, or basic logic puzzles, chatbots may provide accurate solutions,although not in every attempt. However, for more complex mathematical problemsor advanced logic tasks, their answers, although written in a usually""convincing"" way, may not be reliable. Consistency is also an issue, as manytimes a chatbot will provide conflicting answers when given the same questionmore than once. A comparative quantitative evaluation of the three chatbots ismade through scoring their final answers based on correctness. It was foundthat ChatGPT-4 outperforms ChatGPT-3.5 in both sets of questions. Bard comesthird in the original questions of Set A, behind the other two chatbots, whileit has the best performance (first place) in the published questions of Set B.This is probably because Bard has direct access to the internet, in contrast toChatGPT chatbots which do not have any communication with the outside world.",Vagelis Plevris,2023/5/30,2023/5/30,,N/A,"['cs.CL', 'cs.AI', '68T50', 'I.2.0; I.2.7']"
2305.18654v3,Faith and Fate: Limits of Transformers on Compositionality,http://arxiv.org/abs/2305.18654v3,"Transformer large language models (LLMs) have sparked admiration for theirexceptional performance on tasks that demand intricate multi-step reasoning.Yet, these models simultaneously show failures on surprisingly trivialproblems. This begs the question: Are these errors incidental, or do theysignal more substantial limitations? In an attempt to demystify transformerLLMs, we investigate the limits of these models across three representativecompositional tasks -- multi-digit multiplication, logic grid puzzles, and aclassic dynamic programming problem. These tasks require breaking problems downinto sub-steps and synthesizing these steps into a precise answer. We formulatecompositional tasks as computation graphs to systematically quantify the levelof complexity, and break down reasoning steps into intermediate sub-procedures.Our empirical findings suggest that transformer LLMs solve compositional tasksby reducing multi-step compositional reasoning into linearized subgraphmatching, without necessarily developing systematic problem-solving skills. Toround off our empirical study, we provide theoretical arguments on abstractmulti-step reasoning problems that highlight how autoregressive generations'performance can rapidly decay with\,increased\,task\,complexity.",Nouha Dziri,2023/5/29,2023/10/31,,N/A,"['cs.CL', 'cs.AI', 'cs.LG']"
2305.08291v1,Large Language Model Guided Tree-of-Thought,http://arxiv.org/abs/2305.08291v1,"In this paper, we introduce the Tree-of-Thought (ToT) framework, a novelapproach aimed at improving the problem-solving capabilities of auto-regressivelarge language models (LLMs). The ToT technique is inspired by the human mind'sapproach for solving complex reasoning tasks through trial and error. In thisprocess, the human mind explores the solution space through a tree-like thoughtprocess, allowing for backtracking when necessary. To implement ToT as asoftware system, we augment an LLM with additional modules including a prompteragent, a checker module, a memory module, and a ToT controller. In order tosolve a given problem, these modules engage in a multi-round conversation withthe LLM. The memory module records the conversation and state history of theproblem solving process, which allows the system to backtrack to the previoussteps of the thought-process and explore other directions from there. To verifythe effectiveness of the proposed technique, we implemented a ToT-based solverfor the Sudoku Puzzle. Experimental results show that the ToT framework cansignificantly increase the success rate of Sudoku puzzle solving. Ourimplementation of the ToT-based Sudoku solver is available on GitHub:\url{https://github.com/jieyilong/tree-of-thought-puzzle-solver}.",Jieyi Long,2023/5/15,2023/5/15,,N/A,"['cs.AI', 'cs.CL', 'cs.CV', 'cs.LG', 'cs.NE']"
2304.01771v1,Using Language Models For Knowledge Acquisition in Natural Language Reasoning Problems,http://arxiv.org/abs/2304.01771v1,"For a natural language problem that requires some non-trivial reasoning tosolve, there are at least two ways to do it using a large language model (LLM).One is to ask it to solve it directly. The other is to use it to extract thefacts from the problem text and then use a theorem prover to solve it. In thisnote, we compare the two methods using ChatGPT and GPT4 on a series of logicword puzzles, and conclude that the latter is the right approach.",Fangzhen Lin,2023/4/4,2023/4/4,,N/A,['cs.AI']
2302.01973v3,Measuring The Impact Of Programming Language Distribution,http://arxiv.org/abs/2302.01973v3,"Current benchmarks for evaluating neural code models focus on only a smallsubset of programming languages, excluding many popular languages such as Go orRust. To ameliorate this issue, we present the BabelCode framework forexecution-based evaluation of any benchmark in any language. BabelCode enablesnew investigations into the qualitative performance of models' memory, runtime,and individual test case results. Additionally, we present a new codetranslation dataset called Translating Python Programming Puzzles (TP3) fromthe Python Programming Puzzles (Schuster et al. 2021) benchmark that involvestranslating expert-level python functions to any language. With both BabelCodeand the TP3 benchmark, we investigate if balancing the distributions of 14languages in a training dataset improves a large language model's performanceon low-resource languages. Training a model on a balanced corpus results in, onaverage, 12.34% higher $pass@k$ across all tasks and languages compared to thebaseline. We find that this strategy achieves 66.48% better $pass@k$ onlow-resource languages at the cost of only a 12.94% decrease to high-resourcelanguages. In our three translation tasks, this strategy yields, on average,30.77% better low-resource $pass@k$ while having 19.58% worse high-resource$pass@k$.",Gabriel Orlanski,2023/2/3,2023/5/24,,N/A,"['cs.LG', 'cs.CL', 'cs.PL']"
2212.10114v2,True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4,http://arxiv.org/abs/2212.10114v2,"Large language models (LLMs) have demonstrated solid zero-shot reasoningcapabilities, which is reflected in their performance on the current testtasks. This calls for a more challenging benchmark requiring highly advancedreasoning ability to be solved. In this paper, we introduce such a benchmark,consisting of 191 long-form (1200 words on average) mystery narrativesconstructed as detective puzzles. Puzzles are sourced from the ""5 MinuteMystery"" platform and include a multiple-choice question for evaluation. Only47% of humans solve a puzzle successfully on average, while the best humansolvers achieve over 80% success rate. We show that GPT-3 models barelyoutperform random on this benchmark (with 28% accuracy) while state-of-the-artGPT-4 solves only 38% of puzzles. This indicates that there is still asignificant gap in the deep reasoning abilities of LLMs and humans andhighlights the need for further research in this area. Our work introduces achallenging benchmark for future studies on reasoning in language models andcontributes to a better understanding of the limits of LLMs' abilities.",Maksym Del,2022/12/20,2023/6/1,,N/A,['cs.CL']
2212.09993v6,Are Deep Neural Networks SMARTer than Second Graders?,http://arxiv.org/abs/2212.09993v6,"Recent times have witnessed an increasing number of applications of deepneural networks towards solving tasks that require superior cognitiveabilities, e.g., playing Go, generating art, ChatGPT, etc. Such a dramaticprogress raises the question: how generalizable are neural networks in solvingproblems that demand broad skills? To answer this question, we propose SMART: aSimple Multimodal Algorithmic Reasoning Task and the associated SMART-101dataset, for evaluating the abstraction, deduction, and generalizationabilities of neural networks in solving visuo-linguistic puzzles designedspecifically for children in the 6--8 age group. Our dataset consists of 101unique puzzles; each puzzle comprises a picture and a question, and theirsolution needs a mix of several elementary skills, including arithmetic,algebra, and spatial reasoning, among others. To scale our dataset towardstraining deep neural networks, we programmatically generate entirely newinstances for each puzzle, while retaining their solution algorithm. Tobenchmark performances on SMART-101, we propose a vision and languagemeta-learning model using varied state-of-the-art backbones. Our experimentsreveal that while powerful deep models offer reasonable performances on puzzlesin a supervised setting, they are not better than random accuracy when analyzedfor generalization. We also evaluate the recent ChatGPT and other largelanguage models on a subset of SMART-101 and find that while these models showconvincing reasoning abilities, the answers are often incorrect.",Anoop Cherian,2022/12/20,2023/9/11,,N/A,"['cs.AI', 'cs.CV', 'cs.LG']"
2212.08681v1,Plansformer: Generating Symbolic Plans using Transformers,http://arxiv.org/abs/2212.08681v1,"Large Language Models (LLMs) have been the subject of active research,significantly advancing the field of Natural Language Processing (NLP). FromBERT to BLOOM, LLMs have surpassed state-of-the-art results in various naturallanguage tasks such as question answering, summarization, and text generation.Many ongoing efforts focus on understanding LLMs' capabilities, including theirknowledge of the world, syntax, and semantics. However, extending the textualprowess of LLMs to symbolic reasoning has been slow and predominantly focusedon tackling problems related to the mathematical field. In this paper, weexplore the use of LLMs for automated planning - a branch of AI concerned withthe realization of action sequences (plans) to achieve a goal, typicallyexecuted by intelligent agents, autonomous robots, and unmanned vehicles. Weintroduce Plansformer; an LLM fine-tuned on planning problems and capable ofgenerating plans with favorable behavior in terms of correctness and lengthwith reduced knowledge-engineering efforts. We also demonstrate theadaptability of Plansformer in solving different planning domains with varyingcomplexities, owing to the transfer learning abilities of LLMs. For oneconfiguration of Plansformer, we achieve ~97% valid plans, out of which ~95%are optimal for Towers of Hanoi - a puzzle-solving domain.",Vishal Pallagani,2022/12/16,2022/12/16,,N/A,['cs.AI']
2205.10770v2,Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models,http://arxiv.org/abs/2205.10770v2,"Despite their wide adoption, the underlying training and memorizationdynamics of very large language models is not well understood. We empiricallystudy exact memorization in causal and masked language modeling, across modelsizes and throughout the training process. We measure the effects of datasetsize, learning rate, and model size on memorization, finding that largerlanguage models memorize training data faster across all settings.Surprisingly, we show that larger models can memorize a larger portion of thedata before over-fitting and tend to forget less throughout the trainingprocess. We also analyze the memorization dynamics of different parts of speechand find that models memorize nouns and numbers first; we hypothesize andprovide empirical evidence that nouns and numbers act as a unique identifierfor memorizing individual training examples. Together, these findings presentanother piece of the broader puzzle of trying to understand what actuallyimproves as models get bigger.",Kushal Tirumala,2022/5/22,2022/11/2,,N/A,['cs.CL']
