import pandas as pd
from datetime import datetime
import argparse
import os
from collections import defaultdict
import textwrap

# é…ç½®å‚æ•°
DAILY_REPORT_DIR = "/Users/djh/Documents/GitHub/LLM-DailyDigest/updates"
TOP_N_TRENDING = 5
THEME_KEYWORDS = {
    "å¤§æ¨¡å‹": [
        "è‡ªå›å½’", "Transformer", "æ·±åº¦ç¥ç»ç½‘ç»œ", "æ¨¡å‹å‹ç¼©", "é¢„è®­ç»ƒ", "å¾®è°ƒ", 
        "å¤§è§„æ¨¡è®­ç»ƒ", "å¤šæ¨¡æ€", "ç”Ÿæˆå¼æ¨¡å‹", "æ¨¡å‹é›†æˆ", "è‡ªç›‘ç£å­¦ä¹ "
    ],
    "æ¨ç†": [
        "æ¨ç†", "o1",
        "æ¨ç†é€Ÿåº¦", "æ¨ç†ä¼˜åŒ–", "æ¨¡å‹åŠ é€Ÿ", "é‡åŒ–", "ä½ç²¾åº¦æ¨ç†", "GPUåŠ é€Ÿ", 
        "æ¨ç†å»¶è¿Ÿ", "è¾¹ç¼˜è®¡ç®—", "åˆ†å¸ƒå¼æ¨ç†", "å®æ—¶æ¨ç†", "æ¨ç†æ¡†æ¶", "å¢é‡æ¨ç†"
    ],
    "æµ‹è¯„": [
        "å‡†ç¡®ç‡", "å¬å›ç‡", "F1åˆ†æ•°", "AUC", "æ··æ·†çŸ©é˜µ", "æ€§èƒ½è¯„ä¼°", "æ€§èƒ½ç“¶é¢ˆ", 
        "æ¨¡å‹é²æ£’æ€§", "è®¡ç®—å¤æ‚åº¦", "å¯¹æ¯”å®éªŒ", "æ³›åŒ–èƒ½åŠ›", "åœ¨çº¿è¯„æµ‹", "æ¨¡å‹éªŒè¯"
    ],
    "æ¨¡å‹éƒ¨ç½²ä¸åº”ç”¨": [
        "æ¨¡å‹éƒ¨ç½²", "å®¹å™¨åŒ–", "äº‘ç«¯éƒ¨ç½²", "è¾¹ç¼˜è®¡ç®—", "å®æ—¶æ¨ç†", "APIé›†æˆ", 
        "å¾®æœåŠ¡", "è‡ªåŠ¨åŒ–éƒ¨ç½²", "æŒç»­é›†æˆ", "è·¨å¹³å°éƒ¨ç½²"
    ],
    "ç®—æ³•ä¸ä¼˜åŒ–": [
        "ä¼˜åŒ–ç®—æ³•", "æ¢¯åº¦ä¸‹é™", "é—ä¼ ç®—æ³•", "è´å¶æ–¯ä¼˜åŒ–", "è¶…å‚æ•°è°ƒä¼˜", "å¼ºåŒ–å­¦ä¹ ", 
        "è¿ç§»å­¦ä¹ ", "å…ƒå­¦ä¹ ", "å¢é‡å­¦ä¹ ", "è‡ªé€‚åº”ä¼˜åŒ–", "ç›®æ ‡æ£€æµ‹", "å›¾åƒåˆ†å‰²"
    ],
    "æ•°æ®ä¸éšç§": [
        "æ•°æ®å¢å¼º", "æ•°æ®éšç§", "è”é‚¦å­¦ä¹ ", "å·®åˆ†éšç§", "æ•°æ®å®‰å…¨", 
        "æ•°æ®æ ‡æ³¨", "å¤šæ¨¡æ€æ•°æ®", "æ•°æ®æµå¤„ç†", "æ•°æ®æ¸…æ´—"
    ],
    "äººå·¥æ™ºèƒ½ä¼¦ç†ä¸ç¤¾ä¼š": [
        "ä¼¦ç†", "å¯è§£é‡Šæ€§", "å…¬å¹³æ€§", "ç®—æ³•åè§", "è‡ªåŠ¨åŒ–å†³ç­–", "éšç§ä¿æŠ¤", 
        "ç¤¾ä¼šå½±å“", "AIæ²»ç†", "ç®—æ³•é€æ˜åº¦"
    ],
    "è‡ªç„¶è¯­è¨€å¤„ç†": [
        "æ–‡æœ¬ç”Ÿæˆ", "è¯­éŸ³è¯†åˆ«", "æœºå™¨ç¿»è¯‘", "å‘½åå®ä½“è¯†åˆ«", "æƒ…æ„Ÿåˆ†æ", "è¯å‘é‡", 
        "å¥æ³•åˆ†æ", "è‡ªç„¶è¯­è¨€ç†è§£", "å¯¹è¯ç³»ç»Ÿ", "è¯­éŸ³åˆæˆ", "è¯­è¨€æ¨¡å‹", "æ–‡æœ¬åˆ†ç±»"
    ],
    "è®¡ç®—æœºè§†è§‰": [
        "å›¾åƒåˆ†ç±»", "ç›®æ ‡æ£€æµ‹", "å›¾åƒåˆ†å‰²", "äººè„¸è¯†åˆ«", "å§¿æ€ä¼°è®¡", "è§†è§‰è·Ÿè¸ª", 
        "å›¾åƒç”Ÿæˆ", "æ·±åº¦å›¾åƒç”Ÿæˆ", "å›¾åƒæ£€ç´¢", "è§†é¢‘åˆ†æ", "3Dé‡å»º", "è§†è§‰æ„ŸçŸ¥"
    ]
}

def load_data(file_path):
    """
    åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®

    å‚æ•°:
    file_path (str): æ•°æ®æ–‡ä»¶çš„è·¯å¾„

    è¿”å›:
    DataFrame: é¢„å¤„ç†åçš„æ•°æ®æ¡†
    """
    # ä»CSVæ–‡ä»¶ä¸­è¯»å–æ•°æ®ï¼ŒæŒ‡å®šåˆ†éš”ç¬¦ä¸ºåˆ¶è¡¨ç¬¦ï¼Œå¹¶å°†æ—¥æœŸåˆ—è§£æä¸ºæ—¥æœŸç±»å‹
    df = pd.read_csv(file_path, sep=',', parse_dates=['Publish Date', 'Update Date'])
    
    # å°†Starsåˆ—è½¬æ¢ä¸ºæ•°å€¼ç±»å‹ï¼Œé”™è¯¯å€¼å¡«å……ä¸º0
    df['Stars'] = pd.to_numeric(df['Stars'], errors='coerce').fillna(0)
    
    # è¿”å›é¢„å¤„ç†åçš„æ•°æ®æ¡†
    return df

def classify_theme(summary):
    """é€šè¿‡å…³é”®è¯åŒ¹é…è¿›è¡Œä¸»é¢˜åˆ†ç±»"""
    themes = []
    for theme, keywords in THEME_KEYWORDS.items():
        if any(kw in summary for kw in keywords):
            themes.append(theme)
    return themes if themes else ["å…¶ä»–"]

def generate_daily_report(target_date, df):
    """ç”Ÿæˆæ—¥æŠ¥æ ¸å¿ƒå†…å®¹"""
    # å°†ç›®æ ‡æ—¥æœŸæ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
    date_str = target_date.strftime("%Y-%m-%d")
    
    # ç­›é€‰å½“æ—¥å‘å¸ƒçš„æ–°è®ºæ–‡
    new_papers = df[df['Publish Date'].dt.date == target_date.date()]
    # ç­›é€‰å½“æ—¥æ›´æ–°çš„è®ºæ–‡
    updated_papers = df[
        (df['Update Date'].dt.date == target_date.date()) & 
        (df['Publish Date'].dt.date != target_date.date())
    ]
    
    # ç”Ÿæˆè¶‹åŠ¿åˆ†æï¼ŒæŒ‰æ˜Ÿæ˜Ÿæ•°å’Œæ›´æ–°æ—¥æœŸæ’åºï¼Œå–å‰TOP_N_TRENDINGç¯‡
    trending = df.sort_values(by=['Stars', 'Update Date'], ascending=False).head(TOP_N_TRENDING)
    
    # ä¸»é¢˜åˆ†ç±»ç»Ÿè®¡
    theme_dist = defaultdict(list)
    for _, row in df.iterrows():
        # å¯¹æ¯ç¯‡è®ºæ–‡çš„æ‘˜è¦è¿›è¡Œä¸»é¢˜åˆ†ç±»
        themes = classify_theme(row['Summary'])
        for theme in themes:
            # å°†è®ºæ–‡æ·»åŠ åˆ°å¯¹åº”çš„ä¸»é¢˜åˆ—è¡¨ä¸­
            theme_dist[theme].append(row)
    
    # æ„å»ºMarkdownå†…å®¹
    content = []
    # æ·»åŠ æ—¥æŠ¥æ ‡é¢˜
    content.append(f"# å­¦æœ¯æ—¥æŠ¥ {date_str}\n")
    
    # å½“æ—¥æ¦‚è§ˆ
    content.append("## ğŸ“Š å½“æ—¥æ¦‚è§ˆ")
    # æ·»åŠ æ–°å¢è®ºæ–‡æ•°é‡
    content.append(f"- æ–°å¢è®ºæ–‡: {len(new_papers)} ç¯‡")
    # æ·»åŠ æ›´æ–°è®ºæ–‡æ•°é‡
    content.append(f"- æ›´æ–°è®ºæ–‡: {len(updated_papers)} ç¯‡")
    # æ·»åŠ æœ€çƒ­é—¨è®ºæ–‡æ ‡é¢˜å’Œæ˜Ÿæ˜Ÿæ•°
    content.append(f"- æœ€çƒ­é—¨è®ºæ–‡: {trending.iloc[0]['Title'][:30]}... (â­{trending.iloc[0]['Stars']})\n")
    
    # æ–°å¢è®ºæ–‡
    if not new_papers.empty:
        # æ·»åŠ æ–°å¢è®ºæ–‡æ ‡é¢˜
        content.append("## ğŸ†• æ–°å¢è®ºæ–‡")
        for _, paper in new_papers.iterrows():
            # æ ¼å¼åŒ–æ–°å¢è®ºæ–‡ä¿¡æ¯
            content.append(format_paper(paper, "æ–°è®ºæ–‡"))
    
    # æ›´æ–°è®ºæ–‡
    if not updated_papers.empty:
        # æ·»åŠ æ›´æ–°è®ºæ–‡æ ‡é¢˜
        content.append("## ğŸ”„ æ›´æ–°è®ºæ–‡")
        for _, paper in updated_papers.iterrows():
            # æ ¼å¼åŒ–æ›´æ–°è®ºæ–‡ä¿¡æ¯
            content.append(format_paper(paper, "æ›´æ–°"))
    
    # è¶‹åŠ¿è®ºæ–‡
    content.append("## ğŸ“ˆ è¶‹åŠ¿è®ºæ–‡")
    for _, paper in trending.iterrows():
        # æ ¼å¼åŒ–è¶‹åŠ¿è®ºæ–‡ä¿¡æ¯
        content.append(format_paper(paper, "çƒ­é—¨"))
    
    # ä¸»é¢˜åˆ†å¸ƒ
    content.append("## ğŸ§© ä¸»é¢˜åˆ†å¸ƒ")
    for theme, papers in sorted(theme_dist.items(), key=lambda x: len(x[1]), reverse=True):
        # æ·»åŠ ä¸»é¢˜æ ‡é¢˜å’Œè®ºæ–‡æ•°é‡
        content.append(f"### {theme} ({len(papers)}ç¯‡)")
        # æ·»åŠ ä»£è¡¨æ€§è®ºæ–‡æ ‡é¢˜
        content.append(f"**ä»£è¡¨æ€§è®ºæ–‡**: {papers[0]['Title'][:50]}...")
        # æ·»åŠ æœ€æ–°è¿›å±•
        content.append("**æœ€æ–°è¿›å±•**:")
        # æ·»åŠ æ‘˜è¦çš„ç¬¬ä¸€è¡Œ
        content.append(textwrap.wrap(papers[0]['Summary'], width=200)[0] + "...\n")
        # å…¨éƒ¨è®ºæ–‡æ ‡é¢˜
        content.append("**å…¨éƒ¨è®ºæ–‡**:")
        for paper in papers:
            # æ ¼å¼åŒ–ä¸»é¢˜è®ºæ–‡ä¿¡æ¯
            content.append(f"- {paper['Title']} ({paper['First Author']}) [è·³è½¬]({paper['URL']})")
    
    # å°†å†…å®¹åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶è¿”å›
    return "\n".join(content)

def format_paper(paper, badge):
    """æ ¼å¼åŒ–å•ç¯‡è®ºæ–‡ä¿¡æ¯"""
    code_link = f"[ä»£ç ]({paper['Code URL']})" if pd.notna(paper['Code URL']) else "æ— ä»£ç "
    return f"""
### {paper['Title']}
**{badge}** â­{paper['Stars']} | {paper['Publish Date'].date()} | {code_link}  
**ä½œè€…**: {paper['First Author']}  
**æ‘˜è¦**: {textwrap.shorten(paper['Summary'], width=200, placeholder='...')}  
[é˜…è¯»å…¨æ–‡]({paper['URL']})
"""

def main():
    parser = argparse.ArgumentParser(description="ç”Ÿæˆå­¦æœ¯æ—¥æŠ¥")
    parser.add_argument("data_file", help="è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--date", help="æŒ‡å®šæ—¥æœŸ (YYYY-MM-DD)", default=datetime.today().date())
    args = parser.parse_args()

    df = load_data(args.data_file)
    report_content = generate_daily_report(pd.to_datetime(args.date), df)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(DAILY_REPORT_DIR, exist_ok=True)
    
    # ä¿å­˜æ–‡ä»¶
    filename = f"daily_report_{args.date}.md"
    with open(os.path.join(DAILY_REPORT_DIR, filename), 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"æ—¥æŠ¥å·²ç”Ÿæˆï¼š{os.path.join(DAILY_REPORT_DIR, filename)}")

if __name__ == "__main__":
    main()