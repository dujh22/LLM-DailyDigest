{"custom_id": "1_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Making Qwen3 Think in Korean with Reinforcement Learning"}], "temperature": 0.1}}
{"custom_id": "1_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We present a two-stage fine-tuning approach to make the large language modelQwen3 14B \"think\" natively in Korean. In the first stage, supervisedfine-tuning (SFT) on a high-quality Korean reasoning dataset establishes astrong foundation in Korean logical reasoning, yielding notable improvements inKorean-language tasks and even some gains in general reasoning ability. In thesecond stage, we employ reinforcement learning with a customized Group RelativePolicy Optimization (GRPO) algorithm to further enhance both Korean reasoningalignment and overall problem-solving performance. We address criticalstability challenges in GRPO training - such as reward hacking and policycollapse - by introducing an oracle judge model that calibrates the rewardsignal. Our approach achieves stable learning (avoiding the collapse observedin naive GRPO) and leads to steady, incremental performance gains. The finalRL-tuned model demonstrates substantially improved results on advancedreasoning benchmarks (particularly math and coding tasks) while maintainingknowledge and language proficiency, successfully conducting its internalchain-of-thought entirely in Korean."}], "temperature": 0.1}}
{"custom_id": "1_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jungyup Lee"}], "temperature": 0.1}}
{"custom_id": "2_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "From Intent to Execution: Multimodal Chain-of-Thought Reinforcement Learning for Precise CAD Code Generation"}], "temperature": 0.1}}
{"custom_id": "2_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Computer-Aided Design (CAD) plays a vital role in engineering andmanufacturing, yet current CAD workflows require extensive domain expertise andmanual modeling effort. Recent advances in large language models (LLMs) havemade it possible to generate code from natural language, opening newopportunities for automating parametric 3D modeling. However, directlytranslating human design intent into executable CAD code remains highlychallenging, due to the need for logical reasoning, syntactic correctness, andnumerical precision. In this work, we propose CAD-RL, a multimodalChain-of-Thought (CoT) guided reinforcement learning post training frameworkfor CAD modeling code generation. Our method combines CoT-based Cold Start withgoal-driven reinforcement learning post training using three task-specificrewards: executability reward, geometric accuracy reward, and externalevaluation reward. To ensure stable policy learning under sparse andhigh-variance reward conditions, we introduce three targeted optimizationstrategies: Trust Region Stretch for improved exploration, Precision Token Lossfor enhanced dimensions parameter accuracy, and Overlong Filtering to reducenoisy supervision. To support training and benchmarking, we release ExeCAD, anoval dataset comprising 16,540 real-world CAD examples with paired naturallanguage and structured design language descriptions, executable CADQueryscripts, and rendered 3D models. Experiments demonstrate that CAD-RL achievessignificant improvements in reasoning quality, output precision, and codeexecutability over existing VLMs."}], "temperature": 0.1}}
{"custom_id": "2_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ke Niu"}], "temperature": 0.1}}
{"custom_id": "3_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution"}], "temperature": 0.1}}
{"custom_id": "3_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have achieved impressive performance, leading totheir widespread adoption as decision-support tools in resource-constrainedcontexts like hiring and admissions. There is, however, scientific consensusthat AI systems can reflect and exacerbate societal biases, raising concernsabout identity-based harm when used in critical social contexts. Prior work haslaid a solid foundation for assessing bias in LLMs by evaluating demographicdisparities in different language reasoning tasks. In this work, we extendsingle-axis fairness evaluations to examine intersectional bias, recognizingthat when multiple axes of discrimination intersect, they create distinctpatterns of disadvantage. We create a new benchmark called WinoIdentity byaugmenting the WinoBias dataset with 25 demographic markers across 10attributes, including age, nationality, and race, intersected with binarygender, yielding 245,700 prompts to evaluate 50 distinct bias patterns.Focusing on harms of omission due to underrepresentation, we investigate biasthrough the lens of uncertainty and propose a group (un)fairness metric calledCoreference Confidence Disparity which measures whether models are more or lessconfident for some intersectional identities than others. We evaluate fiverecently published LLMs and find confidence disparities as high as 40% alongvarious demographic attributes including body type, sexual orientation andsocio-economic status, with models being most uncertain aboutdoubly-disadvantaged identities in anti-stereotypical settings. Surprisingly,coreference confidence decreases even for hegemonic or privileged markers,indicating that the recent impressive performance of LLMs is more likely due tomemorization than logical reasoning. Notably, these are two independentfailures in value alignment and validity that can compound to cause socialharm."}], "temperature": 0.1}}
{"custom_id": "3_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Falaah Arif Khan"}], "temperature": 0.1}}
{"custom_id": "4_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "ContextGuard-LVLM: Enhancing News Veracity through Fine-grained Cross-modal Contextual Consistency Verification"}], "temperature": 0.1}}
{"custom_id": "4_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The proliferation of digital news media necessitates robust methods forverifying content veracity, particularly regarding the consistency betweenvisual and textual information. Traditional approaches often fall short inaddressing the fine-grained cross-modal contextual consistency (FCCC) problem,which encompasses deeper alignment of visual narrative, emotional tone, andbackground information with text, beyond mere entity matching. To address this,we propose ContextGuard-LVLM, a novel framework built upon advancedVision-Language Large Models (LVLMs) and integrating a multi-stage contextualreasoning mechanism. Our model is uniquely enhanced through reinforced oradversarial learning paradigms, enabling it to detect subtle contextualmisalignments that evade zero-shot baselines. We extend and augment threeestablished datasets (TamperedNews-Ent, News400-Ent, MMG-Ent) with newfine-grained contextual annotations, including \"contextual sentiment,\" \"visualnarrative theme,\" and \"scene-event logical coherence,\" and introduce acomprehensive CTXT (Contextual Coherence) entity type. Extensive experimentsdemonstrate that ContextGuard-LVLM consistently outperforms state-of-the-artzero-shot LVLM baselines (InstructBLIP and LLaVA 1.5) across nearly allfine-grained consistency tasks, showing significant improvements in complexlogical reasoning and nuanced contextual understanding. Furthermore, our modelexhibits superior robustness to subtle perturbations and a higher agreementrate with human expert judgments on challenging samples, affirming its efficacyin discerning sophisticated forms of context detachment."}], "temperature": 0.1}}
{"custom_id": "4_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Sihan Ma"}], "temperature": 0.1}}
{"custom_id": "5_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SABER: Switchable and Balanced Training for Efficient LLM Reasoning"}], "temperature": 0.1}}
{"custom_id": "5_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) empowered by chain-of-thought reasoning haveachieved impressive accuracy on complex tasks but suffer from excessiveinference costs and latency when applied uniformly to all problems. We proposeSABER (Switchable and Balanced Training for Efficient LLM Reasoning), areinforcement learning framework that endows LLMs with user-controllable,token-budgeted reasoning. SABER first profiles each training example'sbase-model thinking token usage and assigns it to one of the predefined budgettiers. During fine-tuning, the model is guided by system prompts andlength-aware rewards to respect its assigned budget. In parallel, weincorporate no-think examples to ensure the model remains reliable even whenexplicit reasoning is turned off. SABER further supports four discreteinference modes - NoThink, FastThink, CoreThink, and DeepThink, enablingflexible trade-offs between latency and reasoning depth. Extensive evaluationson math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tightbudgets, graceful degradation, and effective cross-scale and cross-domaingeneralization. In particular, SABER-FastThink cuts reasoning length by 65.4%and yields a 3.6% accuracy gain compared with the base model on the MATHbenchmark."}], "temperature": 0.1}}
{"custom_id": "5_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Kai Zhao"}], "temperature": 0.1}}
{"custom_id": "6_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines"}], "temperature": 0.1}}
{"custom_id": "6_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Geometry problem solving (GPS) requires models to master diagramcomprehension, logical reasoning, knowledge application, numerical computation,and auxiliary line construction. This presents a significant challenge forMultimodal Large Language Models (MLLMs). However, existing benchmarks forevaluating MLLM geometry skills overlook auxiliary line construction and lackfine-grained process evaluation, making them insufficient for assessing MLLMs'long-step reasoning abilities. To bridge these gaps, we present the GeoLauxbenchmark, comprising 2,186 geometry problems, incorporating both calculationand proving questions. Notably, the problems require an average of 6.51reasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliaryline construction. Building on the dataset, we design a novel five-dimensionalevaluation strategy assessing answer correctness, process correctness, processquality, auxiliary line impact, and error causes. Extensive experiments on 13leading MLLMs (including thinking models and non-thinking models) yield threepivotal findings: First, models exhibit substantial performance degradation inextended reasoning steps (nine models demonstrate over 50% performance drop).Second, compared to calculation problems, MLLMs tend to take shortcuts whensolving proving problems. Third, models lack auxiliary line awareness, andenhancing this capability proves particularly beneficial for overall geometryreasoning improvement. These findings establish GeoLaux as both a benchmark forevaluating MLLMs' long-step geometric reasoning with auxiliary lines and aguide for capability advancement. Our dataset and code are included insupplementary materials and will be released."}], "temperature": 0.1}}
{"custom_id": "6_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yumeng Fu"}], "temperature": 0.1}}
{"custom_id": "7_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Generative AI for Object-Oriented Programming: Writing the Right Code and Reasoning the Right Logic"}], "temperature": 0.1}}
{"custom_id": "7_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We find ourselves in the midst of an explosion in artificial intelligenceresearch, particularly with large language models (LLMs). These models havediverse applications spanning finance, commonsense knowledge graphs, medicine,and visual analysis. In the world of Object-Oriented Programming(OOP), a robustbody of knowledge and methods has been developed for managing complex tasksthrough object-oriented thinking. However, the intersection of LLMs with OOPremains an underexplored territory. Empirically, we currently possess limitedunderstanding of how LLMs can enhance the effectiveness of OOP learning andcode writing, as well as how we can evaluate such AI-powered tools. Our workaims to address this gap by presenting a vision from the perspectives of keystakeholders involved in an OOP task: programmers, mariners, and experiencedprogrammers. We identify critical junctures within typical coding workflowswhere the integration of LLMs can offer significant benefits. Furthermore, wepropose ways to augment existing logical reasoning and code writing, ultimatelyenhancing the programming experience."}], "temperature": 0.1}}
{"custom_id": "7_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Gang Xu"}], "temperature": 0.1}}
{"custom_id": "8_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning"}], "temperature": 0.1}}
{"custom_id": "8_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Despite recent advances in the reasoning capabilities of Large LanguageModels (LLMs), improving the reasoning ability of Small Language Models (SLMs,e.g., $\\leq$ 1.5B) remains challenging. A key obstacle lies in the complexityand variability of natural language: essentially equivalent problems oftenappear in diverse surface forms, often obscured by redundant or distractingdetails. This imposes a dual burden on SLMs: they must first extract the coreproblem from complex linguistic input, and then perform reasoning based on thatunderstanding. The resulting vast and noisy problem space hinders optimization,particularly for models with limited capacity. To address this, we propose anew framework that decouples understanding from reasoning by mapping naturallanguage problems into a canonical problem space-a semantically simplified yetexpressive domain. This enables SLMs to focus on reasoning over standardizedinputs, free from linguistic variability. Within this framework, we introduceDURIT (Decoupled Understanding from Reasoning via Iterative Training), athree-step algorithm that iteratively: (1) mapping natural language problemsvia reinforcement learning, (2) aligns reasoning trajectories throughself-distillation, and (3) trains reasoning policies in the problem space. Themapper and reasoner are co-trained in an alternating loop throughout thisprocess. Experiments show that DURIT substantially improves SLMs' performanceon both in-domain and out-of-domain mathematical and logical reasoning tasks.Beyond improving reasoning capabilities, DURIT also improves the robustness ofreasoning, validating decoupling understanding from reasoning as an effectivestrategy for strengthening SLMs."}], "temperature": 0.1}}
{"custom_id": "8_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Li Wang"}], "temperature": 0.1}}
{"custom_id": "9_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for Belief-Tracked Inference with Pretrained Language Models"}], "temperature": 0.1}}
{"custom_id": "9_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models often fail at logical reasoning when semanticheuristics conflict with decisive evidence - a phenomenon we term cognitivetraps. To address this fundamental limitation, we introduce the DeliberativeReasoning Network (DRN), a novel paradigm that reframes logical reasoning fromprobability maximization to uncertainty minimization. Instead of asking \"Whichanswer is most likely?\", DRN asks \"Which hypothesis has the most internallyconsistent evidence?\". DRN achieves intrinsic interpretability by explicitlytracking belief states and quantifying epistemic uncertainty for competinghypotheses through an iterative evidence synthesis process. We validate ourapproach through two complementary architectures - a bespoke discriminativemodel that embodies the core uncertainty minimization principle, and alightweight verification module that enhances existing generative LLMs.Evaluated on LCR-1000, our new adversarial reasoning benchmark designed toexpose cognitive traps, the bespoke DRN achieves up to 15.2% improvement overstandard baselines. When integrated as a parameter-efficient verifier withMistral-7B, our hybrid system boosts accuracy from 20% to 80% on the mostchallenging problems. Critically, DRN demonstrates strong zero-shotgeneralization, improving TruthfulQA performance by 23.6% without additionaltraining, indicating that uncertainty-driven deliberation learns transferablereasoning principles. We position DRN as a foundational, verifiable System 2reasoning component for building more trustworthy AI systems."}], "temperature": 0.1}}
{"custom_id": "9_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Anran Xu"}], "temperature": 0.1}}
{"custom_id": "10_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Method-Based Reasoning for Large Language Models: Extraction, Reuse, and Continuous Improvement"}], "temperature": 0.1}}
{"custom_id": "10_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have shown impressive capabilities across a widerange of language tasks. However, their reasoning process is primarily guidedby statistical patterns in training data, which limits their ability to handlenovel problems and perform consistent logical reasoning. In this paper, wepropose a method-based model that enhances LLMs with explicit, reusableprocedures extracted from training content, generated responses, and userinteractions. Each method is represented as a pair consisting of a problem andits corresponding solution, stored externally and ranked based on feedback.When a new query is received, the system retrieves and applies the mostrelevant methods to guide the LLM's response. Our model enables continuallearning, method reuse, and logical consistency beyond next-token prediction.Experimental results demonstrate that the system improves factual verificationand generalization in complex prompts, and that newly learned methods canoutperform earlier ones through user-driven refinement."}], "temperature": 0.1}}
{"custom_id": "10_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hong Su"}], "temperature": 0.1}}
{"custom_id": "11_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork"}], "temperature": 0.1}}
{"custom_id": "11_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "AI agents deployed in assistive roles often have to collaborate with otheragents (humans, AI systems) without prior coordination. Methods consideredstate of the art for such ad hoc teamwork often pursue a data-driven approachthat needs a large labeled dataset of prior observations, lacks transparency,and makes it difficult to rapidly revise existing knowledge in response tochanges. As the number of agents increases, the complexity of decision-makingmakes it difficult to collaborate effectively. This paper advocates leveragingthe complementary strengths of knowledge-based and data-driven methods forreasoning and learning for ad hoc teamwork. For any given goal, ourarchitecture enables each ad hoc agent to determine its actions throughnon-monotonic logical reasoning with: (a) prior commonsense domain-specificknowledge; (b) models learned and revised rapidly to predict the behavior ofother agents; and (c) anticipated abstract future goals based on genericknowledge of similar situations in an existing foundation model. Weexperimentally evaluate our architecture's capabilities in VirtualHome, arealistic physics-based 3D simulation environment."}], "temperature": 0.1}}
{"custom_id": "11_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hasra Dodampegama"}], "temperature": 0.1}}
{"custom_id": "12_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Comparative Study of Neurosymbolic AI Approaches to Interpretable Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "12_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "General logical reasoning, defined as the ability to reason deductively ondomain-agnostic tasks, continues to be a challenge for large language models(LLMs). Current LLMs fail to reason deterministically and are notinterpretable. As such, there has been a recent surge in interest inneurosymbolic AI, which attempts to incorporate logic into neural networks. Wefirst identify two main neurosymbolic approaches to improving logicalreasoning: (i) the integrative approach comprising models where symbolicreasoning is contained within the neural network, and (ii) the hybrid approachcomprising models where a symbolic solver, separate from the neural network,performs symbolic reasoning. Both contain AI systems with promising results ondomain-specific logical reasoning benchmarks. However, their performance ondomain-agnostic benchmarks is understudied. To the best of our knowledge, therehas not been a comparison of the contrasting approaches that answers thefollowing question: Which approach is more promising for developing generallogical reasoning? To analyze their potential, the following best-in-classdomain-agnostic models are introduced: Logic Neural Network (LNN), which usesthe integrative approach, and LLM-Symbolic Solver (LLM-SS), which uses thehybrid approach. Using both models as case studies and representatives of eachapproach, our analysis demonstrates that the hybrid approach is more promisingfor developing general logical reasoning because (i) its reasoning chain ismore interpretable, and (ii) it retains the capabilities and advantages ofexisting LLMs. To support future works using the hybrid approach, we propose ageneralizable framework based on LLM-SS that is modular by design,model-agnostic, domain-agnostic, and requires little to no human input."}], "temperature": 0.1}}
{"custom_id": "12_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Michael K. Chen"}], "temperature": 0.1}}
{"custom_id": "13_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs"}], "temperature": 0.1}}
{"custom_id": "13_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs), despite their success in question answering,exhibit limitations in complex multi-hop question answering (MQA) tasks thatnecessitate non-linear, structured reasoning. This limitation stems from theirinability to adequately capture deep conceptual relationships between entities.To overcome this challenge, we present **ORACLE** (**O**ntology-driven**R**easoning **A**nd **C**hain for **L**ogical **E**ucidation), atraining-free framework that combines LLMs' generative capabilities with thestructural benefits of knowledge graphs. Our approach operates through threestages: (1) dynamic construction of question-specific knowledge ontologiesusing LLMs, (2) transformation of these ontologies into First-Order Logicreasoning chains, and (3) systematic decomposition of the original query intologically coherent sub-questions. Experimental results on several standard MQAbenchmarks show that our framework achieves highly competitive performance,rivaling current state-of-the-art models like DeepSeek-R1. Detailed analysesfurther confirm the effectiveness of each component, while demonstrating thatour method generates more logical and interpretable reasoning chains thanexisting approaches."}], "temperature": 0.1}}
{"custom_id": "13_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Haonan Bian"}], "temperature": 0.1}}
{"custom_id": "14_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Loop Invariant Generation: A Hybrid Framework of Reasoning optimised LLMs and SMT Solvers"}], "temperature": 0.1}}
{"custom_id": "14_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Loop invariants are essential for proving the correctness of programs withloops. Developing loop invariants is challenging, and fully automatic synthesiscannot be guaranteed for arbitrary programs. Some approaches have been proposedto synthesize loop invariants using symbolic techniques and more recently usingneural approaches. These approaches are able to correctly synthesize loopinvariants only for subsets of standard benchmarks. In this work, weinvestigate whether modern, reasoning-optimized large language models can dobetter. We integrate OpenAI's O1, O1-mini, and O3-mini into a tightly coupledgenerate-and-check pipeline with the Z3 SMT solver, using solvercounterexamples to iteratively guide invariant refinement. We use Code2Invbenchmark, which provides C programs along with their formal preconditions andpostconditions. On this benchmark of 133 tasks, our framework achieves 100%coverage (133 out of 133), outperforming the previous best of 107 out of 133,while requiring only 1-2 model proposals per instance and 14-55 seconds ofwall-clock time. These results demonstrate that LLMs possess latent logicalreasoning capabilities which can help automate loop invariant synthesis. Whileour experiments target C-specific programs, this approach should begeneralizable to other imperative languages."}], "temperature": 0.1}}
{"custom_id": "14_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Varun Bharti"}], "temperature": 0.1}}
{"custom_id": "15_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via Progressive Reinforcement Learning"}], "temperature": 0.1}}
{"custom_id": "15_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In medical scenarios, effectively retrieving external knowledge andleveraging it for rigorous logical reasoning is of significant importance.Despite their potential, existing work has predominantly focused on enhancingeither retrieval or reasoning capabilities of the models in isolation, withlittle attention given to their joint optimization, which leads to limitedcoordination between the two processes. Additionally, current methods relyheavily on supervised fine-tuning (SFT), which can cause models to memorizeexisting problem-solving pathways, thereby restricting their generalizationability when confronted with novel problem contexts. Furthermore, while somestudies have explored to improve retrieval-augmented reasoning in generaldomains via reinforcement learning, their reward function designs do notadequately capture the specific demands of the medical domain. To address thesechallenges, we introduce **Med-R$^3$**, a **Med**ical **R**etrieval-augmented**R**easoning framework driven by progressive **R**einforcement learning. Inthis framework, we first develop the model's ability to perform logicalreasoning over medical problems. Subsequently, on the basis of this foundation,we adaptively optimize the retrieval capability to better align with thecharacteristics of knowledge corpus and external information utilizationthroughout the reasoning process. Finally, we conduct joint optimization of themodel's retrieval and reasoning coordination. Extensive experiments indicatethat **Med-R$^3$** could achieve state-of-the-art performances, withLLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by3.93\\% at a comparable parameter scale, while Qwen2.5-14B augmented withMed-R$^3$ shows a more substantial gain of 13.53\\%."}], "temperature": 0.1}}
{"custom_id": "15_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Keer Lu"}], "temperature": 0.1}}
{"custom_id": "16_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SLM-SQL: An Exploration of Small Language Models for Text-to-SQL"}], "temperature": 0.1}}
{"custom_id": "16_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have demonstrated strong performance intranslating natural language questions into SQL queries (Text-to-SQL). Incontrast, small language models (SLMs) ranging from 0.5B to 1.5B parameterscurrently underperform on Text-to-SQL tasks due to their limited logicalreasoning capabilities. However, SLMs offer inherent advantages in inferencespeed and suitability for edge deployment. To explore their potential inText-to-SQL applications, we leverage recent advancements in post-trainingtechniques. Specifically, we used the open-source SynSQL-2.5M dataset toconstruct two derived datasets: SynSQL-Think-916K for SQL generation andSynSQL-Merge-Think-310K for SQL merge revision. We then applied supervisedfine-tuning and reinforcement learning-based post-training to the SLM, followedby inference using a corrective self-consistency approach. Experimental resultsvalidate the effectiveness and generalizability of our method, SLM-SQL. On theBIRD development set, the five evaluated models achieved an average improvementof 31.4 points. Notably, the 0.5B model reached 56.87\\% execution accuracy(EX), while the 1.5B model achieved 67.08\\% EX. We will release our dataset,model, and code to github: https://github.com/CycloneBoy/slm_sql."}], "temperature": 0.1}}
{"custom_id": "16_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Lei Sheng"}], "temperature": 0.1}}
{"custom_id": "17_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LLM4VV: Evaluating Cutting-Edge LLMs for Generation and Evaluation of Directive-Based Parallel Programming Model Compiler Tests"}], "temperature": 0.1}}
{"custom_id": "17_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The usage of Large Language Models (LLMs) for software and test developmenthas continued to increase since LLMs were first introduced, but only recentlyhave the expectations of LLMs become more realistic. Verifying the correctnessof code generated by LLMs is key to improving their usefulness, but there havebeen no comprehensive and fully autonomous solutions developed yet.Hallucinations are a major concern when LLMs are applied blindly to problemswithout taking the time and effort to verify their outputs, and an inability toexplain the logical reasoning of LLMs leads to issues with trusting theirresults. To address these challenges while also aiming to effectively applyLLMs, this paper proposes a dual-LLM system (i.e. a generative LLM and adiscriminative LLM) and experiments with the usage of LLMs for the generationof a large volume of compiler tests. We experimented with a number of LLMspossessing varying parameter counts and presented results using tencarefully-chosen metrics that we describe in detail in our narrative. Throughour findings, it is evident that LLMs possess the promising potential togenerate quality compiler tests and verify them automatically."}], "temperature": 0.1}}
{"custom_id": "17_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zachariah Sollenberger"}], "temperature": 0.1}}
{"custom_id": "18_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning"}], "temperature": 0.1}}
{"custom_id": "18_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefitsubstantially from chain-of-thought (CoT) reasoning, yet pushing theirperformance typically requires vast data, large model sizes, and full-parameterfine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost,most existing approaches primarily address domain adaptation or layer-wiseallocation rather than explicitly tailoring data and parameters to differentresponse demands. Inspired by \"Thinking, Fast and Slow,\" which characterizestwo distinct modes of thought-System 1 (fast, intuitive, often automatic) andSystem 2 (slower, more deliberative and analytic)-we draw an analogy thatdifferent \"subregions\" of an LLM's parameters might similarly specialize fortasks that demand quick, intuitive responses versus those requiring multi-steplogical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA frameworkthat partitions both data and parameters by System 1 or System 2 demands, usingfewer yet more focused parameters for each task. Specifically, we classify taskdata via multi-model role-playing and voting, and partition parameters based onimportance scoring, then adopt a two-stage fine-tuning strategy of trainingSystem 1 tasks with supervised fine-tuning (SFT) to enhance knowledge andintuition and refine System 2 tasks with reinforcement learning (RL) toreinforce deeper logical deliberation next. Extensive experiments show that thetwo-stage fine-tuning strategy, SFT and RL, lowers active parameter usage whilematching or surpassing SOTA PEFT baselines."}], "temperature": 0.1}}
{"custom_id": "18_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yining Huang"}], "temperature": 0.1}}
{"custom_id": "19_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "On the Limits of Hierarchically Embedded Logic in Classical Neural Networks"}], "temperature": 0.1}}
{"custom_id": "19_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We propose a formal model of reasoning limitations in large neural net modelsfor language, grounded in the depth of their neural architecture. By treatingneural networks as linear operators over logic predicate space we show thateach layer can encode at most one additional level of logical reasoning. Weprove that a neural network of depth a particular depth cannot faithfullyrepresent predicates in a one higher order logic, such as simple counting overcomplex predicates, implying a strict upper bound on logical expressiveness.This structure induces a nontrivial null space during tokenization andembedding, excluding higher-order predicates from representability. Ourframework offers a natural explanation for phenomena such as hallucination,repetition, and limited planning, while also providing a foundation forunderstanding how approximations to higher-order logic may emerge. Theseresults motivate architectural extensions and interpretability strategies infuture development of language models."}], "temperature": 0.1}}
{"custom_id": "19_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Bill Cochran"}], "temperature": 0.1}}
{"custom_id": "20_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Enhancing Spatial Reasoning through Visual and Textual Thinking"}], "temperature": 0.1}}
{"custom_id": "20_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The spatial reasoning task aims to reason about the spatial relationships in2D and 3D space, which is a fundamental capability for Visual QuestionAnswering (VQA) and robotics. Although vision language models (VLMs) havedeveloped rapidly in recent years, they are still struggling with the spatialreasoning task. In this paper, we introduce a method that can enhance Spatialreasoning through Visual and Textual thinking Simultaneously (SpatialVTS). Inthe spatial visual thinking phase, our model is trained to generatelocation-related specific tokens of essential targets automatically. Not onlyare the objects mentioned in the problem addressed, but also the potentialobjects related to the reasoning are considered. During the spatial textualthinking phase, Our model conducts long-term thinking based on visual cues anddialogues, gradually inferring the answers to spatial reasoning problems. Toeffectively support the model's training, we perform manual corrections to theexisting spatial reasoning dataset, eliminating numerous incorrect labelsresulting from automatic annotation, restructuring the data input format toenhance generalization ability, and developing thinking processes with logicalreasoning details. Without introducing additional information (such as masks ordepth), our model's overall average level in several spatial understandingtasks has significantly improved compared with other models."}], "temperature": 0.1}}
{"custom_id": "20_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xun Liang"}], "temperature": 0.1}}
{"custom_id": "21_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "NIRS: An Ontology for Non-Invasive Respiratory Support in Acute Care"}], "temperature": 0.1}}
{"custom_id": "21_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Objective: Develop a Non Invasive Respiratory Support (NIRS) ontology tosupport knowledge representation in acute care settings.  Materials and Methods: We developed the NIRS ontology using Web OntologyLanguage (OWL) semantics and Protege to organize clinical concepts andrelationships. To enable rule-based clinical reasoning beyond hierarchicalstructures, we added Semantic Web Rule Language (SWRL) rules. We evaluatedlogical reasoning by adding 17 hypothetical patient clinical scenarios. We usedSPARQL queries and data from the Electronic Intensive Care Unit (eICU)Collaborative Research Database to retrieve and test targeted inferences.  Results: The ontology has 132 classes, 12 object properties, and 17 dataproperties across 882 axioms that establish concept relationships. Tostandardize clinical concepts, we added 350 annotations, including descriptivedefinitions based on controlled vocabularies. SPARQL queries successfullyvalidated all test cases (rules) by retrieving appropriate patient outcomes,for instance, a patient treated with HFNC (high-flow nasal cannula) for 2 hoursdue to acute respiratory failure may avoid endotracheal intubation.  Discussion: The NIRS ontology formally represents domain-specific concepts,including ventilation modalities, patient characteristics, therapy parameters,and outcomes. SPARQL query evaluations on clinical scenarios confirmed theability of the ontology to support rule based reasoning and therapyrecommendations, providing a foundation for consistent documentation practices,integration into clinical data models, and advanced analysis of NIRS outcomes.  Conclusion: We unified NIRS concepts into an ontological framework anddemonstrated its applicability through the evaluation of hypothetical patientscenarios and alignment with standardized vocabularies."}], "temperature": 0.1}}
{"custom_id": "21_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Md Fantacher Islam"}], "temperature": 0.1}}
{"custom_id": "22_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)"}], "temperature": 0.1}}
{"custom_id": "22_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Answer Set Programming (ASP) is a powerful paradigm for non-monotonicreasoning. Recently, large language models (LLMs) have demonstrated promisingcapabilities in logical reasoning. Despite this potential, current evaluationsof LLM capabilities in ASP are often limited. Existing works normally employoverly simplified ASP programs, do not support negation, disjunction, ormultiple answer sets. Furthermore, there is a lack of benchmarks that introducetasks specifically designed for ASP solving. To bridge this gap, we introduceASPBench, a comprehensive ASP benchmark, including three ASP specific tasks:ASP entailment, answer set verification, and answer set computation. Ourextensive evaluations on ASPBench reveal that while 14 state-of-the-art LLMs,including \\emph{deepseek-r1}, \\emph{o4-mini}, and\\emph{gemini-2.5-flash-thinking}, perform relatively well on the first twosimpler tasks, they struggle with answer set computation, which is the core ofASP solving. These findings offer insights into the current limitations of LLMsin ASP solving. This highlights the need for new approaches that integratesymbolic reasoning capabilities more effectively. The code and dataset areavailable at https://github.com/HomuraT/ASPBench."}], "temperature": 0.1}}
{"custom_id": "22_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Lin Ren"}], "temperature": 0.1}}
{"custom_id": "23_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Augmented Vision-Language Models: A Systematic Review"}], "temperature": 0.1}}
{"custom_id": "23_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advances in visual-language machine learning models have demonstratedexceptional ability to use natural language and understand visual scenes bytraining on large, unstructured datasets. However, this training paradigmcannot produce interpretable explanations for its outputs, requires retrainingto integrate new information, is highly resource-intensive, and struggles withcertain forms of logical reasoning. One promising solution involves integratingneural networks with external symbolic information systems, forming neuralsymbolic systems that can enhance reasoning and memory abilities. These neuralsymbolic systems provide more interpretable explanations to their outputs andthe capacity to assimilate new information without extensive retraining.Utilizing powerful pre-trained Vision-Language Models (VLMs) as the core neuralcomponent, augmented by external systems, offers a pragmatic approach torealizing the benefits of neural-symbolic integration. This systematicliterature review aims to categorize techniques through which visual-languageunderstanding can be improved by interacting with external symbolic informationsystems."}], "temperature": 0.1}}
{"custom_id": "23_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Anthony C Davis"}], "temperature": 0.1}}
{"custom_id": "24_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Automated Code Review Using Large Language Models with Symbolic Reasoning"}], "temperature": 0.1}}
{"custom_id": "24_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Code review is one of the key processes in the software development lifecycleand is essential to maintain code quality. However, manual code review issubjective and time consuming. Given its rule-based nature, code review is wellsuited for automation. In recent years, significant efforts have been made toautomate this process with the help of artificial intelligence. Recentdevelopments in Large Language Models (LLMs) have also emerged as a promisingtool in this area, but these models often lack the logical reasoningcapabilities needed to fully understand and evaluate code. To overcome thislimitation, this study proposes a hybrid approach that integrates symbolicreasoning techniques with LLMs to automate the code review process. We testedour approach using the CodexGlue dataset, comparing several models, includingCodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combiningsymbolic reasoning and prompting techniques with LLMs. Our results show thatthis approach improves the accuracy and efficiency of automated code review."}], "temperature": 0.1}}
{"custom_id": "24_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Busra Icoz"}], "temperature": 0.1}}
{"custom_id": "25_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning"}], "temperature": 0.1}}
{"custom_id": "25_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as apowerful paradigm for enhancing the reasoning capabilities of LLMs. Existingresearch has predominantly concentrated on isolated reasoning domains such asmathematical problem-solving, coding tasks, or logical reasoning. However, realworld reasoning scenarios inherently demand an integrated application ofmultiple cognitive skills. Despite this, the interplay among these reasoningskills under reinforcement learning remains poorly understood. To bridge thisgap, we present a systematic investigation of multi-domain reasoning within theRLVR framework, explicitly focusing on three primary domains: mathematicalreasoning, code generation, and logical puzzle solving. We conduct acomprehensive study comprising four key components: (1) Leveraging the GRPOalgorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates themodels' in-domain improvements and cross-domain generalization capabilitieswhen trained on single-domain datasets. (2) Additionally, we examine theintricate interactions including mutual enhancements and conflicts that emergeduring combined cross-domain training. (3) To further understand the influenceof SFT on RL, we also analyze and compare performance differences between baseand instruct models under identical RL configurations. (4) Furthermore, wedelve into critical RL training details, systematically exploring the impactsof curriculum learning strategies, variations in reward design, andlanguage-specific factors. Through extensive experiments, our results offersignificant insights into the dynamics governing domain interactions, revealingkey factors influencing both specialized and generalizable reasoningperformance. These findings provide valuable guidance for optimizing RLmethodologies to foster comprehensive, multi-domain reasoning capabilities inLLMs."}], "temperature": 0.1}}
{"custom_id": "25_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yu Li"}], "temperature": 0.1}}
{"custom_id": "26_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs"}], "temperature": 0.1}}
{"custom_id": "26_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Although recent Large Language Models (LLMs) have shown rapid improvement onreasoning benchmarks in English, the evaluation of such LLMs' multilingualreasoning capability across diverse languages and cultural contexts remainslimited. Existing multilingual reasoning benchmarks are typically constructedby translating existing English reasoning benchmarks, biasing these benchmarkstowards reasoning problems with context in English language/cultures. In thiswork, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), abenchmark designed to assess LLMs on more than 1,000 native, linguistic andculturally grounded reasoning questions written by native speakers in French,Spanish, and Chinese. MultiNRC covers four core reasoning categories:language-specific linguistic reasoning, wordplay & riddles, cultural/traditionreasoning, and math reasoning with cultural relevance. For cultural/traditionreasoning and math reasoning with cultural relevance, we also provide Englishequivalent translations of the multilingual questions by manual translationfrom native speakers fluent in English. This set of English equivalents canprovide a direct comparison of LLM reasoning capacity in other languages vs.English on the same reasoning questions. We systematically evaluate current 14leading LLMs covering most LLM families on MultiNRC and its English equivalentset. The results show that (1) current LLMs are still not good at nativemultilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMsexhibit distinct strengths and weaknesses in handling linguistic, cultural, andlogical reasoning tasks; (3) Most models perform substantially better in mathreasoning in English compared to in original languages (+10%), indicatingpersistent challenges with culturally grounded knowledge."}], "temperature": 0.1}}
{"custom_id": "26_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Alexander R. Fabbri"}], "temperature": 0.1}}
{"custom_id": "27_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "CIMR: Contextualized Iterative Multimodal Reasoning for Robust Instruction Following in LVLMs"}], "temperature": 0.1}}
{"custom_id": "27_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The rapid advancement of Large Language Models (LLMs) and LargeVision-Language Models (LVLMs) has enhanced our ability to process and generatehuman language and visual information. However, these models often strugglewith complex, multi-step multi-modal instructions that require logicalreasoning, dynamic feedback integration, and iterative self-correction. Toaddress this, we propose CIMR: Contextualized Iterative Multimodal Reasoning, anovel framework that introduces a context-aware iterative reasoning andself-correction module. CIMR operates in two stages: initial reasoning andresponse generation, followed by iterative refinement using parsed multi-modalfeedback. A dynamic fusion module deeply integrates textual, visual, andcontextual features at each step. We fine-tune LLaVA-1.5-7B on the VisualInstruction Tuning (VIT) dataset and evaluate CIMR on the newly introducedMulti-modal Action Planning (MAP) dataset. CIMR achieves 91.5% accuracy,outperforming state-of-the-art models such as GPT-4V (89.2%), LLaVA-1.5(78.5%), MiniGPT-4 (75.3%), and InstructBLIP (72.8%), demonstrating theefficacy of its iterative reasoning and self-correction capabilities in complextasks."}], "temperature": 0.1}}
{"custom_id": "27_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yangshu Yuan"}], "temperature": 0.1}}
{"custom_id": "28_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Learning Temporal Abstractions via Variational Homomorphisms in Option-Induced Abstract MDPs"}], "temperature": 0.1}}
{"custom_id": "28_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have shown remarkable reasoning ability throughexplicit Chain-of-Thought (CoT) prompting, but generating these step-by-steptextual explanations is computationally expensive and slow. To overcome this,we aim to develop a framework for efficient, implicit reasoning, where themodel \"thinks\" in a latent space without generating explicit text for everystep. We propose that these latent thoughts can be modeled astemporally-extended abstract actions, or options, within a hierarchicalreinforcement learning framework. To effectively learn a diverse library ofoptions as latent embeddings, we first introduce the Variational MarkovianOption Critic (VMOC), an off-policy algorithm that uses variational inferencewithin the HiT-MDP framework. To provide a rigorous foundation for using theseoptions as an abstract reasoning space, we extend the theory of continuous MDPhomomorphisms. This proves that learning a policy in the simplified, abstractlatent space, for which VMOC is suited, preserves the optimality of thesolution to the original, complex problem. Finally, we propose a cold-startprocedure that leverages supervised fine-tuning (SFT) data to distill humanreasoning demonstrations into this latent option space, providing a richinitialization for the model's reasoning capabilities. Extensive experimentsdemonstrate that our approach achieves strong performance on complex logicalreasoning benchmarks and challenging locomotion tasks, validating our frameworkas a principled method for learning abstract skills for both language andcontrol."}], "temperature": 0.1}}
{"custom_id": "28_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chang Li"}], "temperature": 0.1}}
{"custom_id": "29_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through Logical Rule-Guided Chains"}], "temperature": 0.1}}
{"custom_id": "29_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Current knowledge editing methods for large language models (LLMs) struggleto maintain logical consistency when propagating ripple effects to associatedfacts. We propose ChainEdit, a framework that synergizes knowledgegraph-derived logical rules with LLM logical reasoning capabilities to enablesystematic chain updates. By automatically extracting logical patterns fromstructured knowledge bases and aligning them with LLMs' internal logics,ChainEdit dynamically generates and edits logically connected knowledgeclusters. Experiments demonstrate an improvement of more than 30% in logicalgeneralization over baselines while preserving editing reliability andspecificity. We further address evaluation biases in existing benchmarksthrough knowledge-aware protocols that disentangle external dependencies. Thiswork establishes new state-of-the-art performance on ripple effect whileensuring internal logical consistency after knowledge editing."}], "temperature": 0.1}}
{"custom_id": "29_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zilu Dong"}], "temperature": 0.1}}
{"custom_id": "30_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code"}], "temperature": 0.1}}
{"custom_id": "30_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Enhancing reasoning capabilities remains a central focus in the LLM reasearchcommunity. A promising direction involves requiring models to simulate codeexecution step-by-step to derive outputs for given inputs. However, as code isoften designed for large-scale systems, direct application leads toover-reliance on complex data structures and algorithms, even for simple cases,resulting in overfitting to algorithmic patterns rather than core reasoningstructures. To address this, we propose TeaR, which aims at teaching LLMs toreason better. TeaR leverages careful data curation and reinforcement learningto guide models in discovering optimal reasoning paths through code-relatedtasks, thereby improving general reasoning abilities. We conduct extensiveexperiments using two base models and three long-CoT distillation models, withmodel sizes ranging from 1.5 billion to 32 billion parameters, and across 17benchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The resultsconsistently show significant performance improvements. Notably, TeaR achievesa 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B."}], "temperature": 0.1}}
{"custom_id": "30_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Keqin Bao"}], "temperature": 0.1}}
{"custom_id": "31_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning under uncertainty in the game of Cops and Robbers"}], "temperature": 0.1}}
{"custom_id": "31_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The game of Cops and Robbers is an important model for studying computationalqueries in pursuit-evasion environments, among others. As recent logicalexplorations have shown, its structure exhibits appealing analogies with modallogic. In this paper, we enrich the game with a setting in which players mayhave imperfect information. We propose a new formal framework, Epistemic Logicof Cops and Robbers (ELCR), to make the core notions of the game precise, forinstance, players' positions, observational power and inference. Applying ELCRto analyze the game, we obtain an automated way to track interactions betweenplayers and characterize their information updates during the game. The updatemechanism is defined by a novel dynamic operator, and we compare it with somerelevant paradigms from the game and logic perspectives. We study variousproperties of ELCR including axiomatization and decidability. To our knowledge,this is the first attempt to explore these games from a formal point of viewwhere (partial) information available to players is taken into account."}], "temperature": 0.1}}
{"custom_id": "31_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Dazhu Li"}], "temperature": 0.1}}
{"custom_id": "32_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large Language Models"}], "temperature": 0.1}}
{"custom_id": "32_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Advancements in reasoning for large language models (LLMs) have lead tosignificant performance improvements for LLMs in various fields such asmathematics and programming. However, research applying these advances to thefinancial domain, where considerable domain-specific knowledge is necessary tocomplete tasks, remains limited. To address this gap, we introduce FEVO(Financial Evolution), a multi-stage enhancement framework developed to enhanceLLM performance in the financial domain. FEVO systemically enhances LLMperformance by using continued pre-training (CPT) to expand financial domainknowledge, supervised fine-tuning (SFT) to instill structured, elaboratereasoning patterns, and reinforcement learning (RL) to further integrate theexpanded financial domain knowledge with the learned structured reasoning. Toensure effective and efficient training, we leverage frontier reasoning modelsand rule-based filtering to curate FEVO-Train, high-quality datasetsspecifically designed for the different post-training phases. Using ourframework, we train the FEVO series of models - C32B, S32B, R32B - fromQwen2.5-32B and evaluate them on seven benchmarks to assess financial andgeneral capabilities, with results showing that FEVO-R32B achievesstate-of-the-art performance on five financial benchmarks against much largermodels as well as specialist models. More significantly, FEVO-R32B demonstratesmarkedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instructusing only RL), thus validating the effectiveness of financial domain knowledgeexpansion and structured, logical reasoning distillation"}], "temperature": 0.1}}
{"custom_id": "32_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Bo Pang"}], "temperature": 0.1}}
{"custom_id": "33_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Current Practices for Building LLM-Powered Reasoning Tools Are Ad Hoc -- and We Can Do Better"}], "temperature": 0.1}}
{"custom_id": "33_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "There is growing excitement about building software verifiers, synthesizers,and other Automated Reasoning (AR) tools by combining traditional symbolicalgorithms and Large Language Models (LLMs). Unfortunately, the currentpractice for constructing such neurosymbolic AR systems is an ad hocprogramming model that does not have the strong guarantees of traditionalsymbolic algorithms, nor a deep enough synchronization of neural networks andsymbolic reasoning to unlock the full potential of LLM-powered reasoning. Ipropose Neurosymbolic Transition Systems as a principled computational modelthat can underlie infrastructure for building neurosymbolic AR tools. In thismodel, symbolic state is paired with intuition, and state transitions operateover symbols and intuition in parallel. I argue why this new paradigm can scalelogical reasoning beyond current capabilities while retaining the strongguarantees of symbolic algorithms, and I sketch out how the computational modelI propose can be reified in a logic programming language."}], "temperature": 0.1}}
{"custom_id": "33_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Aaron Bembenek"}], "temperature": 0.1}}
{"custom_id": "34_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models for Agent-Based Modelling: Current and possible uses across the modelling cycle"}], "temperature": 0.1}}
{"custom_id": "34_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The emergence of Large Language Models (LLMs) with increasingly sophisticatednatural language understanding and generative capabilities has sparked interestin the Agent-based Modelling (ABM) community. With their ability to summarize,generate, analyze, categorize, transcribe and translate text, answer questions,propose explanations, sustain dialogue, extract information from unstructuredtext, and perform logical reasoning and problem-solving tasks, LLMs have a goodpotential to contribute to the modelling process. After reviewing the currentuse of LLMs in ABM, this study reflects on the opportunities and challenges ofthe potential use of LLMs in ABM. It does so by following the modelling cycle,from problem formulation to documentation and communication of model results,and holding a critical stance."}], "temperature": 0.1}}
{"custom_id": "34_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Lo茂s Vanh茅e"}], "temperature": 0.1}}
{"custom_id": "35_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "MLlm-DR: Towards Explainable Depression Recognition with MultiModal Large Language Models"}], "temperature": 0.1}}
{"custom_id": "35_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Automated depression diagnosis aims to analyze multimodal information frominterview videos to predict participants' depression scores. Previous studiesoften lack clear explanations of how these scores were determined, limitingtheir adoption in clinical practice. While the advent of LLMs provides apossible pathway for explainable depression diagnosis, current LLMs capable ofprocessing multimodal data lack training on interview data, resulting in poordiagnostic performance when used directly. In this paper, we propose a novelmultimodal large language model (MLlm-DR) that can understand multimodalinformation inputs and supports explainable depression diagnosis. MLlm-DRintegrates a smaller LLMs and a lightweight query module (LQ-former).Specifically, the smaller LLMs is designed to generate depression scores andcorresponding evaluation rationales. To enhance its logical reasoning fordomain-specific tasks while maintaining practicality, we constructed a robusttraining dataset to fine-tune it. Meanwhile, the LQ-former capturesdepression-related features from speech and visual data, aiding the model'sability to process multimodal information, to achieve comprehensive depressiondiagnosis. Our approach achieves state-of-the-art results on twointerview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating itseffectiveness and superiority."}], "temperature": 0.1}}
{"custom_id": "35_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Wei Zhang"}], "temperature": 0.1}}
{"custom_id": "36_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System"}], "temperature": 0.1}}
{"custom_id": "36_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Furniture decoration is an important task in various industrial applications.However, achieving a high-quality decorative result is often time-consuming andrequires specialized artistic expertise. To tackle these challenges, we explorehow multi-agent systems can assist in automating the decoration process. Wepropose FurniMAS, a multi-agent system for automatic furniture decoration.Specifically, given a human prompt and a household furniture item such as aworking desk or a TV stand, our system suggests relevant assets withappropriate styles and materials, and arranges them on the item, ensuring thedecorative result meets functionality, aesthetic, and ambiance preferences.FurniMAS assembles a hybrid team of LLM-based and non-LLM agents, eachfulfilling distinct roles in a typical decoration project. These agentscollaborate through communication, logical reasoning, and validation totransform the requirements into the final outcome. Extensive experimentsdemonstrate that our FurniMAS significantly outperforms other baselines ingenerating high-quality 3D decor."}], "temperature": 0.1}}
{"custom_id": "36_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Toan Nguyen"}], "temperature": 0.1}}
{"custom_id": "37_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Uncertainty-aware Reward Design Process"}], "temperature": 0.1}}
{"custom_id": "37_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Designing effective reward functions is a cornerstone of reinforcementlearning (RL), yet it remains a challenging process due to the inefficienciesand inconsistencies inherent in conventional reward engineering methodologies.Recent advances have explored leveraging large language models (LLMs) toautomate reward function design. However, their suboptimal performance innumerical optimization often yields unsatisfactory reward quality, while theevolutionary search paradigm demonstrates inefficient utilization of simulationresources, resulting in prohibitively lengthy design cycles withdisproportionate computational overhead. To address these challenges, wepropose the Uncertainty-aware Reward Design Process (URDP), a novel frameworkthat integrates large language models to streamline reward function design andevaluation in RL environments. URDP quantifies candidate reward functionuncertainty based on self-consistency analysis, enabling simulation-freeidentification of ineffective reward components while discovering novel rewardcomponents. Furthermore, we introduce uncertainty-aware Bayesian optimization(UABO), which incorporates uncertainty estimation to significantly enhancehyperparameter configuration efficiency. Finally, we construct a bi-leveloptimization architecture by decoupling the reward component optimization andthe hyperparameter tuning. URDP orchestrates synergistic collaboration betweenthe reward logic reasoning of the LLMs and the numerical optimization strengthsof the Bayesian Optimization. We conduct a comprehensive evaluation of URDPacross 35 diverse tasks spanning three benchmark environments. Our experimentalresults demonstrate that URDP not only generates higher-quality rewardfunctions but also achieves significant improvements in the efficiency ofautomated reward design compared to existing approaches."}], "temperature": 0.1}}
{"custom_id": "37_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yang Yang"}], "temperature": 0.1}}
{"custom_id": "38_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning"}], "temperature": 0.1}}
{"custom_id": "38_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Event stream based scene text recognition is a newly arising research topicin recent years which performs better than the widely used RGB cameras inextremely challenging scenarios, especially the low illumination, fast motion.Existing works either adopt end-to-end encoder-decoder framework or largelanguage models for enhanced recognition, however, they are still limited bythe challenges of insufficient interpretability and weak contextual logicalreasoning. In this work, we propose a novel chain-of-thought reasoning basedevent stream scene text recognition framework, termed ESTR-CoT. Specifically,we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the inputevent stream into tokens and utilize a Llama tokenizer to encode the givengeneration prompt. A Q-former is used to align the vision token to thepre-trained large language model Vicuna-7B and output both the answer andchain-of-thought (CoT) reasoning process simultaneously. Our framework can beoptimized using supervised fine-tuning in an end-to-end manner. In addition, wealso propose a large-scale CoT dataset to train our framework via a three stageprocessing (i.e., generation, polish, and expert verification). This datasetprovides a solid data foundation for the development of subsequentreasoning-based large models. Extensive experiments on three event stream STRbenchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated theeffectiveness and interpretability of our proposed framework. The source codeand pre-trained models will be released onhttps://github.com/Event-AHU/ESTR-CoT."}], "temperature": 0.1}}
{"custom_id": "38_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xiao Wang"}], "temperature": 0.1}}
{"custom_id": "39_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Synergizing Logical Reasoning, Knowledge Management and Collaboration in Multi-Agent LLM System"}], "temperature": 0.1}}
{"custom_id": "39_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This paper explores the integration of advanced Multi-Agent Systems (MAS)techniques to develop a team of agents with enhanced logical reasoning,long-term knowledge retention, and Theory of Mind (ToM) capabilities. Byuniting these core components with optimized communication protocols, we createa novel framework called SynergyMAS, which fosters collaborative teamwork andsuperior problem-solving skills. The system's effectiveness is demonstratedthrough a product development team case study, where our approach significantlyenhances performance and adaptability. These findings highlight SynergyMAS'spotential to tackle complex, real-world challenges."}], "temperature": 0.1}}
{"custom_id": "39_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Adam Kostka"}], "temperature": 0.1}}
{"custom_id": "40_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Model-Driven Closed-Loop UAV Operation with Semantic Observations"}], "temperature": 0.1}}
{"custom_id": "40_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advances in large Language Models (LLMs) have revolutionized mobilerobots, including unmanned aerial vehicles (UAVs), enabling their intelligentoperation within Internet of Things (IoT) ecosystems. However, LLMs still facechallenges from logical reasoning and complex decision-making, leading toconcerns about the reliability of LLM-driven UAV operations in IoTapplications. In this paper, we propose a LLM-driven closed-loop controlframework that enables reliable UAV operations powered by effective feedbackand refinement using two LLM modules, i.e., a Code Generator and an Evaluator.Our framework transforms numerical state observations from UAV operations intonatural language trajectory descriptions to enhance the evaluator LLM'sunderstanding of UAV dynamics for precise feedback generation. Our frameworkalso enables a simulation-based refinement process, and hence eliminates therisks to physical UAVs caused by incorrect code execution during therefinement. Extensive experiments on UAV control tasks with differentcomplexities are conducted. The experimental results show that our frameworkcan achieve reliable UAV operations using LLMs, which significantly outperformsbaseline approaches in terms of success rate and completeness with the increaseof task complexity."}], "temperature": 0.1}}
{"custom_id": "40_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Wenhao Wang"}], "temperature": 0.1}}
{"custom_id": "41_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "AI4Research: A Survey of Artificial Intelligence for Scientific Research"}], "temperature": 0.1}}
{"custom_id": "41_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advancements in artificial intelligence (AI), particularly in largelanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstratedremarkable capabilities in complex domains such as logical reasoning andexperimental coding. Motivated by these advancements, numerous studies haveexplored the application of AI in the innovation process, particularly in thecontext of scientific research. These AI technologies primarily aim to developsystems that can autonomously conduct research processes across a wide range ofscientific disciplines. Despite these significant strides, a comprehensivesurvey on AI for Research (AI4Research) remains absent, which hampers ourunderstanding and impedes further development in this field. To address thisgap, we present a comprehensive survey and offer a unified perspective onAI4Research. Specifically, the main contributions of our work are as follows:(1) Systematic taxonomy: We first introduce a systematic taxonomy to classifyfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify keyresearch gaps and highlight promising future directions, focusing on the rigorand scalability of automated experiments, as well as the societal impact. (3)Abundant applications and resources: Finally, we compile a wealth of resources,including relevant multidisciplinary applications, data corpora, and tools. Wehope our work will provide the research community with quick access to theseresources and stimulate innovative breakthroughs in AI4Research."}], "temperature": 0.1}}
{"custom_id": "41_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Qiguang Chen"}], "temperature": 0.1}}
{"custom_id": "42_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models"}], "temperature": 0.1}}
{"custom_id": "42_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities atsolving complex reasoning tasks with Chain-of-Thought (CoT) prompting, buttheir decision-making processes remain somewhat blackbox. We introducetextbfinverse reasoning, a novel paradigm enabling LLMs to decompose andexplain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a4-billion-parameter reasoning model, employs a metacognitive structure thatreflects back via attention processes to identify major decision points andgenerate explanations of reasoning choices. While typical CoT approaches aredirected towards forward reasoning generation, inverse reasoning providesinsight into why specific reasoning chains were selected over others. Throughthorough testing of logical reasoning puzzles, math problems and ethicaldilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, wedemonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) forits task, and offers performance almost on par with models like Claude-3.5Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework forLLM self-reflection via inverse reasoning, (ii) a novel metalearning frameworkto reverse the attention flow, (iii) comprehensive evaluation frameworks forreasoning transparency, and (iv) evidence that increasing reasoning usinginverse reasoning improves interpretability along with reasoning performance.Our work creates new avenues for transparent AI systems and closes significantgaps in AI safety, education, and scientific discovery."}], "temperature": 0.1}}
{"custom_id": "42_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Basab Jha"}], "temperature": 0.1}}
{"custom_id": "43_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Do LLMs Dream of Discrete Algorithms?"}], "temperature": 0.1}}
{"custom_id": "43_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have rapidly transformed the landscape ofartificial intelligence, enabling natural language interfaces and dynamicorchestration of software components. However, their reliance on probabilisticinference limits their effectiveness in domains requiring strict logicalreasoning, discrete decision-making, and robust interpretability. This paperinvestigates these limitations and proposes a neurosymbolic approach thataugments LLMs with logic-based reasoning modules, particularly leveragingProlog predicates and composable toolsets. By integrating first-order logic andexplicit rule systems, our framework enables LLMs to decompose complex queriesinto verifiable sub-tasks, orchestrate reliable solutions, and mitigate commonfailure modes such as hallucination and incorrect step decomposition. Wedemonstrate the practical benefits of this hybrid architecture throughexperiments on the DABStep benchmark, showing improved precision, coverage, andsystem documentation in multi-step reasoning tasks. Our results indicate thatcombining LLMs with modular logic reasoning restores engineering rigor,enhances system reliability, and offers a scalable path toward trustworthy,interpretable AI agents across complex domains."}], "temperature": 0.1}}
{"custom_id": "43_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Claudionor Coelho Jr"}], "temperature": 0.1}}
{"custom_id": "44_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning"}], "temperature": 0.1}}
{"custom_id": "44_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This work explores enabling Chain-of-Thought (CoT) reasoning to link visualcues across multiple images. A straightforward solution is to adapt rule-basedreinforcement learning for Vision-Language Models (VLMs). However, such methodstypically rely on manually curated question-answer pairs, which can beparticularly challenging when dealing with fine grained visual details andcomplex logic across images. Inspired by self-supervised visual representationlearning, we observe that images contain inherent constraints that can serve assupervision. Based on this insight, we construct image triplets comprising twoaugmented views of the same image and a third, similar but distinct image.During training, the model is prompted to generate a reasoning process tocompare these images (i.e., determine same or different). Then we optimize themodel with rule-based reinforcement learning. Due to the high visual similarityand the presence of augmentations, the model must attend to subtle visualchanges and perform logical reasoning to succeed. Experiments show that,although trained solely on visual comparison tasks, the learned reasoningability generalizes effectively to a wide range of questions. Without relyingon any human-annotated question-answer pairs, our method achieves significantimprovements on multi-image reasoning benchmarks and shows strong performanceon general vision tasks."}], "temperature": 0.1}}
{"custom_id": "44_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xi Chen"}], "temperature": 0.1}}
{"custom_id": "45_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Query as Test: An Intelligent Driving Test and Data Storage Method for Integrated Cockpit-Vehicle-Road Scenarios"}], "temperature": 0.1}}
{"custom_id": "45_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "With the deep penetration of Artificial Intelligence (AI) in thetransportation sector, intelligent cockpits, autonomous driving, andintelligent road networks are developing at an unprecedented pace. However, thedata ecosystems of these three key areas are increasingly fragmented andincompatible. Especially, existing testing methods rely on data stacking, failto cover all edge cases, and lack flexibility. To address this issue, thispaper introduces the concept of \"Query as Test\" (QaT). This concept shifts thefocus from rigid, prescripted test cases to flexible, on-demand logical queriesagainst a unified data representation. Specifically, we identify the need for afundamental improvement in data storage and representation, leading to ourproposal of \"Extensible Scenarios Notations\" (ESN). ESN is a novel declarativedata framework based on Answer Set Programming (ASP), which uniformlyrepresents heterogeneous multimodal data from the cockpit, vehicle, and road asa collection of logical facts and rules. This approach not only achieves deepsemantic fusion of data, but also brings three core advantages: (1) supportscomplex and flexible semantic querying through logical reasoning; (2) providesnatural interpretability for decision-making processes; (3) allows foron-demand data abstraction through logical rules, enabling fine-grained privacyprotection. We further elaborate on the QaT paradigm, transforming thefunctional validation and safety compliance checks of autonomous drivingsystems into logical queries against the ESN database, significantly enhancingthe expressiveness and formal rigor of the testing. Finally, we introduce theconcept of \"Validation-Driven Development\" (VDD), which suggests to guidedevelopments by logical validation rather than quantitative testing in the eraof Large Language Models, in order to accelerating the iteration anddevelopment process."}], "temperature": 0.1}}
{"custom_id": "45_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Shengyue Yao"}], "temperature": 0.1}}
{"custom_id": "46_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs"}], "temperature": 0.1}}
{"custom_id": "46_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Current Vision-Language Models (VLMs) struggle with fine-grained spatialreasoning, particularly when multi-step logic and precise spatial alignment arerequired. In this work, we introduce SpatialReasoner-R1, a vision-languagereasoning model designed to address these limitations. To constructhigh-quality supervision for spatial reasoning, we design a Multi-Model MonteCarlo Tree Search (M3CTS) method that generates diverse, logically consistentLong Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we proposefine-grained Direct Preference Optimization (fDPO), which introducessegment-specific preference granularity for descriptive grounding and logicalreasoning, guided by a spatial reward mechanism that evaluates candidateresponses based on visual consistency, spatial grounding, and logicalcoherence. Experimental results demonstrate that fDPO achieves an averageimprovement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets anew SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% inaverage accuracy, while maintaining competitive performance on generalvision-language tasks."}], "temperature": 0.1}}
{"custom_id": "46_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yifan Shen"}], "temperature": 0.1}}
{"custom_id": "47_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance"}], "temperature": 0.1}}
{"custom_id": "47_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The increasing use of large language models (LLMs) in natural languageprocessing (NLP) tasks has sparked significant interest in evaluating theireffectiveness across diverse applications. While models like ChatGPT andDeepSeek have shown strong results in many NLP domains, a comprehensiveevaluation is needed to understand their strengths, weaknesses, anddomain-specific abilities. This is critical as these models are applied tovarious tasks, from sentiment analysis to more nuanced tasks like textualentailment and translation. This study aims to evaluate ChatGPT and DeepSeekacross five key NLP tasks: sentiment analysis, topic classification, textsummarization, machine translation, and textual entailment. A structuredexperimental protocol is used to ensure fairness and minimize variability. Bothmodels are tested with identical, neutral prompts and evaluated on twobenchmark datasets per task, covering domains like news, reviews, andformal/informal texts. The results show that DeepSeek excels in classificationstability and logical reasoning, while ChatGPT performs better in tasksrequiring nuanced understanding and flexibility. These findings providevaluable insights for selecting the appropriate LLM based on task requirements."}], "temperature": 0.1}}
{"custom_id": "47_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Wael Etaiwi"}], "temperature": 0.1}}
{"custom_id": "48_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LOGICPO: Efficient Translation of NL-based Logical Problems to FOL using LLMs and Preference Optimization"}], "temperature": 0.1}}
{"custom_id": "48_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning is a key task for artificial intelligence due to it's rolein major downstream tasks such as Question Answering, Summarization. Recentmethods in improving the reasoning ability of LLMs fall short in correctlyconverting a natural language reasoning problem to an equivalent logicalformulation, which hinders the framework's overall ability to reason. Towardsthis, we propose to use finetuning on a preference optimization dataset tolearn to parse and represent a natural language problem as a whole to aconsistent logical program by 1) introducing a new supervised and preferenceoptimization dataset LogicPO, and 2) adopting popular techniques such as DirectPreference Optimization (DPO), Kahneman-Tversky optimization (KTO) to finetuneopen-source LLMs. Our best model with Phi-3.5 consistently outperformsGPT-3.5-turbo's (8-shot) by producing 10% more logically correct and with 14%less syntax errors. Through the framework and our improved evaluation metrics,we offer a promising direction in improving the logical reasoning of LLMs bybetter representing them in their logical formulations."}], "temperature": 0.1}}
{"custom_id": "48_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Koushik Viswanadha"}], "temperature": 0.1}}
{"custom_id": "49_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE"}], "temperature": 0.1}}
{"custom_id": "49_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have demonstrated some significant capabilitiesacross various domains; however, their effectiveness in spreadsheet relatedtasks remains underexplored. This study introduces a foundation for acomprehensive benchmark framework to evaluate the performance of leading LLMsin executing spreadsheet functions, formula generation and data manipulationtasks. The benchmark encompasses tasks ranging from basic formula creation tocomplex, real world spreadsheet scenarios. Our findings reveal that while LLMsexhibit proficiency in straightforward tasks, they often falter in complex,multi step operations, frequently producing plausible yet incorrect outputs.These results underscore the limitations of current LLMs in handlingspreadsheet tasks that require precise logical reasoning and highlight the needfor integrating symbolic reasoning capabilities into LLM architectures. Tosupport this, we introduce FLARE (Formula Logic, Auditing, Reasoning andEvaluation) a new benchmark for evaluating LLM performance on real-worldspreadsheet logic, auditing, and reasoning tasks."}], "temperature": 0.1}}
{"custom_id": "49_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Simon Thorne"}], "temperature": 0.1}}
{"custom_id": "50_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SLR: Automated Synthesis for Scalable Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "50_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We introduce SLR, an end-to-end framework for systematic evaluation andtraining of Large Language Models (LLMs) via Scalable Logical Reasoning. Givena user's task specification, SLR automatically synthesizes (i) an instructionprompt for an inductive reasoning task, (ii) a validation program, executableon model outputs to provide verifiable rewards, and (iii) the latentground-truth rule. This process is fully automated, scalable, requires no humanannotations, and offers precise control over task difficulty. Using SLR, wecreate SLR-Bench, a benchmark comprising 19k prompts organized into 20curriculum levels that progressively increase in relational, arithmetic, andrecursive complexity. Large-scale evaluation reveals that contemporary LLMsreadily produce syntactically valid rules, yet often fail at correct logicalinference. Recent reasoning LLMs demonstrate improved performance but incurvery high test-time computation, with costs exceeding $300 for just 1,000prompts. Finally, curriculum learning via SLR doubles Llama-3-8B accuracy onSLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction ofcomputational cost. Moreover, these reasoning capabilities generalize to a widerange of established benchmarks, underscoring the effectiveness of SLR fordownstream reasoning."}], "temperature": 0.1}}
{"custom_id": "50_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Lukas Helff"}], "temperature": 0.1}}
{"custom_id": "51_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning in LLMs"}], "temperature": 0.1}}
{"custom_id": "51_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advances in Large Reasoning Models (LRMs) trained with LongChain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domaingeneralization capabilities. However, the underlying mechanisms supporting suchtransfer remain poorly understood. We hypothesize that cross-domaingeneralization arises from shared abstract reasoning prototypes -- fundamentalreasoning patterns that capture the essence of problems across domains. Theseprototypes minimize the nuances of the representation, revealing that seeminglydiverse tasks are grounded in shared reasoning structures.Based on thishypothesis, we propose ProtoReasoning, a framework that enhances the reasoningability of LLMs by leveraging scalable and verifiable prototypicalrepresentations (Prolog for logical reasoning, PDDL forplanning).ProtoReasoning features: (1) an automated prototype constructionpipeline that transforms problems into corresponding prototype representations;(2) a comprehensive verification system providing reliable feedback throughProlog/PDDL interpreters; (3) the scalability to synthesize problemsarbitrarily within prototype space while ensuring correctness. Extensiveexperiments show that ProtoReasoning achieves 4.7% improvement over baselinemodels on logical reasoning (Enigmata-Eval), 6.3% improvement on planningtasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics(AIME24). Significantly, our ablation studies confirm that learning inprototype space also demonstrates enhanced generalization to structurallysimilar problems compared to training solely on natural languagerepresentations, validating our hypothesis that reasoning prototypes serve asthe foundation for generalizable reasoning in large language models."}], "temperature": 0.1}}
{"custom_id": "51_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Feng He"}], "temperature": 0.1}}
{"custom_id": "52_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Discrete JEPA: Learning Discrete Token Representations without Reconstruction"}], "temperature": 0.1}}
{"custom_id": "52_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The cornerstone of cognitive intelligence lies in extracting hidden patternsfrom observations and leveraging these principles to systematically predictfuture outcomes. However, current image tokenization methods demonstratesignificant limitations in tasks requiring symbolic abstraction and logicalreasoning capabilities essential for systematic inference. To address thischallenge, we propose Discrete-JEPA, extending the latent predictive codingframework with semantic tokenization and novel complementary objectives tocreate robust tokenization for symbolic reasoning tasks. Discrete-JEPAdramatically outperforms baselines on visual symbolic prediction tasks, whilestriking visual evidence reveals the spontaneous emergence of deliberatesystematic patterns within the learned semantic token space. Though an initialmodel, our approach promises a significant impact for advancing Symbolic worldmodeling and planning capabilities in artificial intelligence systems."}], "temperature": 0.1}}
{"custom_id": "52_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Junyeob Baek"}], "temperature": 0.1}}
{"custom_id": "53_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "AI-Generated Game Commentary: A Survey and a Datasheet Repository"}], "temperature": 0.1}}
{"custom_id": "53_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "AI-Generated Game Commentary (AIGGC) has gained increasing attention due toits market potential and inherent technical challenges. As a comprehensivemultimodal Natural Language Processing (NLP) task, AIGGC imposes substantialdemands on language models, including factual accuracy, logical reasoning,expressive text generation, generation speed, and context management. In thispaper, we introduce a general framework for AIGGC and present a comprehensivesurvey of 45 existing game commentary dataset and methods according to keychallenges they aim to address in this domain. We further classify and comparevarious evaluation metrics commonly used in this domain. To support futureresearch and benchmarking, we also provide a structured datasheet summarizingthe essential attributes of these datasets in appendix, which is meanwhilepublicly available in an open repository."}], "temperature": 0.1}}
{"custom_id": "53_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Qirui Zheng"}], "temperature": 0.1}}
{"custom_id": "54_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mixture of Cognitive Reasoners: Modular Reasoning with Brain-Like Specialization"}], "temperature": 0.1}}
{"custom_id": "54_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Human intelligence emerges from the interaction of specialized brainnetworks, each dedicated to distinct cognitive functions such as languageprocessing, logical reasoning, social understanding, and memory retrieval.Inspired by this biological observation, we introduce the Mixture of CognitiveReasoners (MiCRo) architecture and training paradigm: a modulartransformer-based language model with a training curriculum that encourages theemergence of functional specialization among different modules. Inspired bystudies in neuroscience, we partition the layers of a pretrained transformermodel into four expert modules, each corresponding to a well-studied cognitivebrain network. Our Brain-Like model has three key benefits over the state ofthe art: First, the specialized experts are highly interpretable andfunctionally critical, where removing a module significantly impairsperformance on domain-relevant benchmarks. Second, our model outperformscomparable baselines that lack specialization on seven reasoning benchmarks.And third, the model's behavior can be steered at inference time by selectivelyemphasizing certain expert modules (e.g., favoring social over logicalreasoning), enabling fine-grained control over the style of its response. Ourfindings suggest that biologically inspired inductive biases involved in humancognition lead to significant modeling gains in interpretability, performance,and controllability."}], "temperature": 0.1}}
{"custom_id": "54_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Badr AlKhamissi"}], "temperature": 0.1}}
{"custom_id": "55_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Breaking Thought Patterns: A Multi-Dimensional Reasoning Framework for LLMs"}], "temperature": 0.1}}
{"custom_id": "55_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) are often constrained by rigid reasoningprocesses, limiting their ability to generate creative and diverse responses.To address this, a novel framework called LADDER is proposed, combiningChain-of-Thought (CoT) reasoning, Mixture of Experts (MoE) models, andmulti-dimensional up/down-sampling strategies which breaks the limitations oftraditional LLMs. First, CoT reasoning guides the model through multi-steplogical reasoning, expanding the semantic space and breaking the rigidity ofthought. Next, MoE distributes the reasoning tasks across multiple expertmodules, each focusing on specific sub-tasks. Finally, dimensionality reductionmaps the reasoning outputs back to a lower-dimensional semantic space, yieldingmore precise and creative responses. Extensive experiments across multipletasks demonstrate that LADDER significantly improves task completion,creativity, and fluency, generating innovative and coherent responses thatoutperform traditional models. Ablation studies reveal the critical roles ofCoT and MoE in enhancing reasoning abilities and creative output. This workcontributes to the development of more flexible and creative LLMs, capable ofaddressing complex and novel tasks."}], "temperature": 0.1}}
{"custom_id": "55_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xintong Tang"}], "temperature": 0.1}}
{"custom_id": "56_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models"}], "temperature": 0.1}}
{"custom_id": "56_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "While large language models have shown reasoning capabilities, theirapplication to the audio modality, particularly in large audio-language models(ALMs), remains significantly underdeveloped. Addressing this gap requires asystematic approach, involving a capable base model, high-qualityreasoning-oriented audio data, and effective training algorithms. In thisstudy, we present a comprehensive solution: we introduce the Audio LogicalReasoning (ALR) dataset, consisting of 6,446 text-audio annotated samplesspecifically designed for complex reasoning tasks. Building on this resource,we propose SoundMind, a rule-based reinforcement learning (RL) algorithmtailored to endow ALMs with deep bimodal reasoning abilities. By trainingQwen2.5-Omni-7B on the ALR dataset using SoundMind, our approach achievesstate-of-the-art performance in audio logical reasoning. This work highlightsthe impact of combining high-quality, reasoning-focused datasets withspecialized RL techniques, advancing the frontier of auditory intelligence inlanguage models. Our code and the proposed dataset are available athttps://github.com/xid32/SoundMind."}], "temperature": 0.1}}
{"custom_id": "56_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xingjian Diao"}], "temperature": 0.1}}
{"custom_id": "57_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "CAPO: Reinforcing Consistent Reasoning in Medical Decision-Making"}], "temperature": 0.1}}
{"custom_id": "57_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In medical visual question answering (Med-VQA), achieving accurate responsesrelies on three critical steps: precise perception of medical imaging data,logical reasoning grounded in visual input and textual questions, and coherentanswer derivation from the reasoning process. Recent advances in generalvision-language models (VLMs) show that large-scale reinforcement learning (RL)could significantly enhance both reasoning capabilities and overall modelperformance. However, their application in medical domains is hindered by twofundamental challenges: 1) misalignment between perceptual understanding andreasoning stages, and 2) inconsistency between reasoning pathways and answergeneration, both compounded by the scarcity of high-quality medical datasetsfor effective large-scale RL. In this paper, we first introduce Med-Zero-17K, acurated dataset for pure RL-based training, encompassing over 30 medical imagemodalities and 24 clinical tasks. Moreover, we propose a novel large-scale RLframework for Med-VLMs, Consistency-Aware Preference Optimization (CAPO), whichintegrates rewards to ensure fidelity between perception and reasoning,consistency in reasoning-to-answer derivation, and rule-based accuracy forfinal responses. Extensive experiments on both in-domain and out-of-domainscenarios demonstrate the superiority of our method over strong VLM baselines,showcasing strong generalization capability to 3D Med-VQA benchmarks andR1-like training paradigms."}], "temperature": 0.1}}
{"custom_id": "57_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Songtao Jiang"}], "temperature": 0.1}}
{"custom_id": "58_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mastering Da Vinci Code: A Comparative Study of Transformer, LLM, and PPO-based Agents"}], "temperature": 0.1}}
{"custom_id": "58_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The Da Vinci Code, a game of logical deduction and imperfect information,presents unique challenges for artificial intelligence, demanding nuancedreasoning beyond simple pattern recognition. This paper investigates theefficacy of various AI paradigms in mastering this game. We develop andevaluate three distinct agent architectures: a Transformer-based baseline modelwith limited historical context, several Large Language Model (LLM) agents(including Gemini, DeepSeek, and GPT variants) guided by structured prompts,and an agent based on Proximal Policy Optimization (PPO) employing aTransformer encoder for comprehensive game history processing. Performance isbenchmarked against the baseline, with the PPO-based agent demonstratingsuperior win rates ($58.5\\% \\pm 1.0\\%$), significantly outperforming the LLMcounterparts. Our analysis highlights the strengths of deep reinforcementlearning in policy refinement for complex deductive tasks, particularly inlearning implicit strategies from self-play. We also examine the capabilitiesand inherent limitations of current LLMs in maintaining strict logicalconsistency and strategic depth over extended gameplay, despite sophisticatedprompting. This study contributes to the broader understanding of AI inrecreational games involving hidden information and multi-step logicalreasoning, offering insights into effective agent design and the comparativeadvantages of different AI approaches."}], "temperature": 0.1}}
{"custom_id": "58_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LeCheng Zhang"}], "temperature": 0.1}}
{"custom_id": "59_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving"}], "temperature": 0.1}}
{"custom_id": "59_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The increasing adoption of artificial intelligence in telecommunications hasraised interest in the capability of Large Language Models (LLMs) to addressdomain-specific, mathematically intensive tasks. Although recent advancementshave improved the performance of LLMs in general mathematical reasoning, theireffectiveness within specialized domains, such as signal processing, networkoptimization, and performance analysis, remains largely unexplored. To addressthis gap, we introduce TeleMath, the first benchmark dataset specificallydesigned to evaluate LLM performance in solving mathematical problems withnumerical solutions in the telecommunications domain. Comprising 500question-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in thetelecommunications field. This paper outlines the proposed QnAs generationpipeline, starting from a selected seed of problems crafted by Subject MatterExperts. The evaluation of a wide range of open-source LLMs reveals that bestperformance on TeleMath is achieved by recent models explicitly designed formathematical or logical reasoning. In contrast, general-purpose models, eventhose with a large number of parameters, often struggle with these challenges.We have released the dataset and the evaluation code to ease resultreproducibility and support future research."}], "temperature": 0.1}}
{"custom_id": "59_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Vincenzo Colle"}], "temperature": 0.1}}
{"custom_id": "60_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation"}], "temperature": 0.1}}
{"custom_id": "60_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advances in large language models, especially in natural languageunderstanding and reasoning, have opened new possibilities for text-to-motiongeneration. Although existing approaches have made notable progress in semanticalignment and motion synthesis, they often rely on end-to-end mappingstrategies that fail to capture deep linguistic structures and logicalreasoning. Consequently, generated motions tend to lack controllability,consistency, and diversity. To address these limitations, we propose Motion-R1,a unified motion-language modeling framework that integrates a Chain-of-Thoughtmechanism. By explicitly decomposing complex textual instructions intologically structured action paths, Motion-R1 provides high-level semanticguidance for motion generation, significantly enhancing the model's ability tointerpret and execute multi-step, long-horizon, and compositionally richcommands. To train our model, we adopt Group Relative Policy Optimization, areinforcement learning algorithm designed for large models, which leveragesmotion quality feedback to optimize reasoning chains and motion synthesisjointly. Extensive experiments across multiple benchmark datasets demonstratethat Motion-R1 achieves competitive or superior performance compared tostate-of-the-art methods, particularly in scenarios requiring nuanced semanticunderstanding and long-term temporal coherence. The code, model and data willbe publicly available."}], "temperature": 0.1}}
{"custom_id": "60_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Runqi Ouyang"}], "temperature": 0.1}}
{"custom_id": "61_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games"}], "temperature": 0.1}}
{"custom_id": "61_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large reasoning models (LRMs) have demonstrated impressive reasoningcapabilities across a broad range of tasks including Olympiad-levelmathematical problems, indicating evidence of their complex reasoningabilities. While many reasoning benchmarks focus on the STEM domain, theability of LRMs to reason correctly in broader task domains remainsunderexplored. In this work, we introduce \\textbf{TTT-Bench}, a new benchmarkthat is designed to evaluate basic strategic, spatial, and logical reasoningabilities in LRMs through a suite of four two-player Tic-Tac-Toe-style gamesthat humans can effortlessly solve from a young age. We propose a simple yetscalable programmatic approach for generating verifiable two-player gameproblems for TTT-Bench. Although these games are trivial for humans, theyrequire reasoning about the intentions of the opponent, as well as the gameboard's spatial configurations, to ensure a win. We evaluate a diverse set ofstate-of-the-art LRMs, and \\textbf{discover that the models that excel at hardmath problems frequently fail at these simple reasoning games}. Further testingreveals that our evaluated reasoning models score on average $\\downarrow$ 41\\%\\& $\\downarrow$ 5\\% lower on TTT-Bench compared to MATH 500 \\& AIME 2024respectively, with larger models achieving higher performance using shorterreasoning traces, where most of the models struggle on long-term strategicreasoning situations on simple and new TTT-Bench tasks."}], "temperature": 0.1}}
{"custom_id": "61_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Prakamya Mishra"}], "temperature": 0.1}}
{"custom_id": "62_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mitigating Spurious Correlations in LLMs via Causality-Aware Post-Training"}], "temperature": 0.1}}
{"custom_id": "62_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "While large language models (LLMs) have demonstrated remarkable capabilitiesin language modeling, recent studies reveal that they often fail onout-of-distribution (OOD) samples due to spurious correlations acquired duringpre-training. Here, we aim to mitigate such spurious correlations throughcausality-aware post-training (CAPT). By decomposing a biased prediction intotwo unbiased steps, known as \\textit{event estimation} and \\textit{eventintervention}, we reduce LLMs' pre-training biases without incurring additionalfine-tuning biases, thus enhancing the model's generalization ability.Experiments on the formal causal inference benchmark CLadder and the logicalreasoning dataset PrOntoQA show that 3B-scale language models fine-tuned withCAPT can outperform both traditional SFT and larger LLMs on in-distribution(ID) and OOD tasks using only 100 ID fine-tuning samples, demonstrating theeffectiveness and sample efficiency of CAPT."}], "temperature": 0.1}}
{"custom_id": "62_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Shurui Gui"}], "temperature": 0.1}}
{"custom_id": "63_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Stronger Language Models Produce More Human-Like Errors"}], "temperature": 0.1}}
{"custom_id": "63_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Do language models converge toward human-like reasoning patterns as theyimprove? We provide surprising evidence that while overall reasoningcapabilities increase with model sophistication, the nature of errorsincreasingly mirrors predictable human reasoning fallacies: a previouslyunobserved inverse scaling phenomenon. To investigate this question, we applythe Erotetic Theory of Reasoning (ETR), a formal cognitive framework withempirical support for predicting human reasoning outcomes. Using theopen-source package PyETR, we generate logical reasoning problems where humanspredictably err, evaluating responses from 38 language models across 383reasoning tasks. Our analysis indicates that as models advance in generalcapability (as measured by Chatbot Arena scores), the proportion of theirincorrect answers that align with ETR-predicted human fallacies tends toincrease ($\\rho = 0.360, p = 0.0265$). Notably, as we observe no correlationbetween model sophistication and logical correctness on these tasks, this shiftin error patterns toward human-likeness occurs independently of error rate.These findings challenge the prevailing view that scaling language modelsnaturally obtains normative rationality, suggesting instead a convergencetoward human-like cognition inclusive of our characteristic biases andlimitations, as we further confirm by demonstrating order-effects in languagemodel reasoning."}], "temperature": 0.1}}
{"custom_id": "63_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Andrew Keenan Richardson"}], "temperature": 0.1}}
{"custom_id": "64_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search"}], "temperature": 0.1}}
{"custom_id": "64_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Small language models (SLMs) offer promising and efficient alternatives tolarge language models (LLMs). However, SLMs' limited capacity restricts theirreasoning capabilities and makes them sensitive to prompt variations. Toaddress these challenges, we propose a novel framework that enhances SLMreasoning capabilities through LLM generated blueprints. The blueprints providestructured, high-level reasoning guides that help SLMs systematically tacklerelated problems. Furthermore, our framework integrates a prompt templatesearch mechanism to mitigate the SLMs' sensitivity to prompt variations. Ourframework demonstrates improved SLM performance across various tasks, includingmath (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improvesthe reasoning capabilities of SLMs without increasing model size or requiringadditional training, offering a lightweight and deployment-friendly solutionfor on-device or resource-constrained environments."}], "temperature": 0.1}}
{"custom_id": "64_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Dongge Han"}], "temperature": 0.1}}
{"custom_id": "65_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "RE-oriented Model Development with LLM Support and Deduction-based Verification"}], "temperature": 0.1}}
{"custom_id": "65_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The requirements engineering (RE) phase is pivotal in developing high-qualitysoftware. Integrating advanced modelling techniques with large language models(LLMs) and formal verification in a logical style can significantly enhancethis process. We propose a comprehensive framework that focuses on specificUnified Modelling Language (UML) diagrams for preliminary system development.This framework offers visualisations at various modelling stages and seamlesslyintegrates large language models and logical reasoning engines. The behaviouralmodels generated with the assistance of LLMs are automatically translated intoformal logical specifications. Deductive formal verification ensures thatlogical requirements and interrelations between software artefacts arethoroughly addressed. Ultimately, the framework facilitates the automaticgeneration of program skeletons, streamlining the transition from design toimplementation."}], "temperature": 0.1}}
{"custom_id": "65_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Radoslaw Klimek"}], "temperature": 0.1}}
{"custom_id": "66_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Language-Vision Planner and Executor for Text-to-Visual Reasoning"}], "temperature": 0.1}}
{"custom_id": "66_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The advancement in large language models (LLMs) and large vision models hasfueled the rapid progress in multi-modal visual-text reasoning capabilities.However, existing vision-language models (VLMs) to date suffer fromgeneralization performance. Inspired by recent development in LLMs for visualreasoning, this paper presents VLAgent, an AI system that can create astep-by-step visual reasoning plan with an easy-to-understand script andexecute each step of the plan in real time by integrating planning script withexecution verifications via an automated process supported by VLAgent. In thetask planning phase, VLAgent fine-tunes an LLM through in-context learning togenerate a step-by-step planner for each user-submitted text-visual reasoningtask. During the plan execution phase, VLAgent progressively refines thecomposition of neuro-symbolic executable modules to generate high-confidencereasoning results. VLAgent has three unique design characteristics: First, weimprove the quality of plan generation through in-context learning, improvinglogic reasoning by reducing erroneous logic steps, incorrect programs, and LLMhallucinations. Second, we design a syntax-semantics parser to identify andcorrect additional logic errors of the LLM-generated planning script prior tolaunching the plan executor. Finally, we employ the ensemble method to improvethe generalization performance of our step-executor. Extensive experiments withfour visual reasoning benchmarks (GQA, MME, NLVR2, VQAv2) show that VLAgentachieves significant performance enhancement for multimodal text-visualreasoning applications, compared to the exiting representative VLMs and LLMbased visual composition approaches like ViperGPT and VisProg, thanks to thenovel optimization modules of VLAgent back-engine (SS-Parser, Plan Repairer,Output Verifiers). Code and data will be made available upon paper acceptance."}], "temperature": 0.1}}
{"custom_id": "66_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yichang Xu"}], "temperature": 0.1}}
{"custom_id": "67_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "EVINET: Towards Open-World Graph Learning via Evidential Reasoning Network"}], "temperature": 0.1}}
{"custom_id": "67_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Graph learning has been crucial to many real-world tasks, but they are oftenstudied with a closed-world assumption, with all possible labels of data knowna priori. To enable effective graph learning in an open and noisy environment,it is critical to inform the model users when the model makes a wrongprediction to in-distribution data of a known class, i.e., misclassificationdetection or when the model encounters out-of-distribution from novel classes,i.e., out-of-distribution detection. This paper introduces Evidential ReasoningNetwork (EVINET), a framework that addresses these two challenges byintegrating Beta embedding within a subjective logic framework. EVINET includestwo key modules: Dissonance Reasoning for misclassification detection andVacuity Reasoning for out-of-distribution detection. Extensive experimentsdemonstrate that EVINET outperforms state-of-the-art methods across multiplemetrics in the tasks of in-distribution classification, misclassificationdetection, and out-of-distribution detection. EVINET demonstrates the necessityof uncertainty estimation and logical reasoning for misclassification detectionand out-of-distribution detection and paves the way for open-world graphlearning. Our code and data are available at https://github.com/SSSKJ/EviNET."}], "temperature": 0.1}}
{"custom_id": "67_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Weijie Guan"}], "temperature": 0.1}}
{"custom_id": "68_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "KnowCoder-V2: Deep Knowledge Analysis"}], "temperature": 0.1}}
{"custom_id": "68_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Deep knowledge analysis tasks always involve the systematic extraction andassociation of knowledge from large volumes of data, followed by logicalreasoning to discover insights. However, to solve such complex tasks, existingdeep research frameworks face three major challenges: 1) They lack systematicorganization and management of knowledge; 2) They operate purely online, makingit inefficient for tasks that rely on shared and large-scale knowledge; 3) Theycannot perform complex knowledge computation, limiting their abilities toproduce insightful analytical results. Motivated by these, in this paper, wepropose a \\textbf{K}nowledgeable \\textbf{D}eep \\textbf{R}esearch (\\textbf{KDR})framework that empowers deep research with deep knowledge analysis capability.Specifically, it introduces an independent knowledge organization phase topreprocess large-scale, domain-relevant data into systematic knowledge offline.Based on this knowledge, it extends deep research with an additional kind ofreasoning steps that perform complex knowledge computation in an online manner.To enhance the abilities of LLMs to solve knowledge analysis tasks in the aboveframework, we further introduce \\textbf{\\KCII}, an LLM that bridges knowledgeorganization and reasoning via unified code generation. For knowledgeorganization, it generates instantiation code for predefined classes,transforming data into knowledge objects. For knowledge computation, itgenerates analysis code and executes on the above knowledge objects to obtaindeep analysis results. Experimental results on more than thirty datasets acrosssix knowledge analysis tasks demonstrate the effectiveness of \\KCII. Moreover,when integrated into the KDR framework, \\KCII can generate high-quality reportswith insightful analytical results compared to the mainstream deep researchframework."}], "temperature": 0.1}}
{"custom_id": "68_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zixuan Li"}], "temperature": 0.1}}
{"custom_id": "69_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Cross-lingual Collapse: How Language-Centric Foundation Models Shape Reasoning in Large Language Models"}], "temperature": 0.1}}
{"custom_id": "69_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We identify \\textbf{Cross-lingual Collapse}, a systematic drift in which thechain-of-thought (CoT) of a multilingual language model reverts to its dominantpre-training language even when the prompt is expressed in a differentlanguage. Recent large language models (LLMs) with reinforcement learning withverifiable reward (RLVR) have achieved strong logical reasoning performances byexposing their intermediate reasoning traces, giving rise to large reasoningmodels (LRMs). However, the mechanism behind multilingual reasoning in LRMs isnot yet fully explored. To investigate the issue, we fine-tune multilingualLRMs with Group-Relative Policy Optimization (GRPO) on translated versions ofthe GSM$8$K and SimpleRL-Zoo datasets in three different languages: Chinese,Korean, and Ukrainian. During training, we monitor both task accuracy andlanguage consistency of the reasoning chains. Our experiments reveal three keyfindings: (i) GRPO rapidly amplifies pre-training language imbalances, leadingto the erosion of low-resource languages within just a few hundred updates;(ii) language consistency reward mitigates this drift but does so at theexpense of an almost 5 - 10 pp drop in accuracy. and (iii) the resultinglanguage collapse is severely damaging and largely irreversible, as subsequentfine-tuning struggles to steer the model back toward its originaltarget-language reasoning capabilities. Together, these findings point to aremarkable conclusion: \\textit{not all languages are trained equally forreasoning}. Furthermore, our paper sheds light on the roles of reward shaping,data difficulty, and pre-training priors in eliciting multilingual reasoning."}], "temperature": 0.1}}
{"custom_id": "69_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Cheonbok Park"}], "temperature": 0.1}}
{"custom_id": "70_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study"}], "temperature": 0.1}}
{"custom_id": "70_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning is a core capability for many applications of largelanguage models (LLMs), yet existing benchmarks often rely solely onfinal-answer accuracy, failing to capture the quality and structure of thereasoning process. We propose FineLogic, a fine-grained evaluation frameworkthat assesses logical reasoning across three dimensions: overall benchmarkaccuracy, stepwise soundness, and representation-level alignment. In addition,to better understand how reasoning capabilities emerge, we conduct acomprehensive study on the effects of supervision format during fine-tuning. Weconstruct four supervision styles (one natural language and three symbolicvariants) and train LLMs under each. Our findings reveal that natural languagesupervision yields strong generalization even on out-of-distribution andlong-context tasks, while symbolic reasoning styles promote more structurallysound and atomic inference chains. Further, our representation-level probingshows that fine-tuning primarily improves reasoning behaviors throughstep-by-step generation, rather than enhancing shortcut prediction orinternalized correctness. Together, our framework and analysis provide a morerigorous and interpretable lens for evaluating and improving logical reasoningin LLMs."}], "temperature": 0.1}}
{"custom_id": "70_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yujun Zhou"}], "temperature": 0.1}}
{"custom_id": "71_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Are LLMs Reliable Translators of Logical Reasoning Across Lexically Diversified Contexts?"}], "temperature": 0.1}}
{"custom_id": "71_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neuro-symbolic approaches combining large language models (LLMs) with solversexcels in logical reasoning problems need long reasoning chains. In thisparadigm, LLMs serve as translators, converting natural language reasoningproblems into formal logic formulas. Then reliable symbolic solvers returncorrect solutions. Despite their success, we find that LLMs, as translators,struggle to handle lexical diversification, a common linguistic phenomenon,indicating that LLMs as logic translators are unreliable in real-worldscenarios. Moreover, existing logical reasoning benchmarks lack lexicaldiversity, failing to challenge LLMs' ability to translate such text and thusobscuring this issue. In this work, we propose SCALe, a benchmark designed toaddress this significant gap through **logic-invariant lexicaldiversification**. By using LLMs to transform original benchmark datasets intolexically diversified but logically equivalent versions, we evaluate LLMs'ability to consistently map diverse expressions to uniform logical symbols onthese new datasets. Experiments using SCALe further confirm that current LLMsexhibit deficiencies in this capability. Building directly on the deficienciesidentified through our benchmark, we propose a new method, MenTaL, to addressthis limitation. This method guides LLMs to first construct a table unifyingdiverse expressions before performing translation. Applying MenTaL throughin-context learning and supervised fine-tuning (SFT) significantly improves theperformance of LLM translators on lexically diversified text. Our code is nowavailable at https://github.com/wufeiwuwoshihua/LexicalDiver."}], "temperature": 0.1}}
{"custom_id": "71_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Qingchuan Li"}], "temperature": 0.1}}
{"custom_id": "72_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Around the World in 24 Hours: Probing LLM Knowledge of Time and Place"}], "temperature": 0.1}}
{"custom_id": "72_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning over time and space is essential for understanding our world.However, the abilities of language models in this area are largely unexploredas previous work has tested their abilities for logical reasoning in terms oftime and space in isolation or only in simple or artificial environments. Inthis paper, we present the first evaluation of the ability of language modelsto jointly reason over time and space. To enable our analysis, we createGeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37time zones. Using GeoTemp, we evaluate eight open chat models of threedifferent model families for different combinations of temporal and geographicknowledge. We find that most models perform well on reasoning tasks involvingonly temporal knowledge and that overall performance improves with scale.However, performance remains constrained in tasks that require connectingtemporal and geographical information. We do not find clear correlations ofperformance with specific geographic regions. Instead, we find a significantperformance increase for location names with low model perplexity, suggestingtheir repeated occurrence during model training. We further demonstrate thattheir performance is heavily influenced by prompt formulation - a directinjection of geographical knowledge leads to performance gains, whereas,surprisingly, techniques like chain-of-thought prompting decrease performanceon simpler tasks."}], "temperature": 0.1}}
{"custom_id": "72_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Carolin Holtermann"}], "temperature": 0.1}}
{"custom_id": "73_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Toward Entailment Checking: Explore Eigenmarking Search"}], "temperature": 0.1}}
{"custom_id": "73_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logic entailment is essential to reasoning,  but entailment checking has the worst-case complexity of an exponential ofthe variable size.  With recent development, quantum computing when mature  may allow an effective approach for various combinatorial problems,  including entailment checking.  Grover algorithm uses Grover operations, selective phase inversion andamplitude amplification  to address a search over unstructured data with quadratic improvement from aclassical method.  Its original form is intended to a single-winner scenario: exactly one matchis promised.  Its extension to multiple-winner cases employs probabilistic control over  a number of applications of Grover operations, while a no-winner case ishandled by time-out.  Our study explores various schemes of ``eigenmarking'' approach.  Still relying on Grover operations, but the approach introduces additionalqubits  to tag the eigenstates.  The tagged eigenstates are to facilitate an interpretation of the measuredresults and  enhance identification of a no-winner case (related to no logic violation inentailment context).  Our investigation experiments three variations of eigenmarking on a two-qubitsystem  using an IBM Aer simulator.  The results show strong distinguishability in all schemes with the bestrelative distinguishabilities of 19 and 53 in worst case  and in average case, respectively.  Our findings reveal a viable quantum mechanism to differentiate a no-winnercase  from other scenarios, which could play a pivot role in entailment checkingand logic reasoning in general."}], "temperature": 0.1}}
{"custom_id": "73_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Tatpong Katanyukul"}], "temperature": 0.1}}
{"custom_id": "74_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks"}], "temperature": 0.1}}
{"custom_id": "74_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have demonstrated remarkable performance acrossvarious tasks by effectively utilizing a prompting strategy. However, they arehighly sensitive to input perturbations, such as typographical errors or slightcharacter order errors, which can substantially degrade their performance.Despite advances in prompting techniques, developing a prompting strategy thatexplicitly mitigates the negative impact of such perturbations remains an openchallenge. To bridge this gap, we propose Robustness of Prompting (RoP), anovel prompting strategy specifically designed to enhance the robustness ofLLMs. RoP consists of two stages: Error Correction and Guidance. In the ErrorCorrection stage, RoP applies diverse perturbation methods to generateadversarial examples, which are then used to construct prompts thatautomatically correct input errors. In the Guidance stage, RoP generates anoptimal guidance prompting based on the corrected input, steering the modeltoward more robust and accurate inferences. Through comprehensive experimentsspanning arithmetic, commonsense, and logical reasoning tasks, we demonstratethat RoP significantly improves LLMs' robustness against adversarialperturbations. Notably, it maintains model accuracy with only minimaldegradation compared to clean input scenarios, thereby establishing RoP as apractical and effective approach for enhancing LLM robustness in real-worldapplications."}], "temperature": 0.1}}
{"custom_id": "74_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Lin Mu"}], "temperature": 0.1}}
{"custom_id": "75_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem"}], "temperature": 0.1}}
{"custom_id": "75_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possessimmense reasoning potential inherited from the pre-training stage. Withreinforcement learning (RL), these models can improve dramatically on reasoningtasks. Recent studies have shown that even RL on a single problem can unleashthese models' reasoning capabilities. However, RL is not only expensive butalso unstable. Even one-shot RL requires hundreds of GPU hours. This raises acritical question: Is there a more efficient way to unleash the reasoningpotential of these powerful base LLMs? In this work, we demonstrate thatCritique Fine-Tuning (CFT) on only one problem can effectively unleash thereasoning potential of LLMs. Our method constructs critique data by collectingdiverse model-generated solutions to a single problem and using teacher LLMs toprovide detailed critiques. We fine-tune Qwen and Llama family models, rangingfrom 1.5B to 14B parameters, on the CFT data and observe significantperformance gains across diverse reasoning tasks. For example, with just 5 GPUhours of training, Qwen-Math-7B-CFT show an average improvement of 15% on sixmath benchmarks and 16% on three logic reasoning benchmarks. These results arecomparable to or even surpass the results from RL with 20x less compute.Ablation studies reveal the robustness of one-shot CFT across different promptproblems. These results highlight one-shot CFT as a simple, general, andcompute-efficient approach to unleashing the reasoning capabilities of modernLLMs."}], "temperature": 0.1}}
{"custom_id": "75_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yubo Wang"}], "temperature": 0.1}}
{"custom_id": "76_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Towards Geometry Problem Solving in the Large Model Era: A Survey"}], "temperature": 0.1}}
{"custom_id": "76_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Geometry problem solving (GPS) represents a critical frontier in artificialintelligence, with profound applications in education, computer-aided design,and computational graphics. Despite its significance, automating GPS remainschallenging due to the dual demands of spatial understanding and rigorouslogical reasoning. Recent advances in large models have enabled notablebreakthroughs, particularly for SAT-level problems, yet the field remainsfragmented across methodologies, benchmarks, and evaluation frameworks. Thissurvey systematically synthesizes GPS advancements through three coredimensions: (1) benchmark construction, (2) textual and diagrammatic parsing,and (3) reasoning paradigms. We further propose a unified analytical paradigm,assess current limitations, and identify emerging opportunities to guide futureresearch toward human-level geometric reasoning, including automated benchmarkgeneration and interpretable neuro-symbolic integration."}], "temperature": 0.1}}
{"custom_id": "76_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yurui Zhao"}], "temperature": 0.1}}
{"custom_id": "77_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "MDPO: Multi-Granularity Direct Preference Optimization for Mathematical Reasoning"}], "temperature": 0.1}}
{"custom_id": "77_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mathematical reasoning presents a significant challenge for Large LanguageModels (LLMs) as it requires ensuring the correctness of each reasoning step.Researchers have been strengthening the mathematical reasoning abilities ofLLMs through supervised fine-tuning, but due to the inability to suppressincorrect outputs, illusions can easily arise. Recently, Direct PreferenceOptimization (DPO) has been widely adopted for aligning human intent by usingpreference data to prevent LLMs from generating incorrect outputs. However, ithas shown limited benefits in long-chain mathematical reasoning, mainly becauseDPO struggles to effectively capture the differences between accepted andrejected answers from preferences in long-chain data. The inconsistency betweenDPO training and LLMs' generation metrics also affects the effectiveness ofsuppressing incorrect outputs. We propose the Multi-Granularity DirectPreference Optimization (MDPO) method, optimizing the mathematical reasoning ofLLMs at three granularities: Solution2Solution, Inference2Inference, andStep2Step. Solution2Solution focuses on the correctness of entire long-chainreasoning; Inference2Inference concentrates on logical reasoning between steps;Step2Step corrects computational errors in steps, enhancing the computationalcapabilities of LLMs. Additionally, we unify the training objectives of thethree granularities to align with the generation metrics. We conductedexperiments on the open-source models Qwen2 and Llama3, achieving improvementsof 1.7% and 0.9% on the GSM8K dataset, and 2.3% and 1.2% on the MATH dataset,outperforming DPO and other DPO variant methods. Furthermore, we also provide apipeline for constructing MDPO training data that is simple and does notrequire manual annotation costs."}], "temperature": 0.1}}
{"custom_id": "77_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yunze Lin"}], "temperature": 0.1}}
{"custom_id": "78_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL"}], "temperature": 0.1}}
{"custom_id": "78_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Vision language models (VLMs) are expected to perform effective multimodalreasoning and make logically coherent decisions, which is critical to taskssuch as diagram understanding and spatial problem solving. However, current VLMreasoning lacks large-scale and well-structured training datasets. To bridgethis gap, we propose VisualSphinx, a first-of-its-kind large-scale syntheticvisual logical reasoning training data. To tackle the challenge of imagesynthesis with grounding answers, we propose a rule-to-image synthesispipeline, which extracts and expands puzzle rules from seed questions andgenerates the code of grounding synthesis image synthesis for puzzle sampleassembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinxbenefit from logical coherence and readability of our dataset and exhibitimproved performance on logical reasoning tasks. The enhanced reasoningcapabilities developed from VisualSphinx also benefit other reasoning taskssuch as algebraic reasoning, arithmetic reasoning and geometry reasoning."}], "temperature": 0.1}}
{"custom_id": "78_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yichen Feng"}], "temperature": 0.1}}
{"custom_id": "79_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Continuous Chain of Thought Enables Parallel Exploration and Reasoning"}], "temperature": 0.1}}
{"custom_id": "79_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Current language models generate chain-of-thought traces by autoregressivelysampling tokens from a finite vocabulary. While this discrete sampling hasachieved remarkable success, conducting chain-of-thought withcontinuously-valued tokens (CoT2) offers a richer and more expressivealternative. Our work examines the benefits of CoT2 through logical reasoningtasks that inherently require search capabilities and provide optimization andexploration methods for CoT2. Theoretically, we show that CoT2 allows the modelto track multiple traces in parallel and quantify its benefits for inferenceefficiency. Notably, one layer transformer equipped with CoT2 can provablysolve the combinatorial \"subset sum problem\" given sufficient embeddingdimension. These insights lead to a novel and effective supervision strategywhere we match the softmax outputs to the empirical token distributions of aset of target traces. Complementing this, we introduce sampling strategies thatunlock policy optimization and self-improvement for CoT2. Our first strategysamples and composes $K$ discrete tokens at each decoding step to control thelevel of parallelism, and reduces to standard CoT when $K=1$. Our secondstrategy relies on continuous exploration over the probability simplex.Experiments confirm that policy optimization with CoT2 indeed improves theperformance of the model beyond its initial discrete or continuous supervision."}], "temperature": 0.1}}
{"custom_id": "79_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Halil Alperen Gozeten"}], "temperature": 0.1}}
{"custom_id": "80_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models"}], "temperature": 0.1}}
{"custom_id": "80_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advancements in large language models (LLMs) have demonstratedsubstantial progress in reasoning capabilities, such as DeepSeek-R1, whichleverages rule-based reinforcement learning to enhance logical reasoningsignificantly. However, extending these achievements to multimodal largelanguage models (MLLMs) presents critical challenges, which are frequently morepronounced for Multimodal Small Language Models (MSLMs) given their typicallyweaker foundational reasoning abilities: (1) the scarcity of high-qualitymultimodal reasoning datasets, (2) the degradation of reasoning capabilitiesdue to the integration of visual processing, and (3) the risk that directapplication of reinforcement learning may produce complex yet incorrectreasoning processes. To address these challenges, we design a novel frameworkInfi-MMR to systematically unlock the reasoning potential of MSLMs through acurriculum of three carefully structured phases and propose our multimodalreasoning model Infi-MMR-3B. The first phase, Foundational ReasoningActivation, leverages high-quality textual reasoning datasets to activate andstrengthen the model's logical reasoning capabilities. The second phase,Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data tofacilitate the progressive transfer of reasoning skills to multimodal contexts.The third phase, Multimodal Reasoning Enhancement, employs curated,caption-free multimodal data to mitigate linguistic biases and promote robustcross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodalmath reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVisiontest, and 21.33% on OlympiadBench) and general reasoning ability (67.2% onMathVista testmini). Resources are available athttps://huggingface.co/Reallm-Labs/Infi-MMR-3B."}], "temperature": 0.1}}
{"custom_id": "80_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zeyu Liu"}], "temperature": 0.1}}
{"custom_id": "81_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Climate Finance Bench"}], "temperature": 0.1}}
{"custom_id": "81_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Climate Finance Bench introduces an open benchmark that targetsquestion-answering over corporate climate disclosures using Large LanguageModels. We curate 33 recent sustainability reports in English drawn fromcompanies across all 11 GICS sectors and annotate 330 expert-validatedquestion-answer pairs that span pure extraction, numerical reasoning, andlogical reasoning. Building on this dataset, we propose a comparison of RAG(retrieval-augmented generation) approaches. We show that the retriever'sability to locate passages that actually contain the answer is the chiefperformance bottleneck. We further argue for transparent carbon reporting inAI-for-climate applications, highlighting advantages of techniques such asWeight Quantization."}], "temperature": 0.1}}
{"custom_id": "81_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Rafik Mankour"}], "temperature": 0.1}}
{"custom_id": "82_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?"}], "temperature": 0.1}}
{"custom_id": "82_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) demonstrate impressive reasoning capabilities infamiliar contexts, but struggle when the context conflicts with theirparametric knowledge. To investigate this phenomenon, we introduceCounterLogic, a dataset containing 1,800 examples across 9 logical schemas,explicitly designed to evaluate logical reasoning through counterfactual(hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11LLMs across 6 different datasets reveals a consistent performance degradation,with accuracies dropping by 27% on average when reasoning throughcounterfactual information. We propose Self-Segregate, a prompting methodenabling metacognitive awareness (explicitly identifying knowledge conflicts)before reasoning. Our method dramatically narrows the average performance gapsfrom 27% to just 11%, while significantly increasing the overall accuracy(+7.5%). We discuss the implications of these findings and draw parallels tohuman cognitive processes, particularly on how humans disambiguate conflictinginformation during reasoning tasks. Our findings offer practical insights forunderstanding and enhancing LLMs reasoning capabilities in real-worldapplications, especially where models must logically reason independently oftheir factual knowledge."}], "temperature": 0.1}}
{"custom_id": "82_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ishwar B Balappanawar"}], "temperature": 0.1}}
{"custom_id": "83_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Structured Unplugged Approach for Foundational AI Literacy in Primary Education"}], "temperature": 0.1}}
{"custom_id": "83_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Younger generations are growing up in a world increasingly shaped byintelligent technologies, making early AI literacy crucial for developing theskills to critically understand and navigate them. However, education in thisfield often emphasizes tool-based learning, prioritizing usage overunderstanding the underlying concepts. This lack of knowledge leavesnon-experts, especially children, prone to misconceptions, unrealisticexpectations, and difficulties in recognizing biases and stereotypes. In thispaper, we propose a structured and replicable teaching approach that fostersfoundational AI literacy in primary students, by building upon coremathematical elements closely connected to and of interest in primarycurricula, to strengthen conceptualization, data representation, classificationreasoning, and evaluation of AI. To assess the effectiveness of our approach,we conducted an empirical study with thirty-one fifth-grade students across twoclasses, evaluating their progress through a post-test and a satisfactionsurvey. Our results indicate improvements in terminology understanding andusage, features description, logical reasoning, and evaluative skills, withstudents showing a deeper comprehension of decision-making processes and theirlimitations. Moreover, the approach proved engaging, with students particularlyenjoying activities that linked AI concepts to real-world reasoning. Materials:https://github.com/tail-unica/ai-literacy-primary-ed."}], "temperature": 0.1}}
{"custom_id": "83_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Maria Cristina Carrisi"}], "temperature": 0.1}}
{"custom_id": "84_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs"}], "temperature": 0.1}}
{"custom_id": "84_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning is a fundamental aspect of human intelligence and anessential capability for multimodal large language models (MLLMs). Despite thesignificant advancement in multimodal reasoning, existing benchmarks fail tocomprehensively evaluate their reasoning abilities due to the lack of explicitcategorization for logical reasoning types and an unclear understanding ofreasoning. To address these issues, we introduce MME-Reasoning, a comprehensivebenchmark designed to evaluate the reasoning ability of MLLMs, which covers allthree types of reasoning (i.e., inductive, deductive, and abductive) in itsquestions. We carefully curate the data to ensure that each questioneffectively evaluates reasoning ability rather than perceptual skills orknowledge breadth, and extend the evaluation protocols to cover the evaluationof diverse questions. Our evaluation reveals substantial limitations ofstate-of-the-art MLLMs when subjected to holistic assessments of logicalreasoning capabilities. Even the most advanced MLLMs show limited performancein comprehensive logical reasoning, with notable performance imbalances acrossreasoning types. In addition, we conducted an in-depth analysis of approachessuch as ``thinking mode'' and Rule-based RL, which are commonly believed toenhance reasoning abilities. These findings highlight the critical limitationsand performance imbalances of current MLLMs in diverse logical reasoningscenarios, providing comprehensive and systematic insights into theunderstanding and evaluation of reasoning capabilities."}], "temperature": 0.1}}
{"custom_id": "84_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jiakang Yuan"}], "temperature": 0.1}}
{"custom_id": "85_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation"}], "temperature": 0.1}}
{"custom_id": "85_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Vision-and-Language Navigation (VLN) requires the agent to navigate byfollowing natural instructions under partial observability, making it difficultto align perception with language. Recent methods mitigate this by imaginingfuture scenes, yet they rely on vision-based synthesis, leading to highcomputational cost and redundant details. To this end, we propose to adaptivelyimagine key environmental semantics via \\textit{language} form, enabling a morereliable and efficient strategy. Specifically, we introduce a novel AdaptiveText Dreamer (ATD), a dual-branch self-guided imagination policy built upon alarge language model (LLM). ATD is designed with a human-like left-right brainarchitecture, where the left brain focuses on logical integration, and theright brain is responsible for imaginative prediction of future scenes. Toachieve this, we fine-tune only the Q-former within both brains to efficientlyactivate domain-specific knowledge in the LLM, enabling dynamic updates oflogical reasoning and imagination during navigation. Furthermore, we introducea cross-interaction mechanism to regularize the imagined outputs and injectthem into a navigation expert module, allowing ATD to jointly exploit both thereasoning capacity of the LLM and the expertise of the navigation model. Weconduct extensive experiments on the R2R benchmark, where ATD achievesstate-of-the-art performance with fewer parameters. The code is\\href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}."}], "temperature": 0.1}}
{"custom_id": "85_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Pingrui Zhang"}], "temperature": 0.1}}
{"custom_id": "86_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Step-Wise Formal Verification for LLM-Based Mathematical Problem Solving"}], "temperature": 0.1}}
{"custom_id": "86_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have demonstrated formidable capabilities insolving mathematical problems, yet they may still commit logical reasoning andcomputational errors during the problem-solving process. Thus, this paperproposes a framework, MATH-VF, which includes a Formalizer and a Critic, forformally verifying the correctness of the solutions generated by large languagemodels. Our framework first utilizes a Formalizer which employs an LLM totranslate a natural language solution into a formal context. Afterward, ourCritic (which integrates various external tools such as a Computer AlgebraSystem and an SMT solver) evaluates the correctness of each statement withinthe formal context, and when a statement is incorrect, our Critic providescorrective feedback. We empirically investigate the effectiveness of MATH-VF intwo scenarios: 1) Verification: MATH-VF is utilized to determine thecorrectness of a solution to a given problem. 2) Refinement: When MATH-VFidentifies errors in the solution generated by an LLM-based solution generatorfor a given problem, it submits the corrective suggestions proposed by theCritic to the solution generator to regenerate the solution. We evaluate ourframework on widely used mathematical benchmarks: MATH500 and ProcessBench,demonstrating the superiority of our approach over existing approaches."}], "temperature": 0.1}}
{"custom_id": "86_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Kuo Zhou"}], "temperature": 0.1}}
{"custom_id": "87_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SV-TrustEval-C: Evaluating Structure and Semantic Reasoning in Large Language Models for Source Code Vulnerability Analysis"}], "temperature": 0.1}}
{"custom_id": "87_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "As Large Language Models (LLMs) evolve in understanding and generating code,accurately evaluating their reliability in analyzing source codevulnerabilities becomes increasingly vital. While studies have examined LLMcapabilities in tasks like vulnerability detection and repair, they oftenoverlook the importance of both structure and semantic reasoning crucial fortrustworthy vulnerability analysis. To address this gap, we introduceSV-TrustEval-C, a benchmark designed to evaluate LLMs' abilities forvulnerability analysis of code written in the C programming language throughtwo key dimensions: structure reasoning - assessing how models identifyrelationships between code elements under varying data and control flowcomplexities; and semantic reasoning - examining their logical consistency inscenarios where code is structurally and semantically perturbed. Our resultsshow that current LLMs are far from satisfactory in understanding complex coderelationships and that their vulnerability analyses rely more on patternmatching than on robust logical reasoning. These findings underscore theeffectiveness of the SV-TrustEval-C benchmark and highlight critical areas forenhancing the reasoning capabilities and trustworthiness of LLMs in real-worldvulnerability analysis tasks. Our initial benchmark dataset is publiclyavailable."}], "temperature": 0.1}}
{"custom_id": "87_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yansong Li"}], "temperature": 0.1}}
{"custom_id": "88_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision"}], "temperature": 0.1}}
{"custom_id": "88_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have shown promising performance in mathematicaland logical reasoning benchmarks. However, recent studies have pointed tomemorization, rather than generalization, as one of the leading causes for suchperformance. LLMs, in fact, are susceptible to content variations,demonstrating a lack of robust symbolic abstractions supporting their reasoningprocess. To improve reliability, many attempts have been made to combine LLMswith symbolic methods. Nevertheless, existing approaches fail to effectivelyleverage symbolic representations due to the challenges involved in developingreliable and scalable verification mechanisms. In this paper, we propose toovercome such limitations by generating symbolic reasoning trajectories andselect the high-quality ones using a process reward model automatically tunedbased on Monte Carlo estimation. The trajectories are then employed viafine-tuning methods to improve logical reasoning and generalization. Ourresults on logical reasoning benchmarks such as FOLIO and LogicAsker show theeffectiveness of the proposed method with large gains on frontier andopen-weight models. Moreover, additional experiments on claim verificationreveal that fine-tuning on the generated symbolic reasoning trajectoriesenhances out-of-domain generalizability, suggesting the potential impact ofsymbolically-guided process supervision in alleviating the effect ofmemorization on LLM reasoning."}], "temperature": 0.1}}
{"custom_id": "88_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xingwei Tan"}], "temperature": 0.1}}
{"custom_id": "89_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"}], "temperature": 0.1}}
{"custom_id": "89_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The rapid evolution of large language models in natural language processinghas substantially elevated their semantic understanding and logical reasoningcapabilities. Such proficiencies have been leveraged in autonomous drivingsystems, contributing to significant improvements in system performance. Modelssuch as OpenAI o1 and DeepSeek-R1, leverage Chain-of-Thought (CoT) reasoning,an advanced cognitive method that simulates human thinking processes,demonstrating remarkable reasoning capabilities in complex tasks. Bystructuring complex driving scenarios within a systematic reasoning framework,this approach has emerged as a prominent research focus in autonomous driving,substantially improving the system's ability to handle challenging cases. Thispaper investigates how CoT methods improve the reasoning abilities ofautonomous driving models. Based on a comprehensive literature review, wepresent a systematic analysis of the motivations, methodologies, challenges,and future research directions of CoT in autonomous driving. Furthermore, wepropose the insight of combining CoT with self-learning to facilitateself-evolution in driving systems. To ensure the relevance and timeliness ofthis study, we have compiled a dynamic repository of literature and open-sourceprojects, diligently updated to incorporate forefront developments. Therepository is publicly available athttps://github.com/cuiyx1720/Awesome-CoT4AD."}], "temperature": 0.1}}
{"custom_id": "89_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yixin Cui"}], "temperature": 0.1}}
{"custom_id": "90_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "CP-Router: An Uncertainty-Aware Router Between LLM and LRM"}], "temperature": 0.1}}
{"custom_id": "90_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advances in Large Reasoning Models (LRMs) have significantly improvedlong-chain reasoning capabilities over Large Language Models (LLMs). However,LRMs often produce unnecessarily lengthy outputs even for simple queries,leading to inefficiencies or even accuracy degradation compared to LLMs. Toovercome this, we propose CP-Router, a training-free and model-agnostic routingframework that dynamically selects between an LLM and an LRM, demonstrated withmultiple-choice question answering (MCQA) prompts. The routing decision isguided by the prediction uncertainty estimates derived via Conformal Prediction(CP), which provides rigorous coverage guarantees. To further refine theuncertainty differentiation across inputs, we introduce Full and Binary Entropy(FBE), a novel entropy-based criterion that adaptively selects the appropriateCP threshold. Experiments across diverse MCQA benchmarks, includingmathematics, logical reasoning, and Chinese chemistry, demonstrate thatCP-Router efficiently reduces token usage while maintaining or even improvingaccuracy compared to using LRM alone. We also extend CP-Router to diverse modelpairings and open-ended QA, where it continues to demonstrate strongperformance, validating its generality and robustness."}], "temperature": 0.1}}
{"custom_id": "90_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jiayuan Su"}], "temperature": 0.1}}
{"custom_id": "91_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles"}], "temperature": 0.1}}
{"custom_id": "91_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel atadvanced reasoning tasks like math and coding via Reinforcement Learning withVerifiable Rewards (RLVR), but still struggle with puzzles solvable by humanswithout domain knowledge. We introduce Enigmata, the first comprehensive suitetailored for improving LLMs with puzzle reasoning skills. It includes 36 tasksacross seven categories, each with 1) a generator that produces unlimitedexamples with controllable difficulty and 2) a rule-based verifier forautomatic evaluation. This generator-verifier design supports scalable,multi-task RL training, fine-grained analysis, and seamless RLVR integration.We further propose Enigmata-Eval, a rigorous benchmark, and develop optimizedmulti-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarkslike Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizeswell to out-of-domain puzzle benchmarks and mathematical reasoning, with littlemulti-tasking trade-off. When trained on larger models like Seed1.5-Thinking(20B activated parameters and 200B total parameters), puzzle data from Enigmatafurther boosts SoTA performance on advanced math and STEM reasoning tasks suchas AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalizationbenefits of Enigmata. This work offers a unified, controllable framework foradvancing logical reasoning in LLMs. Resources of this work can be found athttps://seed-enigmata.github.io."}], "temperature": 0.1}}
{"custom_id": "91_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jiangjie Chen"}], "temperature": 0.1}}
{"custom_id": "92_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants"}], "temperature": 0.1}}
{"custom_id": "92_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Proprietary giants are increasingly dominating the race for ever-largerlanguage models. Can open-source, smaller models remain competitive across abroad range of tasks? In this paper, we present the Avengers -- a simple recipethat leverages the collective intelligence of these smaller models. TheAvengers builds upon four lightweight operations: (i) embedding: encode queriesusing a text embedding model; (ii) clustering: group queries based on theirsemantic similarity; (iii) scoring: scores each model's performance within eachcluster; and (iv) voting: improve outputs via repeated sampling and voting. Atinference time, each query is embedded and assigned to its nearest cluster. Thetop-performing model(s) within that cluster are selected to generate theresponse with repeated sampling. Remarkably, with 10 open-source models (~7Bparameters each), the Avengers surpasses GPT-4o, 4.1, and 4.5 in averageperformance across 15 diverse datasets spanning mathematics, coding, logicalreasoning, general knowledge, and affective tasks. In particular, it surpassesGPT-4.1 on mathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore,the Avengers delivers superior out-of-distribution generalization, and remainsrobust across various embedding models, clustering algorithms, ensemblestrategies, and values of its sole parameter -- the number of clusters."}], "temperature": 0.1}}
{"custom_id": "92_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yiqun Zhang"}], "temperature": 0.1}}
{"custom_id": "93_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models for Planning: A Comprehensive and Systematic Survey"}], "temperature": 0.1}}
{"custom_id": "93_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Planning represents a fundamental capability of intelligent agents, requiringcomprehensive environmental understanding, rigorous logical reasoning, andeffective sequential decision-making. While Large Language Models (LLMs) havedemonstrated remarkable performance on certain planning tasks, their broaderapplication in this domain warrants systematic investigation. This paperpresents a comprehensive review of LLM-based planning. Specifically, thissurvey is structured as follows: First, we establish the theoreticalfoundations by introducing essential definitions and categories about automatedplanning. Next, we provide a detailed taxonomy and analysis of contemporaryLLM-based planning methodologies, categorizing them into three principalapproaches: 1) External Module Augmented Methods that combine LLMs withadditional components for planning, 2) Finetuning-based Methods that involveusing trajectory data and feedback signals to adjust LLMs in order to improvetheir planning abilities, and 3) Searching-based Methods that break downcomplex tasks into simpler components, navigate the planning space, or enhancedecoding strategies to find the best solutions. Subsequently, we systematicallysummarize existing evaluation frameworks, including benchmark datasets,evaluation metrics and performance comparisons between representative planningmethods. Finally, we discuss the underlying mechanisms enabling LLM-basedplanning and outline promising research directions for this rapidly evolvingfield. We hope this survey will serve as a valuable resource to inspireinnovation and drive progress in this field."}], "temperature": 0.1}}
{"custom_id": "93_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Pengfei Cao"}], "temperature": 0.1}}
{"custom_id": "94_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond"}], "temperature": 0.1}}
{"custom_id": "94_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated thepotential of Reinforcement Learning (RL) to enhance reasoning abilities inLarge Language Models (LLMs). While open-source replication efforts haveprimarily focused on mathematical and coding domains, methods and resources fordeveloping general reasoning capabilities remain underexplored. This gap ispartly due to the challenge of collecting diverse and verifiable reasoning datasuitable for RL. We hypothesize that logical reasoning is critical fordeveloping general reasoning capabilities, as logic forms a fundamentalbuilding block of reasoning. In this work, we present SynLogic, a datasynthesis framework and dataset that generates diverse logical reasoning dataat scale, encompassing 35 diverse logical reasoning tasks. The SynLogicapproach enables controlled synthesis of data with adjustable difficulty andquantity. Importantly, all examples can be verified by simple rules, makingthem ideally suited for RL with verifiable rewards. In our experiments, wevalidate the effectiveness of RL training on the SynLogic dataset based on 7Band 32B models. SynLogic leads to state-of-the-art logical reasoningperformance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32Bby 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical andcoding tasks improves the training efficiency of these domains andsignificantly enhances reasoning generalization. Notably, our mixed trainingmodel outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. Thesefindings position SynLogic as a valuable resource for advancing the broaderreasoning capabilities of LLMs. We open-source both the data synthesis pipelineand the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic."}], "temperature": 0.1}}
{"custom_id": "94_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Junteng Liu"}], "temperature": 0.1}}
{"custom_id": "95_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Interleaved Reasoning for Large Language Models via Reinforcement Learning"}], "temperature": 0.1}}
{"custom_id": "95_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Long chain-of-thought (CoT) significantly enhances large language models'(LLM) reasoning capabilities. However, the extensive reasoning traces lead toinefficiencies and an increased time-to-first-token (TTFT). We propose a noveltraining paradigm that uses reinforcement learning (RL) to guide reasoning LLMsto interleave thinking and answering for multi-hop questions. We observe thatmodels inherently possess the ability to perform interleaved reasoning, whichcan be further enhanced through RL. We introduce a simple yet effectiverule-based reward to incentivize correct intermediate steps, which guides thepolicy model toward correct reasoning paths by leveraging intermediate signalsgenerated during interleaved reasoning. Extensive experiments conducted acrossfive diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++)demonstrate consistent improvements over traditional think-answer reasoning,without requiring external tools. Specifically, our approach reduces TTFT byover 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore,our method, trained solely on question answering and logical reasoningdatasets, exhibits strong generalization ability to complex reasoning datasetssuch as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis toreveal several valuable insights into conditional reward modeling."}], "temperature": 0.1}}
{"custom_id": "95_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Roy Xie"}], "temperature": 0.1}}
{"custom_id": "96_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers"}], "temperature": 0.1}}
{"custom_id": "96_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models have achieved remarkable success in natural languageprocessing tasks, with Reinforcement Learning playing a key role in adaptingthem to specific applications. However, obtaining ground truth answers fortraining LLMs in mathematical problem-solving is often challenging, costly, andsometimes unfeasible. This research delves into the utilization of format andlength as surrogate signals to train LLMs for mathematical problem-solving,bypassing the need for traditional ground truth answers. Our study shows that areward function centered on format correctness alone can yield performanceimprovements comparable to the standard GRPO algorithm in early phases.Recognizing the limitations of format-only rewards in the later phases, weincorporate length-based rewards. The resulting GRPO approach, leveragingformat-length surrogate signals, not only matches but surpasses the performanceof the standard GRPO algorithm relying on ground truth answers in certainscenarios, achieving 40.0% accuracy on AIME2024 with a 7B base model. Throughsystematic exploration and experimentation, this research not only offers apractical solution for training LLMs to solve mathematical problems andreducing the dependence on extensive ground truth data collection, but alsoreveals the essence of why our label-free approach succeeds: the powerful basemodel is like an excellent student who has already mastered mathematical andlogical reasoning skills, but performs poorly on the test paper, it simplyneeds to develop good answering habits to achieve outstanding results in exams,to unlock the capabilities it already possesses."}], "temperature": 0.1}}
{"custom_id": "96_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Rihui Xin"}], "temperature": 0.1}}
{"custom_id": "97_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "ChartSketcher: Reasoning with Multimodal Feedback and Reflection for Chart Understanding"}], "temperature": 0.1}}
{"custom_id": "97_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Charts are high-density visualization carriers for complex data, serving as acrucial medium for information extraction and analysis. Automated chartunderstanding poses significant challenges to existing multimodal largelanguage models (MLLMs) due to the need for precise and complex visualreasoning. Current step-by-step reasoning models primarily focus on text-basedlogical reasoning for chart understanding. However, they struggle to refine orcorrect their reasoning when errors stem from flawed visual understanding, asthey lack the ability to leverage multimodal interaction for deepercomprehension. Inspired by human cognitive behavior, we propose ChartSketcher,a multimodal feedback-driven step-by-step reasoning method designed to addressthese limitations. ChartSketcher is a chart understanding model that employsSketch-CoT, enabling MLLMs to annotate intermediate reasoning steps directlyonto charts using a programmatic sketching library, iteratively feeding thesevisual annotations back into the reasoning process. This mechanism enables themodel to visually ground its reasoning and refine its understanding overmultiple steps. We employ a two-stage training strategy: a cold start phase tolearn sketch-based reasoning patterns, followed by off-policy reinforcementlearning to enhance reflection and generalization. Experiments demonstrate thatChartSketcher achieves promising performance on chart understanding benchmarksand general vision tasks, providing an interactive and interpretable approachto chart comprehension."}], "temperature": 0.1}}
{"custom_id": "97_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Muye Huang"}], "temperature": 0.1}}
{"custom_id": "98_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "MARCO: Meta-Reflection with Cross-Referencing for Code Reasoning"}], "temperature": 0.1}}
{"custom_id": "98_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The ability to reason is one of the most fundamental capabilities of largelanguage models (LLMs), enabling a wide range of downstream tasks throughsophisticated problem-solving. A critical aspect of this is code reasoning,which involves logical reasoning with formal languages (i.e., programmingcode). In this paper, we enhance this capability of LLMs by exploring thefollowing question: how can an LLM agent become progressively smarter in codereasoning with each solution it proposes, thereby achieving substantialcumulative improvement? Most existing research takes a static perspective,focusing on isolated problem-solving using frozen LLMs. In contrast, we adopt acognitive-evolving perspective and propose a novel framework namedMeta-Reflection with Cross-Referencing (MARCO) that enables the LLM to evolvedynamically during inference through self-improvement. From the perspective ofhuman cognitive development, we leverage both knowledge accumulation and lessonsharing. In particular, to accumulate knowledge during problem-solving, wepropose meta-reflection that reflects on the reasoning paths of the currentproblem to obtain knowledge and experience for future consideration. Moreover,to effectively utilize the lessons from other agents, we proposecross-referencing that incorporates the solution and feedback from other agentsinto the current problem-solving process. We conduct experiments across variousdatasets in code reasoning, and the results demonstrate the effectiveness ofMARCO."}], "temperature": 0.1}}
{"custom_id": "98_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yusheng Zhao"}], "temperature": 0.1}}
{"custom_id": "99_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?"}], "temperature": 0.1}}
{"custom_id": "99_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have been shown to achieve breakthroughperformance on complex logical reasoning tasks. Nevertheless, most existingresearch focuses on employing formal language to guide LLMs to derive reliablereasoning paths, while systematic evaluations of these capabilities are stilllimited. In this paper, we aim to conduct a comprehensive evaluation of LLMsacross various logical reasoning problems utilizing formal languages. From theperspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, andformat of trajectories, our key findings are: 1) Thinking models significantlyoutperform Instruct models, especially when formal language is employed; 2) AllLLMs exhibit limitations in inductive reasoning capability, irrespective ofwhether they use a formal language; 3) Data with PoT format achieves the bestgeneralization performance across other languages. Additionally, we also curatethe formal-relative training data to further enhance the small language models,and the experimental results indicate that a simple rejected fine-tuning methodcan better enable LLMs to generalize across formal languages and achieve thebest overall performance. Our codes and reports are available athttps://github.com/jiangjin1999/FormalEval."}], "temperature": 0.1}}
{"custom_id": "99_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jin Jiang"}], "temperature": 0.1}}
{"custom_id": "100_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning in Neurosymbolic AI"}], "temperature": 0.1}}
{"custom_id": "100_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Knowledge representation and reasoning in neural networks have been along-standing endeavor which has attracted much attention recently. Theprincipled integration of reasoning and learning in neural networks is a mainobjective of the area of neurosymbolic Artificial Intelligence (AI). In thischapter, a simple energy-based neurosymbolic AI system is described that canrepresent and reason formally about any propositional logic formula. Thiscreates a powerful combination of learning from data and knowledge and logicalreasoning. We start by positioning neurosymbolic AI in the context of thecurrent AI landscape that is unsurprisingly dominated by Large Language Models(LLMs). We identify important challenges of data efficiency, fairness andsafety of LLMs that might be addressed by neurosymbolic reasoning systems withformal reasoning capabilities. We then discuss the representation of logic bythe specific energy-based system, including illustrative examples and empiricalevaluation of the correspondence between logical reasoning and energyminimization using Restricted Boltzmann Machines (RBM). Learning from data andknowledge is also evaluated empirically and compared with a symbolic, neuraland a neurosymbolic system. Results reported in this chapter in an accessibleway are expected to reignite the research on the use of neural networks asmassively-parallel models for logical reasoning and promote the principledintegration of reasoning and learning in deep networks. We conclude the chapterwith a discussion of the importance of positioning neurosymbolic AI within abroader framework of formal reasoning and accountability in AI, discussing thechallenges for neurosynbolic AI to tackle the various known problems ofreliability of deep learning."}], "temperature": 0.1}}
{"custom_id": "100_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Son Tran"}], "temperature": 0.1}}
{"custom_id": "101_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Towards Competent AI for Fundamental Analysis in Finance: A Benchmark Dataset and Evaluation"}], "temperature": 0.1}}
{"custom_id": "101_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Generative AI, particularly large language models (LLMs), is beginning totransform the financial industry by automating tasks and helping to make senseof complex financial information. One especially promising use case is theautomatic creation of fundamental analysis reports, which are essential formaking informed investment decisions, evaluating credit risks, guidingcorporate mergers, etc. While LLMs attempt to generate these reports from asingle prompt, the risks of inaccuracy are significant. Poor analysis can leadto misguided investments, regulatory issues, and loss of trust. Existingfinancial benchmarks mainly evaluate how well LLMs answer financial questionsbut do not reflect performance in real-world tasks like generating financialanalysis reports. In this paper, we propose FinAR-Bench, a solid benchmarkdataset focusing on financial statement analysis, a core competence offundamental analysis. To make the evaluation more precise and reliable, webreak this task into three measurable steps: extracting key information,calculating financial indicators, and applying logical reasoning. Thisstructured approach allows us to objectively assess how well LLMs perform eachstep of the process. Our findings offer a clear understanding of LLMs currentstrengths and limitations in fundamental analysis and provide a more practicalway to benchmark their performance in real-world financial settings."}], "temperature": 0.1}}
{"custom_id": "101_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zonghan Wu"}], "temperature": 0.1}}
{"custom_id": "102_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Sudoku-Bench: Evaluating creative reasoning with Sudoku variants"}], "temperature": 0.1}}
{"custom_id": "102_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Existing reasoning benchmarks for large language models (LLMs) frequentlyfail to capture authentic creativity, often rewarding memorization ofpreviously observed patterns. We address this shortcoming with Sudoku-Bench, acurated benchmark of challenging and unconventional Sudoku variantsspecifically selected to evaluate creative, multi-step logical reasoning.Sudoku variants form an unusually effective domain for reasoning research: eachpuzzle introduces unique or subtly interacting constraints, making memorizationinfeasible and requiring solvers to identify novel logical breakthroughs(``break-ins''). Despite their diversity, Sudoku variants maintain a common andcompact structure, enabling clear and consistent evaluation. Sudoku-Benchincludes a carefully chosen puzzle set, a standardized text-based puzzlerepresentation, and flexible tools compatible with thousands of publiclyavailable puzzles -- making it easy to extend into a general researchenvironment. Baseline experiments show that state-of-the-art LLMs solve fewerthan 15\\% of puzzles unaided, highlighting significant opportunities to advancelong-horizon, strategic reasoning capabilities."}], "temperature": 0.1}}
{"custom_id": "102_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jeffrey Seely"}], "temperature": 0.1}}
{"custom_id": "103_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Learning to Reason via Mixture-of-Thought for Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "103_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Human beings naturally utilize multiple reasoning modalities to learn andsolve logical problems, i.e., different representational formats such asnatural language, code, and symbolic logic. In contrast, most existingLLM-based approaches operate with a single reasoning modality during training,typically natural language. Although some methods explored modality selectionor augmentation at inference time, the training process remains modality-blind,limiting synergy among modalities. To fill in this gap, we proposeMixture-of-Thought (MoT), a framework that enables LLMs to reason across threecomplementary modalities: natural language, code, and a newly introducedsymbolic modality, truth-table, which systematically enumerates logical casesand partially mitigates key failure modes in natural language reasoning. MoTadopts a two-phase design: (1) self-evolving MoT training, which jointly learnsfrom filtered, self-generated rationales across modalities; and (2) MoTinference, which fully leverages the synergy of three modalities to producebetter predictions. Experiments on logical reasoning benchmarks including FOLIOand ProofWriter demonstrate that our MoT framework consistently andsignificantly outperforms strong LLM baselines with single-modalitychain-of-thought approaches, achieving up to +11.7pp average accuracy gain.Further analyses show that our MoT framework benefits both training andinference stages; that it is particularly effective on harder logical reasoningproblems; and that different modalities contribute complementary strengths,with truth-table reasoning helping to overcome key bottlenecks in naturallanguage inference."}], "temperature": 0.1}}
{"custom_id": "103_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Tong Zheng"}], "temperature": 0.1}}
{"custom_id": "104_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Systematic Evaluation of Machine-Generated Reasoning and PHQ-9 Labeling for Depression Detection Using Large Language Models"}], "temperature": 0.1}}
{"custom_id": "104_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent research leverages large language models (LLMs) for early mentalhealth detection, such as depression, often optimized with machine-generateddata. However, their detection may be subject to unknown weaknesses. Meanwhile,quality control has not been applied to these generated corpora besides limitedhuman verifications. Our goal is to systematically evaluate LLM reasoning andreveal potential weaknesses. To this end, we first provide a systematicevaluation of the reasoning over machine-generated detection andinterpretation. Then we use the models' reasoning abilities to exploremitigation strategies for enhanced performance. Specifically, we do thefollowing: A. Design an LLM instruction strategy that allows for systematicanalysis of the detection by breaking down the task into several subtasks. B.Design contrastive few-shot and chain-of-thought prompts by selecting typicalpositive and negative examples of detection reasoning. C. Perform humanannotation for the subtasks identified in the first step and evaluate theperformance. D. Identify human-preferred detection with desired logicalreasoning from the few-shot generation and use them to explore differentoptimization strategies. We conducted extensive comparisons on the DepTweetdataset across the following subtasks: 1. identifying whether the speaker isdescribing their own depression; 2. accurately detecting the presence of PHQ-9symptoms, and 3. finally, detecting depression. Human verification ofstatistical outliers shows that LLMs demonstrate greater accuracy in analyzingand detecting explicit language of depression as opposed to implicitexpressions of depression. Two optimization methods are used for performanceenhancement and reduction of the statistic bias: supervised fine-tuning (SFT)and direct preference optimization (DPO). Notably, the DPO approach achievessignificant performance improvement."}], "temperature": 0.1}}
{"custom_id": "104_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zongru Shao"}], "temperature": 0.1}}
{"custom_id": "105_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "FOL-Pretrain: A complexity annotated corpus of first-order logic"}], "temperature": 0.1}}
{"custom_id": "105_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Transformer-based large language models (LLMs) have demonstrated remarkablereasoning capabilities such as coding and solving mathematical problems tocommonsense inference. While these tasks vary in complexity, they all requiremodels to integrate and compute over structured information. Despite recentefforts to reverse-engineer LLM behavior through controlled experiments, ourunderstanding of how these models internalize and execute complex algorithmsremains limited. Progress has largely been confined to small-scale studies orshallow tasks such as basic arithmetic and grammatical pattern matching. Onebarrier to deeper understanding is the nature of pretraining data -- vast,heterogeneous, and often poorly annotated, making it difficult to isolatemechanisms of reasoning. To bridge this gap, we introduce a large-scale, fullyopen, complexity-annotated dataset of first-order logic reasoning traces,designed to probe and analyze algorithmic reasoning in LLMs. The datasetconsists of 3.5 billion tokens, including 8.8 million LLM-augmented,human-annotated examples and 7.5 million synthetically generated examples. Eachsynthetic example is verifiably correct, produced by a custom automated theoremsolver, and accompanied by metadata tracing its algorithmic provenance. We aimto provide a scalable, interpretable artifact for studying how LLMs learn andgeneralize symbolic reasoning processes, paving the way for more transparentand targeted investigations into the algorithmic capabilities of modern models."}], "temperature": 0.1}}
{"custom_id": "105_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Isabelle Lee"}], "temperature": 0.1}}
{"custom_id": "106_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning"}], "temperature": 0.1}}
{"custom_id": "106_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have achieved remarkable progress onmathematical tasks through Chain-of-Thought (CoT) reasoning. However, existingmathematical CoT datasets often suffer from Thought Leaps due to expertsomitting intermediate steps, which negatively impacts model learning andgeneralization. We propose the CoT Thought Leap Bridge Task, which aims toautomatically detect leaps and generate missing intermediate reasoning steps torestore the completeness and coherence of CoT. To facilitate this, weconstructed a specialized training dataset called ScaleQM+, based on thestructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thoughtleaps. Through comprehensive experiments on mathematical reasoning benchmarks,we demonstrate that models fine-tuned on bridged datasets consistentlyoutperform those trained on original datasets, with improvements of up to+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)and provides better starting points for reinforcement learning (+3.1%),functioning as a plug-and-play module compatible with existing optimizationtechniques. Furthermore, CoT-Bridge demonstrate improved generalization toout-of-domain logical reasoning tasks, confirming that enhancing reasoningcompleteness yields broadly applicable benefits."}], "temperature": 0.1}}
{"custom_id": "106_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Haolei Xu"}], "temperature": 0.1}}
{"custom_id": "107_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas"}], "temperature": 0.1}}
{"custom_id": "107_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We introduce SATBench, a benchmark for evaluating the logical reasoningcapabilities of large language models (LLMs) through logical puzzles derivedfrom Boolean satisfiability (SAT) problems. Unlike prior work that focuses oninference rule-based reasoning, which often involves deducing conclusions froma set of premises, our approach leverages the search-based nature of SATproblems, where the objective is to find a solution that fulfills a specifiedset of logical constraints. Each instance in SATBench is generated from a SATformula, then translated into a story context and conditions using LLMs. Thegeneration process is fully automated and allows for adjustable difficulty byvarying the number of clauses. All 2100 puzzles are validated through bothLLM-assisted and solver-based consistency checks, with human validation on asubset. Experimental results show that even the strongest model, o4-mini,achieves only 65.0% accuracy on hard UNSAT problems, close to the randombaseline of 50%. SATBench exposes fundamental limitations in the search-basedlogical reasoning abilities of current LLMs and provides a scalable testbed forfuture research in logical reasoning."}], "temperature": 0.1}}
{"custom_id": "107_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Anjiang Wei"}], "temperature": 0.1}}
{"custom_id": "108_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning-OCR: Can Large Multimodal Models Solve Complex Logical Reasoning Problems from OCR Cues?"}], "temperature": 0.1}}
{"custom_id": "108_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Multimodal Models (LMMs) have become increasingly versatile,accompanied by impressive Optical Character Recognition (OCR) relatedcapabilities. Existing OCR-related benchmarks emphasize evaluating LMMs'abilities of relatively simple visual question answering, visual-text parsing,etc. However, the extent to which LMMs can deal with complex logical reasoningproblems based on OCR cues is relatively unexplored. To this end, we introducethe Reasoning-OCR benchmark, which challenges LMMs to solve complex reasoningproblems based on the cues that can be extracted from rich visual-text.Reasoning-OCR covers six visual scenarios and encompasses 150 meticulouslydesigned questions categorized into six reasoning challenges. Additionally,Reasoning-OCR minimizes the impact of field-specialized knowledge. Ourevaluation offers some insights for proprietary and open-source LMMs indifferent reasoning challenges, underscoring the urgent to improve thereasoning performance. We hope Reasoning-OCR can inspire and facilitate futureresearch on enhancing complex reasoning ability based on OCR cues.Reasoning-OCR is publicly available athttps://github.com/Hxyz-123/ReasoningOCR."}], "temperature": 0.1}}
{"custom_id": "108_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Haibin He"}], "temperature": 0.1}}
{"custom_id": "109_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?"}], "temperature": 0.1}}
{"custom_id": "109_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advances in Large Multimodal Models (LMMs) have significantly improvedtheir reasoning and Optical Character Recognition (OCR) capabilities. However,their performance on complex logical reasoning tasks involving text-rich imagesremains underexplored. To bridge this gap, we introduce LogicOCR, a benchmarkcomprising 1,100 multiple-choice questions designed to evaluate LMMs' logicalreasoning abilities on text-rich images, while minimizing reliance ondomain-specific knowledge (e.g., mathematics). We construct LogicOCR bycurating a text corpus from the Chinese National Civil Servant Examination anddevelop a scalable, automated pipeline to convert it into multimodal samples.First, we design prompt templates to steer GPT-Image-1 to generate images withdiverse backgrounds, interleaved text-illustration layouts, and varied fonts,ensuring contextual relevance and visual realism. Then, the generated imagesare manually verified, with low-quality examples discarded. We evaluate a rangeof representative open-source and proprietary LMMs under both Chain-of-Thought(CoT) and direct-answer settings. Our multi-dimensional analysis reveals keyinsights, such as the impact of test-time scaling, input modality differences,and sensitivity to visual-text orientation. Notably, LMMs still lag inmultimodal reasoning compared to text-only inputs, indicating that they havenot fully bridged visual reading with reasoning. We hope LogicOCR will serve asa valuable resource for advancing multimodal reasoning research. The dataset isavailable at https://github.com/MiliLab/LogicOCR."}], "temperature": 0.1}}
{"custom_id": "109_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Maoyuan Ye"}], "temperature": 0.1}}
{"custom_id": "110_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Efficient RL Training for Reasoning Models via Length-Aware Optimization"}], "temperature": 0.1}}
{"custom_id": "110_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstratedremarkable performance on reasoning tasks but often incur a long reasoning pathwith significant memory and time costs. Existing methods primarily aim toshorten reasoning paths by introducing additional training data and stages. Inthis paper, we propose three critical reward designs integrated directly intothe reinforcement learning process of large reasoning models, which reduce theresponse length without extra training stages. Experiments on four settingsshow that our method significantly decreases response length while maintainingor even improving performance. Specifically, in a logic reasoning setting, weachieve a 40% reduction in response length averaged by steps alongside a 14%gain in performance. For math problems, we reduce response length averaged bysteps by 33% while preserving performance."}], "temperature": 0.1}}
{"custom_id": "110_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Danlong Yuan"}], "temperature": 0.1}}
{"custom_id": "111_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Curriculum Abductive Learning"}], "temperature": 0.1}}
{"custom_id": "111_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Abductive Learning (ABL) integrates machine learning with logical reasoningin a loop: a learning model predicts symbolic concept labels from raw inputs,which are revised through abduction using domain knowledge and then fed backfor retraining. However, due to the nondeterminism of abduction, the trainingprocess often suffers from instability, especially when the knowledge base islarge and complex, resulting in a prohibitively large abduction space. Whileprior works focus on improving candidate selection within this space, theytypically treat the knowledge base as a static black box. In this work, wepropose Curriculum Abductive Learning (C-ABL), a method that explicitlyleverages the internal structure of the knowledge base to address the ABLtraining challenges. C-ABL partitions the knowledge base into a sequence ofsub-bases, progressively introduced during training. This reduces the abductionspace throughout training and enables the model to incorporate logic in astepwise, smooth way. Experiments across multiple tasks show that C-ABLoutperforms previous ABL implementations, significantly improves trainingstability, convergence speed, and final accuracy, especially under complexknowledge setting."}], "temperature": 0.1}}
{"custom_id": "111_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Wen-Chao Hu"}], "temperature": 0.1}}
{"custom_id": "112_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs"}], "temperature": 0.1}}
{"custom_id": "112_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advances in Large Reasoning Models (LRMs) have shown impressivecapabilities in mathematical and logical reasoning. However, current LRMsrarely admit ignorance or respond with \"I don't know\". Instead, they oftenproduce incorrect answers while showing undue confidence, raising concernsabout their factual reliability. In this work, we identify two pathologicalreasoning patterns characterized by overthinking that contribute to theoverconfident and incorrect answers: last-minute guessing and second-thoughtspiraling. To address these issues, we propose BARREL-a novel framework thatpromotes concise and boundary-aware factual reasoning. Our experiments showthat BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8Bfrom 39.33% to 61.48%, while still achieving accuracy comparable to modelsfinetuned on reasoning data generated by R1. These results demonstrate that ourpilot study is inspiring to build more reliable and factual System 2 LRMs."}], "temperature": 0.1}}
{"custom_id": "112_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Junxiao Yang"}], "temperature": 0.1}}
{"custom_id": "113_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Evaluating the Logical Reasoning Abilities of Large Reasoning Models"}], "temperature": 0.1}}
{"custom_id": "113_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large reasoning models, often post-trained on long chain-of-thought (longCoT) data with reinforcement learning, achieve state-of-the-art performance onmathematical, coding, and domain-specific reasoning benchmarks. However, theirlogical reasoning capabilities - fundamental to human cognition and independentof domain knowledge - remain understudied. To address this gap, we introduceLogiEval, a holistic benchmark for evaluating logical reasoning in largereasoning models. LogiEval spans diverse reasoning types (deductive, inductive,analogical, and abductive) and task formats (e.g., logical sequence, argumentanalysis), sourced from high-quality human examinations (e.g., LSAT, GMAT). Ourexperiments demonstrate that modern reasoning models excel at 4-choice argumentanalysis problems and analogical reasoning, surpassing human performance, yetexhibit uneven capabilities across reasoning types and formats, highlightinglimitations in their generalization. Our analysis reveals that humanperformance does not mirror model failure distributions. To foster furtherresearch, we curate LogiEval-Hard, a challenging subset identified through anovel screening paradigm where small-model failures (Qwen3-30B-A3B) reliablypredict difficulties for larger models. Modern models show striking, consistentfailures on LogiEval-Hard. This demonstrates that fundamental reasoningbottlenecks persist across model scales, and establishes LogiEval-Hard as botha diagnostic tool and a rigorous testbed for advancing logical reasoning inLLMs."}], "temperature": 0.1}}
{"custom_id": "113_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hanmeng Liu"}], "temperature": 0.1}}
{"custom_id": "114_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Rhetorical XAI: Explaining AI's Benefits as well as its Use via Rhetorical Design"}], "temperature": 0.1}}
{"custom_id": "114_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This paper explores potential benefits of incorporating Rhetorical Designinto the design of Explainable Artificial Intelligence (XAI) systems. While XAIis traditionally framed around explaining individual predictions or overallsystem behavior, explanations also function as a form of argumentation, shapinghow users evaluate system perceived usefulness, credibility, and fosterappropriate trust. Rhetorical Design offers a useful framework to analyze thecommunicative role of explanations between AI systems and users, focusing on:(1) logical reasoning conveyed through different types of explanations, (2)credibility projected by the system and its developers, and (3) emotionalresonance elicited in users. Together, these rhetorical appeals help usunderstand how explanations influence user perceptions and facilitate AIadoption. This paper synthesizes design strategies from prior XAI work thatalign with these three rhetorical appeals and highlights both opportunities andchallenges of integrating rhetorical design into XAI design."}], "temperature": 0.1}}
{"custom_id": "114_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Houjiang Liu"}], "temperature": 0.1}}
{"custom_id": "115_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection"}], "temperature": 0.1}}
{"custom_id": "115_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have gained widespread adoption across diverseapplications due to their impressive generative capabilities. Theirplug-and-play nature enables both developers and end users to interact withthese models through simple prompts. However, as LLMs become more integratedinto various systems in diverse domains, concerns around their security aregrowing. Existing studies mainly focus on threats arising from user prompts(e.g. prompt injection attack) and model output (e.g. model inversion attack),while the security of system prompts remains largely overlooked. This workbridges the critical gap. We introduce system prompt poisoning, a new attackvector against LLMs that, unlike traditional user prompt injection, poisonssystem prompts hence persistently impacts all subsequent user interactions andmodel responses. We systematically investigate four practical attack strategiesin various poisoning scenarios. Through demonstration on both generative andreasoning LLMs, we show that system prompt poisoning is highly feasible withoutrequiring jailbreak techniques, and effective across a wide range of tasks,including those in mathematics, coding, logical reasoning, and natural languageprocessing. Importantly, our findings reveal that the attack remains effectiveeven when user prompts employ advanced prompting techniques likechain-of-thought (CoT). We also show that such techniques, including CoT andretrieval-augmentation-generation (RAG), which are proven to be effective forimproving LLM performance in a wide range of tasks, are significantly weakenedin their effectiveness by system prompt poisoning."}], "temperature": 0.1}}
{"custom_id": "115_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jiawei Guo"}], "temperature": 0.1}}
{"custom_id": "116_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time"}], "temperature": 0.1}}
{"custom_id": "116_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This paper proposes an integration of temporal logical reasoning andPartially Observable Markov Decision Processes (POMDPs) to achieveinterpretable decision-making under uncertainty with macro-actions. Our methodleverages a fragment of Linear Temporal Logic (LTL) based on Event Calculus(EC) to generate \\emph{persistent} (i.e., constant) macro-actions, which guideMonte Carlo Tree Search (MCTS)-based POMDP solvers over a time horizon,significantly reducing inference time while ensuring robust performance. Suchmacro-actions are learnt via Inductive Logic Programming (ILP) from a fewtraces of execution (belief-action pairs), thus eliminating the need formanually designed heuristics and requiring only the specification of the POMDPtransition model. In the Pocman and Rocksample benchmark scenarios, our learnedmacro-actions demonstrate increased expressiveness and generality when comparedto time-independent heuristics, indeed offering substantial computationalefficiency improvements."}], "temperature": 0.1}}
{"custom_id": "116_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Celeste Veronese"}], "temperature": 0.1}}
{"custom_id": "117_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking"}], "temperature": 0.1}}
{"custom_id": "117_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advancements have significantly enhanced the performance of largelanguage models (LLMs) in tackling complex reasoning tasks, achieving notablesuccess in domains like mathematical and logical reasoning. However, thesemethods encounter challenges with complex planning tasks, primarily due toextended reasoning steps, diverse constraints, and the challenge of handlingmultiple distinct sub-tasks. To address these challenges, we propose HyperTreePlanning (HTP), a novel reasoning paradigm that constructs hypertree-structuredplanning outlines for effective planning. The hypertree structure enables LLMsto engage in hierarchical thinking by flexibly employing the divide-and-conquerstrategy, effectively breaking down intricate reasoning steps, accommodatingdiverse constraints, and managing multiple distinct sub-tasks in awell-organized manner. We further introduce an autonomous planning frameworkthat completes the planning process by iteratively refining and expanding thehypertree-structured planning outlines. Experiments demonstrate theeffectiveness of HTP, achieving state-of-the-art accuracy on the TravelPlannerbenchmark with Gemini-1.5-Pro, resulting in a 3.6 times performance improvementover o1-preview."}], "temperature": 0.1}}
{"custom_id": "117_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Runquan Gui"}], "temperature": 0.1}}
{"custom_id": "118_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning Capabilities and Invariability of Large Language Models"}], "temperature": 0.1}}
{"custom_id": "118_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have shown remarkable capabilities inmanipulating natural language across multiple applications, but their abilityto handle simple reasoning tasks is often questioned. In this work, we aim toprovide a comprehensive analysis of LLMs' reasoning competence, specificallyfocusing on their prompt dependency. In particular, we introduce a newbenchmark dataset with a series of simple reasoning questions demanding shallowlogical reasoning. Aligned with cognitive psychology standards, the questionsare confined to a basic domain revolving around geometric figures, ensuringthat responses are independent of any pre-existing intuition about the worldand rely solely on deduction. An empirical analysis involving zero-shot andfew-shot prompting across 24 LLMs of different sizes reveals that, while LLMswith over 70 billion parameters perform better in the zero-shot setting, thereis still a large room for improvement. An additional test with chain-of-thoughtprompting over 22 LLMs shows that this additional prompt can aid or damage theperformance of models, depending on whether the rationale is required before orafter the answer."}], "temperature": 0.1}}
{"custom_id": "118_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Alessandro Raganato"}], "temperature": 0.1}}
{"custom_id": "119_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Report on the llms evaluating the high school questions"}], "temperature": 0.1}}
{"custom_id": "119_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This report aims to evaluate the performance of large language models (LLMs)in solving high school science questions and to explore their potentialapplications in the educational field. With the rapid development of LLMs inthe field of natural language processing, their application in education hasattracted widespread attention. This study selected mathematics exam questionsfrom the college entrance examinations (2019-2023) as evaluation data andutilized at least eight LLM APIs to provide answers. A comprehensive assessmentwas conducted based on metrics such as accuracy, response time, logicalreasoning, and creativity. Through an in-depth analysis of the evaluationresults, this report reveals the strengths and weaknesses of LLMs in handlinghigh school science questions and discusses their implications for educationalpractice. The findings indicate that although LLMs perform excellently incertain aspects, there is still room for improvement in logical reasoning andcreative problem-solving. This report provides an empirical foundation forfurther research and application of LLMs in the educational field and offerssuggestions for improvement."}], "temperature": 0.1}}
{"custom_id": "119_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhu Jiawei"}], "temperature": 0.1}}
{"custom_id": "120_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Temperature Mapping in Urban Biomes Using an Infrared Thermometer: An Investigative Approach to Physics Education"}], "temperature": 0.1}}
{"custom_id": "120_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In recent years, the frequency of extreme weather events on Earth hasincreased significantly. This phenomenon is driven by the intensification ofthe greenhouse effect caused by anthropogenic activities, leading totemperature variations in urban environments that affect thermal comfort andquality of life. Given this context, the present study investigates temperaturemapping in urban biomes using an infrared thermometer, conducted as part of ahands-on workshop offered during the 21st National Science and Technology Week.The initiative involved students from the public school system and was groundedin Physics education, aiming to foster scientific enculturation. Participantsengaged in a problem-based learning experience, actively contributing to allstages of the knowledge construction process. The objective was to examine therelationship between vegetation presence and its impact on temperature in urbanenvironments. A qualitative and quantitative methodological approach wasadopted, enabling the identification of scientific literacy indicators such asinformation sequencing, data organization, logical reasoning, hypothesisformulation, justification, and explanation of observed phenomena. The analysisof students' statements and activity guidelines provided insights into theircritical thinking development. The findings indicate that students developedessential skills for understanding physical and environmental phenomena,effectively linking collected data to scientific concepts and proposingwell-supported interpretations. Moreover, the experience reinforced theperception of science as a dynamic and investigative process, fosteringcuriosity and enhancing students' argumentative abilities. Thus, the workshopproved to be an effective strategy for promoting scientific literacy andengaging participants in the study of environmental impacts in urban contexts."}], "temperature": 0.1}}
{"custom_id": "120_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Welington Fabr铆cio dos Santos Costa"}], "temperature": 0.1}}
{"custom_id": "121_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework"}], "temperature": 0.1}}
{"custom_id": "121_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This paper investigates the logical reasoning capabilities of large languagemodels (LLMs). For a precisely defined yet tractable formulation, we choose theconceptually simple but technically complex task of constructing proofs inBoolean logic. A trained LLM receives as input a set of assumptions and a goal,and produces as output a proof that formally derives the goal from theassumptions. Incorrect proofs are caught by an automated proof checker. Acritical obstacle for training is the scarcity of real-world proofs. We proposean efficient, randomized procedure for synthesizing valid proofs and introduceTemplate Transformation, a data augmentation technique that enhances themodel's ability to handle complex logical expressions. The central evaluationquestion is whether an LLM has indeed learned to reason. We propose tests tomeasure the reasoning ability of a black-box LLM. By these measures,experiments demonstrate strong reasoning capabilities for assertions with shortproofs, which decline with proof complexity. Notably, template transformationimproves accuracy even for smaller models, suggesting its effectiveness acrossmodel scales."}], "temperature": 0.1}}
{"custom_id": "121_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yuan Xia"}], "temperature": 0.1}}
{"custom_id": "122_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models"}], "temperature": 0.1}}
{"custom_id": "122_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have dramatically advanced machine learningresearch including natural language processing, computer vision, data mining,etc., yet they still exhibit critical limitations in reasoning, factualconsistency, and interpretability. In this paper, we introduce a novel learningparadigm -- Modular Machine Learning (MML) -- as an essential approach towardnew-generation LLMs. MML decomposes the complex structure of LLMs into threeinterdependent components: modular representation, modular model, and modularreasoning, aiming to enhance LLMs' capability of counterfactual reasoning,mitigating hallucinations, as well as promoting fairness, safety, andtransparency. Specifically, the proposed MML paradigm can: i) clarify theinternal working mechanism of LLMs through the disentanglement of semanticcomponents; ii) allow for flexible and task-adaptive model design; iii) enableinterpretable and logic-driven decision-making process. We present a feasibleimplementation of MML-based LLMs via leveraging advanced techniques such asdisentangled representation learning, neural architecture search andneuro-symbolic learning. We critically identify key challenges, such as theintegration of continuous neural and discrete symbolic processes, jointoptimization, and computational scalability, present promising future researchdirections that deserve further exploration. Ultimately, the integration of theMML paradigm with LLMs has the potential to bridge the gap between statistical(deep) learning and formal (logical) reasoning, thereby paving the way forrobust, adaptable, and trustworthy AI systems across a wide range of real-worldapplications."}], "temperature": 0.1}}
{"custom_id": "122_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xin Wang"}], "temperature": 0.1}}
{"custom_id": "123_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Formal Framework for Naturally Specifying and Verifying Sequential Algorithms"}], "temperature": 0.1}}
{"custom_id": "123_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Current approaches for formal verification of algorithms face importantlimitations. For specification, they cannot express algorithms naturally andconcisely, especially for algorithms with states and flexible control flow. Forverification, formal proof based on Hoare logic cannot reflect the logicalstructure of natural proof. To address these challenges, we introduce a formalframework for naturally specifying and verifying sequential algorithms in Coq.We use the state relation monad to integrate Coq's expressive type system withthe flexible control flow of imperative languages. It supports nondeterministicoperations and customizable program states, enabling specifying algorithms atan appropriate level of abstraction. For verification, we build a Hoare logicfor the monad and propose a novel two-stage proof approach that separatesnatural logical reasoning from mechanical composition. It reflects the logicalstructure of natural proof, enhancing modularity and readability. We evaluatethe framework by formalizing the Depth-First Search (DFS) algorithm andverifying the Knuth-Morris-Pratt (KMP) algorithm."}], "temperature": 0.1}}
{"custom_id": "123_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chengxi Yang"}], "temperature": 0.1}}
{"custom_id": "124_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LR-IAD:Mask-Free Industrial Anomaly Detection with Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "124_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Industrial Anomaly Detection (IAD) is critical for ensuring product qualityby identifying defects. Traditional methods such as feature embedding andreconstruction-based approaches require large datasets and struggle withscalability. Existing vision-language models (VLMs) and Multimodal LargeLanguage Models (MLLMs) address some limitations but rely on mask annotations,leading to high implementation costs and false positives. Additionally,industrial datasets like MVTec-AD and VisA suffer from severe class imbalance,with defect samples constituting only 23.8% and 11.1% of total datarespectively. To address these challenges, we propose a reward function thatdynamically prioritizes rare defect patterns during training to handle classimbalance. We also introduce a mask-free reasoning framework using Chain ofThought (CoT) and Group Relative Policy Optimization (GRPO) mechanisms,enabling anomaly detection directly from raw images without annotated masks.This approach generates interpretable step-by-step explanations for defectlocalization. Our method achieves state-of-the-art performance, outperformingprior approaches by 36% in accuracy on MVTec-AD and 16% on VisA. By eliminatingmask dependency and reducing costs while providing explainable outputs, thiswork advances industrial anomaly detection and supports scalable qualitycontrol in manufacturing. Code to reproduce the experiment is available athttps://github.com/LilaKen/LR-IAD."}], "temperature": 0.1}}
{"custom_id": "124_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Peijian Zeng"}], "temperature": 0.1}}
{"custom_id": "125_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "125_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks thatrequire intermediate reasoning steps prior to generating a final answer. Inthis paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning modelinspired by the finetuning strategy based on Group Relative PolicyOptimization. We also leverage a high-quality Vietnamese synthesized reasoningdataset and design two reward functions to tackle the main limitations of thistechnique: (i) language mixing, where we explicitly detect the presence ofbiased language characters during the process of sampling tokens, and (ii) weleverage Sentence Transformer-based models to ensure that the generatedreasoning content maintains factual correctness and does not distort the finaloutput. Experimental results on the Vietnamese dataset from the VLSP 2023Challenge demonstrate that our model outperforms prior works and enhanceslinguistic consistency in its responses. Furthermore, we extend our evaluationto SeaExam-a multilingual multiple-choice dataset, showing the effectiveness ofour reasoning method compared to few-shot prompting techniques."}], "temperature": 0.1}}
{"custom_id": "125_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Luu Quy Tung"}], "temperature": 0.1}}
{"custom_id": "126_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "POLYRAG: Integrating Polyviews into Retrieval-Augmented Generation for Medical Applications"}], "temperature": 0.1}}
{"custom_id": "126_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have become a disruptive force in the industry,introducing unprecedented capabilities in natural language processing, logicalreasoning and so on. However, the challenges of knowledge updates andhallucination issues have limited the application of LLMs in medical scenarios,where retrieval-augmented generation (RAG) can offer significant assistance.Nevertheless, existing retrieve-then-read approaches generally digest theretrieved documents, without considering the timeliness, authoritativeness andcommonality of retrieval. We argue that these approaches can be suboptimal,especially in real-world applications where information from different sourcesmight conflict with each other and even information from the same source indifferent time scale might be different, and totally relying on this woulddeteriorate the performance of RAG approaches. We propose PolyRAG thatcarefully incorporate judges from different perspectives and finally integratethe polyviews for retrieval augmented generation in medical applications. Dueto the scarcity of real-world benchmarks for evaluation, to bridge the gap wepropose PolyEVAL, a benchmark consists of queries and documents collected fromreal-world medical scenarios (including medical policy, hospital & doctorinquiry and healthcare) with multiple tagging (e.g., timeliness,authoritativeness) on them. Extensive experiments and analysis on PolyEVAL havedemonstrated the superiority of PolyRAG."}], "temperature": 0.1}}
{"custom_id": "126_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chunjing Gan"}], "temperature": 0.1}}
{"custom_id": "127_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "CRAVE: A Conflicting Reasoning Approach for Explainable Claim Verification Using LLMs"}], "temperature": 0.1}}
{"custom_id": "127_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The rapid spread of misinformation, driven by digital media and AI-generatedcontent, has made automatic claim verification essential. Traditional methods,which depend on expert-annotated evidence, are labor-intensive and notscalable. Although recent automated systems have improved, they still strugglewith complex claims that require nuanced reasoning. To address this, we proposeCRAVE, a Conflicting Reasoning Approach for explainable claim VErification,that verify the complex claims based on the conflicting rationales reasoned bylarge language models (LLMs). Specifically, CRAVE introduces a three-moduleframework. Ambiguity Elimination enchanced Evidence Retrieval module performsambiguity elimination and entity-based search to gather relevant evidencerelated to claim verification from external sources like Wikipedia. ConflictingPerspective Reasoning and Preliminary Judgment module with LLMs adopts LLMs toreason rationales with conflicting stances about claim verification fromretrieved evidence across four dimensions, i.e., direct evidence, semanticrelationships, linguistic patterns, and logical reasoning and make apreliminary judgment. Finally, Small Language Model (SLM) based Judge module isfine-tuned to make use of preliminary judgment from LLMs to assess theconfidence of the conflicting rationales and make a final authenticityjudgment. This methodology allows CRAVE to capture subtle inconsistencies incomplex claims, improving both the accuracy and transparency of claimverification. Extensive experiments on two public claim verification datasetsdemonstrate that our CRAVE model achieves much better performance thanstate-of-the-art methods and exhibits a superior capacity for finding relevantevidence and explaining the model predictions. The code is provided athttps://github.com/8zym/CRAVE."}], "temperature": 0.1}}
{"custom_id": "127_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yingming Zheng"}], "temperature": 0.1}}
{"custom_id": "128_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners"}], "temperature": 0.1}}
{"custom_id": "128_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Multimodal Large Language Models (MLLMs) have powered Graphical UserInterface (GUI) Agents, showing promise in automating tasks on computingdevices. Recent works have begun exploring reasoning in GUI tasks withencouraging results. However, many current approaches rely on manually designedreasoning templates, which may result in reasoning that is not sufficientlyrobust and adaptive for complex GUI environments. Meanwhile, some existingagents continue to operate as Reactive Actors, relying primarily on implicitreasoning that may lack sufficient depth for GUI tasks demanding planning anderror recovery. We argue that advancing these agents requires a shift fromreactive acting towards acting based on deliberate reasoning. To facilitatethis transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developedthrough our Actor2Reasoner framework, a reasoning-centric, two-stage trainingapproach designed to progressively evolve agents from Reactive Actors toDeliberative Reasoners. The first stage, Reasoning Injection, focuses onestablishing a basic reasoner. We employ Spatial Reasoning Distillation totransfer cross-modal spatial reasoning capabilities from teacher models toMLLMs through trajectories with explicit reasoning steps, enabling models tointegrate GUI visual-spatial information with logical reasoning before actiongeneration. The second stage, Deliberation Enhancement, refines the basicreasoner into a deliberative one using Reinforcement Learning. This stageintroduces two approaches: Sub-goal Guidance, which rewards models forgenerating accurate intermediate sub-goals, and Error Recovery ScenarioConstruction, which creates failure-and-recovery training scenarios fromidentified prone-to-error steps. Experimental results show InfiGUI-R1 achievesstrong performance in GUI grounding and trajectory tasks. Resources athttps://github.com/Reallm-Labs/InfiGUI-R1."}], "temperature": 0.1}}
{"custom_id": "128_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yuhang Liu"}], "temperature": 0.1}}
{"custom_id": "129_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Explainable Recommendation with Simulated Human Feedback"}], "temperature": 0.1}}
{"custom_id": "129_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advancements in explainable recommendation have greatly bolstered userexperience by elucidating the decision-making rationale. However, the existingmethods actually fail to provide effective feedback signals for potentiallybetter or worse generated explanations due to their reliance on traditionalsupervised learning paradigms in sparse interaction data. To address theseissues, we propose a novel human-like feedback-driven optimization framework.This framework employs a dynamic interactive optimization mechanism forachieving human-centered explainable requirements without incurring high laborcosts. Specifically, we propose to utilize large language models (LLMs) ashuman simulators to predict human-like feedback for guiding the learningprocess. To enable the LLMs to deeply understand the task essence and meetuser's diverse personalized requirements, we introduce a human-inducedcustomized reward scoring method, which helps stimulate the languageunderstanding and logical reasoning capabilities of LLMs. Furthermore,considering the potential conflicts between different perspectives ofexplanation quality, we introduce a principled Pareto optimization thattransforms the multi-perspective quality enhancement task into amulti-objective optimization problem for improving explanation performance. Atlast, to achieve efficient model training, we design an off-policy optimizationpipeline. By incorporating a replay buffer and addressing the data distributionbiases, we can effectively improve data utilization and enhance modelgenerality. Extensive experiments on four datasets demonstrate the superiorityof our approach."}], "temperature": 0.1}}
{"custom_id": "129_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jiakai Tang"}], "temperature": 0.1}}
{"custom_id": "130_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models"}], "temperature": 0.1}}
{"custom_id": "130_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have achieved remarkable multi-step reasoningcapabilities across various domains. However, LLMs still face distinctchallenges in complex logical reasoning, as (1) proof-finding requiressystematic exploration and the maintenance of logical coherence and (2)searching the right combination of premises at each reasoning step isinherently challenging in tasks with large premise space. To address this, wepropose LogicTree, an inference-time modular framework employingalgorithm-guided search to automate structured proof exploration and ensurelogical coherence. Advancing beyond tree-of-thought (ToT), we incorporatecaching mechanism into LogicTree to enable effective utilization of historicalknowledge, preventing reasoning stagnation and minimizing redundancy.Furthermore, we address the combinatorial complexity of premise search bydecomposing it into a linear process. The refined premise selection restrictssubsequent inference to at most one derivation per step, enhancing reasoninggranularity and enforcing strict step-by-step reasoning. Additionally, weintroduce two LLM-free heuristics for premise prioritization, enablingstrategic proof search. Experimental results on five datasets demonstrate thatLogicTree optimally scales inference-time computation to achieve higher proofaccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%and 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4ooutperforms o3-mini by 7.6% on average."}], "temperature": 0.1}}
{"custom_id": "130_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Kang He"}], "temperature": 0.1}}
{"custom_id": "131_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Multi-Stage Retrieval for Operational Technology Cybersecurity Compliance Using Large Language Models: A Railway Casestudy"}], "temperature": 0.1}}
{"custom_id": "131_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Operational Technology Cybersecurity (OTCS) continues to be a dominantchallenge for critical infrastructure such as railways. As these systems becomeincreasingly vulnerable to malicious attacks due to digitalization, effectivedocumentation and compliance processes are essential to protect thesesafety-critical systems. This paper proposes a novel system that leveragesLarge Language Models (LLMs) and multi-stage retrieval to enhance thecompliance verification process against standards like IEC 62443 and therail-specific IEC 63452. We first evaluate a Baseline Compliance Architecture(BCA) for answering OTCS compliance queries, then develop an extended approachcalled Parallel Compliance Architecture (PCA) that incorporates additionalcontext from regulatory standards. Through empirical evaluation comparingOpenAI-gpt-4o and Claude-3.5-haiku models in these architectures, wedemonstrate that the PCA significantly improves both correctness and reasoningquality in compliance verification. Our research establishes metrics forresponse correctness, logical reasoning, and hallucination detection,highlighting the strengths and limitations of using LLMs for complianceverification in railway cybersecurity. The results suggest thatretrieval-augmented approaches can significantly improve the efficiency andaccuracy of compliance assessments, particularly valuable in an industry facinga shortage of cybersecurity expertise."}], "temperature": 0.1}}
{"custom_id": "131_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Regan Bolton"}], "temperature": 0.1}}
{"custom_id": "132_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Context-Awareness and Interpretability of Rare Occurrences for Discovery and Formalization of Critical Failure Modes"}], "temperature": 0.1}}
{"custom_id": "132_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Vision systems are increasingly deployed in critical domains such assurveillance, law enforcement, and transportation. However, theirvulnerabilities to rare or unforeseen scenarios pose significant safety risks.To address these challenges, we introduce Context-Awareness andInterpretability of Rare Occurrences (CAIRO), an ontology-based human-assistivediscovery framework for failure cases (or CP - Critical Phenomena) detectionand formalization. CAIRO by design incentivizes human-in-the-loop for testingand evaluation of criticality that arises from misdetections, adversarialattacks, and hallucinations in AI black-box models. Our robust analysis ofobject detection model(s) failures in automated driving systems (ADS) showcasesscalable and interpretable ways of formalizing the observed gaps between cameraperception and real-world contexts, resulting in test cases stored as explicitknowledge graphs (in OWL/XML format) amenable for sharing, downstream analysis,logical reasoning, and accountability."}], "temperature": 0.1}}
{"custom_id": "132_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Sridevi Polavaram"}], "temperature": 0.1}}
{"custom_id": "133_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via Symbolic-Neural Integration"}], "temperature": 0.1}}
{"custom_id": "133_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advances in Multimodal Large Language Models (MLLMs) have achievedremarkable progress in general domains and demonstrated promise in multimodalmathematical reasoning. However, applying MLLMs to geometry problem solving(GPS) remains challenging due to lack of accurate step-by-step solution dataand severe hallucinations during reasoning. In this paper, we propose GeoGen, apipeline that can automatically generates step-wise reasoning paths forgeometry diagrams. By leveraging the precise symbolic reasoning,\\textbf{GeoGen} produces large-scale, high-quality question-answer pairs. Tofurther enhance the logical reasoning ability of MLLMs, we train\\textbf{GeoLogic}, a Large Language Model (LLM) using synthetic data generatedby GeoGen. Serving as a bridge between natural language and symbolic systems,GeoLogic enables symbolic tools to help verifying MLLM outputs, making thereasoning process more rigorous and alleviating hallucinations. Experimentalresults show that our approach consistently improves the performance of MLLMs,achieving remarkable results on benchmarks for geometric reasoning tasks. Thisimprovement stems from our integration of the strengths of LLMs and symbolicsystems, which enables a more reliable and interpretable approach for the GPStask. Codes are available at https://github.com/ycpNotFound/GeoGen."}], "temperature": 0.1}}
{"custom_id": "133_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yicheng Pan"}], "temperature": 0.1}}
{"custom_id": "134_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LAD-Reasoner: Tiny Multimodal Models are Good Reasoners for Logical Anomaly Detection"}], "temperature": 0.1}}
{"custom_id": "134_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advances in industrial anomaly detection have highlighted the need fordeeper logical anomaly analysis, where unexpected relationships among objects,counts, and spatial configurations must be identified and explained. Existingapproaches often rely on large-scale external reasoning modules or elaboratepipeline designs, hindering practical deployment and interpretability. Toaddress these limitations, we introduce a new task, Reasoning Logical AnomalyDetection (RLAD), which extends traditional anomaly detection by incorporatinglogical reasoning. We propose a new framework, LAD-Reasoner, a customized tinymultimodal language model built on Qwen2.5-VL 3B. Our approach leverages atwo-stage training paradigm that first employs Supervised Fine-Tuning (SFT) forfine-grained visual understanding, followed by Group Relative PolicyOptimization (GRPO) to refine logical anomaly detection and enforce coherent,human-readable reasoning. Crucially, reward signals are derived from both thedetection accuracy and the structural quality of the outputs, obviating theneed for building chain of thought (CoT) reasoning data. Experiments on theMVTec LOCO AD dataset show that LAD-Reasoner, though significantly smaller,matches the performance of Qwen2.5-VL-72B in accuracy and F1 score, and furtherexcels in producing concise and interpretable rationales. This unified designreduces reliance on large models and complex pipelines, while offeringtransparent and interpretable insights into logical anomaly detection. Code anddata will be released."}], "temperature": 0.1}}
{"custom_id": "134_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Weijia Li"}], "temperature": 0.1}}
{"custom_id": "135_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "MediSee: Reasoning-based Pixel-level Perception in Medical Images"}], "temperature": 0.1}}
{"custom_id": "135_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Despite remarkable advancements in pixel-level medical image perception,existing methods are either limited to specific tasks or heavily rely onaccurate bounding boxes or text labels as input prompts. However, the medicalknowledge required for input is a huge obstacle for general public, whichgreatly reduces the universality of these methods. Compared with thesedomain-specialized auxiliary information, general users tend to rely on oralqueries that require logical reasoning. In this paper, we introduce a novelmedical vision task: Medical Reasoning Segmentation and Detection (MedSD),which aims to comprehend implicit queries about medical images and generate thecorresponding segmentation mask and bounding box for the target object. Toaccomplish this task, we first introduce a Multi-perspective, Logic-drivenMedical Reasoning Segmentation and Detection (MLMR-SD) dataset, whichencompasses a substantial collection of medical entity targets along with theircorresponding reasoning. Furthermore, we propose MediSee, an effective baselinemodel designed for medical reasoning segmentation and detection. Theexperimental results indicate that the proposed method can effectively addressMedSD with implicit colloquial queries and outperform traditional medicalreferring segmentation methods."}], "temperature": 0.1}}
{"custom_id": "135_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Qinyue Tong"}], "temperature": 0.1}}
{"custom_id": "136_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "PuzzleBench: A Fully Dynamic Evaluation Framework for Large Multimodal Models on Puzzle Solving"}], "temperature": 0.1}}
{"custom_id": "136_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Multimodal Models (LMMs) have demonstrated impressive capabilitiesacross a wide range of multimodal tasks, achieving ever-increasing performanceon various evaluation benchmarks. However, existing benchmarks are typicallystatic and often overlap with pre-training datasets, leading to fixedcomplexity constraints and substantial data contamination issues. Meanwhile,manually annotated datasets are labor-intensive, time-consuming, and subject tohuman bias and inconsistency, leading to reliability and reproducibilityissues. To address these problems, we propose a fully dynamic multimodalevaluation framework, named Open-ended Visual Puzzle Generation (OVPG), whichaims to generate fresh, diverse, and verifiable evaluation data automaticallyin puzzle-solving tasks. Specifically, the OVPG pipeline consists of a rawmaterial sampling module, a visual content generation module, and a puzzle ruledesign module, which ensures that each evaluation instance is primitive, highlyrandomized, and uniquely solvable, enabling continual adaptation to theevolving capabilities of LMMs. Built upon OVPG, we construct PuzzleBench, adynamic and scalable benchmark comprising 11,840 VQA samples. It features sixcarefully designed puzzle tasks targeting three core LMM competencies, visualrecognition, logical reasoning, and context understanding. PuzzleBench differsfrom static benchmarks that quickly become outdated. It enables ongoing datasetrefreshing through OVPG and a rich set of open-ended puzzle designs, allowingseamless adaptation to the evolving capabilities of LMMs."}], "temperature": 0.1}}
{"custom_id": "136_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zeyu Zhang"}], "temperature": 0.1}}
{"custom_id": "137_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge"}], "temperature": 0.1}}
{"custom_id": "137_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Current multimodal benchmarks often conflate reasoning with domain-specificknowledge, making it difficult to isolate and evaluate general reasoningabilities in non-expert settings. To address this, we introduce VisualPuzzles,a benchmark that targets visual reasoning while deliberately minimizingreliance on specialized knowledge. VisualPuzzles consists of diverse questionsspanning five categories: algorithmic, analogical, deductive, inductive, andspatial reasoning. One major source of our questions is manually translatedlogical reasoning questions from the Chinese Civil Service Examination.Experiments show that VisualPuzzles requires significantly less intensivedomain-specific knowledge and more complex reasoning compared to benchmarkslike MMMU, enabling us to better evaluate genuine multimodal reasoning.Evaluations show that state-of-the-art multimodal large language modelsconsistently lag behind human performance on VisualPuzzles, and that strongperformance on knowledge-intensive benchmarks does not necessarily translate tosuccess on reasoning-focused, knowledge-light tasks. Additionally, reasoningenhancements such as scaling up inference compute (with \"thinking\" modes) yieldinconsistent gains across models and task types, and we observe no clearcorrelation between model size and performance. We also found that modelsexhibit different reasoning and answering patterns on VisualPuzzles compared tobenchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearerlens through which to evaluate reasoning capabilities beyond factual recall anddomain knowledge."}], "temperature": 0.1}}
{"custom_id": "137_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yueqi Song"}], "temperature": 0.1}}
{"custom_id": "138_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training"}], "temperature": 0.1}}
{"custom_id": "138_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advances in reinforcement learning (RL)-based post-training have ledto notable improvements in large language models (LLMs), particularly inenhancing their reasoning capabilities to handle complex tasks. However, mostexisting methods treat the training data as a unified whole, overlooking thefact that modern LLM training often involves a mixture of data from diversedistributions-varying in both source and difficulty. This heterogeneityintroduces a key challenge: how to adaptively schedule training acrossdistributions to optimize learning efficiency. In this paper, we present aprincipled curriculum learning framework grounded in the notion ofdistribution-level learnability. Our core insight is that the magnitude ofpolicy advantages reflects how much a model can still benefit from furthertraining on a given distribution. Based on this, we propose adistribution-level curriculum learning framework for RL-based LLMpost-training, which leverages the Upper Confidence Bound (UCB) principle todynamically adjust sampling probabilities for different distrubutions. Thisapproach prioritizes distributions with either high average advantage(exploitation) or low sample count (exploration), yielding an adaptive andtheoretically grounded training schedule. We instantiate our curriculumlearning framework with GRPO as the underlying RL algorithm and demonstrate itseffectiveness on logic reasoning datasets with multiple difficulties andsources. Our experiments show that our framework significantly improvesconvergence speed and final performance, highlighting the value ofdistribution-aware curriculum strategies in LLM post-training. Code:https://github.com/ZhentingWang/DUMP."}], "temperature": 0.1}}
{"custom_id": "138_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhenting Wang"}], "temperature": 0.1}}
{"custom_id": "139_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SDIGLM: Leveraging Large Language Models and Multi-Modal Chain of Thought for Structural Damage Identification"}], "temperature": 0.1}}
{"custom_id": "139_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Existing computer vision(CV)-based structural damage identification modelsdemonstrate notable accuracy in categorizing and localizing damage. However,these models present several critical limitations that hinder their practicalapplication in civil engineering(CE). Primarily, their ability to recognizedamage types remains constrained, preventing comprehensive analysis of thehighly varied and complex conditions encountered in real-world CE structures.Second, these models lack linguistic capabilities, rendering them unable toarticulate structural damage characteristics through natural languagedescriptions. With the continuous advancement of artificial intelligence(AI),large multi-modal models(LMMs) have emerged as a transformative solution,enabling the unified encoding and alignment of textual and visual data. Thesemodels can autonomously generate detailed descriptive narratives of structuraldamage while demonstrating robust generalization across diverse scenarios andtasks. This study introduces SDIGLM, an innovative LMM for structural damageidentification, developed based on the open-source VisualGLM-6B architecture.To address the challenge of adapting LMMs to the intricate and varied operatingconditions in CE, this work integrates a U-Net-based semantic segmentationmodule to generate defect segmentation maps as visual Chain of Thought(CoT).Additionally, a multi-round dialogue fine-tuning dataset is constructed toenhance logical reasoning, complemented by a language CoT formed through promptengineering. By leveraging this multi-modal CoT, SDIGLM surpassesgeneral-purpose LMMs in structural damage identification, achieving an accuracyof 95.24% across various infrastructure types. Moreover, the model effectivelydescribes damage characteristics such as hole size, crack direction, andcorrosion severity."}], "temperature": 0.1}}
{"custom_id": "139_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yunkai Zhang"}], "temperature": 0.1}}
{"custom_id": "140_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Towards Stepwise Domain Knowledge-Driven Reasoning Optimization and Reflection Improvement"}], "temperature": 0.1}}
{"custom_id": "140_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recently, stepwise supervision on Chain of Thoughts (CoTs) presents anenhancement on the logical reasoning tasks such as coding and math, with thehelp of Monte Carlo Tree Search (MCTS). However, its contribution to tasksrequiring domain-specific expertise and knowledge remains unexplored. Motivatedby the interest, we identify several potential challenges of vanilla MCTSwithin this context, and propose the framework of Stepwise DomainKnowledge-Driven Reasoning Optimization, employing the MCTS algorithm todevelop step-level supervision for problems that require essentialcomprehension, reasoning, and specialized knowledge. Additionally, we alsointroduce the Preference Optimization towards Reflection Paths, whichiteratively learns self-reflection on the reasoning thoughts from betterperspectives. We have conducted extensive experiments to evaluate the advantageof the methodologies. Empirical results demonstrate the effectiveness onvarious legal-domain problems. We also report a diverse set of valuablefindings, hoping to encourage the enthusiasm to the research of domain-specificLLMs and MCTS."}], "temperature": 0.1}}
{"custom_id": "140_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chengyuan Liu"}], "temperature": 0.1}}
{"custom_id": "141_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Alice: Proactive Learning with Teacher's Demonstrations for Weak-to-Strong Generalization"}], "temperature": 0.1}}
{"custom_id": "141_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The growing capabilities of large language models (LLMs) present a keychallenge of maintaining effective human oversight. Weak-to-stronggeneralization (W2SG) offers a promising framework for supervising increasinglycapable LLMs using weaker ones. Traditional W2SG methods rely on passivelearning, where a weak teacher provides noisy demonstrations to train a strongstudent. This hinders students from employing their knowledge during trainingand reaching their full potential. In this work, we introduce Alice(pro{A}ctive {l}earning w{i}th tea{c}her's D{e}monstrations), a framework thatleverages complementary knowledge between teacher and student to enhance thelearning process. We probe the knowledge base of the teacher model by elicitingtheir uncertainty, and then use these insights together with teachers'responses as demonstrations to guide student models in self-generating improvedresponses for supervision. In addition, for situations with significantcapability gaps between teacher and student models, we introduce cascade Alice,which employs a hierarchical training approach where weak teachers initiallysupervise intermediate models, who then guide stronger models in sequence.Experimental results demonstrate that our method significantly enhances theW2SG performance, yielding substantial improvements in three key tasks comparedto the original W2SG: knowledge-based reasoning (+4.0%), mathematical reasoning(+22.62%), and logical reasoning (+12.11%). This highlights the effectivenessof our new W2SG paradigm that enables more robust knowledge transfer andsupervision outcome."}], "temperature": 0.1}}
{"custom_id": "141_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Shujin Wu"}], "temperature": 0.1}}
{"custom_id": "142_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "MovSAM: A Single-image Moving Object Segmentation Framework Based on Deep Thinking"}], "temperature": 0.1}}
{"custom_id": "142_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Moving object segmentation plays a vital role in understanding dynamic visualenvironments. While existing methods rely on multi-frame image sequences toidentify moving objects, single-image MOS is critical for applications likemotion intention prediction and handling camera frame drops. However,segmenting moving objects from a single image remains challenging for existingmethods due to the absence of temporal cues. To address this gap, we proposeMovSAM, the first framework for single-image moving object segmentation. MovSAMleverages a Multimodal Large Language Model (MLLM) enhanced withChain-of-Thought (CoT) prompting to search the moving object and generate textprompts based on deep thinking for segmentation. These prompts are cross-fusedwith visual features from the Segment Anything Model (SAM) and aVision-Language Model (VLM), enabling logic-driven moving object segmentation.The segmentation results then undergo a deep thinking refinement loop, allowingMovSAM to iteratively improve its understanding of the scene context andinter-object relationships with logical reasoning. This innovative approachenables MovSAM to segment moving objects in single images by considering sceneunderstanding. We implement MovSAM in the real world to validate its practicalapplication and effectiveness for autonomous driving scenarios where themulti-frame methods fail. Furthermore, despite the inherent advantage ofmulti-frame methods in utilizing temporal information, MovSAM achievesstate-of-the-art performance across public MOS benchmarks, reaching 92.5\\% onJ\\&F. Our implementation will be available athttps://github.com/IRMVLab/MovSAM."}], "temperature": 0.1}}
{"custom_id": "142_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chang Nie"}], "temperature": 0.1}}
{"custom_id": "143_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large Language Models with Logic Programming-based Test Oracles"}], "temperature": 0.1}}
{"custom_id": "143_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have achieved significant progress in languageunderstanding and reasoning. Evaluating and analyzing their logical reasoningabilities has therefore become essential. However, existing datasets andbenchmarks are often limited to overly simplistic, unnatural, or contextuallyconstrained examples. In response to the growing demand, we introduceSmartyPat-Bench, a challenging, naturally expressed, and systematically labeledbenchmark derived from real-world high-quality Reddit posts containing subtlelogical fallacies. Unlike existing datasets and benchmarks, it provides moredetailed annotations of logical fallacies and features more diverse data. Tofurther scale up the study and address the limitations of manual datacollection and labeling - such as fallacy-type imbalance and labor-intensiveannotation - we introduce SmartyPat, an automated framework powered by logicprogramming-based oracles. SmartyPat utilizes Prolog rules to systematicallygenerate logically fallacious statements, which are then refined into fluentnatural-language sentences by LLMs, ensuring precise fallacy representation.Extensive evaluation demonstrates that SmartyPat produces fallacies comparablein subtlety and quality to human-generated content and significantlyoutperforms baseline methods. Finally, experiments reveal nuanced insights intoLLM capabilities, highlighting that while excessive reasoning steps hinderfallacy detection accuracy, structured reasoning enhances fallacycategorization performance."}], "temperature": 0.1}}
{"custom_id": "143_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zihao Xu"}], "temperature": 0.1}}
{"custom_id": "144_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Don't Let It Hallucinate: Premise Verification via Retrieval-Augmented Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "144_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have shown substantial capacity for generatingfluent, contextually appropriate responses. However, they can producehallucinated outputs, especially when a user query includes one or more falsepremises-claims that contradict established facts. Such premises can misleadLLMs into offering fabricated or misleading details. Existing approachesinclude pretraining, fine-tuning, and inference-time techniques that often relyon access to logits or address hallucinations after they occur. These methodstend to be computationally expensive, require extensive training data, or lackproactive mechanisms to prevent hallucination before generation, limiting theirefficiency in real-time applications. We propose a retrieval-based frameworkthat identifies and addresses false premises before generation. Our methodfirst transforms a user's query into a logical representation, then appliesretrieval-augmented generation (RAG) to assess the validity of each premiseusing factual sources. Finally, we incorporate the verification results intothe LLM's prompt to maintain factual consistency in the final output.Experiments show that this approach effectively reduces hallucinations,improves factual accuracy, and does not require access to model logits orlarge-scale fine-tuning."}], "temperature": 0.1}}
{"custom_id": "144_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yuehan Qin"}], "temperature": 0.1}}
{"custom_id": "145_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning Models Know When They're Right: Probing Hidden States for Self-Verification"}], "temperature": 0.1}}
{"custom_id": "145_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning models have achieved remarkable performance on tasks like math andlogical reasoning thanks to their ability to search during reasoning. However,they still suffer from overthinking, often performing unnecessary reasoningsteps even after reaching the correct answer. This raises the question: canmodels evaluate the correctness of their intermediate answers during reasoning?In this work, we study whether reasoning models encode information about answercorrectness through probing the model's hidden states. The resulting probe canverify intermediate answers with high accuracy and produces highly calibratedscores. Additionally, we find models' hidden states encode correctness offuture answers, enabling early prediction of the correctness before theintermediate answer is fully formulated. We then use the probe as a verifier todecide whether to exit reasoning at intermediate answers during inference,reducing the number of inference tokens by 24\\% without compromisingperformance. These findings confirm that reasoning models do encode a notion ofcorrectness yet fail to exploit it, revealing substantial untapped potential toenhance their efficiency."}], "temperature": 0.1}}
{"custom_id": "145_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Anqi Zhang"}], "temperature": 0.1}}
{"custom_id": "146_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Provable Failure of Language Models in Learning Majority Boolean Logic via Gradient Descent"}], "temperature": 0.1}}
{"custom_id": "146_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advancements in Transformer-based architectures have led to impressivebreakthroughs in natural language processing tasks, with models such as GPT-4,Claude, and Gemini demonstrating human-level reasoning abilities. However,despite their high performance, concerns remain about the inherent limitationsof these models, especially when it comes to learning basic logical functions.While complexity-theoretic analyses indicate that Transformers can representsimple logic functions (e.g., $\\mathsf{AND}$, $\\mathsf{OR}$, and majoritygates) by its nature of belonging to the $\\mathsf{TC}^0$ class, these resultsassume ideal parameter settings and do not account for the constraints imposedby gradient descent-based training methods. In this work, we investigatewhether Transformers can truly learn simple majority functions when trainedusing gradient-based methods. We focus on a simplified variant of theTransformer architecture and consider both $n=\\mathrm{poly}(d)$ and$n=\\exp(\\Omega(d))$ number of training samples, where each sample is a $d$-sizebinary string paired with the output of a basic majority function. Our analysisdemonstrates that even after $\\mathrm{poly}(d)$ gradient queries, thegeneralization error of the Transformer model still remains substantiallylarge, growing exponentially with $d$. This work highlights fundamentaloptimization challenges in training Transformers for the simplest logicalreasoning tasks and provides new insights into their theoretical limitations."}], "temperature": 0.1}}
{"custom_id": "146_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Bo Chen"}], "temperature": 0.1}}
{"custom_id": "147_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Have Large Language Models Learned to Reason? A Characterization via 3-SAT Phase Transition"}], "temperature": 0.1}}
{"custom_id": "147_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have been touted as AI models possessingadvanced reasoning abilities. In theory, autoregressive LLMs withChain-of-Thought (CoT) can perform more serial computations to solve complexreasoning tasks. However, recent studies suggest that, despite this capacity,LLMs do not truly learn to reason but instead fit on statistical features. Tostudy the reasoning capabilities in a principled fashion, we adopt acomputational theory perspective and propose an experimental protocol centeredon 3-SAT -- the prototypical NP-complete problem lying at the core of logicalreasoning and constraint satisfaction tasks. Specifically, we examine the phasetransitions in random 3-SAT and characterize the reasoning abilities ofstate-of-the-art LLMs by varying the inherent hardness of the probleminstances. By comparing DeepSeek R1 with other LLMs, our findings reveal twokey insights (1) LLM accuracy drops significantly on harder instances,suggesting all current models struggle when statistical shortcuts areunavailable (2) Unlike other LLMs, R1 shows signs of having learned theunderlying reasoning. Following a principled experimental protocol, our studymoves beyond the benchmark-driven evidence often found in LLM reasoningresearch. Our findings highlight important gaps and suggest clear directionsfor future research."}], "temperature": 0.1}}
{"custom_id": "147_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Rishi Hazra"}], "temperature": 0.1}}
{"custom_id": "148_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing"}], "temperature": 0.1}}
{"custom_id": "148_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Multi-modality Models (LMMs) have made significant progress in visualunderstanding and generation, but they still face challenges in General VisualEditing, particularly in following complex instructions, preserving appearanceconsistency, and supporting flexible input formats. To study this gap, weintroduce RISEBench, the first benchmark for evaluating Reasoning-InformedviSual Editing (RISE). RISEBench focuses on four key reasoning categories:Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality testcases for each category and propose an robust evaluation framework thatassesses Instruction Reasoning, Appearance Consistency, and Visual Plausibilitywith both human judges and the LMM-as-a-judge approach. We conductedexperiments evaluating nine prominent visual editing models, comprising bothopen-source and proprietary models. The evaluation results demonstrate thatcurrent models face significant challenges in reasoning-based editing tasks.Even the most powerful model evaluated, GPT-4o-Image, achieves an accuracy ofmerely 28.8%. RISEBench effectively highlights the limitations of contemporaryediting models, provides valuable insights, and indicates potential futuredirections for the field of reasoning-aware visual editing. Our code and datahave been released at https://github.com/PhoenixZ810/RISEBench."}], "temperature": 0.1}}
{"custom_id": "148_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xiangyu Zhao"}], "temperature": 0.1}}
{"custom_id": "149_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Adaptive Rectification Sampling for Test-Time Compute Scaling"}], "temperature": 0.1}}
{"custom_id": "149_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The newly released OpenAI-o1 and DeepSeek-R1 have demonstrated that test-timescaling can significantly improve model performance, especially in complextasks such as logical reasoning. Common test-time scaling methods involvegenerating more chain of thoughts (CoTs) or longer CoTs with self-correction.However, while self-correction can improve performance, it may lead tosignificant token waste and reduce readability of the CoT if the reasoningsteps are already correct. To demonstrate that large language models (LLMs) canrectify errors at a more fine-grained level, we propose Adaptive RectificationSampling (AR-Sampling), which can guide the LLMs to self-correction at theappropriate step. AR-Sampling leverages a process-supervised reward model (PRM)as a verifier and constructed trigger sentences to guide the model in adaptivestep-level rethinking. Through the experiments on GSM8K and MATH500, itindicate that our approach enables the models to rethink in more fine-grainedlevel, improving the accuracy of solutions, while generating a reasonablenumber of additional tokens."}], "temperature": 0.1}}
{"custom_id": "149_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhendong Tan"}], "temperature": 0.1}}
{"custom_id": "150_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1"}], "temperature": 0.1}}
{"custom_id": "150_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advancements in Chain of Thought (COT) generation have significantlyimproved the reasoning capabilities of Large Language Models (LLMs), withreinforcement learning (RL) emerging as an effective post-training approach.Multimodal Large Language Models (MLLMs) inherit this reasoning potential butremain underexplored in tasks requiring both perception and logical reasoning.To address this, we introduce SEED-Bench-R1, a benchmark designed tosystematically evaluate post-training methods for MLLMs in video understanding.It includes intricate real-world videos and complex everyday planning tasks inthe format of multiple-choice questions, requiring sophisticated perception andreasoning. SEED-Bench-R1 assesses generalization through a three-levelhierarchy: in-distribution, cross-environment, and cross-environment-taskscenarios, equipped with a large-scale training dataset with easily verifiableground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RLwith supervised fine-tuning (SFT), demonstrating RL's data efficiency andsuperior performance on both in-distribution and out-of-distribution tasks,even outperforming SFT on general video understanding benchmarks likeLongVideoBench. Our detailed analysis reveals that RL enhances visualperception but often produces less logically coherent reasoning chains. Weidentify key limitations such as inconsistent reasoning and overlooked visualcues, and suggest future improvements in base model reasoning, reward modeling,and RL robustness against noisy signals."}], "temperature": 0.1}}
{"custom_id": "150_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yi Chen"}], "temperature": 0.1}}
{"custom_id": "151_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models"}], "temperature": 0.1}}
{"custom_id": "151_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Vision-Language Models (LVLMs) struggle with puzzles, which requireprecise perception, rule comprehension, and logical reasoning. Assessing andenhancing their performance in this domain is crucial, as it reflects theirability to engage in structured reasoning - an essential skill for real-worldproblem-solving. However, existing benchmarks primarily evaluate pre-trainedmodels without additional training or fine-tuning, often lack a dedicated focuson reasoning, and fail to establish a systematic evaluation framework. Toaddress these limitations, we introduce VGRP-Bench, a Visual Grid ReasoningPuzzle Benchmark featuring 20 diverse puzzles. VGRP-Bench spans multipledifficulty levels, and includes extensive experiments not only on existing chatLVLMs (e.g., GPT-4o), but also on reasoning LVLMs (e.g., Gemini-Thinking). Ourresults reveal that even the state-of-the-art LVLMs struggle with thesepuzzles, highlighting fundamental limitations in their puzzle-solvingcapabilities. Most importantly, through systematic experiments, we identify andanalyze key factors influencing LVLMs' puzzle-solving performance, includingthe number of clues, grid size, and rule complexity. Furthermore, we exploretwo Supervised Fine-Tuning (SFT) strategies that can be used in post-training:SFT on solutions (S-SFT) and SFT on synthetic reasoning processes (R-SFT).While both methods significantly improve performance on trained puzzles, theyexhibit limited generalization to unseen ones. We will release VGRP-Bench tofacilitate further research on LVLMs for complex, real-world problem-solving.Project page: https://yufan-ren.com/subpage/VGRP-Bench/."}], "temperature": 0.1}}
{"custom_id": "151_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yufan Ren"}], "temperature": 0.1}}
{"custom_id": "152_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "QuestBench: Can LLMs ask the right question to acquire information in reasoning tasks?"}], "temperature": 0.1}}
{"custom_id": "152_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recently, a large amount of work has focused on improving large languagemodels' (LLMs') performance on reasoning benchmarks such as math and logic.However, past work has largely assumed that tasks are well-defined. In the realworld, queries to LLMs are often underspecified, only solvable throughacquiring missing information. We formalize this as a constraint satisfactionproblem (CSP) with missing variable assignments. Using a special case of thisformalism where only one necessary variable assignment is missing, we canrigorously evaluate an LLM's ability to identify the minimal necessary questionto ask and quantify axes of difficulty levels for each problem. We presentQuestBench, a set of underspecified reasoning tasks solvable by asking at mostone question, which includes: (1) Logic-Q: Logical reasoning tasks with onemissing proposition, (2) Planning-Q: PDDL planning problems with initial statesthat are partially-observed, (3) GSM-Q: Human-annotated grade school mathproblems with one missing variable assignment, and (4) GSME-Q: a version ofGSM-Q where word problems are translated into equations by human annotators.The LLM is tasked with selecting the correct clarification question(s) from alist of options. While state-of-the-art models excel at GSM-Q and GSME-Q, theiraccuracy is only 40-50% on Logic-Q and Planning-Q. Analysis demonstrates thatthe ability to solve well-specified reasoning problems may not be sufficientfor success on our benchmark: models have difficulty identifying the rightquestion to ask, even when they can solve the fully specified version of theproblem. Furthermore, in the Planning-Q domain, LLMs tend not to hedge, evenwhen explicitly presented with the option to predict ``not sure.'' Thishighlights the need for deeper investigation into models' informationacquisition capabilities."}], "temperature": 0.1}}
{"custom_id": "152_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Belinda Z. Li"}], "temperature": 0.1}}
{"custom_id": "153_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Negation: A Pink Elephant in the Large Language Models' Room?"}], "temperature": 0.1}}
{"custom_id": "153_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Negations are key to determining sentence meaning, making them essential forlogical reasoning. Despite their importance, negations pose a substantialchallenge for large language models (LLMs) and remain underexplored.  We constructed and published two new textual entailment datasets NoFEVER-MLand NoSNLI-ML in four languages (English, Czech, German, and Ukrainian) with  examples differing in negation. It allows investigation of the root causes ofthe negation problem and its exemplification: how popular LLM model propertiesand language impact their inability to handle negation correctly.  Contrary to previous work, we show that increasing the model size may improvethe models' ability to handle negations. Furthermore, we find that both themodels' reasoning accuracy and robustness to negation are language-dependentand that the length and explicitness of the premise have an impact onrobustness. There is better accuracy in projective language with fixed order,such as English, than in non-projective ones, such as German or Czech.  Our entailment datasets pave the way to further research for explanation andexemplification of the negation problem, minimization of LLM hallucinations,and improvement of LLM reasoning in multilingual settings."}], "temperature": 0.1}}
{"custom_id": "153_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Tereza Vrabcov谩"}], "temperature": 0.1}}
{"custom_id": "154_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "ShieldAgent: Shielding Agents via Verifiable Safety Policy Reasoning"}], "temperature": 0.1}}
{"custom_id": "154_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Autonomous agents powered by foundation models have seen widespread adoptionacross various real-world applications. However, they remain highly vulnerableto malicious instructions and attacks, which can result in severe consequencessuch as privacy breaches and financial losses. More critically, existingguardrails for LLMs are not applicable due to the complex and dynamic nature ofagents. To tackle these challenges, we propose ShieldAgent, the first guardrailagent designed to enforce explicit safety policy compliance for the actiontrajectory of other protected agents through logical reasoning. Specifically,ShieldAgent first constructs a safety policy model by extracting verifiablerules from policy documents and structuring them into a set of action-basedprobabilistic rule circuits. Given the action trajectory of the protectedagent, ShieldAgent retrieves relevant rule circuits and generates a shieldingplan, leveraging its comprehensive tool library and executable code for formalverification. In addition, given the lack of guardrail benchmarks for agents,we introduce ShieldAgent-Bench, a dataset with 3K safety-related pairs of agentinstructions and action trajectories, collected via SOTA attacks across 6 webenvironments and 7 risk categories. Experiments show that ShieldAgent achievesSOTA on ShieldAgent-Bench and three existing benchmarks, outperforming priormethods by 11.3% on average with a high recall of 90.1%. Additionally,ShieldAgent reduces API queries by 64.7% and inference time by 58.2%,demonstrating its high precision and efficiency in safeguarding agents."}], "temperature": 0.1}}
{"custom_id": "154_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhaorun Chen"}], "temperature": 0.1}}
{"custom_id": "155_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning"}], "temperature": 0.1}}
{"custom_id": "155_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) are primarily trained on high-resource naturallanguages, limiting their effectiveness in low-resource settings and in tasksrequiring deep logical reasoning. This research introduces Rosetta-PL, abenchmark designed to evaluate LLMs' logical reasoning and generalizationcapabilities in a controlled environment. We construct Rosetta-PL bytranslating a dataset of logical propositions from Lean into a custom logicallanguage, which is then used to fine-tune an LLM (e.g., GPT-4o). Ourexperiments analyze the impact of the size of the dataset and the translationmethodology on the performance of the model. Our results indicate thatpreserving logical relationships in the translation process significantlyboosts precision, with accuracy plateauing beyond roughly 20,000 trainingsamples. These insights provide valuable guidelines for optimizing LLM trainingin formal reasoning tasks and improving performance in various low-resourcelanguage applications."}], "temperature": 0.1}}
{"custom_id": "155_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Shaun Baek"}], "temperature": 0.1}}
{"custom_id": "156_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Study on Neuro-Symbolic Artificial Intelligence: Healthcare Perspectives"}], "temperature": 0.1}}
{"custom_id": "156_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Over the last few decades, Artificial Intelligence (AI) scientists have beenconducting investigations to attain human-level performance by a machine inaccomplishing a cognitive task. Within machine learning, the ultimateaspiration is to attain Artificial General Intelligence (AGI) through amachine. This pursuit has led to the exploration of two distinct AI paradigms.Symbolic AI, also known as classical or GOFAI (Good Old-Fashioned AI) andConnectionist (Sub-symbolic) AI, represented by Neural Systems, are twomutually exclusive paradigms. Symbolic AI excels in reasoning, explainability,and knowledge representation but faces challenges in processing complexreal-world data with noise. Conversely, deep learning (Black-Box systems)research breakthroughs in neural networks are notable, yet they lack reasoningand interpretability. Neuro-symbolic AI (NeSy), an emerging area of AIresearch, attempts to bridge this gap by integrating logical reasoning intoneural networks, enabling them to learn and reason with symbolicrepresentations. While a long path, this strategy has made significant progresstowards achieving common sense reasoning by systems. This article conducts anextensive review of over 977 studies from prominent scientific databases (DBLP,ACL, IEEExplore, Scopus, PubMed, ICML, ICLR), thoroughly examining themultifaceted capabilities of Neuro-Symbolic AI, with a particular focus on itshealthcare applications, particularly in drug discovery, and Proteinengineering research. The survey addresses vital themes, including reasoning,explainability, integration strategies, 41 healthcare-related use cases,benchmarking, datasets, current approach limitations from both healthcare andbroader perspectives, and proposed novel approaches for future experiments."}], "temperature": 0.1}}
{"custom_id": "156_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Delower Hossain"}], "temperature": 0.1}}
{"custom_id": "157_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "(G)I-DLE: Generative Inference via Distribution-preserving Logit Exclusion with KL Divergence Minimization for Constrained Decoding"}], "temperature": 0.1}}
{"custom_id": "157_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We propose (G)I-DLE, a new approach to constrained decoding that leverages KLdivergence minimization to preserve the intrinsic conditional probabilitydistribution of autoregressive language models while excluding undesirabletokens. Unlike conventional methods that naively set banned tokens' logits to$-\\infty$, which can distort the conversion from raw logits to posteriorprobabilities and increase output variance, (G)I-DLE re-normalizes the allowedtoken probabilities to minimize such distortion. We validate our method on theK2-Eval dataset, specifically designed to assess Korean language fluency,logical reasoning, and cultural appropriateness. Experimental results onQwen2.5 models (ranging from 1.5B to 14B) demonstrate that G-IDLE not onlyboosts mean evaluation scores but also substantially reduces the variance ofoutput quality."}], "temperature": 0.1}}
{"custom_id": "157_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hanwool Lee"}], "temperature": 0.1}}
{"custom_id": "158_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Enhancing Retrieval Systems with Inference-Time Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "158_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Traditional retrieval methods rely on transforming user queries into vectorrepresentations and retrieving documents based on cosine similarity within anembedding space. While efficient and scalable, this approach often fails tohandle complex queries involving logical constructs such as negations,conjunctions, and disjunctions. In this paper, we propose a novelinference-time logical reasoning framework that explicitly incorporates logicalreasoning into the retrieval process. Our method extracts logical reasoningstructures from natural language queries and then composes the individualcosine similarity scores to formulate the final document scores. This approachenables the retrieval process to handle complex logical reasoning withoutcompromising computational efficiency. Our results on both synthetic andreal-world benchmarks demonstrate that the proposed method consistentlyoutperforms traditional retrieval methods across different models and datasets,significantly improving retrieval performance for complex queries."}], "temperature": 0.1}}
{"custom_id": "158_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Felix Faltings"}], "temperature": 0.1}}
{"custom_id": "159_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "MedAgent-Pro: Towards Evidence-based Multi-modal Medical Diagnosis via Reasoning Agentic Workflow"}], "temperature": 0.1}}
{"custom_id": "159_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In modern medicine, clinical diagnosis relies on the comprehensive analysisof primarily textual and visual data, drawing on medical expertise to ensuresystematic and rigorous reasoning. Recent advances in large Vision-LanguageModels (VLMs) and agent-based methods hold great potential for medicaldiagnosis, thanks to the ability to effectively integrate multi-modal patientdata. However, they often provide direct answers and draw empirical-drivenconclusions without quantitative analysis, which reduces their reliability andclinical usability. We propose MedAgent-Pro, a new agentic reasoning paradigmthat follows the diagnosis principle in modern medicine, to decouple theprocess into sequential components for step-by-step, evidence-based reasoning.Our MedAgent-Pro workflow presents a hierarchical diagnostic structure tomirror this principle, consisting of disease-level standardized plan generationand patient-level personalized step-by-step reasoning. To support disease-levelplanning, an RAG-based agent is designed to retrieve medical guidelines toensure alignment with clinical standards. For patient-level reasoning, wepropose to integrate professional tools such as visual models to enablequantitative assessments. Meanwhile, we propose to verify the reliability ofeach step to achieve evidence-based diagnosis, enforcing rigorous logicalreasoning and a well-founded conclusion. Extensive experiments across a widerange of anatomical regions, imaging modalities, and diseases demonstrate thesuperiority of MedAgent-Pro to mainstream VLMs, agentic systems andstate-of-the-art expert models. Ablation studies and human evaluation byclinical experts further validate its robustness and clinical relevance. Codeis available at https://github.com/jinlab-imvr/MedAgent-Pro."}], "temperature": 0.1}}
{"custom_id": "159_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ziyue Wang"}], "temperature": 0.1}}
{"custom_id": "160_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in Reinforcement Learning"}], "temperature": 0.1}}
{"custom_id": "160_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Deep Reinforcement Learning (DRL) has demonstrated strong performance inrobotic control but remains susceptible to out-of-distribution (OOD) states,often resulting in unreliable actions and task failure. While previous methodshave focused on minimizing or preventing OOD occurrences, they largely neglectrecovery once an agent encounters such states. Although the latest research hasattempted to address this by guiding agents back to in-distribution states,their reliance on uncertainty estimation hinders scalability in complexenvironments. To overcome this limitation, we introduce Language Models forOut-of-Distribution Recovery (LaMOuR), which enables recovery learning withoutrelying on uncertainty estimation. LaMOuR generates dense reward codes thatguide the agent back to a state where it can successfully perform its originaltask, leveraging the capabilities of LVLMs in image description, logicalreasoning, and code generation. Experimental results show that LaMOuRsubstantially enhances recovery efficiency across diverse locomotion tasks andeven generalizes effectively to complex environments, including humanoidlocomotion and mobile manipulation, where existing methods struggle. The codeand supplementary materials are available at https://lamour-rl.github.io/."}], "temperature": 0.1}}
{"custom_id": "160_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chan Kim"}], "temperature": 0.1}}
{"custom_id": "161_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Bridging Technology and Humanities: Evaluating the Impact of Large Language Models on Social Sciences Research with DeepSeek-R1"}], "temperature": 0.1}}
{"custom_id": "161_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In recent years, the development of Large Language Models (LLMs) has madesignificant breakthroughs in the field of natural language processing and hasgradually been applied to the field of humanities and social sciences research.LLMs have a wide range of application value in the field of humanities andsocial sciences because of its strong text understanding, generation andreasoning capabilities. In humanities and social sciences research, LLMs cananalyze large-scale text data and make inferences.  This article analyzes the large language model DeepSeek-R1 from sevenaspects: low-resource language translation, educational question-answering,student writing improvement in higher education, logical reasoning, educationalmeasurement and psychometrics, public health policy analysis, and art education. Then we compare the answers given by DeepSeek-R1 in the seven aspects withthe answers given by o1-preview. DeepSeek-R1 performs well in the humanitiesand social sciences, answering most questions correctly and logically, and cangive reasonable analysis processes and explanations. Compared with o1-preview,it can automatically generate reasoning processes and provide more detailedexplanations, which is suitable for beginners or people who need to have adetailed understanding of this knowledge, while o1-preview is more suitable forquick reading.  Through analysis, it is found that LLM has broad application potential in thefield of humanities and social sciences, and shows great advantages inimproving text analysis efficiency, language communication and other fields.LLM's powerful language understanding and generation capabilities enable it todeeply explore complex problems in the field of humanities and social sciences,and provide innovative tools for academic research and practical applications."}], "temperature": 0.1}}
{"custom_id": "161_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Peiran Gu"}], "temperature": 0.1}}
{"custom_id": "162_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "From Chaos to Order: The Atomic Reasoner Framework for Fine-grained Reasoning in Large Language Models"}], "temperature": 0.1}}
{"custom_id": "162_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advances in large language models (LLMs) have shown remarkableprogress, yet their capacity for logical ``slow-thinking'' reasoning persistsas a critical research frontier. Current inference scaling paradigms sufferfrom two fundamental constraints: fragmented thought flows compromising logicalcoherence, and intensively computational complexity that escalates with searchspace dimensions. To overcome these limitations, we present \\textbf{AtomicReasoner} (\\textbf{AR}), a cognitive inference strategy that enablesfine-grained reasoning through systematic atomic-level operations. ARdecomposes the reasoning process into atomic cognitive units, employing acognitive routing mechanism to dynamically construct reasoning representationsand orchestrate inference pathways. This systematic methodology implementsstepwise, structured cognition, which ensures logical coherence whilesignificantly reducing cognitive load, effectively simulating the cognitivepatterns observed in human deep thinking processes. Extensive experimentalresults demonstrate AR's superior reasoning capabilities without thecomputational burden of exhaustive solution searches, particularly excelling inlinguistic logic puzzles. These findings substantiate AR's effectiveness inenhancing LLMs' capacity for robust, long-sequence logical reasoning anddeliberation."}], "temperature": 0.1}}
{"custom_id": "162_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jinyi Liu"}], "temperature": 0.1}}
{"custom_id": "163_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Think Like Human Developers: Harnessing Community Knowledge for Structured Code Reasoning"}], "temperature": 0.1}}
{"custom_id": "163_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have significantly advanced automated codegeneration, yet they struggle with complex coding tasks requiring multi-steplogical reasoning. High-quality reasoning data is crucial for improving LLMs'reasoning capabilities, but such datasets remain scarce. Existing approacheseither rely on computationally expensive reinforcement learning (RL) orerror-prone reasoning chains synthesized by LLMs, posing challenges inscalability and accuracy.  To address this challenge, we propose SVRC (Structured and ValidatedReasoning Chains for Code Generation), a novel framework that mines,restructures, and enriches reasoning chains from community-driven discussionson software engineering platforms. SVRC refines unstructured and incompletediscussions of coding problems by aligning them with Software Development LifeCycle (SDLC) principles, ensuring that reasoning chains capture real-worldproblem-solving strategies and support iterative refinement.  To evaluate the effectiveness of SVRC, we introduce CodeThinker, an LLMfine-tuned on 12,444 reasoning-augmented samples generated by SVRC. Experimentson LiveCodeBench show that CodeThinker surpasses its base model by 42.86\\% onmedium-level code problems in terms of pass@1 and outperforms GPT-4o-mini andGPT-4o by 73.14\\% and 115.86\\%, respectively. Our ablation study furtherhighlights that each component of SVRC contributes to the reasoningcapabilities of CodeThinker."}], "temperature": 0.1}}
{"custom_id": "163_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chengran Yang"}], "temperature": 0.1}}
{"custom_id": "164_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Measuring AI Ability to Complete Long Tasks"}], "temperature": 0.1}}
{"custom_id": "164_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmarkperformance remains unclear. To quantify the capabilities of AI systems interms of human capabilities, we propose a new metric: 50%-task-completion timehorizon. This is the time humans typically take to complete tasks that AImodels can complete with 50% success rate. We first timed humans with relevantdomain expertise on a combination of RE-Bench, HCAST, and 66 novel shortertasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnethave a 50% time horizon of around 50 minutes. Furthermore, frontier AI timehorizon has been doubling approximately every seven months since 2019, thoughthe trend may have accelerated in 2024. The increase in AI models' timehorizons seems to be primarily driven by greater reliability and ability toadapt to mistakes, combined with better logical reasoning and tool usecapabilities. We discuss the limitations of our results -- including theirdegree of external validity -- and the implications of increased autonomy fordangerous capabilities. If these results generalize to real-world softwaretasks, extrapolation of this trend predicts that within 5 years, AI systemswill be capable of automating many software tasks that currently take humans amonth."}], "temperature": 0.1}}
{"custom_id": "164_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Thomas Kwa"}], "temperature": 0.1}}
{"custom_id": "165_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting Attack"}], "temperature": 0.1}}
{"custom_id": "165_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Batch prompting, which combines a batch of multiple queries sharing the samecontext in one inference, has emerged as a promising solution to reduceinference costs. However, our study reveals a significant securityvulnerability in batch prompting: malicious users can inject attackinstructions into a batch, leading to unwanted interference across all queries,which can result in the inclusion of harmful content, such as phishing links,or the disruption of logical reasoning. In this paper, we constructBATCHSAFEBENCH, a comprehensive benchmark comprising 150 attack instructions oftwo types and 8k batch instances, to study the batch prompting vulnerabilitysystematically. Our evaluation of both closed-source and open-weight LLMsdemonstrates that all LLMs are susceptible to batch-prompting attacks. We thenexplore multiple defending approaches. While the prompting-based defense showslimited effectiveness for smaller LLMs, the probing-based approach achievesabout 95% accuracy in detecting attacks. Additionally, we perform a mechanisticanalysis to understand the attack and identify attention heads that areresponsible for it."}], "temperature": 0.1}}
{"custom_id": "165_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Murong Yue"}], "temperature": 0.1}}
{"custom_id": "166_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "3DAxisPrompt: Promoting the 3D Grounding and Reasoning in GPT-4o"}], "temperature": 0.1}}
{"custom_id": "166_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Multimodal Large Language Models (MLLMs) exhibit impressive capabilitiesacross a variety of tasks, especially when equipped with carefully designedvisual prompts. However, existing studies primarily focus on logical reasoningand visual understanding, while the capability of MLLMs to operate effectivelyin 3D vision remains an ongoing area of exploration. In this paper, weintroduce a novel visual prompting method, called 3DAxisPrompt, to elicit the3D understanding capabilities of MLLMs in real-world scenes. More specifically,our method leverages the 3D coordinate axis and masks generated from theSegment Anything Model (SAM) to provide explicit geometric priors to MLLMs andthen extend their impressive 2D grounding and reasoning ability to real-world3D scenarios. Besides, we first provide a thorough investigation of thepotential visual prompting formats and conclude our findings to reveal thepotential and limits of 3D understanding capabilities in GPT-4o, as arepresentative of MLLMs. Finally, we build evaluation environments with fourdatasets, i.e., ScanRefer, ScanNet, FMB, and nuScene datasets, covering various3D tasks. Based on this, we conduct extensive quantitative and qualitativeexperiments, which demonstrate the effectiveness of the proposed method.Overall, our study reveals that MLLMs, with the help of 3DAxisPrompt, caneffectively perceive an object's 3D position in real-world scenarios.Nevertheless, a single prompt engineering approach does not consistentlyachieve the best outcomes for all 3D tasks. This study highlights thefeasibility of leveraging MLLMs for 3D vision grounding/reasoning with promptengineering techniques."}], "temperature": 0.1}}
{"custom_id": "166_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Dingning Liu"}], "temperature": 0.1}}
{"custom_id": "167_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Aristotle's Original Idea: For and Against Logic in the era of AI"}], "temperature": 0.1}}
{"custom_id": "167_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Aristotle is generally accepted as the father of logic. The ideas that heraised in his study of logical reasoning carried the development of scienceover the centuries. Today, in the era of AI, this title of the fatherhood oflogic has a renewed significance. Behind it lies his original idea that humanreasoning could be studied as a process and that perhaps there exist universalsystems of reasoning that underly all human reasoning irrespective of thecontent of what we are reasoning about. In this article, we look intoAristotle's work on human thought, his work on reasoning itself but also on howit relates to science and human endeavor more generally, from a modernperspective of Artificial Intelligence and ask if this can help enlighten ourunderstanding of AI and Science more generally."}], "temperature": 0.1}}
{"custom_id": "167_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Antonis C. Kakas"}], "temperature": 0.1}}
{"custom_id": "168_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models"}], "temperature": 0.1}}
{"custom_id": "168_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advancements in reasoning with large language models (RLLMs), such asOpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities incomplex domains like mathematics and coding. A central factor in their successlies in the application of long chain-of-thought (Long CoT) characteristics,which enhance reasoning abilities and enable the solution of intricateproblems. However, despite these developments, a comprehensive survey on LongCoT is still lacking, limiting our understanding of its distinctions fromtraditional short chain-of-thought (Short CoT) and complicating ongoing debateson issues like \"overthinking\" and \"inference-time scaling.\" This survey seeksto fill this gap by offering a unified perspective on Long CoT. (1) We firstdistinguish Long CoT from Short CoT and introduce a novel taxonomy tocategorize current reasoning paradigms. (2) Next, we explore the keycharacteristics of Long CoT: deep reasoning, extensive exploration, andfeasible reflection, which enable models to handle more complex tasks andproduce more efficient, coherent outcomes compared to the shallower Short CoT.(3) We then investigate key phenomena such as the emergence of Long CoT withthese characteristics, including overthinking, and inference-time scaling,offering insights into how these processes manifest in practice. (4) Finally,we identify significant research gaps and highlight promising futuredirections, including the integration of multi-modal reasoning, efficiencyimprovements, and enhanced knowledge frameworks. By providing a structuredoverview, this survey aims to inspire future research and further thedevelopment of logical reasoning in artificial intelligence."}], "temperature": 0.1}}
{"custom_id": "168_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Qiguang Chen"}], "temperature": 0.1}}
{"custom_id": "169_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning is All You Need for Video Generalization: A Counterfactual Benchmark with Sub-question Evaluation"}], "temperature": 0.1}}
{"custom_id": "169_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Counterfactual reasoning is crucial for robust video understanding butremains underexplored in existing multimodal benchmarks. In this paper, weintroduce \\textbf{COVER} (\\textbf{\\underline{CO}}unterfactual\\textbf{\\underline{V}}id\\textbf{\\underline{E}}o\\textbf{\\underline{R}}easoning), a multidimensional multimodal benchmark thatsystematically evaluates MLLMs across the abstract-concrete andperception-cognition dimensions. Beyond prior multimodal benchmarks, COVERdecomposes complex queries into structured sub-questions, enabling fine-grainedreasoning analysis. Experiments on commercial and open-source models reveal astrong correlation between sub-question accuracy and counterfactual reasoningperformance, highlighting the role of structured inference in videounderstanding. Furthermore, our results suggest a key insight: enhancing thereasoning capability of models is essential for improving the robustness ofvideo understanding. COVER establishes a new standard for assessing MLLMs'logical reasoning abilities in dynamic environments. Our work is available athttps://github.com/gongyifan-hash/COVER-Benchmark."}], "temperature": 0.1}}
{"custom_id": "169_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Qiji Zhou"}], "temperature": 0.1}}
{"custom_id": "170_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LightPlanner: Unleashing the Reasoning Capabilities of Lightweight Large Language Models in Task Planning"}], "temperature": 0.1}}
{"custom_id": "170_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In recent years, lightweight large language models (LLMs) have garneredsignificant attention in the robotics field due to their low computationalresource requirements and suitability for edge deployment. However, in taskplanning -- particularly for complex tasks that involve dynamic semantic logicreasoning -- lightweight LLMs have underperformed. To address this limitation,we propose a novel task planner, LightPlanner, which enhances the performanceof lightweight LLMs in complex task planning by fully leveraging theirreasoning capabilities. Unlike conventional planners that use fixed skilltemplates, LightPlanner controls robot actions via parameterized functioncalls, dynamically generating parameter values. This approach allows forfine-grained skill control and improves task planning success rates in complexscenarios. Furthermore, we introduce hierarchical deep reasoning. Beforegenerating each action decision step, LightPlanner thoroughly considers threelevels: action execution (feedback verification), semantic parsing (goalconsistency verification), and parameter generation (parameter validityverification). This ensures the correctness of subsequent action controls.Additionally, we incorporate a memory module to store historical actions,thereby reducing context length and enhancing planning efficiency for long-termtasks. We train the LightPlanner-1.5B model on our LightPlan-40k dataset, whichcomprises 40,000 action controls across tasks with 2 to 13 action steps.Experiments demonstrate that our model achieves the highest task success ratedespite having the smallest number of parameters. In tasks involving spatialsemantic reasoning, the success rate exceeds that of ReAct by 14.9 percent.Moreover, we demonstrate LightPlanner's potential to operate on edge devices."}], "temperature": 0.1}}
{"custom_id": "170_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Weijie Zhou"}], "temperature": 0.1}}
{"custom_id": "171_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL"}], "temperature": 0.1}}
{"custom_id": "171_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challengesfrom the complex interplay between visual perception and logical reasoning,particularly in compact 3B-parameter architectures where architecturalconstraints limit reasoning capacity and modality alignment.  While rule-based reinforcement learning (RL) excels in text-only domains, itsmultimodal extension confronts two critical barriers: (1) data limitations dueto ambiguous answers and scarce complex reasoning examples, and (2) degradedfoundational reasoning induced by multimodal pretraining. To address thesechallenges, we propose \\textbf{LMM-R1}, a two-stage framework adaptingrule-based RL for multimodal reasoning through \\textbf{Foundational ReasoningEnhancement (FRE)} followed by \\textbf{Multimodal Generalization Training(MGT)}. The FRE stage first strengthens reasoning abilities using text-onlydata with rule-based RL, then the MGT stage generalizes these reasoningcapabilities to multimodal domains.  Experiments on Qwen2.5-VL-Instruct-3B demonstrate that LMM-R1 achieves 4.83\\%and 4.5\\% average improvements over baselines in multimodal and text-onlybenchmarks, respectively, with a 3.63\\% gain in complex Football Game tasks.These results validate that text-based reasoning enhancement enables effectivemultimodal generalization, offering a data-efficient paradigm that bypassescostly high-quality multimodal training data."}], "temperature": 0.1}}
{"custom_id": "171_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yingzhe Peng"}], "temperature": 0.1}}
{"custom_id": "172_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Towards Superior Quantization Accuracy: A Layer-sensitive Approach"}], "temperature": 0.1}}
{"custom_id": "172_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Vision and Language Models have exhibited remarkable human-likeintelligence in tasks such as natural language comprehension, problem-solving,logical reasoning, and knowledge retrieval. However, training and serving thesemodels require substantial computational resources, posing a significantbarrier to their widespread application and further research. To mitigate thischallenge, various model compression techniques have been developed to reducecomputational requirements. Nevertheless, existing methods often employ uniformquantization configurations, failing to account for the varying difficultiesacross different layers in quantizing large neural network models. This papertackles this issue by leveraging layer-sensitivity features, such as activationsensitivity and weight distribution Kurtosis, to identify layers that arechallenging to quantize accurately and allocate additional memory budget. Theproposed methods, named SensiBoost and KurtBoost, respectively, demonstratenotable improvement in quantization accuracy, achieving up to 9% lowerperplexity with only a 2% increase in memory budget on LLama models compared tothe baseline."}], "temperature": 0.1}}
{"custom_id": "172_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Feng Zhang"}], "temperature": 0.1}}
{"custom_id": "173_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Pre-Training Meta-Rule Selection Policy for Visual Generative Abductive Learning"}], "temperature": 0.1}}
{"custom_id": "173_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Visual generative abductive learning studies jointly training symbol-groundedneural visual generator and inducing logic rules from data, such that afterlearning, the visual generation process is guided by the induced logic rules. Amajor challenge for this task is to reduce the time cost of logic abductionduring learning, an essential step when the logic symbol set is large and thelogic rule to induce is complicated. To address this challenge, we propose apre-training method for obtaining meta-rule selection policy for the recentlyproposed visual generative learning approach AbdGen [Peng et al., 2023], aimingat significantly reducing the candidate meta-rule set and pruning the searchspace. The selection model is built based on the embedding representation ofboth symbol grounding of cases and meta-rules, which can be effectivelyintegrated with both neural model and logic reasoning system. The pre-trainingprocess is done on pure symbol data, not involving symbol grounding learning ofraw visual inputs, making the entire learning process low-cost. An additionalinteresting observation is that the selection policy can rectify symbolgrounding errors unseen during pre-training, which is resulted from thememorization ability of attention mechanism and the relative stability ofsymbolic patterns. Experimental results show that our method is able toeffectively address the meta-rule selection problem for visual abduction,boosting the efficiency of visual generative abductive learning. Code isavailable at https://github.com/future-item/metarule-select."}], "temperature": 0.1}}
{"custom_id": "173_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yu Jin"}], "temperature": 0.1}}
{"custom_id": "174_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The Society of HiveMind: Multi-Agent Optimization of Foundation Model Swarms to Unlock the Potential of Collective Intelligence"}], "temperature": 0.1}}
{"custom_id": "174_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Multi-agent systems address issues of accessibility and scalability ofartificial intelligence (AI) foundation models, which are often represented bylarge language models. We develop a framework - the \"Society of HiveMind\"(SOHM) - that orchestrates the interaction between multiple AI foundationmodels, imitating the observed behavior of animal swarms in nature by followingmodern evolutionary theories. On the one hand, we find that the SOHM provides anegligible benefit on tasks that mainly require real-world knowledge. On theother hand, we remark a significant improvement on tasks that require intensivelogical reasoning, indicating that multi-agent systems are capable ofincreasing the reasoning capabilities of the collective compared to theindividual agents. Our findings demonstrate the potential of combining amultitude of diverse AI foundation models to form an artificial swarmintelligence capable of self-improvement through interactions with a givenenvironment."}], "temperature": 0.1}}
{"custom_id": "174_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Noah Mamie"}], "temperature": 0.1}}
{"custom_id": "175_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "DB-Explore: Automated Database Exploration and Instruction Synthesis for Text-to-SQL"}], "temperature": 0.1}}
{"custom_id": "175_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent text-to-SQL systems powered by large language models (LLMs) havedemonstrated remarkable performance in translating natural language queriesinto SQL. However, these systems often struggle with complex databasestructures and domain-specific queries, as they primarily focus on enhancinglogical reasoning and SQL syntax while overlooking the critical need forcomprehensive database understanding. To address this limitation, we proposeDB-Explore, a novel framework that systematically aligns LLMs with databaseknowledge through automated exploration and instruction synthesis. DB-Exploreconstructs database graphs to capture complex relational schemas, leveragesGPT-4 to systematically mine structural patterns and semantic knowledge, andsynthesizes instructions to distill this knowledge for efficient fine-tuning ofLLMs. Our framework enables comprehensive database understanding throughdiverse sampling strategies and automated instruction generation, bridging thegap between database structures and language models. Experiments conducted onthe SPIDER and BIRD benchmarks validate the effectiveness of DB-Explore,achieving an execution accuracy of 67.0% on BIRD and 87.8% on SPIDER. Notably,our open-source implementation based on Qwen2.5-Coder-7B achievesstate-of-the-art results at minimal computational cost, outperforming severalGPT-4-driven Text-to-SQL systems."}], "temperature": 0.1}}
{"custom_id": "175_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Haoyuan Ma"}], "temperature": 0.1}}
{"custom_id": "176_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "HelpSteer3: Human-Annotated Feedback and Edit Data to Empower Inference-Time Scaling in Open-Ended General-Domain Tasks"}], "temperature": 0.1}}
{"custom_id": "176_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Inference-Time Scaling has been critical to the success of recent models suchas OpenAI o1 and DeepSeek R1. However, many techniques used to train models forinference-time scaling require tasks to have answers that can be verified,limiting their application to domains such as math, coding and logicalreasoning. We take inspiration from how humans make first attempts, ask fordetailed feedback from others and make improvements based on such feedbackacross a wide spectrum of open-ended endeavors. To this end, we collectHelpSteer3 data to train dedicated Feedback and Edit Models that are capable ofperforming inference-time scaling for open-ended general-domain tasks. In oursetup, one model generates an initial response, which are given feedback by asecond model, that are then used by a third model to edit the response. We showthat performance on Arena Hard, a benchmark strongly predictive of ChatbotArena Elo can be boosted by scaling the number of initial response drafts,effective feedback and edited responses. When scaled optimally, our setup basedon 70B models from the Llama 3 family can reach SoTA performance on Arena Hardat 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 andDeepSeek R1 with 92.3."}], "temperature": 0.1}}
{"custom_id": "176_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhilin Wang"}], "temperature": 0.1}}
{"custom_id": "177_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Three tiers of computation in transformers and in brain architectures"}], "temperature": 0.1}}
{"custom_id": "177_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Human language and logic abilities are computationally quantified within thewell-studied grammar-automata hierarchy. We identify three hierarchical tiersand two corresponding transitions and show their correspondence to specificabilities in transformer-based language models (LMs). These emergent abilitieshave often been described in terms of scaling; we show that it is thetransition between tiers, rather than scaled size itself, that determines asystem's capabilities. Specifically, humans effortlessly process language yetrequire critical training to perform arithmetic or logical reasoning tasks; andLMs possess language abilities absent from predecessor systems, yet stillstruggle with logical processing. We submit a novel benchmark of computationalpower, provide empirical evaluations of humans and fifteen LMs, and, mostsignificantly, provide a theoretically grounded framework to promote carefulthinking about these crucial topics. The resulting principled analyses provideexplanatory accounts of the abilities and shortfalls of LMs, and suggestactionable insights into the expansion of their logic abilities."}], "temperature": 0.1}}
{"custom_id": "177_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "E Graham"}], "temperature": 0.1}}
{"custom_id": "178_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Psy-Insight: Explainable Multi-turn Bilingual Dataset for Mental Health Counseling"}], "temperature": 0.1}}
{"custom_id": "178_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The in-context learning capabilities of large language models (LLMs) showgreat potential in mental health support. However, the lack of counselingdatasets, particularly in Chinese corpora, restricts their application in thisfield. To address this, we constructed Psy-Insight, the first mentalhealth-oriented explainable multi-task bilingual dataset. We collectedface-to-face multi-turn counseling dialogues, which are annotated withmulti-task labels and conversation process explanations. Our annotationsinclude psychotherapy, emotion, strategy, and topic labels, as well asturn-level reasoning and session-level guidance. Psy-Insight is not onlysuitable for tasks such as label recognition but also meets the need fortraining LLMs to act as empathetic counselors through logical reasoning.Experiments show that training LLMs on Psy-Insight enables the models to notonly mimic the conversation style but also understand the underlying strategiesand reasoning of counseling."}], "temperature": 0.1}}
{"custom_id": "178_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Keqi Chen"}], "temperature": 0.1}}
{"custom_id": "179_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "DeLTa: A Decoding Strategy based on Logit Trajectory Prediction Improves Factuality and Reasoning Ability"}], "temperature": 0.1}}
{"custom_id": "179_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) are increasingly being used in real-worldapplications. However, concerns about the reliability of the content theygenerate persist, as it frequently deviates from factual correctness orexhibits deficiencies in logical reasoning. This paper proposes a noveldecoding strategy aimed at enhancing both factual accuracy and inferentialreasoning without requiring any modifications to the architecture orpre-trained parameters of LLMs. Our approach adjusts next-token probabilitiesby analyzing the trajectory of logits from lower to higher layers inTransformers and applying linear regression. We find that this Decoding byLogit Trajectory-based approach (DeLTa) effectively reinforces factuality andreasoning while mitigating incorrect generation. Experiments on TruthfulQAdemonstrate that DeLTa attains up to a 4.9% improvement over the baseline.Furthermore, it enhances performance by up to 8.1% on StrategyQA and 7.3% onGSM8K, both of which demand strong reasoning capabilities."}], "temperature": 0.1}}
{"custom_id": "179_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yunzhen He"}], "temperature": 0.1}}
{"custom_id": "180_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "KGCompiler: Deep Learning Compilation Optimization for Knowledge Graph Complex Logical Query Answering"}], "temperature": 0.1}}
{"custom_id": "180_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Complex Logical Query Answering (CLQA) involves intricate multi-hop logicalreasoning over large-scale and potentially incomplete Knowledge Graphs (KGs).Although existing CLQA algorithms achieve high accuracy in answering suchqueries, their reasoning time and memory usage scale significantly with thenumber of First-Order Logic (FOL) operators involved, creating seriouschallenges for practical deployment. In addition, current research primarilyfocuses on algorithm-level optimizations for CLQA tasks, often overlookingcompiler-level optimizations, which can offer greater generality andscalability. To address these limitations, we introduce a Knowledge GraphCompiler, namely KGCompiler, the first deep learning compiler specificallydesigned for CLQA tasks. By incorporating KG-specific optimizations proposed inthis paper, KGCompiler enhances the reasoning performance of CLQA algorithmswithout requiring additional manual modifications to their implementations. Atthe same time, it significantly reduces memory usage. Extensive experimentsdemonstrate that KGCompiler accelerates CLQA algorithms by factors ranging from1.04x to 8.26x, with an average speedup of 3.71x. We also provide an interfaceto enable hands-on experience with KGCompiler."}], "temperature": 0.1}}
{"custom_id": "180_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hongyu Lin"}], "temperature": 0.1}}
{"custom_id": "181_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs"}], "temperature": 0.1}}
{"custom_id": "181_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "An Achilles heel of Large Language Models (LLMs) is their tendency tohallucinate non-factual statements. A response mixed of factual and non-factualstatements poses a challenge for humans to verify and accurately base theirdecisions on. To combat this problem, we propose Highlighted Chain-of-ThoughtPrompting (HoT), a technique for prompting LLMs to generate responses with XMLtags that ground facts to those provided in the query. That is, given an inputquestion, LLMs would first re-format the question to add XML tags highlightingkey facts, and then, generate a response with highlights over the factsreferenced from the input. Interestingly, in few-shot settings, HoT outperformsvanilla chain of thought prompting (CoT) on a wide range of 17 tasks fromarithmetic, reading comprehension to logical reasoning. When asking humans toverify LLM responses, highlights help time-limited participants to moreaccurately and efficiently recognize when LLMs are correct. Yet, surprisingly,when LLMs are wrong, HoTs tend to make users believe that an answer is correct."}], "temperature": 0.1}}
{"custom_id": "181_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Tin Nguyen"}], "temperature": 0.1}}
{"custom_id": "182_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Order Doesn't Matter, But Reasoning Does: Training LLMs with Order-Centric Augmentation"}], "temperature": 0.1}}
{"custom_id": "182_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning is essential for large language models (LLMs) to ensureaccurate and coherent inference. However, LLMs struggle with reasoning ordervariations and fail to generalize across logically equivalent transformations.LLMs often rely on fixed sequential patterns rather than true logicalunderstanding. To address this issue, we introduce an order-centric dataaugmentation framework based on commutativity in logical reasoning. We firstrandomly shuffle independent premises to introduce condition orderaugmentation. For reasoning steps, we construct a directed acyclic graph (DAG)to model dependencies between steps, which allows us to identify validreorderings of steps while preserving logical correctness. By leveragingorder-centric augmentations, models can develop a more flexible and generalizedreasoning process. Finally, we conduct extensive experiments across multiplelogical reasoning benchmarks, demonstrating that our method significantlyenhances LLMs' reasoning performance and adaptability to diverse logicalstructures. We release our codes and augmented data inhttps://anonymous.4open.science/r/Order-Centric-Data-Augmentation-822C/."}], "temperature": 0.1}}
{"custom_id": "182_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Qianxi He"}], "temperature": 0.1}}
{"custom_id": "183_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "TextGames: Learning to Self-Play Text-Based Puzzle Games via Language Model Reasoning"}], "temperature": 0.1}}
{"custom_id": "183_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning is a fundamental capability of large language models (LLMs),enabling them to comprehend, analyze, and solve complex problems. In thispaper, we introduce TextGames, an innovative benchmark specifically crafted toassess LLMs through demanding text-based games that require advanced skills inpattern recognition, spatial awareness, arithmetic, and logical reasoning. Ouranalysis probes LLMs' performance in both single-turn and multi-turn reasoning,and their abilities in leveraging feedback to correct subsequent answersthrough self-reflection. Our findings reveal that, although LLMs exhibitproficiency in addressing most easy and medium-level problems, they facesignificant challenges with more difficult tasks. In contrast, humans arecapable of solving all tasks when given sufficient time. Moreover, we observethat LLMs show improved performance in multi-turn predictions throughself-reflection, yet they still struggle with sequencing, counting, andfollowing complex rules consistently. Additionally, models optimized forreasoning outperform pre-trained LLMs that prioritize instruction following,highlighting the crucial role of reasoning skills in addressing highly complexproblems."}], "temperature": 0.1}}
{"custom_id": "183_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Frederikus Hudi"}], "temperature": 0.1}}
{"custom_id": "184_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logic Haystacks: Probing LLMs Long-Context Logical Reasoning (Without Easily Identifiable Unrelated Padding)"}], "temperature": 0.1}}
{"custom_id": "184_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models demonstrate promising long context processingcapabilities, with recent models touting context windows close to one milliontokens. However, the evaluations supporting these claims often involve simpleretrieval tasks or synthetic tasks padded with irrelevant text, which themodels may easily detect and discard. In this work, we generate lengthysimplified English text with first-order logic representations spanning up to2048 clauses (around 25k GPT-4 tokens). We formulate an evaluation task withevidence retrieval for contradiction detection. The long, homogeneous text isfilled with distractors that are both hard to distinguish from relevantevidences and provably not interfering with them. Our evaluation of evidenceretrieval shows that the effective context window is much smaller withrealistic distractors, already crumbling at 128 clauses."}], "temperature": 0.1}}
{"custom_id": "184_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Damien Sileo"}], "temperature": 0.1}}
{"custom_id": "185_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Quantifying Logical Consistency in Transformers via Query-Key Alignment"}], "temperature": 0.1}}
{"custom_id": "185_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have demonstrated impressive performance invarious natural language processing tasks, yet their ability to performmulti-step logical reasoning remains an open challenge. AlthoughChain-of-Thought prompting has improved logical reasoning by enabling models togenerate intermediate steps, it lacks mechanisms to assess the coherence ofthese logical transitions. In this paper, we propose a novel, lightweightevaluation strategy for logical reasoning that uses query-key alignments insidetransformer attention heads. By computing a single forward pass and extractinga \"QK-score\" from carefully chosen heads, our method reveals latentrepresentations that reliably separate valid from invalid inferences, offeringa scalable alternative to traditional ablation-based techniques. We alsoprovide an empirical validation on multiple logical reasoning benchmarks,demonstrating improved robustness of our evaluation method against distractorsand increased reasoning depth. The experiments were conducted on a diverse setof models, ranging from 1.5B to 70B parameters."}], "temperature": 0.1}}
{"custom_id": "185_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Eduard Tulchinskii"}], "temperature": 0.1}}
{"custom_id": "186_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Autoregressive Image Generation with Vision Full-view Prompt"}], "temperature": 0.1}}
{"custom_id": "186_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In autoregressive (AR) image generation, models based on the 'next-tokenprediction' paradigm of LLMs have shown comparable performance to diffusionmodels by reducing inductive biases. However, directly applying LLMs to compleximage generation can struggle with reconstructing the image's structure anddetails, impacting the generation's accuracy and stability. Additionally, the'next-token prediction' paradigm in the AR model does not align with thecontextual scanning and logical reasoning processes involved in human visualperception, limiting effective image generation. Prompt engineering, as a keytechnique for guiding LLMs, leverages specifically designed prompts to improvemodel performance on complex natural language processing (NLP) tasks, enhancingaccuracy and stability of generation while maintaining contextual coherence andlogical consistency, similar to human reasoning. Inspired by prompt engineeringfrom the field of NLP, we propose Vision Full-view prompt (VF prompt) toenhance autoregressive image generation. Specifically, we design specializedimage-related VF prompts for AR image generation to simulate the process ofhuman image creation. This enhances contextual logic ability by allowing themodel to first perceive overall distribution information before generating theimage, and improve generation stability by increasing the inference steps.Compared to the AR method without VF prompts, our method shows outstandingperformance and achieves an approximate improvement of 20%."}], "temperature": 0.1}}
{"custom_id": "186_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Miaomiao Cai"}], "temperature": 0.1}}
{"custom_id": "187_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning Abilities of Large Language Models"}], "temperature": 0.1}}
{"custom_id": "187_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "While logical reasoning evaluation of Large Language Models (LLMs) hasattracted significant attention, existing benchmarks predominantly rely onmultiple-choice formats that are vulnerable to random guessing, leading tooverestimated performance and substantial performance fluctuations. To obtainmore accurate assessments of models' reasoning capabilities, we propose anautomated method for synthesizing open-ended logic puzzles, and use it todevelop a bilingual benchmark, AutoLogi. Our approach features program-basedverification and controllable difficulty levels, enabling more reliableevaluation that better distinguishes models' reasoning abilities. Extensiveevaluation of eight modern LLMs shows that AutoLogi can better reflect truemodel capabilities, with performance scores spanning from 35% to 73% comparedto the narrower range of 21% to 37% on the source multiple-choice dataset.Beyond benchmark creation, this synthesis method can generate high-qualitytraining data by incorporating program verifiers into the rejection samplingprocess, enabling systematic enhancement of LLMs' reasoning capabilities acrossdiverse datasets."}], "temperature": 0.1}}
{"custom_id": "187_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Qin Zhu"}], "temperature": 0.1}}
{"custom_id": "188_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Identifying Features that Shape Perceived Consciousness in Large Language Model-based AI: A Quantitative Study of Human Responses"}], "temperature": 0.1}}
{"custom_id": "188_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This study quantitively examines which features of AI-generated text leadhumans to perceive subjective consciousness in large language model (LLM)-basedAI systems. Drawing on 99 passages from conversations with Claude 3 Opus andfocusing on eight features -- metacognitive self-reflection, logical reasoning,empathy, emotionality, knowledge, fluency, unexpectedness, and subjectiveexpressiveness -- we conducted a survey with 123 participants. Using regressionand clustering analyses, we investigated how these features influenceparticipants' perceptions of AI consciousness. The results reveal thatmetacognitive self-reflection and the AI's expression of its own emotionssignificantly increased perceived consciousness, while a heavy emphasis onknowledge reduced it. Participants clustered into seven subgroups, each showingdistinct feature-weighting patterns. Additionally, higher prior knowledge ofLLMs and more frequent usage of LLM-based chatbots were associated with greateroverall likelihood assessments of AI consciousness. This study underscores themultidimensional and individualized nature of perceived AI consciousness andprovides a foundation for better understanding the psychosocial implications ofhuman-AI interaction."}], "temperature": 0.1}}
{"custom_id": "188_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Bongsu Kang"}], "temperature": 0.1}}
{"custom_id": "189_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "CrossVTON: Mimicking the Logic Reasoning on Cross-category Virtual Try-on guided by Tri-zone Priors"}], "temperature": 0.1}}
{"custom_id": "189_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Despite remarkable progress in image-based virtual try-on systems, generatingrealistic and robust fitting images for cross-category virtual try-on remains achallenging task. The primary difficulty arises from the absence of human-likereasoning, which involves addressing size mismatches between garments andmodels while recognizing and leveraging the distinct functionalities of variousregions within the model images. To address this issue, we draw inspirationfrom human cognitive processes and disentangle the complex reasoning requiredfor cross-category try-on into a structured framework. This frameworksystematically decomposes the model image into three distinct regions: try-on,reconstruction, and imagination zones. Each zone plays a specific role inaccommodating the garment and facilitating realistic synthesis. To endow themodel with robust reasoning capabilities for cross-category scenarios, wepropose an iterative data constructor. This constructor encompasses diversescenarios, including intra-category try-on, any-to-dress transformations(replacing any garment category with a dress), and dress-to-any transformations(replacing a dress with another garment category). Utilizing the generateddataset, we introduce a tri-zone priors generator that intelligently predictsthe try-on, reconstruction, and imagination zones by analyzing how the inputgarment is expected to align with the model image. Guided by these tri-zonepriors, our proposed method, CrossVTON, achieves state-of-the-art performance,surpassing existing baselines in both qualitative and quantitative evaluations.Notably, it demonstrates superior capability in handling cross-category virtualtry-on, meeting the complex demands of real-world applications."}], "temperature": 0.1}}
{"custom_id": "189_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Donghao Luo"}], "temperature": 0.1}}
{"custom_id": "190_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "On the logical skills of large language models: evaluations using arbitrarily complex first-order logic problems"}], "temperature": 0.1}}
{"custom_id": "190_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We present a method of generating first-order logic statements whosecomplexity can be controlled along multiple dimensions. We use this method toautomatically create several datasets consisting of questions asking for thetruth or falsity of first-order logic statements in Zermelo-Fraenkel settheory. While the resolution of these questions does not require any knowledgebeyond basic notation of first-order logic and set theory, it does require adegree of planning and logical reasoning, which can be controlled up toarbitrarily high difficulty by the complexity of the generated statements.Furthermore, we do extensive evaluations of the performance of various largelanguage models, including recent models such as DeepSeek-R1 and OpenAI'so3-mini, on these datasets. All of the datasets along with the code used forgenerating them, as well as all data from the evaluations is publicly availableat https://github.com/bkuckuck/logical-skills-of-llms."}], "temperature": 0.1}}
{"custom_id": "190_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Shokhrukh Ibragimov"}], "temperature": 0.1}}
{"custom_id": "191_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SPPD: Self-training with Process Preference Learning Using Dynamic Value Margin"}], "temperature": 0.1}}
{"custom_id": "191_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recently, enhancing the numerical and logical reasoning capability of LargeLanguage Models (LLMs) has emerged as a research hotspot. Existing methods faceseveral limitations: inference-phase techniques (e.g., Chain of Thoughts) relyon prompt selection and the pretrained knowledge; sentence-level SupervisedFine-Tuning (SFT) and Direct Preference Optimization (DPO) struggle withstep-wise mathematical correctness and depend on stronger models distillationor human annotations; while Reinforcement Learning (RL) approaches incur highGPU memory costs and unstable training. To address these, we propose\\textbf{S}elf-training framework integrating \\textbf{P}rocess\\textbf{P}reference learning using \\textbf{D}ynamic value margin (SPPD). SPPDleverages a process-based Markov Decision Process (MDP) and Bellman optimalityequation to derive \\textbf{dynamic value margin} on step-level preferenceoptimization, which employs tree-based self-sampling on model responses\\textbf{without any distillation} from other models. Furthermore, wetheoretically prove that SPPD is \\textbf{equivalent to on-policy policygradient methods} under reward constraints. Experiments on 7B-scale modelsdemonstrate superior performance across in-domain and out-domain mathematicalbenchmarks. We open-source our code at\\href{https://anonymous.4open.science/r/SSDPO-D-DCDD}{https://anonymous.4open.science/r/SPPD-DCDD}."}], "temperature": 0.1}}
{"custom_id": "191_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hao Yi"}], "temperature": 0.1}}
{"custom_id": "192_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Detecting LLM Fact-conflicting Hallucinations Enhanced by Temporal-logic-based Reasoning"}], "temperature": 0.1}}
{"custom_id": "192_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) face the challenge of hallucinations -- outputsthat seem coherent but are actually incorrect. A particularly damaging type isfact-conflicting hallucination (FCH), where generated content contradictsestablished facts. Addressing FCH presents three main challenges: 1)Automatically constructing and maintaining large-scale benchmark datasets isdifficult and resource-intensive; 2) Generating complex and efficient testcases that the LLM has not been trained on -- especially those involvingintricate temporal features -- is challenging, yet crucial for elicitinghallucinations; and 3) Validating the reasoning behind LLM outputs isinherently difficult, particularly with complex logical relationships, as itrequires transparency in the model's decision-making process.  This paper presents Drowzee, an innovative end-to-end metamorphic testingframework that utilizes temporal logic to identify fact-conflictinghallucinations (FCH) in large language models (LLMs). Drowzee builds acomprehensive factual knowledge base by crawling sources like Wikipedia anduses automated temporal-logic reasoning to convert this knowledge into a large,extensible set of test cases with ground truth answers. LLMs are tested usingthese cases through template-based prompts, which require them to generate bothanswers and reasoning steps. To validate the reasoning, we propose twosemantic-aware oracles that compare the semantic structure of LLM outputs tothe ground truths. Across nine LLMs in nine different knowledge domains,experimental results show that Drowzee effectively identifies rates ofnon-temporal-related hallucinations ranging from 24.7% to 59.8%, and rates oftemporal-related hallucinations ranging from 16.7% to 39.2%."}], "temperature": 0.1}}
{"custom_id": "192_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ningke Li"}], "temperature": 0.1}}
{"custom_id": "193_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Smooth Transition Between Induction and Deduction: Fast Abductive Learning Based on Probabilistic Symbol Perception"}], "temperature": 0.1}}
{"custom_id": "193_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Abductive learning (ABL) that integrates strengths of machine learning andlogical reasoning to improve the learning generalization, has been recentlyshown effective. However, its efficiency is affected by the transition betweennumerical induction and symbolical deduction, leading to high computationalcosts in the worst-case scenario. Efforts on this issue remain to be limited.In this paper, we identified three reasons why previous optimization algorithmsfor ABL were not effective: insufficient utilization of prediction, symbolrelationships, and accumulated experience in successful abductive processes,resulting in redundant calculations to the knowledge base. To address thesechallenges, we introduce an optimization algorithm named as ProbabilisticSymbol Perception (PSP), which makes a smooth transition between induction anddeduction and keeps the correctness of ABL unchanged. We leverage probabilityas a bridge and present an efficient data structure, achieving the transferfrom a continuous probability sequence to discrete Boolean sequences with lowcomputational complexity. Experiments demonstrate the promising results."}], "temperature": 0.1}}
{"custom_id": "193_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Lin-Han Jia"}], "temperature": 0.1}}
{"custom_id": "194_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions"}], "temperature": 0.1}}
{"custom_id": "194_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chain-of-Though (CoT) represents a common strategy for reasoning in LargeLanguage Models (LLMs) by decomposing complex tasks into intermediate inferencesteps. However, explanations generated via CoT are susceptible to contentbiases that negatively affect their robustness and faithfulness. To mitigateexisting limitations, recent work has proposed using logical formalisms coupledwith external symbolic solvers. However, fully symbolic approaches possess thebottleneck of requiring a complete translation from natural language to formallanguages, a process that affects efficiency and flexibility. To achieve atrade-off, this paper investigates methods to disentangle content from logicalreasoning without a complete formalisation. In particular, we present QuaSAR(for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs tooperate at a higher level of abstraction via quasi-symbolic explanations. Ourframework leverages the capability of LLMs to formalise only relevant variablesand predicates, enabling the coexistence of symbolic elements with naturallanguage. We show the impact of QuaSAR for in-context learning and forconstructing demonstrations to improve the reasoning capabilities of smallermodels. Our experiments show that quasi-symbolic abstractions can improveCoT-based methods by up to 8% accuracy, enhancing robustness and consistency onchallenging adversarial variations on both natural language (i.e. MMLU-Redux)and symbolic reasoning tasks (i.e., GSM-Symbolic)."}], "temperature": 0.1}}
{"custom_id": "194_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Leonardo Ranaldi"}], "temperature": 0.1}}
{"custom_id": "195_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights"}], "temperature": 0.1}}
{"custom_id": "195_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We examine the reasoning and planning capabilities of large language models(LLMs) in solving complex tasks. Recent advances in inference-time techniquesdemonstrate the potential to enhance LLM reasoning without additional trainingby exploring intermediate steps during inference. Notably, OpenAI's o1 modelshows promising performance through its novel use of multi-step reasoning andverification. Here, we explore how scaling inference-time techniques canimprove reasoning and planning, focusing on understanding the tradeoff betweencomputational cost and performance. To this end, we construct a comprehensivebenchmark, known as Sys2Bench, and perform extensive experiments evaluatingexisting inference-time techniques on eleven diverse tasks across fivecategories, including arithmetic reasoning, logical reasoning, common sensereasoning, algorithmic reasoning, and planning. Our findings indicate thatsimply scaling inference-time computation has limitations, as no singleinference-time technique consistently performs well across all reasoning andplanning tasks."}], "temperature": 0.1}}
{"custom_id": "195_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Shubham Parashar"}], "temperature": 0.1}}
{"custom_id": "196_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Warmup Generations: A Task-Agnostic Approach for Guiding Sequence-to-Sequence Learning with Unsupervised Initial State Generation"}], "temperature": 0.1}}
{"custom_id": "196_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Traditional supervised fine-tuning (SFT) strategies for sequence-to-sequencetasks often train models to directly generate the target output. Recent workhas shown that guiding models with intermediate steps, such as keywords,outlines, or reasoning chains, can significantly improve performance,coherence, and interpretability. However, these methods often depend onpredefined intermediate formats and annotated data, limiting their scalabilityand generalizability. In this work, we introduce a task-agnostic framework thatenables models to generate intermediate \"warmup\" sequences. These warmupsequences, serving as an initial state for subsequent generation, are optimizedto enhance the probability of generating the target sequence without relying onexternal supervision or human-designed structures. Drawing inspiration fromreinforcement learning principles, our method iteratively refines theseintermediate steps to maximize their contribution to the final output, similarto reward-driven optimization in reinforcement learning with human feedback.Experimental results across tasks such as translation, summarization, andmulti-choice question answering for logical reasoning show that our approachoutperforms traditional SFT methods, and offers a scalable and flexiblesolution for sequence-to-sequence tasks."}], "temperature": 0.1}}
{"custom_id": "196_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Senyu Li"}], "temperature": 0.1}}
{"custom_id": "197_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Unveiling the Magic of Code Reasoning through Hypothesis Decomposition and Amendment"}], "temperature": 0.1}}
{"custom_id": "197_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The reasoning abilities are one of the most enigmatic and captivating aspectsof large language models (LLMs). Numerous studies are dedicated to exploringand expanding the boundaries of this reasoning capability. However, tasks thatembody both reasoning and recall characteristics are often overlooked. In thispaper, we introduce such a novel task, code reasoning, to provide a newperspective for the reasoning abilities of LLMs. We summarize threemeta-benchmarks based on established forms of logical reasoning, andinstantiate these into eight specific benchmark tasks. Our testing on thesebenchmarks reveals that LLMs continue to struggle with identifying satisfactoryreasoning pathways. Additionally, we present a new pathway exploration pipelineinspired by human intricate problem-solving methods. This Reflective HypothesisDecomposition and Amendment (RHDA) pipeline consists of the following iterativesteps: (1) Proposing potential hypotheses based on observations and decomposingthem; (2) Utilizing tools to validate hypotheses and reflection outcomes; (3)Revising hypothesis in light of observations. Our approach effectivelymitigates logical chain collapses arising from forgetting or hallucinationissues in multi-step reasoning, resulting in performance gains of up to$3\\times$. Finally, we expanded this pipeline by applying it to simulatecomplex household tasks in real-world scenarios, specifically in VirtualHome,enhancing the handling of failure cases. We release our code and all of resultsat https://github.com/TnTWoW/code_reasoning."}], "temperature": 0.1}}
{"custom_id": "197_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yuze Zhao"}], "temperature": 0.1}}
{"custom_id": "198_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Dialogue-based Explanations for Logical Reasoning using Structured Argumentation"}], "temperature": 0.1}}
{"custom_id": "198_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The problem of explaining inconsistency-tolerant reasoning in knowledge bases(KBs) is a prominent topic in Artificial Intelligence (AI). While there is somework on this problem, the explanations provided by existing approaches oftenlack critical information or fail to be expressive enough for non-binaryconflicts. In this paper, we identify structural weaknesses of thestate-of-the-art and propose a generic argumentation-based approach to addressthese problems. This approach is defined for logics involving reasoning withmaximal consistent subsets and shows how any such logic can be translated toargumentation. Our work provides dialogue models as dialectic-proof proceduresto compute and explain a query answer wrt inconsistency-tolerant semantics.This allows us to construct dialectical proof trees as explanations, which aremore expressive and arguably more intuitive than existing explanationformalisms."}], "temperature": 0.1}}
{"custom_id": "198_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Loan Ho"}], "temperature": 0.1}}
{"custom_id": "199_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical forms complement probability in understanding language model (and human) performance"}], "temperature": 0.1}}
{"custom_id": "199_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "With the increasing interest in using large language models (LLMs) forplanning in natural language, understanding their behaviors becomes animportant research question. This work conducts a systematic investigation ofLLMs' ability to perform logical reasoning in natural language. We introduce acontrolled dataset of hypothetical and disjunctive syllogisms in propositionaland modal logic and use it as the testbed for understanding LLM performance.Our results lead to novel insights in predicting LLM behaviors: in addition tothe probability of input (Gonen et al., 2023; McCoy et al., 2024), logicalforms should be considered as important factors. In addition, we showsimilarities and discrepancies between the logical reasoning performances ofhumans and LLMs by collecting and comparing behavioral data from both."}], "temperature": 0.1}}
{"custom_id": "199_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yixuan Wang"}], "temperature": 0.1}}
{"custom_id": "200_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The Multilingual Mind : A Survey of Multilingual Reasoning in Language Models"}], "temperature": 0.1}}
{"custom_id": "200_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "While reasoning and multilingual capabilities in Language Models (LMs) haveachieved remarkable progress in recent years, their integration into a unifiedparadigm, multilingual reasoning, is at a nascent stage. Multilingual reasoningrequires language models to handle logical reasoning across languages whileaddressing misalignment, biases, and challenges in low-resource settings. Thissurvey provides the first in-depth review of multilingual reasoning in LMs. Inthis survey, we provide a systematic overview of existing methods that leverageLMs for multilingual reasoning, specifically outlining the challenges,motivations, and foundational aspects of applying language models to reasonacross diverse languages. We provide an overview of the standard data resourcesused for training multilingual reasoning in LMs and the evaluation benchmarksemployed to assess their multilingual capabilities. Next, we analyze variousstate-of-the-art methods and their performance on these benchmarks. Finally, weexplore future research opportunities to improve multilingual reasoning in LMs,focusing on enhancing their ability to handle diverse languages and complexreasoning tasks."}], "temperature": 0.1}}
{"custom_id": "200_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Akash Ghosh"}], "temperature": 0.1}}
{"custom_id": "201_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical Lease Litigation: Prolog and LLMs for Rental Law Compliance in New York"}], "temperature": 0.1}}
{"custom_id": "201_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Legal cases require careful logical reasoning following the laws, whereasinteractions with non-technical users must be in natural language. As anapplication combining logical reasoning using Prolog and natural languageprocessing using large language models (LLMs), this paper presents a novelapproach and system, LogicLease, to automate the analysis of landlord-tenantlegal cases in the state of New York. LogicLease determines compliance withrelevant legal requirements by analyzing case descriptions and citing allrelevant laws. It leverages LLMs for information extraction and Prolog forlegal reasoning. By separating information extraction from legal reasoning,LogicLease achieves greater transparency and control over the legal logicapplied to each case. We evaluate the accuracy, efficiency, and robustness ofLogicLease through a series of tests, achieving 100% accuracy and an averageprocessing time of 2.57 seconds. LogicLease presents advantages overstate-of-the-art LLM-based legal analysis systems by providing clear,step-by-step reasoning, citing specific laws, and distinguishing itself by itsability to avoid hallucinations -- a common issue in LLMs."}], "temperature": 0.1}}
{"custom_id": "201_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Sanskar Sehgal"}], "temperature": 0.1}}
{"custom_id": "202_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical Reasoning in Large Language Models: A Survey"}], "temperature": 0.1}}
{"custom_id": "202_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "With the emergence of advanced reasoning models like OpenAI o3 andDeepSeek-R1, large language models (LLMs) have demonstrated remarkablereasoning capabilities. However, their ability to perform rigorous logicalreasoning remains an open question. This survey synthesizes recent advancementsin logical reasoning within LLMs, a critical area of AI research. It outlinesthe scope of logical reasoning in LLMs, its theoretical foundations, and thebenchmarks used to evaluate reasoning proficiency. We analyze existingcapabilities across different reasoning paradigms - deductive, inductive,abductive, and analogical - and assess strategies to enhance reasoningperformance, including data-centric tuning, reinforcement learning, decodingstrategies, and neuro-symbolic approaches. The review concludes with futuredirections, emphasizing the need for further exploration to strengthen logicalreasoning in AI systems."}], "temperature": 0.1}}
{"custom_id": "202_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hanmeng Liu"}], "temperature": 0.1}}
{"custom_id": "203_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mechanistic Unveiling of Transformer Circuits: Self-Influence as a Key to Model Reasoning"}], "temperature": 0.1}}
{"custom_id": "203_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Transformer-based language models have achieved significant success; however,their internal mechanisms remain largely opaque due to the complexity ofnon-linear interactions and high-dimensional operations. While previous studieshave demonstrated that these models implicitly embed reasoning trees, humanstypically employ various distinct logical reasoning mechanisms to complete thesame task. It is still unclear which multi-step reasoning mechanisms are usedby language models to solve such tasks. In this paper, we aim to address thisquestion by investigating the mechanistic interpretability of language models,particularly in the context of multi-step reasoning tasks. Specifically, weemploy circuit analysis and self-influence functions to evaluate the changingimportance of each token throughout the reasoning process, allowing us to mapthe reasoning paths adopted by the model. We apply this methodology to theGPT-2 model on a prediction task (IOI) and demonstrate that the underlyingcircuits reveal a human-interpretable reasoning process used by the model."}], "temperature": 0.1}}
{"custom_id": "203_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Lin Zhang"}], "temperature": 0.1}}
{"custom_id": "204_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "DMWM: Dual-Mind World Model with Long-Term Imagination"}], "temperature": 0.1}}
{"custom_id": "204_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Imagination in world models is crucial for enabling agents to learnlong-horizon policy in a sample-efficient manner. Existing recurrentstate-space model (RSSM)-based world models depend on single-step statisticalinference to capture the environment dynamics, and, hence, they are unable toperform long-term imagination tasks due to the accumulation of predictionerrors. Inspired by the dual-process theory of human cognition, we propose anovel dual-mind world model (DMWM) framework that integrates logical reasoningto enable imagination with logical consistency. DMWM is composed of twocomponents: an RSSM-based System 1 (RSSM-S1) component that handles statetransitions in an intuitive manner and a logic-integrated neural network-basedSystem 2 (LINN-S2) component that guides the imagination process throughhierarchical deep logical reasoning. The inter-system feedback mechanism isdesigned to ensure that the imagination process follows the logical rules ofthe real environment. The proposed framework is evaluated on benchmark tasksthat require long-term planning from the DMControl suite. Extensiveexperimental results demonstrate that the proposed framework yields significantimprovements in terms of logical coherence, trial efficiency, data efficiencyand long-term imagination over the state-of-the-art world models."}], "temperature": 0.1}}
{"custom_id": "204_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Lingyi Wang"}], "temperature": 0.1}}
{"custom_id": "205_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation"}], "temperature": 0.1}}
{"custom_id": "205_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "First-order logic (FOL) reasoning, which involves sequential deduction, ispivotal for intelligent systems and serves as a valuable task for evaluatingreasoning capabilities, particularly in chain-of-thought (CoT) contexts.Existing benchmarks often rely on extensive human annotation or handcraftedtemplates, making it difficult to achieve the necessary complexity,scalability, and diversity for robust evaluation. To address these limitations,we propose a novel framework called ProverGen that synergizes the generativestrengths of Large Language Models (LLMs) with the rigor and precision ofsymbolic provers, enabling the creation of a scalable, diverse, andhigh-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished byits inclusion of accessible and logically coherent intermediate reasoning stepsfor each problem. Our evaluation shows that state-of-the-art LLMs struggle tosolve ProverQA problems, even with CoT prompting, highlighting the dataset'schallenging nature. We also finetune Llama3.1-8B-Instruct on a separatetraining set generated by our framework. The finetuned model demonstratesconsistent improvements on both in-distribution and out-of-distribution testsets, suggesting the value of our proposed data generation framework. Codeavailable at: https://github.com/opendatalab/ProverGen"}], "temperature": 0.1}}
{"custom_id": "205_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chengwen Qi"}], "temperature": 0.1}}
{"custom_id": "206_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "XiHeFusion: Harnessing Large Language Models for Science Communication in Nuclear Fusion"}], "temperature": 0.1}}
{"custom_id": "206_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Nuclear fusion is one of the most promising ways for humans to obtaininfinite energy. Currently, with the rapid development of artificialintelligence, the mission of nuclear fusion has also entered a critical periodof its development. How to let more people to understand nuclear fusion andjoin in its research is one of the effective means to accelerate theimplementation of fusion. This paper proposes the first large model in thefield of nuclear fusion, XiHeFusion, which is obtained through supervisedfine-tuning based on the open-source large model Qwen2.5-14B. We have collectedmulti-source knowledge about nuclear fusion tasks to support the training ofthis model, including the common crawl, eBooks, arXiv, dissertation, etc. Afterthe model has mastered the knowledge of the nuclear fusion field, we furtherused the chain of thought to enhance its logical reasoning ability, makingXiHeFusion able to provide more accurate and logical answers. In addition, wepropose a test questionnaire containing 180+ questions to assess theconversational ability of this science popularization large model. Extensiveexperimental results show that our nuclear fusion dialogue model, XiHeFusion,can perform well in answering science popularization knowledge. The pre-trainedXiHeFusion model is released on https://github.com/Event-AHU/XiHeFusion."}], "temperature": 0.1}}
{"custom_id": "206_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xiao Wang"}], "temperature": 0.1}}
{"custom_id": "207_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SymAgent: A Neural-Symbolic Self-Learning Agent Framework for Complex Reasoning over Knowledge Graphs"}], "temperature": 0.1}}
{"custom_id": "207_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advancements have highlighted that Large Language Models (LLMs) areprone to hallucinations when solving complex reasoning problems, leading toerroneous results. To tackle this issue, researchers incorporate KnowledgeGraphs (KGs) to improve the reasoning ability of LLMs. However, existingmethods face two limitations: 1) they typically assume that all answers to thequestions are contained in KGs, neglecting the incompleteness issue of KGs, and2) they treat the KG as a static repository and overlook the implicit logicalreasoning structures inherent in KGs. In this paper, we introduce SymAgent, aninnovative neural-symbolic agent framework that achieves collaborativeaugmentation between KGs and LLMs. We conceptualize KGs as dynamic environmentsand transform complex reasoning tasks into a multi-step interactive process,enabling KGs to participate deeply in the reasoning process. SymAgent consistsof two modules: Agent-Planner and Agent-Executor. The Agent-Planner leveragesLLM's inductive reasoning capability to extract symbolic rules from KGs,guiding efficient question decomposition. The Agent-Executor autonomouslyinvokes predefined action tools to integrate information from KGs and externaldocuments, addressing the issues of KG incompleteness. Furthermore, we design aself-learning framework comprising online exploration and offline iterativepolicy updating phases, enabling the agent to automatically synthesizereasoning trajectories and improve performance. Experimental resultsdemonstrate that SymAgent with weak LLM backbones (i.e., 7B series) yieldsbetter or comparable performance compared to various strong baselines. Furtheranalysis reveals that our agent can identify missing triples, facilitatingautomatic KG updates."}], "temperature": 0.1}}
{"custom_id": "207_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ben Liu"}], "temperature": 0.1}}
{"custom_id": "208_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Artificial Intelligence and Legal Analysis: Implications for Legal Education and the Profession"}], "temperature": 0.1}}
{"custom_id": "208_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This article reports the results of a study examining the ability of legaland non-legal Large Language Models to perform legal analysis using theIssue-Rule-Application-Conclusion framework. LLMs were tested on legalreasoning tasks involving rule analysis and analogical reasoning. The resultsshow that LLMs can conduct basic IRAC analysis, but are limited by briefresponses lacking detail, an inability to commit to answers, false confidence,and hallucinations. The study compares legal and nonlegal LLMs, identifiesshortcomings, and explores traits that may hinder their ability to think like alawyer. It also discusses the implications for legal education and practice,highlighting the need for critical thinking skills in future lawyers and thepotential pitfalls of overreliance on artificial intelligence AI resulting in aloss of logic, reasoning, and critical thinking skills."}], "temperature": 0.1}}
{"custom_id": "208_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Lee Peoples"}], "temperature": 0.1}}
{"custom_id": "209_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Standard Neural Computation Alone Is Insufficient for Logical Intelligence"}], "temperature": 0.1}}
{"custom_id": "209_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neural networks, as currently designed, fall short of achieving true logicalintelligence. Modern AI models rely on standard neuralcomputation-inner-product-based transformations and nonlinear activations-toapproximate patterns from data. While effective for inductive learning, thisarchitecture lacks the structural guarantees necessary for deductive inferenceand logical consistency. As a result, deep networks struggle with rule-basedreasoning, structured generalization, and interpretability without extensivepost-hoc modifications. This position paper argues that standard neural layersmust be fundamentally rethought to integrate logical reasoning. We advocate forLogical Neural Units (LNUs)-modular components that embed differentiableapproximations of logical operations (e.g., AND, OR, NOT) directly withinneural architectures. We critique existing neurosymbolic approaches, highlightthe limitations of standard neural computation for logical inference, andpresent LNUs as a necessary paradigm shift in AI. Finally, we outline a roadmapfor implementation, discussing theoretical foundations, architecturalintegration, and key challenges for future research."}], "temperature": 0.1}}
{"custom_id": "209_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Youngsung Kim"}], "temperature": 0.1}}
{"custom_id": "210_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Enhancing Large Language Model Efficiencyvia Symbolic Compression: A Formal Approach Towards Interpretability"}], "temperature": 0.1}}
{"custom_id": "210_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) face significant token efficiency bottlenecks incode generation and logical reasoning tasks, a challenge that directly impactsinference cost and model interpretability. This paper proposes a formalframework based on symbolic compression,integrating combinatory logic,information-theoretic optimal encoding, and context-aware inference techniquesto achieve a step-change improvement in token efficiency while preservingsemantic integrity. We establish a mathematical framework within a functionalprogramming paradigm, derive the quantitative relationship between symbolicdensity and model interpretability, and propose a differentiable compressionfactor metric to evaluate encoding efficiency. Furthermore, we leverageparameter-efficient fine-tuning (PEFT) techniques to achieve a low-costapplication of the GAEL language. Experimental results show that this methodachieves a 78.3% token compression rate in code generation tasks whileimproving logical traceability by 62% through structural explicitness. Thisresearch provides new theoretical tools for efficient inference in LLMs andopens a symbolic path for modelinterpretability research."}], "temperature": 0.1}}
{"custom_id": "210_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Lumen AI"}], "temperature": 0.1}}
{"custom_id": "211_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Town Hall Debate Prompting: Enhancing Logical Reasoning in LLMs through Multi-Persona Interaction"}], "temperature": 0.1}}
{"custom_id": "211_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Debate is a commonly used form of human communication catered towardsproblem-solving because of its efficiency. Debate fundamentally allows multipleviewpoints to be brought up in problem-solving, and for complex problems, eachviewpoint opens a new path for problem-solving. In this work, we apply thisconcept to LLM decision-making by proposing town hall-style debate prompting(THDP), a prompting method that splices a language model into multiple personasthat will debate one another to reach a conclusion. Our experimental pipelinevaries both the number of personas and the personality types of each persona tofind the optimum town hall size and personality for benchmark performance asmeasured by ZebraLogic bench, a reasoning-intensive benchmark characterized byboth multiple-choice and fill-in-the-blank questions. Our experimental resultsdemonstrate that a town hall size of 5 personas with LLM-determined personalitytypes performs optimally on ZebraLogic, achieving a 13\\% improvement overone-shot CoT baselines in per-cell accuracy in GPT-4o, 9% puzzle accuracyincrease in Claude 3.5 Sonnet, and an improvement in hard puzzle accuracy from10-15%."}], "temperature": 0.1}}
{"custom_id": "211_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Vivaan Sandwar"}], "temperature": 0.1}}
{"custom_id": "212_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "DBRouting: Routing End User Queries to Databases for Answerability"}], "temperature": 0.1}}
{"custom_id": "212_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Enterprise level data is often distributed across multiple sources andidentifying the correct set-of data-sources with relevant information for aknowledge request is a fundamental challenge. In this work, we define the noveltask of routing an end-user query to the appropriate data-source, where thedata-sources are databases. We synthesize datasets by extending existingdatasets designed for NL-to-SQL semantic parsing. We create baselines on thesedatasets by using open-source LLMs, using both pre-trained and task specificembeddings fine-tuned using the training data. With these baselines wedemonstrate that open-source LLMs perform better than embedding based approach,but suffer from token length limitations. Embedding based approaches benefitfrom task specific fine-tuning, more so when there is availability of data interms of database specific questions for training. We further find that thetask becomes more difficult (i) with an increase in the number of data-sources,(ii) having data-sources closer in terms of their domains,(iii) havingdatabases without external domain knowledge required to interpret its entitiesand (iv) with ambiguous and complex queries requiring more fine-grainedunderstanding of the data-sources or logical reasoning for routing to anappropriate source. This calls for the need for developing more sophisticatedsolutions to better address the task."}], "temperature": 0.1}}
{"custom_id": "212_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Priyangshu Mandal"}], "temperature": 0.1}}
{"custom_id": "213_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SedarEval: Automated Evaluation using Self-Adaptive Rubrics"}], "temperature": 0.1}}
{"custom_id": "213_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The evaluation paradigm of LLM-as-judge gains popularity due to itssignificant reduction in human labor and time costs. This approach utilizes oneor more large language models (LLMs) to assess the quality of outputs fromother LLMs. However, existing methods rely on generic scoring rubrics that failto consider the specificities of each question and its problem-solving process,compromising precision and stability in assessments. Inspired by humanexamination scoring processes, we propose a new evaluation paradigm based onself-adaptive rubrics. Specifically, we create detailed scoring rubrics foreach question, capturing the primary and secondary criteria in a structuredformat of scoring and deduction points that mimic a human evaluator'sanalytical process. Building on this paradigm, we further develop a novelbenchmark called SedarEval, which covers a range of domains including long-tailknowledge, mathematics, coding, and logical reasoning. SedarEval consists of1,000 meticulously crafted questions, each with its own self-adaptive rubric.To further streamline the evaluation, we train a specialized evaluator languagemodel (evaluator LM) to supplant human graders. Using the same training data,our evaluator LM achieves a higher concordance rate with human grading resultsthan other paradigms, including GPT-4, highlighting the superiority andefficiency of our approach. We release our dataset athttps://github.com/wwn1233/sedareval."}], "temperature": 0.1}}
{"custom_id": "213_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhiyuan Fan"}], "temperature": 0.1}}
{"custom_id": "214_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Causality-aware Paradigm for Evaluating Creativity of Multimodal Large Language Models"}], "temperature": 0.1}}
{"custom_id": "214_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recently, numerous benchmarks have been developed to evaluate the logicalreasoning abilities of large language models (LLMs). However, assessing theequally important creative capabilities of LLMs is challenging due to thesubjective, diverse, and data-scarce nature of creativity, especially inmultimodal scenarios. In this paper, we consider the comprehensive pipeline forevaluating the creativity of multimodal LLMs, with a focus on suitableevaluation platforms and methodologies. First, we find the Oogiri game, acreativity-driven task requiring humor, associative thinking, and the abilityto produce unexpected responses to text, images, or both. This game aligns wellwith the input-output structure of modern multimodal LLMs and benefits from arich repository of high-quality, human-annotated creative responses, making itan ideal platform for studying LLM creativity. Next, beyond using the Oogirigame for standard evaluations like ranking and selection, we propose LoTbench,an interactive, causality-aware evaluation framework, to further address someintrinsic risks in standard evaluations, such as information leakage andlimited interpretability. The proposed LoTbench not only quantifies LLMcreativity more effectively but also visualizes the underlying creative thoughtprocesses. Our results show that while most LLMs exhibit constrainedcreativity, the performance gap between LLMs and humans is not insurmountable.Furthermore, we observe a strong correlation between results from themultimodal cognition benchmark MMMU and LoTbench, but only a weak connectionwith traditional creativity metrics. This suggests that LoTbench better alignswith human cognitive theories, highlighting cognition as a critical foundationin the early stages of creativity and enabling the bridging of diverseconcepts. https://lotbench.github.io"}], "temperature": 0.1}}
{"custom_id": "214_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhongzhan Huang"}], "temperature": 0.1}}
{"custom_id": "215_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic Reasoning"}], "temperature": 0.1}}
{"custom_id": "215_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A recent approach to neurosymbolic reasoning is to explicitly combine thestrengths of large language models (LLMs) and symbolic solvers to tacklecomplex reasoning tasks. However, current approaches face significantlimitations, including poor generalizability due to task-specific prompts,inefficiencies caused by the lack of separation between knowledge and queries,and restricted inferential capabilities. These shortcomings hinder theirscalability and applicability across diverse domains. In this paper, weintroduce VERUS-LM, a novel framework designed to address these challenges.VERUS-LM employs a generic prompting mechanism, clearly separates domainknowledge from queries, and supports a wide range of different logicalreasoning tasks. This framework enhances adaptability, reduces computationalcost, and allows for richer forms of reasoning, such as optimization andconstraint satisfaction. We show that our approach succeeds in diversereasoning on a novel dataset, markedly outperforming LLMs. Additionally, oursystem achieves competitive results on common reasoning benchmarks whencompared to other state-of-the-art approaches, and significantly surpasses themon the difficult AR-LSAT dataset. By pushing the boundaries of hybridreasoning, VERUS-LM represents a significant step towards more versatileneurosymbolic AI systems"}], "temperature": 0.1}}
{"custom_id": "215_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Benjamin Callewaert"}], "temperature": 0.1}}
{"custom_id": "216_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "PIKE-RAG: sPecIalized KnowledgE and Rationale Augmented Generation"}], "temperature": 0.1}}
{"custom_id": "216_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Despite notable advancements in Retrieval-Augmented Generation (RAG) systemsthat expand large language model (LLM) capabilities through external retrieval,these systems often struggle to meet the complex and diverse needs ofreal-world industrial applications. The reliance on retrieval alone provesinsufficient for extracting deep, domain-specific knowledge performing inlogical reasoning from specialized corpora. To address this, we introducesPecIalized KnowledgE and Rationale Augmentation Generation (PIKE-RAG),focusing on extracting, understanding, and applying specialized knowledge,while constructing coherent rationale to incrementally steer LLMs towardaccurate responses. Recognizing the diverse challenges of industrial tasks, weintroduce a new paradigm that classifies tasks based on their complexity inknowledge extraction and application, allowing for a systematic evaluation ofRAG systems' problem-solving capabilities. This strategic approach offers aroadmap for the phased development and enhancement of RAG systems, tailored tomeet the evolving demands of industrial applications. Furthermore, we proposeknowledge atomizing and knowledge-aware task decomposition to effectivelyextract multifaceted knowledge from the data chunks and iteratively constructthe rationale based on original query and the accumulated knowledge,respectively, showcasing exceptional performance across various benchmarks."}], "temperature": 0.1}}
{"custom_id": "216_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jinyu Wang"}], "temperature": 0.1}}
{"custom_id": "217_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Assessing the Alignment of FOL Closeness Metrics with Human Judgement"}], "temperature": 0.1}}
{"custom_id": "217_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The recent successful paradigm of solving logical reasoning problems withtool-augmented large language models (LLMs) leverages translation of naturallanguage statements into First-Order Logic~(FOL) and external theorem provers.However, the correctness of FOL statements, comprising operators and textpredicates, often goes unverified due to the lack of a reliable evaluationmetric for comparing generated and ground-truth FOLs. In this paper, we presenta comprehensive study of sensitivity of existing metrics and their alignmentwith human judgement on FOL evaluation. Using ground-truth FOLs, we carefullydesigned various perturbations on the ground-truth to assess metricsensitivity. We sample FOL translation candidates for natural languagestatements and measure the ranking alignment between automatic metrics andhuman annotators. Our empirical findings highlight oversensitivity in then-gram metric BLEU for text perturbations, the semantic graph metric Smatch++for structural perturbations, and FOL metric for operator perturbation. We alsoobserve a closer alignment between BertScore and human judgement. Additionally,we show that combining metrics enhances both alignment and sensitivity comparedto using individual metrics."}], "temperature": 0.1}}
{"custom_id": "217_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ramya Keerthy Thatikonda"}], "temperature": 0.1}}
{"custom_id": "218_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LeapVAD: A Leap in Autonomous Driving via Cognitive Perception and Dual-Process Thinking"}], "temperature": 0.1}}
{"custom_id": "218_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "While autonomous driving technology has made remarkable strides, data-drivenapproaches still struggle with complex scenarios due to their limited reasoningcapabilities. Meanwhile, knowledge-driven autonomous driving systems haveevolved considerably with the popularization of visual language models. In thispaper, we propose LeapVAD, a novel method based on cognitive perception anddual-process thinking. Our approach implements a human-attentional mechanism toidentify and focus on critical traffic elements that influence drivingdecisions. By characterizing these objects through comprehensive attributes -including appearance, motion patterns, and associated risks - LeapVAD achievesmore effective environmental representation and streamlines the decision-makingprocess. Furthermore, LeapVAD incorporates an innovative dual-processdecision-making module miming the human-driving learning process. The systemconsists of an Analytic Process (System-II) that accumulates driving experiencethrough logical reasoning and a Heuristic Process (System-I) that refines thisknowledge via fine-tuning and few-shot learning. LeapVAD also includesreflective mechanisms and a growing memory bank, enabling it to learn from pastmistakes and continuously improve its performance in a closed-loop environment.To enhance efficiency, we develop a scene encoder network that generatescompact scene representations for rapid retrieval of relevant drivingexperiences. Extensive evaluations conducted on two leading autonomous drivingsimulators, CARLA and DriveArena, demonstrate that LeapVAD achieves superiorperformance compared to camera-only approaches despite limited training data.Comprehensive ablation studies further emphasize its effectiveness incontinuous learning and domain adaptation. Project page:https://pjlab-adg.github.io/LeapVAD/."}], "temperature": 0.1}}
{"custom_id": "218_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yukai Ma"}], "temperature": 0.1}}
{"custom_id": "219_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning with Graphs: Structuring Implicit Knowledge to Enhance LLMs Reasoning"}], "temperature": 0.1}}
{"custom_id": "219_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have demonstrated remarkable success across awide range of tasks; however, they still encounter challenges in reasoningtasks that require understanding and inferring relationships between distinctpieces of information within text sequences. This challenge is particularlypronounced in tasks involving multi-step processes, such as logical reasoningand multi-hop question answering, where understanding implicit relationshipsbetween entities and leveraging multi-hop connections in the given context arecrucial. Graphs, as fundamental data structures, explicitly represent pairwiserelationships between entities, thereby offering the potential to enhance LLMs'reasoning capabilities. External graphs have proven effective in supportingLLMs across multiple tasks. However, in many reasoning tasks, no pre-existinggraph structure is provided. Can we structure implicit knowledge derived fromcontext into graphs to assist LLMs in reasoning? In this paper, we proposeReasoning with Graphs (RwG) by first constructing explicit graphs from thecontext and then leveraging these graphs to enhance LLM reasoning performanceon reasoning tasks. Extensive experiments demonstrate the effectiveness of theproposed method in improving both logical reasoning and multi-hop questionanswering tasks."}], "temperature": 0.1}}
{"custom_id": "219_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Haoyu Han"}], "temperature": 0.1}}
{"custom_id": "220_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "TimeLogic: A Temporal Logic Benchmark for Video QA"}], "temperature": 0.1}}
{"custom_id": "220_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Temporal logical understanding, a core facet of human cognition, plays apivotal role in capturing complex sequential events and their temporalrelationships within videos. This capability is particularly crucial in taskslike Video Question Answering (VideoQA), where the goal is to process visualdata over time together with textual data to provide coherent answers. However,current VideoQA benchmarks devote little focus to evaluating this criticalskill due to the challenge of annotating temporal logic. Despite theadvancement of vision-language models, assessing their temporal logicalreasoning powers remains a challenge, primarily due to the lack QA pairs thatdemand formal, complex temporal reasoning. To bridge this gap, we introduce theTimeLogic QA (TLQA) framework to automatically generate the QA pairs,specifically designed to evaluate the temporal logical understanding. To thisend, TLQA leverages temporal annotations from existing video datasets togetherwith temporal operators derived from logic theory to construct questions thattest understanding of event sequences and their temporal relationships. TLQAframework is generic and scalable, capable of leveraging both, existing videoaction datasets with temporal action segmentation annotations, or videodatasets with temporal scene graph annotations, to automatically generatetemporal logical questions. We leverage 4 datasets, STAR, Breakfast, AGQA, andCrossTask, and generate two VideoQA dataset variants - small (TLQA-S) and large(TLQA-L) - containing 2k and 10k QA pairs for each category, resulting in 32kand 160k total pairs per dataset. We undertake a comprehensive evaluation ofleading-edge VideoQA models, employing the TLQA to benchmark their temporallogical understanding capabilities. We assess the VideoQA model's temporalreasoning performance on 16 categories of temporal logic with varying temporalcomplexity."}], "temperature": 0.1}}
{"custom_id": "220_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Sirnam Swetha"}], "temperature": 0.1}}
{"custom_id": "221_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neural Probabilistic Circuits: Enabling Compositional and Interpretable Predictions through Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "221_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "End-to-end deep neural networks have achieved remarkable success acrossvarious domains but are often criticized for their lack of interpretability.While post hoc explanation methods attempt to address this issue, they oftenfail to accurately represent these black-box models, resulting in misleading orincomplete explanations. To overcome these challenges, we propose an inherentlytransparent model architecture called Neural Probabilistic Circuits (NPCs),which enable compositional and interpretable predictions through logicalreasoning. In particular, an NPC consists of two modules: an attributerecognition model, which predicts probabilities for various attributes, and atask predictor built on a probabilistic circuit, which enables logicalreasoning over recognized attributes to make class predictions. To train NPCs,we introduce a three-stage training algorithm comprising attribute recognition,circuit construction, and joint optimization. Moreover, we theoreticallydemonstrate that an NPC's error is upper-bounded by a linear combination of theerrors from its modules. To further demonstrate the interpretability of NPC, weprovide both the most probable explanations and the counterfactualexplanations. Empirical results on four benchmark datasets show that NPCsstrike a balance between interpretability and performance, achieving resultscompetitive even with those of end-to-end black-box models while providingenhanced interpretability."}], "temperature": 0.1}}
{"custom_id": "221_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Weixin Chen"}], "temperature": 0.1}}
{"custom_id": "222_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Multimodal-to-Text Prompt Engineering in Large Language Models Using Feature Embeddings for GNSS Interference Characterization"}], "temperature": 0.1}}
{"custom_id": "222_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) are advanced AI systems applied across variousdomains, including NLP, information retrieval, and recommendation systems.Despite their adaptability and efficiency, LLMs have not been extensivelyexplored for signal processing tasks, particularly in the domain of globalnavigation satellite system (GNSS) interference monitoring. GNSS interferencemonitoring is essential to ensure the reliability of vehicle localization onroads, a critical requirement for numerous applications. However, GNSS-basedpositioning is vulnerable to interference from jamming devices, which cancompromise its accuracy. The primary objective is to identify, classify, andmitigate these interferences. Interpreting GNSS snapshots and the associatedinterferences presents significant challenges due to the inherent complexity,including multipath effects, diverse interference types, varying sensorcharacteristics, and satellite constellations. In this paper, we extractfeatures from a large GNSS dataset and employ LLaVA to retrieve relevantinformation from an extensive knowledge base. We employ prompt engineering tointerpret the interferences and environmental factors, and utilize t-SNE toanalyze the feature embeddings. Our findings demonstrate that the proposedmethod is capable of visual and logical reasoning within the GNSS context.Furthermore, our pipeline outperforms state-of-the-art machine learning modelsin interference classification tasks."}], "temperature": 0.1}}
{"custom_id": "222_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Harshith Manjunath"}], "temperature": 0.1}}
{"custom_id": "223_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LogicAD: Explainable Anomaly Detection via VLM-based Text Feature Extraction"}], "temperature": 0.1}}
{"custom_id": "223_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical image understanding involves interpreting and reasoning about therelationships and consistency within an image's visual content. This capabilityis essential in applications such as industrial inspection, where logicalanomaly detection is critical for maintaining high-quality standards andminimizing costly recalls. Previous research in anomaly detection (AD) hasrelied on prior knowledge for designing algorithms, which often requiresextensive manual annotations, significant computing power, and large amounts ofdata for training. Autoregressive, multimodal Vision Language Models (AVLMs)offer a promising alternative due to their exceptional performance in visualreasoning across various domains. Despite this, their application to logical ADremains unexplored. In this work, we investigate using AVLMs for logical AD anddemonstrate that they are well-suited to the task. Combining AVLMs with formatembedding and a logic reasoner, we achieve SOTA performance on publicbenchmarks, MVTec LOCO AD, with an AUROC of 86.0% and F1-max of 83.7%, alongwith explanations of anomalies. This significantly outperforms the existingSOTA method by a large margin."}], "temperature": 0.1}}
{"custom_id": "223_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Er Jin"}], "temperature": 0.1}}
{"custom_id": "224_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Formalization of Biological Circuit Block Diagrams for formally analyzing Biomedical Control Systems in pHRI Applications"}], "temperature": 0.1}}
{"custom_id": "224_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The control of Biomedical Systems in Physical Human-Robot Interaction (pHRI)plays a pivotal role in achieving the desired behavior by ensuring the intendedtransfer function and stability of subsystems within the overall system.Traditionally, the control aspects of biomedical systems have been analyzedusing manual proofs and computer based analysis tools. However, theseapproaches provide inaccurate results due to human error in manual proofs andunverified algorithms and round-off errors in computer-based tools. We argueusing Interactive reasoning, or frequently called theorem proving, to analyzecontrol systems of biomedical engineering applications, specifically in thecontext of Physical Human-Robot Interaction (pHRI). Our methodology involvesconstructing mathematical models of the control components using Higher-orderLogic (HOL) and analyzing them through deductive reasoning in the HOL Lighttheorem prover. We propose to model these control systems in terms of theirblock diagram representations, which in turn utilize the correspondingdifferential equations and their transfer function-based representation usingthe Laplace Transform (LT). These formally represented block diagrams are thenanalyzed through logical reasoning in the trusted environment of a theoremprover to ensure the correctness of the results. For illustration, we present areal-world case study by analyzing the control system of the ultrafilterationdialysis process."}], "temperature": 0.1}}
{"custom_id": "224_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Adnan Rashid"}], "temperature": 0.1}}
{"custom_id": "225_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for LLMs in Cybersecurity"}], "temperature": 0.1}}
{"custom_id": "225_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Evaluating Large Language Models (LLMs) is crucial for understanding theircapabilities and limitations across various applications, including naturallanguage processing and code generation. Existing benchmarks like MMLU, C-Eval,and HumanEval assess general LLM performance but lack focus on specific expertdomains such as cybersecurity. Previous attempts to create cybersecuritydatasets have faced limitations, including insufficient data volume and areliance on multiple-choice questions (MCQs). To address these gaps, we proposeSecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs inthe cybersecurity domain. SecBench includes questions in various formats (MCQsand short-answer questions (SAQs)), at different capability levels (KnowledgeRetention and Logical Reasoning), in multiple languages (Chinese and English),and across various sub-domains. The dataset was constructed by collectinghigh-quality data from open sources and organizing a Cybersecurity QuestionDesign Contest, resulting in 44,823 MCQs and 3,087 SAQs. Particularly, we usedthe powerful while cost-effective LLMs to (1). label the data and (2).constructing a grading agent for automatic evaluation of SAQs. Benchmarkingresults on 16 SOTA LLMs demonstrate the usability of SecBench, which isarguably the largest and most comprehensive benchmark dataset for LLMs incybersecurity. More information about SecBench can be found at our website, andthe dataset can be accessed via the artifact link."}], "temperature": 0.1}}
{"custom_id": "225_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Pengfei Jing"}], "temperature": 0.1}}
{"custom_id": "226_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Harnessing Large Language Models for Knowledge Graph Question Answering via Adaptive Multi-Aspect Retrieval-Augmentation"}], "temperature": 0.1}}
{"custom_id": "226_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) demonstrate remarkable capabilities, yetstruggle with hallucination and outdated knowledge when tasked with complexknowledge reasoning, resulting in factually incorrect outputs. Previous studieshave attempted to mitigate it by retrieving factual knowledge from large-scaleknowledge graphs (KGs) to assist LLMs in logical reasoning and prediction ofanswers. However, this kind of approach often introduces noise and irrelevantdata, especially in situations with extensive context from multiple knowledgeaspects. In this way, LLM attention can be potentially mislead from questionand relevant information. In our study, we introduce an Adaptive Multi-AspectRetrieval-augmented over KGs (Amar) framework. This method retrieves knowledgeincluding entities, relations, and subgraphs, and converts each piece ofretrieved text into prompt embeddings. The Amar framework comprises two keysub-components: 1) a self-alignment module that aligns commonalities amongentities, relations, and subgraphs to enhance retrieved text, thereby reducingnoise interference; 2) a relevance gating module that employs a soft gate tolearn the relevance score between question and multi-aspect retrieved data, todetermine which information should be used to enhance LLMs' output, or evenfiltered altogether. Our method has achieved state-of-the-art performance ontwo common datasets, WebQSP and CWQ, showing a 1.9\\% improvement in accuracyover its best competitor and a 6.6\\% improvement in logical form generationover a method that directly uses retrieved text as context prompts. Theseresults demonstrate the effectiveness of Amar in improving the reasoning ofLLMs."}], "temperature": 0.1}}
{"custom_id": "226_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Derong Xu"}], "temperature": 0.1}}
{"custom_id": "227_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "ADC: Enhancing Function Calling Via Adversarial Datasets and Code Line-Level Feedback"}], "temperature": 0.1}}
{"custom_id": "227_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have made significant strides in NaturalLanguage Processing and coding, yet they struggle with robustness and accuracyin complex function calls. To tackle these challenges, this paper introducesADC, an innovative approach that enhances LLMs' ability to follow functionformats and match complex parameters. ADC utilizes a high-quality codefine-tuning dataset with line-level execution feedback, providing granularprocess supervision that fosters strong logical reasoning and adherence tofunction formats. It also employs an adversarial dataset generation process toimprove parameter matching. The staged training methodology capitalizes on bothenriched code datasets and refined adversarial datasets, leading to markedimprovements in function calling capabilities on the Berkeley Function-CallingLeaderboard (BFCL) Benchmark. The innovation of ADC lies in its strategiccombination of process supervision, adversarial refinement, and incrementallearning, setting a new standard for LLM proficiency in complex functioncalling."}], "temperature": 0.1}}
{"custom_id": "227_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Wei Zhang"}], "temperature": 0.1}}
{"custom_id": "228_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "DR-Encoder: Encode Low-rank Gradients with Random Prior for Large Language Models Differentially Privately"}], "temperature": 0.1}}
{"custom_id": "228_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The emergence of the Large Language Model (LLM) has shown their superiorityin a wide range of disciplines, including language understanding andtranslation, relational logic reasoning, and even partial differentialequations solving. The transformer is the pervasive backbone architecture forthe foundation model construction. It is vital to research how to adjust theTransformer architecture to achieve an end-to-end privacy guarantee in LLMfine-tuning. In this paper, we investigate three potential information leakageduring a federated fine-tuning procedure for LLM (FedLLM). Based on thepotential information leakage, we provide an end-to-end privacy guaranteesolution for FedLLM by inserting two-stage randomness. The first stage is totrain a gradient auto-encoder with a Gaussian random prior based on thestatistical information of the gradients generated by local clients. The secondstage is to fine-tune the overall LLM with a differential privacy guarantee byadopting appropriate Gaussian noises. We show the efficiency and accuracy gainsof our proposed method with several foundation models and two popularevaluation benchmarks. Furthermore, we present a comprehensive privacy analysiswith Gaussian Differential Privacy (GDP) and Renyi Differential Privacy (RDP)."}], "temperature": 0.1}}
{"custom_id": "228_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Huiwen Wu"}], "temperature": 0.1}}
{"custom_id": "229_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Aristotle: Mastering Logical Reasoning with A Logic-Complete Decompose-Search-Resolve Framework"}], "temperature": 0.1}}
{"custom_id": "229_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In the context of large language models (LLMs), current advanced reasoningmethods have made impressive strides in various reasoning tasks. However, whenit comes to logical reasoning tasks, major challenges remain in both efficacyand efficiency. This is rooted in the fact that these systems fail to fullyleverage the inherent structure of logical tasks throughout the reasoningprocesses such as decomposition, search, and resolution. To address this, wepropose a logic-complete reasoning framework, Aristotle, with three keycomponents: Logical Decomposer, Logical Search Router, and Logical Resolver. Inour framework, symbolic expressions and logical rules are comprehensivelyintegrated into the entire reasoning process, significantly alleviating thebottlenecks of logical reasoning, i.e., reducing sub-task complexity,minimizing search errors, and resolving logical contradictions. Theexperimental results on several datasets demonstrate that Aristotleconsistently outperforms state-of-the-art reasoning frameworks in both accuracyand efficiency, particularly excelling in complex logical reasoning scenarios.We will open-source all our code at https://github.com/Aiden0526/Aristotle."}], "temperature": 0.1}}
{"custom_id": "229_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jundong Xu"}], "temperature": 0.1}}
{"custom_id": "230_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Formal Language Knowledge Corpus for Retrieval Augmented Generation"}], "temperature": 0.1}}
{"custom_id": "230_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The integration of retrieval-augmented techniques with LLMs has shown promisein improving performance across various domains. However, their utility intasks requiring advanced reasoning, such as generating and evaluatingmathematical statements and proofs, remains underexplored. This study exploresthe use of Lean, a programming language for writing mathematical proofs, topopulate the knowledge corpus used by RAG systems. We hope for this to lay thefoundation to exploring different methods of using RAGs to improve theperformance of LLMs in advanced logical reasoning tasks."}], "temperature": 0.1}}
{"custom_id": "230_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Majd Zayyad"}], "temperature": 0.1}}
{"custom_id": "231_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical Consistency of Large Language Models in Fact-checking"}], "temperature": 0.1}}
{"custom_id": "231_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In recent years, large language models (LLMs) have demonstrated significantsuccess in performing varied natural language tasks such as languagetranslation, question-answering, summarizing, fact-checking, etc. Despite LLMs'impressive ability to generate human-like texts, LLMs are infamous for theirinconsistent responses - a meaning-preserving change in the input query resultsin an inconsistent response and attributes to vulnerabilities of LLMs such ashallucination. Consequently, existing research focuses on simpleparaphrasing-based consistency assessment of LLMs, and ignores complex queriesthat necessitate an even better understanding of logical reasoning by an LLM.Our work therefore addresses the logical inconsistency of LLMs under complexlogical queries with primitive logical operators, e.g., negation, conjunction,and disjunction. As a test bed, we consider retrieval-augmented LLMs on afact-checking task involving propositional logic queries from knowledge graphs(KGs). Our contributions are threefold. Benchmark: We introduce three logicalfact-checking datasets over KGs for community development towards logicallyconsistent LLMs. Assessment: We propose consistency measures of LLMs onpropositional logic queries and demonstrate that existing LLMs lack logicalconsistency, especially on complex queries. Improvement: We employ supervisedfine-tuning to improve the logical consistency of LLMs on the complexfact-checking task with KG contexts. We have made our source code andbenchmarks available."}], "temperature": 0.1}}
{"custom_id": "231_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Bishwamittra Ghosh"}], "temperature": 0.1}}
{"custom_id": "232_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "To Rely or Not to Rely? Evaluating Interventions for Appropriate Reliance on Large Language Models"}], "temperature": 0.1}}
{"custom_id": "232_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "As Large Language Models become integral to decision-making, optimism abouttheir power is tempered with concern over their errors. Users may over-rely onLLM advice that is confidently stated but wrong, or under-rely due to mistrust.Reliance interventions have been developed to help users of LLMs, but they lackrigorous evaluation for appropriate reliance. We benchmark the performance ofthree relevant interventions by conducting a randomized online experiment with400 participants attempting two challenging tasks: LSAT logical reasoning andimage-based numerical estimation. For each question, participants firstanswered independently, then received LLM advice modified by one of threereliance interventions and answered the question again. Our findings indicatethat while interventions reduce over-reliance, they generally fail to improveappropriate reliance. Furthermore, people became more confident after makingwrong reliance decisions in certain contexts, demonstrating poor calibration.Based on our findings, we discuss implications for designing effective relianceinterventions in human-LLM collaboration."}], "temperature": 0.1}}
{"custom_id": "232_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jessica Y. Bo"}], "temperature": 0.1}}
{"custom_id": "233_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "WiseAD: Knowledge Augmented End-to-End Autonomous Driving with Vision-Language Model"}], "temperature": 0.1}}
{"custom_id": "233_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The emergence of general human knowledge and impressive logical reasoningcapacity in rapidly progressed vision-language models (VLMs) have drivenincreasing interest in applying VLMs to high-level autonomous driving tasks,such as scene understanding and decision-making. However, an in-depth study onthe relationship between knowledge proficiency, especially essential drivingexpertise, and closed-loop autonomous driving performance requires furtherexploration. In this paper, we investigate the effects of the depth and breadthof fundamental driving knowledge on closed-loop trajectory planning andintroduce WiseAD, a specialized VLM tailored for end-to-end autonomous drivingcapable of driving reasoning, action justification, object recognition, riskanalysis, driving suggestions, and trajectory planning across diversescenarios. We employ joint training on driving knowledge and planning datasets,enabling the model to perform knowledge-aligned trajectory planningaccordingly. Extensive experiments indicate that as the diversity of drivingknowledge extends, critical accidents are notably reduced, contributing 11.9%and 12.4% improvements in the driving score and route completion on the Carlaclosed-loop evaluations, achieving state-of-the-art performance. Moreover,WiseAD also demonstrates remarkable performance in knowledge evaluations onboth in-domain and out-of-domain datasets."}], "temperature": 0.1}}
{"custom_id": "233_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Songyan Zhang"}], "temperature": 0.1}}
{"custom_id": "234_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Benchmarking Table Comprehension In The Wild"}], "temperature": 0.1}}
{"custom_id": "234_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs), while being increasingly dominant on a myriadof knowledge-intensive activities, have only had limited success understandinglengthy table-text mixtures, such as academic papers and financial reports.Recent advances of long-context LLMs have opened up new possibilities for thisfield. Nonetheless, we identify two roadblocks: (1) Prior benchmarks of tablequestion answering (TableQA) have focused on isolated tables without context,making it hard to evaluate models in real-world scenarios. (2) Prior benchmarkshave focused on some narrow skill sets of table comprehension such as tablerecognition, data manipulation/calculation, table summarization etc., while askilled human employs those skills collectively. In this work, we introduceTableQuest, a new benchmark designed to evaluate the holistic tablecomprehension capabilities of LLMs in the natural table-rich context offinancial reports. We employ a rigorous data processing and filtering procedureto ensure that the question-answer pairs are logical, reasonable, and diverse.We experiment with 7 state-of-the-art models, and find that despite reasonableaccuracy in locating facts, they often falter when required to execute moresophisticated reasoning or multi-step calculations. We conclude with aqualitative study of the failure modes and discuss the challenges ofconstructing a challenging benchmark. We make the evaluation data, judgingprocedure and results of this study publicly available to facilitate researchin this field."}], "temperature": 0.1}}
{"custom_id": "234_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yikang Pan"}], "temperature": 0.1}}
{"custom_id": "235_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning-Aware Query-Focused Summarization over Multi-Table Data"}], "temperature": 0.1}}
{"custom_id": "235_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Query-focused summarization over multi-table data is a challenging yetcritical task for extracting precise and relevant information from structureddata. Existing methods often rely on complex preprocessing steps and struggleto generalize across domains or handle the logical reasoning required formulti-table queries. In this paper, we propose QueryTableSummarizer++, anend-to-end generative framework leveraging large language models (LLMs)enhanced with table-aware pre-training, query-aligned fine-tuning, andreinforcement learning with feedback. Our method eliminates the need forintermediate serialization steps and directly generates query-relevantsummaries. Experiments on a benchmark dataset demonstrate thatQueryTableSummarizer++ significantly outperforms state-of-the-art baselines interms of BLEU, ROUGE, and F1-score. Additional analyses highlight itsscalability, generalization across domains, and robust handling of complexqueries. Human evaluation further validates the superior quality and practicalapplicability of the generated summaries, establishing QueryTableSummarizer++as a highly effective solution for multi-table summarization tasks."}], "temperature": 0.1}}
{"custom_id": "235_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xiaochuan Lin"}], "temperature": 0.1}}
{"custom_id": "236_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Federated In-Context LLM Agent Learning"}], "temperature": 0.1}}
{"custom_id": "236_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have revolutionized intelligent services byenabling logical reasoning, tool use, and interaction with external systems asagents. The advancement of LLMs is frequently hindered by the scarcity ofhigh-quality data, much of which is inherently sensitive. Federated learning(FL) offers a potential solution by facilitating the collaborative training ofdistributed LLMs while safeguarding private data. However, FL frameworks facesignificant bandwidth and computational demands, along with challenges fromheterogeneous data distributions. The emerging in-context learning capabilityof LLMs offers a promising approach by aggregating natural language rather thanbulky model parameters. Yet, this method risks privacy leakage, as itnecessitates the collection and presentation of data samples from variousclients during aggregation. In this paper, we propose a novelprivacy-preserving Federated In-Context LLM Agent Learning (FICAL) algorithm,which to our best knowledge for the first work unleashes the power ofin-context learning to train diverse LLM agents through FL. In our design,knowledge compendiums generated by a novel LLM-enhanced Knowledge CompendiumsGeneration (KCG) module are transmitted between clients and the server insteadof model parameters in previous FL methods. Apart from that, an incredibleRetrieval Augmented Generation (RAG) based Tool Learning and Utilizing (TLU)module is designed and we incorporate the aggregated global knowledgecompendium as a teacher to teach LLM agents the usage of tools. We conductedextensive experiments and the results show that FICAL has competitiveperformance compared to other SOTA baselines with a significant communicationcost decrease of $\\mathbf{3.33\\times10^5}$ times."}], "temperature": 0.1}}
{"custom_id": "236_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Panlong Wu"}], "temperature": 0.1}}
{"custom_id": "237_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware"}], "temperature": 0.1}}
{"custom_id": "237_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "While Transformers and other sequence-parallelizable neural networkarchitectures seem like the current state of the art in sequence modeling, theyspecifically lack state-tracking capabilities. These are important fortime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,as well as modern variants like sLSTM do have these capabilities at the cost ofstrictly sequential processing. While this is often seen as a stronglimitation, we show how fast these networks can get with ourhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to theregister level on modern GPUs. We extend traditional RNNs with aparallelization variant that processes multiple RNNs of smaller hidden state inparallel, similar to the head-wise processing in Transformers. To enableflexibility on different GPU variants, we introduce a new optimizationframework for hardware-internal cache sizes, memory and compute handling. Itmodels the hardware in a setting using polyhedral-like constraints, includingthe notion of divisibility. This speeds up the solution process in ourConstrINT library for general integer constraint satisfaction problems (integerCSPs). We show that our kernels can achieve 50x speed-ups over a vanillaPyTorch implementation and allow 40x larger hidden sizes compared to our Tritonimplementation. Our open-source kernels and the optimization library arereleased here to boost research in the direction of state-tracking enabled RNNsand sequence modeling: https://github.com/NX-AI/flashrnn"}], "temperature": 0.1}}
{"custom_id": "237_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Korbinian P枚ppel"}], "temperature": 0.1}}
{"custom_id": "238_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Algorithmic Phase Transitions in Language Models: A Mechanistic Case Study of Arithmetic"}], "temperature": 0.1}}
{"custom_id": "238_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zero-shot capabilities of large language models make them powerful tools forsolving a range of tasks without explicit training. It remains unclear,however, how these models achieve such performance, or why they can zero-shotsome tasks but not others. In this paper, we shed some light on this phenomenonby defining and investigating algorithmic stability in language models --changes in problem-solving strategy employed by the model as a result ofchanges in task specification. We focus on a task where algorithmic stabilityis needed for generalization: two-operand arithmetic. Surprisingly, we findthat Gemma-2-2b employs substantially different computational models on closelyrelated subtasks, i.e. four-digit versus eight-digit addition. Our findingssuggest that algorithmic instability may be a contributing factor to languagemodels' poor zero-shot performance across certain logical reasoning tasks, asthey struggle to abstract different problem-solving strategies and smoothlytransition between them."}], "temperature": 0.1}}
{"custom_id": "238_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Alan Sun"}], "temperature": 0.1}}
{"custom_id": "239_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Training Large Language Models to Reason in a Continuous Latent Space"}], "temperature": 0.1}}
{"custom_id": "239_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) are restricted to reason in the \"languagespace\", where they typically express the reasoning process with achain-of-thought (CoT) to solve a complex reasoning problem. However, we arguethat language space may not always be optimal for reasoning. For example, mostword tokens are primarily for textual coherence and not essential forreasoning, while some critical tokens require complex planning and pose hugechallenges to LLMs. To explore the potential of LLM reasoning in anunrestricted latent space instead of using natural language, we introduce a newparadigm Coconut (Chain of Continuous Thought). We utilize the last hiddenstate of the LLM as a representation of the reasoning state (termed \"continuousthought\"). Rather than decoding this into a word token, we feed it back to theLLM as the subsequent input embedding directly in the continuous space.Experiments show that Coconut can effectively augment the LLM on severalreasoning tasks. This novel latent reasoning paradigm leads to emergentadvanced reasoning patterns: the continuous thought can encode multiplealternative next reasoning steps, allowing the model to perform a breadth-firstsearch (BFS) to solve the problem, rather than prematurely committing to asingle deterministic path like CoT. Coconut outperforms CoT in certain logicalreasoning tasks that require substantial backtracking during planning, withfewer thinking tokens during inference. These findings demonstrate the promiseof latent reasoning and offer valuable insights for future research."}], "temperature": 0.1}}
{"custom_id": "239_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Shibo Hao"}], "temperature": 0.1}}
{"custom_id": "240_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Can OpenAI o1 outperform humans in higher-order cognitive thinking?"}], "temperature": 0.1}}
{"custom_id": "240_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This study evaluates the performance of OpenAI's o1-preview model inhigher-order cognitive domains, including critical thinking, systematicthinking, computational thinking, data literacy, creative thinking, logicalreasoning, and scientific reasoning. Using established benchmarks, we comparedthe o1-preview models's performance to human participants from diverseeducational levels. o1-preview achieved a mean score of 24.33 on the Ennis-WeirCritical Thinking Essay Test (EWCTET), surpassing undergraduate (13.8) andpostgraduate (18.39) participants (z = 1.60 and 0.90, respectively). Insystematic thinking, it scored 46.1, SD = 4.12 on the Lake Urmia Vignette,significantly outperforming the human mean (20.08, SD = 8.13, z = 3.20). Fordata literacy, o1-preview scored 8.60, SD = 0.70 on Merk et al.'s \"Use Data\"dimension, compared to the human post-test mean of 4.17, SD = 2.02 (z = 2.19).On creative thinking tasks, the model achieved originality scores of 2.98, SD =0.73, higher than the human mean of 1.74 (z = 0.71). In logical reasoning(LogiQA), it outperformed humans with average 90%, SD = 10% accuracy versus86%, SD = 6.5% (z = 0.62). For scientific reasoning, it achieved near-perfectperformance (mean = 0.99, SD = 0.12) on the TOSLS,, exceeding the highest humanscores of 0.85, SD = 0.13 (z = 1.78). While o1-preview excelled in structuredtasks, it showed limitations in problem-solving and adaptive reasoning. Theseresults demonstrate the potential of AI to complement education in structuredassessments but highlight the need for ethical oversight and refinement forbroader applications."}], "temperature": 0.1}}
{"custom_id": "240_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ehsan Latif"}], "temperature": 0.1}}
{"custom_id": "241_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Who Speaks Next? Multi-party AI Discussion Leveraging the Systematics of Turn-taking in Murder Mystery Games"}], "temperature": 0.1}}
{"custom_id": "241_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Multi-agent systems utilizing large language models (LLMs) have shown greatpromise in achieving natural dialogue. However, smooth dialogue control andautonomous decision making among agents still remain challenges. In this study,we focus on conversational norms such as adjacency pairs and turn-taking foundin conversation analysis and propose a new framework called \"Murder MysteryAgents\" that applies these norms to AI agents' dialogue control. As anevaluation target, we employed the \"Murder Mystery\" game, a reasoning-typetable-top role-playing game that requires complex social reasoning andinformation manipulation. In this game, players need to unravel the truth ofthe case based on fragmentary information through cooperation and bargaining.The proposed framework integrates next speaker selection based on adjacencypairs and a self-selection mechanism that takes agents' internal states intoaccount to achieve more natural and strategic dialogue. To verify theeffectiveness of this new approach, we analyzed utterances that led to dialoguebreakdowns and conducted automatic evaluation using LLMs, as well as humanevaluation using evaluation criteria developed for the Murder Mystery game.Experimental results showed that the implementation of the next speakerselection mechanism significantly reduced dialogue breakdowns and improved theability of agents to share information and perform logical reasoning. Theresults of this study demonstrate that the systematics of turn-taking in humanconversation are also effective in controlling dialogue among AI agents, andprovide design guidelines for more advanced multi-agent dialogue systems."}], "temperature": 0.1}}
{"custom_id": "241_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ryota Nonomura"}], "temperature": 0.1}}
{"custom_id": "242_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Guidance is All You Need: Temperature-Guided Reasoning in Large Language Models"}], "temperature": 0.1}}
{"custom_id": "242_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We present Quasar-1, a novel architecture that introduces temperature-guidedreasoning to large language models through the Token Temperature Mechanism(TTM) and Guided Sequence of Thought (GSoT). Our approach leverages the conceptof hot and cold tokens, where hot tokens are prioritized for their contextualrelevance, while cold tokens provide supplementary information. This dynamicmodulation of token importance enables the model to achieve superior logicalreasoning capabilities compared to traditional chain-of-thought approaches.Through rigorous mathematical analysis, we prove that our temperature-guidedattention mechanism converges to optimal reasoning paths with exponentialguarantees. Empirical results show significant improvements in reasoningaccuracy and computational efficiency across a wide range of tasks, makingadvanced AI reasoning accessible to a broader range of applications."}], "temperature": 0.1}}
{"custom_id": "242_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Eyad Gomaa"}], "temperature": 0.1}}
{"custom_id": "243_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "MTMT: Consolidating Multiple Thinking Modes to Form a Thought Tree for Strengthening LLM"}], "temperature": 0.1}}
{"custom_id": "243_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have shown limitations in tasks requiringcomplex logical reasoning and multi-step problem-solving. To address thesechallenges, researchers have employed carefully designed prompts andflowcharts, simulating human cognitive processes to enhance LLM performance,such as the Chain of Thought approach. In this paper, we introduce MTMT(Multi-thinking Modes Tree), a novel method that interacts with LLMs toconstruct a thought tree, simulating various advanced cognitive processes,including but not limited to association, counterfactual thinking, taskdecomposition, and comparison. By breaking down the original complex task intosimpler sub-questions, MTMT facilitates easier problem-solving for LLMs,enabling more effective utilization of the latent knowledge within LLMs. Weevaluate the performance of MTMT under different parameter configurations,using GPT-4o mini as the base model. Our results demonstrate that integratingmultiple modes of thinking significantly enhances the ability of LLMs to handlecomplex tasks."}], "temperature": 0.1}}
{"custom_id": "243_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Changcheng Li"}], "temperature": 0.1}}
{"custom_id": "244_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reverse Thinking Makes LLMs Stronger Reasoners"}], "temperature": 0.1}}
{"custom_id": "244_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reverse thinking plays a crucial role in human reasoning. Humans can reasonnot only from a problem to a solution but also in reverse, i.e., start from thesolution and reason towards the problem. This often enhances overall reasoningperformance as it enables consistency checks between their forward and backwardthinking. To enable Large Language Models (LLMs) to perform reverse thinking,we introduce Reverse-Enhanced Thinking (RevThink), a framework composed of dataaugmentation and learning objectives. In RevThink, we augment the dataset bycollecting structured forward-backward reasoning from a teacher model,consisting of: (1) the original question, (2) forward reasoning, (3) backwardquestion, and (4) backward reasoning. We then employ three objectives to traina smaller student model in a multi-task learning fashion: (a) generate forwardreasoning from a question, (b) generate a backward question from a question,and (c) generate backward reasoning from the backward question. Experimentsacross 12 datasets covering commonsense, math, and logical reasoning show anaverage 13.53% improvement over the student model's zero-shot performance and a6.84% improvement over the strongest knowledge distillation baselines.Moreover, our method demonstrates sample efficiency -- using only 10% of thecorrect forward reasoning from the training data, it outperforms a standardfine-tuning method trained on 10x more forward reasoning. RevThink alsoexhibits strong generalization to out-of-distribution held-out datasets."}], "temperature": 0.1}}
{"custom_id": "244_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Justin Chih-Yao Chen"}], "temperature": 0.1}}
{"custom_id": "245_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "ScratchEval: Are GPT-4o Smarter than My Child? Evaluating Large Multimodal Models with Visual Programming Challenges"}], "temperature": 0.1}}
{"custom_id": "245_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advancements in large multimodal models (LMMs) have showcasedimpressive code generation capabilities, primarily evaluated throughimage-to-code benchmarks. However, these benchmarks are limited to specificvisual programming scenarios where the logic reasoning and the multimodalunderstanding capacities are split apart. To fill this gap, we proposeScratchEval, a novel benchmark designed to evaluate the visual programmingreasoning ability of LMMs. ScratchEval is based on Scratch, a block-basedvisual programming language widely used in children's programming education. Byintegrating visual elements and embedded programming logic, ScratchEvalrequires the model to process both visual information and code structure,thereby comprehensively evaluating its programming intent understandingability. Our evaluation approach goes beyond the traditional image-to-codemapping and focuses on unified logical thinking and problem-solving abilities,providing a more comprehensive and challenging framework for evaluating thevisual programming ability of LMMs. ScratchEval not only fills the gap inexisting evaluation methods, but also provides new insights for the futuredevelopment of LMMs in the field of visual programming. Our benchmark can beaccessed at https://github.com/HKBUNLP/ScratchEval ."}], "temperature": 0.1}}
{"custom_id": "245_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Rao Fu"}], "temperature": 0.1}}
{"custom_id": "246_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs"}], "temperature": 0.1}}
{"custom_id": "246_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities acrossvarious tasks, yet they often struggle with spatial reasoning. This paperpresents a novel neural-symbolic framework that enhances LLMs' spatialreasoning abilities through iterative feedback between LLMs and Answer SetProgramming (ASP). We evaluate our approach on two benchmark datasets: StepGameand SparQA, implementing three distinct strategies: (1) direct promptingbaseline, (2) Facts+Rules prompting, and (3) DSPy-based LLM+ASP pipeline withiterative refinement. Our experimental results demonstrate that the LLM+ASPpipeline significantly outperforms baseline methods, achieving an average 82%accuracy on StepGame and 69% on SparQA, marking improvements of 40-50% and8-15% respectively over direct prompting. The success stems from three keyinnovations: (1) effective separation of semantic parsing and logical reasoningthrough a modular pipeline, (2) iterative feedback mechanism between LLMs andASP solvers that improves program rate, and (3) robust error handling thataddresses parsing, grounding, and solving failures. Additionally, we proposeFacts+Rules as a lightweight alternative that achieves comparable performanceon complex SparQA dataset, while reducing computational overhead.Our analysisacross different LLM architectures (Deepseek, Llama3-70B, GPT-4.0 mini)demonstrates the framework's generalizability and provides insights into thetrade-offs between implementation complexity and reasoning capability,contributing to the development of more interpretable and reliable AI systems."}], "temperature": 0.1}}
{"custom_id": "246_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Rong Wang"}], "temperature": 0.1}}
{"custom_id": "247_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Learning for Long-Horizon Planning via Neuro-Symbolic Abductive Imitation"}], "temperature": 0.1}}
{"custom_id": "247_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent learning-to-imitation methods have shown promising results in planningvia imitating within the observation-action space. However, their ability inopen environments remains constrained, particularly in long-horizon tasks. Incontrast, traditional symbolic planning excels in long-horizon tasks throughlogical reasoning over human-defined symbolic spaces but struggles to handleobservations beyond symbolic states, such as high-dimensional visual inputsencountered in real-world scenarios. In this work, we draw inspiration fromabductive learning and introduce a novel framework \\textbf{AB}ductive\\textbf{I}mitation \\textbf{L}earning (ABIL) that integrates the benefits ofdata-driven learning and symbolic-based reasoning, enabling long-horizonplanning. Specifically, we employ abductive reasoning to understand thedemonstrations in symbolic space and design the principles of sequentialconsistency to resolve the conflicts between perception and reasoning. ABILgenerates predicate candidates to facilitate the perception from rawobservations to symbolic space without laborious predicate annotations,providing a groundwork for symbolic planning. With the symbolic understanding,we further develop a policy ensemble whose base policies are built withdifferent logical objectives and managed through symbolic reasoning.Experiments show that our proposal successfully understands the observationswith the task-relevant symbolics to assist the imitation learning. Importantly,ABIL demonstrates significantly improved data efficiency and generalizationacross various long-horizon tasks, highlighting it as a promising solution forlong-horizon planning. Project website:\\url{https://www.lamda.nju.edu.cn/shaojj/KDD25_ABIL/}."}], "temperature": 0.1}}
{"custom_id": "247_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jie-Jing Shao"}], "temperature": 0.1}}
{"custom_id": "248_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SentiXRL: An advanced large language Model Framework for Multilingual Fine-Grained Emotion Classification in Complex Text Environment"}], "temperature": 0.1}}
{"custom_id": "248_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "With strong expressive capabilities in Large Language Models(LLMs),generative models effectively capture sentiment structures and deep semantics,however, challenges remain in fine-grained sentiment classification acrossmulti-lingual and complex contexts. To address this, we propose the SentimentCross-Lingual Recognition and Logic Framework (SentiXRL), which incorporatestwo modules,an emotion retrieval enhancement module to improve sentimentclassification accuracy in complex contexts through historical dialogue andlogical reasoning,and a self-circulating analysis negotiation mechanism(SANM)to facilitates autonomous decision-making within a single model forclassification tasks.We have validated SentiXRL's superiority on multiplestandard datasets, outperforming existing models on CPED and CH-SIMS,andachieving overall better performance on MELD,Emorynlp and IEMOCAP. Notably, weunified labels across several fine-grained sentiment annotation datasets andconducted category confusion experiments, revealing challenges and impacts ofclass imbalance in standard datasets."}], "temperature": 0.1}}
{"custom_id": "248_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jie Wang"}], "temperature": 0.1}}
{"custom_id": "249_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Object-centric proto-symbolic behavioural reasoning from pixels"}], "temperature": 0.1}}
{"custom_id": "249_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Autonomous intelligent agents must bridge computational challenges atdisparate levels of abstraction, from the low-level spaces of sensory input andmotor commands to the high-level domain of abstract reasoning and planning. Akey question in designing such agents is how best to instantiate therepresentational space that will interface between these two levels -- ideallywithout requiring supervision in the form of expensive data annotations. Theseobjectives can be efficiently achieved by representing the world in terms ofobjects (grounded in perception and action). In this work, we present a novel,brain-inspired, deep-learning architecture that learns from pixels tointerpret, control, and reason about its environment, using object-centricrepresentations. We show the utility of our approach through tasks in syntheticenvironments that require a combination of (high-level) logical reasoning and(low-level) continuous control. Results show that the agent can learn emergentconditional behavioural reasoning, such as $(A \\to B) \\land (\\neg A \\to C)$, aswell as logical composition $(A \\to B) \\land (A \\to C) \\vdash A \\to (B \\landC)$ and XOR operations, and successfully controls its environment to satisfyobjectives deduced from these logical rules. The agent can adapt online tounexpected changes in its environment and is robust to mild violations of itsworld model, thanks to dynamic internal desired goal generation. While thepresent results are limited to synthetic settings (2D and 3D activated versionsof dSprites), which fall short of real-world levels of complexity, the proposedarchitecture shows how to manipulate grounded object representations, as a keyinductive bias for unsupervised learning, to enable behavioral reasoning."}], "temperature": 0.1}}
{"custom_id": "249_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ruben van Bergen"}], "temperature": 0.1}}
{"custom_id": "250_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Interactive Visual Assessment for Text-to-Image Generation Models"}], "temperature": 0.1}}
{"custom_id": "250_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Visual generation models have achieved remarkable progress in computergraphics applications but still face significant challenges in real-worlddeployment. Current assessment approaches for visual generation tasks typicallyfollow an isolated three-phase framework: test input collection, model outputgeneration, and user assessment. These fashions suffer from fixed coverage,evolving difficulty, and data leakage risks, limiting their effectiveness incomprehensively evaluating increasingly complex generation models. To addressthese limitations, we propose DyEval, an LLM-powered dynamic interactive visualassessment framework that facilitates collaborative evaluation between humansand generative models for text-to-image systems. DyEval features an intuitivevisual interface that enables users to interactively explore and analyze modelbehaviors, while adaptively generating hierarchical, fine-grained, and diversetextual inputs to continuously probe the capability boundaries of the modelsbased on their feedback. Additionally, to provide interpretable analysis forusers to further improve tested models, we develop a contextual reflectionmodule that mines failure triggers of test inputs and reflects model potentialfailure patterns supporting in-depth analysis using the logical reasoningability of LLM. Qualitative and quantitative experiments demonstrate thatDyEval can effectively help users identify max up to 2.56 times generationfailures than conventional methods, and uncover complex and rare failurepatterns, such as issues with pronoun generation and specific cultural contextgeneration. Our framework provides valuable insights for improving generativemodels and has broad implications for advancing the reliability andcapabilities of visual generation systems across various domains."}], "temperature": 0.1}}
{"custom_id": "250_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xiaoyue Mi"}], "temperature": 0.1}}
{"custom_id": "251_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "XAgents: A Framework for Interpretable Rule-Based Multi-Agents Cooperation"}], "temperature": 0.1}}
{"custom_id": "251_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Extracting implicit knowledge and logical reasoning abilities from largelanguage models (LLMs) has consistently been a significant challenge. Theadvancement of multi-agent systems has further en-hanced the capabilities ofLLMs. Inspired by the structure of multi-polar neurons (MNs), we propose theXAgents framework, an in-terpretable multi-agent cooperative framework based onthe IF-THEN rule-based system. The IF-Parts of the rules are responsible forlogical reasoning and domain membership calculation, while the THEN-Parts arecomprised of domain expert agents that generate domain-specific contents.Following the calculation of the member-ship, XAgetns transmits the task to thedisparate domain rules, which subsequently generate the various responses.These re-sponses are analogous to the answers provided by different experts tothe same question. The final response is reached at by eliminat-ing thehallucinations and erroneous knowledge of the LLM through membershipcomputation and semantic adversarial genera-tion of the various domain rules.The incorporation of rule-based interpretability serves to bolster userconfidence in the XAgents framework. We evaluate the efficacy of XAgentsthrough a com-parative analysis with the latest AutoAgents, in which XAgentsdemonstrated superior performance across three distinct datasets. We performpost-hoc interpretable studies with SHAP algorithm and case studies, provingthe interpretability of XAgent in terms of input-output feature correlation andrule-based semantics."}], "temperature": 0.1}}
{"custom_id": "251_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hailong Yang"}], "temperature": 0.1}}
{"custom_id": "252_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus"}], "temperature": 0.1}}
{"custom_id": "252_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) are capable of solving a wide range of tasks,yet they have struggled with reasoning. To address this, we propose$\\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs'reasoning capabilities by program-generated logical reasoning samples. We firstestablish principles for designing high-quality samples by integrating symboliclogic theory and previous empirical insights. Then, based on these principles,we construct a synthetic corpus named $\\textbf{Formal Logic Deduction Diverse}$($\\textbf{FLD}$$_{\\times 2}$), comprising numerous samples of multi-stepdeduction with unknown facts, diverse reasoning rules, diverse linguisticexpressions, and challenging distractors. Finally, we empirically show that ALTon FLD$_{\\times2}$ substantially enhances the reasoning capabilities ofstate-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains ofup to 30 points on logical reasoning benchmarks, up to 10 points on math andcoding benchmarks, and 5 points on the benchmark suite BBH."}], "temperature": 0.1}}
{"custom_id": "252_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Terufumi Morishita"}], "temperature": 0.1}}
{"custom_id": "253_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Testing Uncertainty of Large Language Models for Physics Knowledge and Reasoning"}], "temperature": 0.1}}
{"custom_id": "253_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have gained significant popularity in recentyears for their ability to answer questions in various fields. However, thesemodels have a tendency to \"hallucinate\" their responses, making it challengingto evaluate their performance. A major challenge is determining how to assessthe certainty of a model's predictions and how it correlates with accuracy. Inthis work, we introduce an analysis for evaluating the performance of popularopen-source LLMs, as well as gpt-3.5 Turbo, on multiple choice physicsquestionnaires. We focus on the relationship between answer accuracy andvariability in topics related to physics. Our findings suggest that most modelsprovide accurate replies in cases where they are certain, but this is by farnot a general behavior. The relationship between accuracy and uncertaintyexposes a broad horizontal bell-shaped distribution. We report how theasymmetry between accuracy and uncertainty intensifies as the questions demandmore logical reasoning of the LLM agent, while the same relationship remainssharp for knowledge retrieval tasks."}], "temperature": 0.1}}
{"custom_id": "253_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Elizaveta Reganova"}], "temperature": 0.1}}
{"custom_id": "254_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) as Traffic Control Systems at Urban Intersections: A New Paradigm"}], "temperature": 0.1}}
{"custom_id": "254_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This study introduces a novel approach for traffic control systems by usingLarge Language Models (LLMs) as traffic controllers. The study utilizes theirlogical reasoning, scene understanding, and decision-making capabilities tooptimize throughput and provide feedback based on traffic conditions inreal-time. LLMs centralize traditionally disconnected traffic control processesand can integrate traffic data from diverse sources to provide context-awaredecisions. LLMs can also deliver tailored outputs using various means such aswireless signals and visuals to drivers, infrastructures, and autonomousvehicles. To evaluate LLMs ability as traffic controllers, this study proposeda four-stage methodology. The methodology includes data creation andenvironment initialization, prompt engineering, conflict identification, andfine-tuning. We simulated multi-lane four-leg intersection scenarios andgenerates detailed datasets to enable conflict detection using LLMs and Pythonsimulation as a ground truth. We used chain-of-thought prompts to lead LLMs inunderstanding the context, detecting conflicts, resolving them using trafficrules, and delivering context-sensitive traffic management solutions. Weevaluated the prformance GPT-mini, Gemini, and Llama as traffic controllers.Results showed that the fine-tuned GPT-mini achieved 83% accuracy and anF1-score of 0.84. GPT-mini model exhibited a promising performance ingenerating actionable traffic management insights, with high ROUGE-L scoresacross conflict identification of 0.95, decision-making of 0.91, priorityassignment of 0.94, and waiting time optimization of 0.92. We demonstrated thatLLMs can offer precise recommendations to drivers in real-time includingyielding, slowing, or stopping based on vehicle dynamics."}], "temperature": 0.1}}
{"custom_id": "254_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Sari Masri"}], "temperature": 0.1}}
{"custom_id": "255_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "AutoIoT: Automated IoT Platform Using Large Language Models"}], "temperature": 0.1}}
{"custom_id": "255_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "IoT platforms, particularly smart home platforms providing significantconvenience to people's lives such as Apple HomeKit and Samsung SmartThings,allow users to create automation rules through trigger-action programming.However, some users may lack the necessary knowledge to formulate automationrules, thus preventing them from fully benefiting from the conveniences offeredby smart home technology. To address this, smart home platforms providepre-defined automation policies based on the smart home devices registered bythe user. Nevertheless, these policies, being pre-generated and relativelysimple, fail to adequately cover the diverse needs of users. Furthermore,conflicts may arise between automation rules, and integrating conflictdetection into the IoT platform increases the burden on developers. In thispaper, we propose AutoIoT, an automated IoT platform based on Large LanguageModels (LLMs) and formal verification techniques, designed to achieveend-to-end automation through device information extraction, LLM-based rulegeneration, conflict detection, and avoidance. AutoIoT can help users generateconflict-free automation rules and assist developers in generating codes forconflict detection, thereby enhancing their experience. A code adapter has beendesigned to separate logical reasoning from the syntactic details of codegeneration, enabling LLMs to generate code for programming languages beyondtheir training data. Finally, we evaluated the performance of AutoIoT andpresented a case study demonstrating how AutoIoT can integrate with existingIoT platforms."}], "temperature": 0.1}}
{"custom_id": "255_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ye Cheng"}], "temperature": 0.1}}
{"custom_id": "256_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Evaluating Creativity and Deception in Large Language Models: A Simulation Framework for Multi-Agent Balderdash"}], "temperature": 0.1}}
{"custom_id": "256_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have shown impressive capabilities in complextasks and interactive environments, yet their creativity remains underexplored.This paper introduces a simulation framework utilizing the game Balderdash toevaluate both the creativity and logical reasoning of LLMs. In Balderdash,players generate fictitious definitions for obscure terms to deceive otherswhile identifying correct definitions. Our framework enables multiple LLMagents to participate in this game, assessing their ability to produceplausible definitions and strategize based on game rules and history. Weimplemented a centralized game engine featuring various LLMs as participantsand a judge LLM to evaluate semantic equivalence. Through a series ofexperiments, we analyzed the performance of different LLMs, examining metricssuch as True Definition Ratio, Deception Ratio, and Correct Guess Ratio. Theresults provide insights into the creative and deceptive capabilities of LLMs,highlighting their strengths and areas for improvement. Specifically, the studyreveals that infrequent vocabulary in LLMs' input leads to poor reasoning ongame rules and historical context(https://github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash)."}], "temperature": 0.1}}
{"custom_id": "256_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Parsa Hejabi"}], "temperature": 0.1}}
{"custom_id": "257_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Building Trustworthy AI: Transparent AI Systems via Large Language Models, Ontologies, and Logical Reasoning (TranspNet)"}], "temperature": 0.1}}
{"custom_id": "257_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Growing concerns over the lack of transparency in AI, particularly inhigh-stakes fields like healthcare and finance, drive the need for explainableand trustworthy systems. While Large Language Models (LLMs) performexceptionally well in generating accurate outputs, their \"black box\" natureposes significant challenges to transparency and trust. To address this, thepaper proposes the TranspNet pipeline, which integrates symbolic AI with LLMs.By leveraging domain expert knowledge, retrieval-augmented generation (RAG),and formal reasoning frameworks like Answer Set Programming (ASP), TranspNetenhances LLM outputs with structured reasoning and verification.This approachstrives to help AI systems deliver results that are as accurate, explainable,and trustworthy as possible, aligning with regulatory expectations fortransparency and accountability. TranspNet provides a solution for developingAI systems that are reliable and interpretable, making it suitable forreal-world applications where trust is critical."}], "temperature": 0.1}}
{"custom_id": "257_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Fadi Al Machot"}], "temperature": 0.1}}
{"custom_id": "258_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Symbolic-AI-Fusion Deep Learning (SAIF-DL): Encoding Knowledge into Training with Answer Set Programming Loss Penalties by a Novel Loss Function Approach"}], "temperature": 0.1}}
{"custom_id": "258_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This paper presents a hybrid methodology that enhances the training processof deep learning (DL) models by embedding domain expert knowledge usingontologies and answer set programming (ASP). By integrating these symbolic AImethods, we encode domain-specific constraints, rules, and logical reasoningdirectly into the model's learning process, thereby improving both performanceand trustworthiness. The proposed approach is flexible and applicable to bothregression and classification tasks, demonstrating generalizability acrossvarious fields such as healthcare, autonomous systems, engineering, and batterymanufacturing applications. Unlike other state-of-the-art methods, the strengthof our approach lies in its scalability across different domains. The designallows for the automation of the loss function by simply updating the ASPrules, making the system highly scalable and user-friendly. This facilitatesseamless adaptation to new domains without significant redesign, offering apractical solution for integrating expert knowledge into DL models inindustrial settings such as battery manufacturing."}], "temperature": 0.1}}
{"custom_id": "258_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Fadi Al Machot"}], "temperature": 0.1}}
{"custom_id": "259_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Knowledge Authoring with Factual English, Rules, and Actions"}], "temperature": 0.1}}
{"custom_id": "259_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Knowledge representation and reasoning systems represent knowledge ascollections of facts and rules. KRRs can represent complex concepts andrelations, and they can query and manipulate information in sophisticated ways.Unfortunately, the KRR technology has been hindered by the fact that specifyingthe requisite knowledge requires skills that most domain experts do not have,and professional knowledge engineers are hard to find. Some recent CNL-basedapproaches, such as the Knowledge Authoring Logic Machine (KALM), have shown tohave very high accuracy compared to others, and a natural question is to whatextent the CNL restrictions can be lifted. Besides the CNL restrictions, KALMhas limitations in terms of the types of knowledge it can represent. To addressthese issues, we propose an extension of KALM called KALM for Factual Language(KALMF). KALMF uses a neural parser for natural language, MS, to parse what wecall factual English sentences, which require little grammar training to use.Building upon KALMF, we propose KALM for Rules and Actions (KALMR), torepresent and reason with rules and actions. Furthermore, we identify thereasons behind the slow speed of KALM and make optimizations to address thisissue. Our evaluation using multiple benchmarks shows that our approachesachieve a high level of correctness on fact and query authoring (95%) and onrule authoring (100%). When used for authoring and reasoning with actions, ourapproach achieves more than 99.3% correctness, demonstrating its effectivenessin enabling more sophisticated knowledge representation and reasoning. We alsoillustrate the logical reasoning capabilities of our approach by drawingattention to the problems faced by the famous AI, ChatGPT. Finally, theevaluation of the newly proposed speed optimization points not only to a 68%runtime improvement but also yields better accuracy of the overall system."}], "temperature": 0.1}}
{"custom_id": "259_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yuheng Wang"}], "temperature": 0.1}}
{"custom_id": "260_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "OpenAI-o1 AB Testing: Does the o1 model really do good reasoning in math problem solving?"}], "temperature": 0.1}}
{"custom_id": "260_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The Orion-1 model by OpenAI is claimed to have more robust logical reasoningcapabilities than previous large language models. However, some suggest theexcellence might be partially due to the model \"memorizing\" solutions,resulting in less satisfactory performance when prompted with problems not inthe training data. We conduct a comparison experiment using two datasets: oneconsisting of International Mathematics Olympiad (IMO) problems, which iseasily accessible; the other one consisting of Chinese National Team Trainingcamp (CNT) problems, which have similar difficulty but not as publicallyaccessible. We label the response for each problem and compare the performancebetween the two datasets. We conclude that there is no significant evidence toshow that the model relies on memorizing problems and solutions. Also, weperform case studies to analyze some features of the model's response."}], "temperature": 0.1}}
{"custom_id": "260_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Leo Li"}], "temperature": 0.1}}
{"custom_id": "261_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Formal Logic-guided Robust Federated Learning against Poisoning Attacks"}], "temperature": 0.1}}
{"custom_id": "261_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Federated Learning (FL) offers a promising solution to the privacy concernsassociated with centralized Machine Learning (ML) by enabling decentralized,collaborative learning. However, FL is vulnerable to various security threats,including poisoning attacks, where adversarial clients manipulate the trainingdata or model updates to degrade overall model performance. Recognizing thisthreat, researchers have focused on developing defense mechanisms to counteractpoisoning attacks in FL systems. However, existing robust FL methodspredominantly focus on computer vision tasks, leaving a gap in addressing theunique challenges of FL with time series data. In this paper, we presentFLORAL, a defense mechanism designed to mitigate poisoning attacks in federatedlearning for time-series tasks, even in scenarios with heterogeneous clientdata and a large number of adversarial participants. Unlike traditionalmodel-centric defenses, FLORAL leverages logical reasoning to evaluate clienttrustworthiness by aligning their predictions with global time-series patterns,rather than relying solely on the similarity of client updates. Our approachextracts logical reasoning properties from clients, then hierarchically infersglobal properties, and uses these to verify client updates. Through formallogic verification, we assess the robustness of each client contribution,identifying deviations indicative of adversarial behavior. Experimental resultson two datasets demonstrate the superior performance of our approach comparedto existing baseline methods, highlighting its potential to enhance therobustness of FL to time series applications. Notably, FLORAL reduced theprediction error by 93.27% in the best-case scenario compared to thesecond-best baseline. Our code is available athttps://anonymous.4open.science/r/FLORAL-Robust-FTS."}], "temperature": 0.1}}
{"custom_id": "261_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Dung Thuy Nguyen"}], "temperature": 0.1}}
{"custom_id": "262_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The LLM Language Network: A Neuroscientific Approach for Identifying Causally Task-Relevant Units"}], "temperature": 0.1}}
{"custom_id": "262_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) exhibit remarkable capabilities on not justlanguage tasks, but also various tasks that are not linguistic in nature, suchas logical reasoning and social inference. In the human brain, neuroscience hasidentified a core language system that selectively and causally supportslanguage processing. We here ask whether similar specialization for languageemerges in LLMs. We identify language-selective units within 18 popular LLMs,using the same localization approach that is used in neuroscience. We thenestablish the causal role of these units by demonstrating that ablating LLMlanguage-selective units -- but not random units -- leads to drastic deficitsin language tasks. Correspondingly, language-selective LLM units are morealigned to brain recordings from the human language system than random units.Finally, we investigate whether our localization method extends to othercognitive domains: while we find specialized networks in some LLMs forreasoning and social capabilities, there are substantial differences amongmodels. These findings provide functional and causal evidence forspecialization in large language models, and highlight parallels with thefunctional organization in the brain."}], "temperature": 0.1}}
{"custom_id": "262_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Badr AlKhamissi"}], "temperature": 0.1}}
{"custom_id": "263_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent"}], "temperature": 0.1}}
{"custom_id": "263_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In this paper, we introduce Hunyuan-Large, which is currently the largestopen-source Transformer-based mixture of experts model, with a total of 389billion parameters and 52 billion activation parameters, capable of handling upto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superiorperformance across various benchmarks including language understanding andgeneration, logical reasoning, mathematical problem-solving, coding,long-context, and aggregated tasks, where it outperforms LLama3.1-70B andexhibits comparable performance when compared to the significantly largerLLama3.1-405B model. Key practice of Hunyuan-Large include large-scalesynthetic data that is orders larger than in previous literature, a mixedexpert routing strategy, a key-value cache compression technique, and anexpert-specific learning rate strategy. Additionally, we also investigate thescaling laws and learning rate schedule of mixture of experts models, providingvaluable insights and guidances for future model development and optimization.The code and checkpoints of Hunyuan-Large are released to facilitate futureinnovations and applications.  Codes: https://github.com/Tencent/Hunyuan-Large  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large"}], "temperature": 0.1}}
{"custom_id": "263_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xingwu Sun"}], "temperature": 0.1}}
{"custom_id": "264_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "On Memorization of Large Language Models in Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "264_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) achieve good performance on challengingreasoning benchmarks, yet could also make basic reasoning mistakes. Thiscontrasting behavior is puzzling when it comes to understanding the mechanismsbehind LLMs' reasoning capabilities. One hypothesis is that the increasinglyhigh and nearly saturated performance on common reasoning benchmarks could bedue to the memorization of similar problems. In this paper, we systematicallyinvestigate this hypothesis with a quantitative measurement of memorization inreasoning tasks, using a dynamically generated logical reasoning benchmarkbased on Knights and Knaves (K&K) puzzles. We find that LLMs could interpolateand memorize the training puzzles (achieving near-perfect accuracy) afterfine-tuning, yet they struggle with slight variations of these puzzles. On theother hand, we show that while fine-tuning leads to heavy memorization, it alsoconsistently improves generalization performance. Through in-depth analyseswith perturbation tests, cross difficulty-level transferability, probing modelinternals, and fine-tuning with wrong answers, we establish that LLMs developreasoning skills on K&K puzzles alongside memorization. Finally, our analysisbased on a per-sample memorization score sheds light on how LLMs switch betweenreasoning and memorization when solving logical puzzles. Our code and data areavailable at https://memkklogic.github.io."}], "temperature": 0.1}}
{"custom_id": "264_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chulin Xie"}], "temperature": 0.1}}
{"custom_id": "265_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Leveraging LLMs for Hypothetical Deduction in Logical Inference: A Neuro-Symbolic Approach"}], "temperature": 0.1}}
{"custom_id": "265_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have exhibited remarkable potential across awide array of reasoning tasks, including logical reasoning. Although massiveefforts have been made to empower the logical reasoning ability of LLMs viaexternal logical symbolic solvers, crucial challenges of the poorgeneralization ability to questions with different features and inevitablequestion information loss of symbolic solver-driven approaches remainunresolved. To mitigate these issues, we introduce LINA, a LLM-drivenneuro-symbolic approach for faithful logical reasoning. By enabling an LLM toautonomously perform the transition from propositional logic extraction tosophisticated logical reasoning, LINA not only bolsters the resilience of thereasoning process but also eliminates the dependency on external solvers.Additionally, through its adoption of a hypothetical-deductive reasoningparadigm, LINA effectively circumvents the expansive search space challengethat plagues traditional forward reasoning methods. Empirical evaluationsdemonstrate that LINA substantially outperforms both established propositionallogic frameworks and conventional prompting techniques across a spectrum offive logical reasoning tasks. Specifically, LINA achieves an improvement of24.34% over LINC on the FOLIO dataset, while also surpassing promptingstrategies like CoT and CoT-SC by up to 24.02%. Our code is available athttps://github.com/wufeiwuwoshihua/nshy."}], "temperature": 0.1}}
{"custom_id": "265_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Qingchuan Li"}], "temperature": 0.1}}
{"custom_id": "266_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neuro-symbolic Learning Yielding Logical Constraints"}], "temperature": 0.1}}
{"custom_id": "266_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neuro-symbolic systems combine the abilities of neural perception and logicalreasoning. However, end-to-end learning of neuro-symbolic systems is still anunsolved challenge. This paper proposes a natural framework that fuses neuralnetwork training, symbol grounding, and logical constraint synthesis into acoherent and efficient end-to-end learning process. The capability of thisframework comes from the improved interactions between the neural and thesymbolic parts of the system in both the training and inference stages.Technically, to bridge the gap between the continuous neural network and thediscrete logical constraint, we introduce a difference-of-convex programmingtechnique to relax the logical constraints while maintaining their precision.We also employ cardinality constraints as the language for logical constraintlearning and incorporate a trust region method to avoid the degeneracy oflogical constraint in learning. Both theoretical analyses and empiricalevaluations substantiate the effectiveness of the proposed framework."}], "temperature": 0.1}}
{"custom_id": "266_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zenan Li"}], "temperature": 0.1}}
{"custom_id": "267_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Combining Domain-Specific Models and LLMs for Automated Disease Phenotyping from Survey Data"}], "temperature": 0.1}}
{"custom_id": "267_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This exploratory pilot study investigated the potential of combining adomain-specific model, BERN2, with large language models (LLMs) to enhanceautomated disease phenotyping from research survey data. Motivated by the needfor efficient and accurate methods to harmonize the growing volume of surveydata with standardized disease ontologies, we employed BERN2, a biomedicalnamed entity recognition and normalization model, to extract diseaseinformation from the ORIGINS birth cohort survey data. After rigorouslyevaluating BERN2's performance against a manually curated ground truth dataset,we integrated various LLMs using prompt engineering, Retrieval-AugmentedGeneration (RAG), and Instructional Fine-Tuning (IFT) to refine the model'soutputs. BERN2 demonstrated high performance in extracting and normalizingdisease mentions, and the integration of LLMs, particularly with Few ShotInference and RAG orchestration, further improved accuracy. This approach,especially when incorporating structured examples, logical reasoning prompts,and detailed context, offers a promising avenue for developing tools to enableefficient cohort profiling and data harmonization across large, heterogeneousresearch datasets."}], "temperature": 0.1}}
{"custom_id": "267_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Gal Beeri"}], "temperature": 0.1}}
{"custom_id": "268_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning or a Semblance of it? A Diagnostic Study of Transitive Reasoning in LLMs"}], "temperature": 0.1}}
{"custom_id": "268_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Evaluating Large Language Models (LLMs) on reasoning benchmarks demonstratestheir ability to solve compositional questions. However, little is known ofwhether these models engage in genuine logical reasoning or simply rely onimplicit cues to generate answers. In this paper, we investigate the transitivereasoning capabilities of two distinct LLM architectures, LLaMA 2 and Flan-T5,by manipulating facts within two compositional datasets: QASC and Bamboogle. Wecontrolled for potential cues that might influence the models' performance,including (a) word/phrase overlaps across sections of test input; (b) models'inherent knowledge during pre-training or fine-tuning; and (c) Named Entities.Our findings reveal that while both models leverage (a), Flan-T5 shows moreresilience to experiments (b and c), having less variance than LLaMA 2. Thissuggests that models may develop an understanding of transitivity throughfine-tuning on knowingly relevant datasets, a hypothesis we leave to futurework."}], "temperature": 0.1}}
{"custom_id": "268_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Houman Mehrafarin"}], "temperature": 0.1}}
{"custom_id": "269_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Improving Small-Scale Large Language Models Function Calling for Reasoning Tasks"}], "temperature": 0.1}}
{"custom_id": "269_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advancements in Large Language Models (LLMs) have demonstratedexceptional capabilities in natural language understanding and generation.While these models excel in general complex reasoning tasks, they still facechallenges in mathematical problem-solving and logical reasoning. To addressthese limitations, researchers have explored function calling abilities,allowing LLMs to execute provided functions and utilize their outputs for taskcompletion. However, concentrating on specific tasks can be very inefficientfor large-scale LLMs to be used, because of the expensive cost of training andinference stages they need in terms of computational resources. This studyintroduces a novel framework for training smaller language models in functioncalling, focusing on specific logical and mathematical reasoning tasks. Theapproach aims to improve performances of small-scale models for these tasksusing function calling, ensuring a high level of accuracy. Our frameworkemploys an agent that, given a problem and a set of callable functions, queriesthe LLM by injecting a description and examples of the usable functions intothe prompt and managing their calls in a step-by-step reasoning chain. Thisprocess is used to create a dataset of correct and incorrect reasoning chainchat completions from a large-scale LLM. This dataset is used to train asmaller LLM using Reinforcement Learning from Human Feedback (RLHF),specifically employing the Direct Preference Optimization (DPO) technique.Experimental results demonstrate how the proposed approach balances thetrade-off between model size and performance, improving the ability of functioncalling for reasoning tasks, in smaller models."}], "temperature": 0.1}}
{"custom_id": "269_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Graziano A. Manduzio"}], "temperature": 0.1}}
{"custom_id": "270_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Aligning CodeLLMs with Direct Preference Optimization"}], "temperature": 0.1}}
{"custom_id": "270_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The last year has witnessed the rapid progress of large language models(LLMs) across diverse domains. Among them, CodeLLMs have garnered particularattention because they can not only assist in completing various programmingtasks but also represent the decision-making and logical reasoning capabilitiesof LLMs. However, current CodeLLMs mainly focus on pre-training and supervisedfine-tuning scenarios, leaving the alignment stage, which is important forpost-training LLMs, under-explored. This work first identifies that thecommonly used PPO algorithm may be suboptimal for the alignment of CodeLLMbecause the involved reward rules are routinely coarse-grained and potentiallyflawed. We then advocate addressing this using the DPO algorithm. Based on onlypreference data pairs, DPO can render the model rank data automatically, givingrise to a fine-grained rewarding pattern more robust than human intervention.We also contribute a pipeline for collecting preference pairs for DPO onCodeLLMs. Studies show that our method significantly improves the performanceof existing CodeLLMs on benchmarks such as MBPP and HumanEval."}], "temperature": 0.1}}
{"custom_id": "270_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yibo Miao"}], "temperature": 0.1}}
{"custom_id": "271_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LLM-Aided Efficient Hardware Design Automation"}], "temperature": 0.1}}
{"custom_id": "271_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "With the rapidly increasing complexity of modern chips, hardware engineersare required to invest more effort in tasks such as circuit design,verification, and physical implementation. These workflows often involvecontinuous modifications, which are labor-intensive and prone to errors.Therefore, there is an increasing need for more efficient and cost-effectiveElectronic Design Automation (EDA) solutions to accelerate new hardwaredevelopment. Recently, large language models (LLMs) have made significantadvancements in contextual understanding, logical reasoning, and responsegeneration. Since hardware designs and intermediate scripts can be expressed intext format, it is reasonable to explore whether integrating LLMs into EDAcould simplify and fully automate the entire workflow. Accordingly, this paperdiscusses such possibilities in several aspects, covering hardware descriptionlanguage (HDL) generation, code debugging, design verification, and physicalimplementation. Two case studies, along with their future outlook, areintroduced to highlight the capabilities of LLMs in code repair and testbenchgeneration. Finally, future directions and challenges are highlighted tofurther explore the potential of LLMs in shaping the next-generation EDA"}], "temperature": 0.1}}
{"custom_id": "271_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Kangwei Xu"}], "temperature": 0.1}}
{"custom_id": "272_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "MedLogic-AQA: Enhancing Medical Question Answering with Abstractive Models Focusing on Logical Structures"}], "temperature": 0.1}}
{"custom_id": "272_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In Medical question-answering (QA) tasks, the need for effective systems ispivotal in delivering accurate responses to intricate medical queries. However,existing approaches often struggle to grasp the intricate logical structuresand relationships inherent in medical contexts, thus limiting their capacity tofurnish precise and nuanced answers. In this work, we address this gap byproposing a novel Abstractive QA system MedLogic-AQA that harnesses First OrderLogic (FOL) based rules extracted from both context and questions to generatewell-grounded answers. Through initial experimentation, we identified sixpertinent first-order logical rules, which were then used to train aLogic-Understanding (LU) model capable of generating logical triples for agiven context, question, and answer. These logic triples are then integratedinto the training of MedLogic-AQA, enabling effective and coherent reasoningduring answer generation. This distinctive fusion of logical reasoning withabstractive QA equips our system to produce answers that are logically sound,relevant, and engaging. Evaluation with respect to both automated andhuman-based demonstrates the robustness of MedLogic-AQA against strongbaselines. Through empirical assessments and case studies, we validate theefficacy of MedLogic-AQA in elevating the quality and comprehensiveness ofanswers in terms of reasoning as well as informativeness"}], "temperature": 0.1}}
{"custom_id": "272_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Aizan Zafar"}], "temperature": 0.1}}
{"custom_id": "273_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Uncovering Autoregressive LLM Knowledge of Thematic Fit in Event Representation"}], "temperature": 0.1}}
{"custom_id": "273_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The thematic fit estimation task measures the compatibility between apredicate (typically a verb), an argument (typically a noun phrase), and aspecific semantic role assigned to the argument. Previous state-of-the-art workhas focused on modeling thematic fit through distributional or neural models ofevent representation, trained in a supervised fashion with indirect labels. Inthis work, we assess whether pre-trained autoregressive LLMs possessconsistent, expressible knowledge about thematic fit. We evaluate both closedand open state-of-the-art LLMs on several psycholinguistic datasets, alongthree axes: (1) Reasoning Form: multi-step logical reasoning (chain-of-thoughtprompting) vs. simple prompting. (2) Input Form: providing context (generatedsentences) vs. raw tuples <predicate, argument, role>. (3) Output Form:categorical vs. numeric. Our results show that chain-of-thought reasoning ismore effective on datasets with self-explanatory semantic role labels,especially Location. Generated sentences helped only in few settings, andlowered results in many others. Predefined categorical (compared to numeric)output raised GPT's results across the board with few exceptions, but loweredLlama's. We saw that semantically incoherent generated sentences, which themodels lack the ability to consistently filter out, hurt reasoning and overallperformance too. Our GPT-powered methods set new state-of-the-art on all testeddatasets."}], "temperature": 0.1}}
{"custom_id": "273_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Safeyah Khaled Alshemali"}], "temperature": 0.1}}
{"custom_id": "274_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition"}], "temperature": 0.1}}
{"custom_id": "274_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We examine the language capabilities of language models (LMs) from thecritical perspective of human language acquisition. Building on classicallanguage development theories, we propose a three-stage framework to assess theabilities of LMs, ranging from preliminary word understanding to complexgrammar and complex logical reasoning. Using this framework, we evaluate thegenerative capacities of LMs using methods from linguistic research. Resultsindicate that although recent LMs outperform earlier models in overallperformance, their developmental trajectory does not strictly follow the pathof human language acquisition. Notably, in generation tasks, LMs are moresimilar to human performance in areas where information is easier to extractfrom the corpus, such as average word length, clauses, and auxiliary verbs.Newer LMs did not exhibit significant progress in terms of specific dimensions,such as clauses and auxiliary verbs, where the variation across corpora isrelatively limited. Register theory offers a plausible explanation for theseobservations, suggesting that the linguistic features of the training data havea substantial impact on the models' abilities."}], "temperature": 0.1}}
{"custom_id": "274_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Qiyuan Yang"}], "temperature": 0.1}}
{"custom_id": "275_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "\"Let's Argue Both Sides\": Argument Generation Can Force Small Models to Utilize Previously Inaccessible Reasoning Capabilities"}], "temperature": 0.1}}
{"custom_id": "275_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs), despite achieving state-of-the-art results in anumber of evaluation tasks, struggle to maintain their performance when logicalreasoning is strictly required to correctly infer a prediction. In this work,we propose Argument Generation as a method of forcing models to utilize theirreasoning capabilities when other approaches such as chain-of-thought reasoningprove insufficient. Our method involves the generation of arguments for eachpossible inference result, and asking the end model to rank the generatedarguments. We show that Argument Generation can serve as an appropriatesubstitute for zero-shot prompting techniques without the requirement to addlayers of complexity. Furthermore, we argue that knowledge-probing techniquessuch as chain-of-thought reasoning and Argument Generation are only useful whenfurther reasoning is required to infer a prediction, making them auxiliary tomore common zero-shot approaches. Finally, we demonstrate that our approachforces larger gains in smaller language models, showcasing a complexrelationship between model size and prompting methods in foundation models."}], "temperature": 0.1}}
{"custom_id": "275_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Kaveh Eskandari Miandoab"}], "temperature": 0.1}}
{"custom_id": "276_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Benchmarking Defeasible Reasoning with Large Language Models -- Initial Experiments and Future Directions"}], "temperature": 0.1}}
{"custom_id": "276_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have gained prominence in the AI landscape dueto their exceptional performance. Thus, it is essential to gain a betterunderstanding of their capabilities and limitations, among others in terms ofnonmonotonic reasoning. This paper proposes a benchmark that corresponds tovarious defeasible rule-based reasoning patterns. We modified an existingbenchmark for defeasible logic reasoners by translating defeasible rules intotext suitable for LLMs. We conducted preliminary experiments on nonmonotonicrule-based reasoning using ChatGPT and compared it with reasoning patternsdefined by defeasible logic."}], "temperature": 0.1}}
{"custom_id": "276_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ilias Tachmazidis"}], "temperature": 0.1}}
{"custom_id": "277_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Exploiting LLMs' Reasoning Capability to Infer Implicit Concepts in Legal Information Retrieval"}], "temperature": 0.1}}
{"custom_id": "277_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Statutory law retrieval is a typical problem in legal language processing,that has various practical applications in law engineering. Modern deeplearning-based retrieval methods have achieved significant results for thisproblem. However, retrieval systems relying on semantic and lexicalcorrelations often exhibit limitations, particularly when handling queries thatinvolve real-life scenarios, or use the vocabulary that is not specific to thelegal domain. In this work, we focus on overcoming this weaknesses by utilizingthe logical reasoning capabilities of large language models (LLMs) to identifyrelevant legal terms and facts related to the situation mentioned in the query.The proposed retrieval system integrates additional information from theterm--based expansion and query reformulation to improve the retrievalaccuracy. The experiments on COLIEE 2022 and COLIEE 2023 datasets show thatextra knowledge from LLMs helps to improve the retrieval result of both lexicaland semantic ranking models. The final ensemble retrieval system outperformedthe highest results among all participating teams in the COLIEE 2022 and 2023competitions."}], "temperature": 0.1}}
{"custom_id": "277_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hai-Long Nguyen"}], "temperature": 0.1}}
{"custom_id": "278_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Transformer-based Language Models for Reasoning in the Description Logic ALCQ"}], "temperature": 0.1}}
{"custom_id": "278_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advancements in transformer-based language models have sparkedresearch into their logical reasoning capabilities. Most of the benchmarks usedto evaluate these models are simple: generated from short (fragments of)first-order logic sentences with only a few logical operators and quantifiers.We construct the natural language dataset, DELTA$_D$, using the expressivedescription logic language $\\mathcal{ALCQ}$. DELTA$_D$ comprises 384K examplesand increases in two dimensions: i) reasoning depth, and ii) linguisticcomplexity. In this way, we systematically investigate the logical reasoningcapabilities of a supervised fine-tuned DeBERTa-based model and two largelanguage models (GPT-3.5, GPT-4) with few-shot prompting. We show that theDeBERTa-based model fine-tuned on our dataset can master the entailmentchecking task. Moreover, the performance of GPTs can improve significantly evenwhen a small number of samples is provided (9 shots). We open-source our codeand datasets."}], "temperature": 0.1}}
{"custom_id": "278_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Angelos Poulis"}], "temperature": 0.1}}
{"custom_id": "279_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Boosting Deductive Reasoning with Step Signals In RLHF"}], "temperature": 0.1}}
{"custom_id": "279_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning is a crucial task for Large Language Models (LLMs),enabling them to tackle complex problems. Among reasoning tasks, multi-stepreasoning poses a particular challenge. Grounded in the theory of formal logic,we have developed an automated method, Multi-step Deduction (MuseD), fordeductive reasoning data. MuseD has allowed us to create training and testingdatasets for multi-step reasoning. Our generation method enables control overthe complexity of the generated instructions, facilitating training andevaluation of models across different difficulty levels. Through RLHF training,our training data has demonstrated significant improvements in logicalcapabilities for both in-domain of out-of-domain reasoning tasks. Additionally,we have conducted tests to assess the multi-step reasoning abilities of variousmodels."}], "temperature": 0.1}}
{"custom_id": "279_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jialian Li"}], "temperature": 0.1}}
{"custom_id": "280_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Systematic Assessment of OpenAI o1-Preview for Higher Order Thinking in Education"}], "temperature": 0.1}}
{"custom_id": "280_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "As artificial intelligence (AI) continues to advance, it demonstratescapabilities comparable to human intelligence, with significant potential totransform education and workforce development. This study evaluates OpenAIo1-preview's ability to perform higher-order cognitive tasks across 14dimensions, including critical thinking, systems thinking, computationalthinking, design thinking, metacognition, data literacy, creative thinking,abstract reasoning, quantitative reasoning, logical reasoning, analogicalreasoning, and scientific reasoning. We used validated instruments like theEnnis-Weir Critical Thinking Essay Test and the Biological Systems ThinkingTest to compare the o1-preview's performance with human performancesystematically. Our findings reveal that o1-preview outperforms humans in mostcategories, achieving 150% better results in systems thinking, computationalthinking, data literacy, creative thinking, scientific reasoning, and abstractreasoning. However, compared to humans, it underperforms by around 25% inlogical reasoning, critical thinking, and quantitative reasoning. In analogicalreasoning, both o1-preview and humans achieved perfect scores. Despite thesestrengths, the o1-preview shows limitations in abstract reasoning, where humanpsychology students outperform it, highlighting the continued importance ofhuman oversight in tasks requiring high-level abstraction. These results havesignificant educational implications, suggesting a shift toward developinghuman skills that complement AI, such as creativity, abstract reasoning, andcritical thinking. This study emphasizes the transformative potential of AI ineducation and calls for a recalibration of educational goals, teaching methods,and curricula to align with an AI-driven world."}], "temperature": 0.1}}
{"custom_id": "280_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ehsan Latif"}], "temperature": 0.1}}
{"custom_id": "281_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains"}], "temperature": 0.1}}
{"custom_id": "281_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Existing methods on understanding the capabilities of LLMs in logicalreasoning rely on binary entailment classification or synthetically derivedrationales, which are not sufficient for proper investigation of model'scapabilities. We present P-FOLIO, a human-annotated dataset consisting ofdiverse and complex reasoning chains for a set of realistic logical reasoningstories also written by humans. P-FOLIO is collected with an annotationprotocol that facilitates humans to annotate well-structured natural languageproofs for first-order logic reasoning problems in a step-by-step manner. Thenumber of reasoning steps in P-FOLIO span from 0 to 20. We further use P-FOLIOto evaluate and improve large-language-model (LLM) reasoning capabilities. Weevaluate LLM reasoning capabilities at a fine granularity via single-stepinference rule classification, with more diverse inference rules of morediverse and higher levels of complexities than previous works. Given that asingle model-generated reasoning chain could take a completely different paththan the human-annotated one, we sample multiple reasoning chains from a modeland use pass@k metrics for evaluating the quality of model-generated reasoningchains. We show that human-written reasoning chains significantly boost thelogical reasoning capabilities of LLMs via many-shot prompting and fine-tuning.Furthermore, fine-tuning Llama3-7B on P-FOLIO improves the model performance by10% or more on three other out-of-domain logical reasoning datasets. We alsoconduct detailed analysis to show where most powerful LLMs fall short inreasoning. We will release the dataset and code publicly."}], "temperature": 0.1}}
{"custom_id": "281_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Simeng Han"}], "temperature": 0.1}}
{"custom_id": "282_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "KnowGraph: Knowledge-Enabled Anomaly Detection via Logical Reasoning on Graph Data"}], "temperature": 0.1}}
{"custom_id": "282_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Graph-based anomaly detection is pivotal in diverse security applications,such as fraud detection in transaction networks and intrusion detection fornetwork traffic. Standard approaches, including Graph Neural Networks (GNNs),often struggle to generalize across shifting data distributions. Meanwhile,real-world domain knowledge is more stable and a common existing component ofreal-world detection strategies. To explicitly integrate such knowledge intodata-driven models such as GCNs, we propose KnowGraph, which integrates domainknowledge with data-driven learning for enhanced graph-based anomaly detection.KnowGraph comprises two principal components: (1) a statistical learningcomponent that utilizes a main model for the overarching detection task,augmented by multiple specialized knowledge models that predict domain-specificsemantic entities; (2) a reasoning component that employs probabilisticgraphical models to execute logical inferences based on model outputs, encodingdomain knowledge through weighted first-order logic formulas. Extensiveexperiments on these large-scale real-world datasets show that KnowGraphconsistently outperforms state-of-the-art baselines in both transductive andinductive settings, achieving substantial gains in average precision whengeneralizing to completely unseen test graphs. Further ablation studiesdemonstrate the effectiveness of the proposed reasoning component in improvingdetection performance, especially under extreme class imbalance. These resultshighlight the potential of integrating domain knowledge into data-driven modelsfor high-stakes, graph-based security applications."}], "temperature": 0.1}}
{"custom_id": "282_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Andy Zhou"}], "temperature": 0.1}}
{"custom_id": "283_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Think Beyond Size: Adaptive Prompting for More Effective Reasoning"}], "temperature": 0.1}}
{"custom_id": "283_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Pretrained large language models (LLMs) are increasingly utilized across awide range of natural language processing (NLP) tasks due to their impressivecapabilities as few-shot learners. Recent techniques, such as chain-of-thought(CoT) prompting, have significantly advanced multi-step reasoning byintroducing step-by-step decomposition, achieving state-of-the-art results oncomplex reasoning benchmarks. However, these approaches often rely on staticprompting templates that do not adapt to task complexity or errors during thereasoning process. In this work, we introduce Adaptive Prompting, a dynamic anditerative framework designed to enhance reasoning by incorporating real-timeadjustments to prompt structures and validation mechanisms.Experimental resultsdemonstrate that Adaptive Prompting significantly improves performance ondiverse reasoning benchmarks, including arithmetic reasoning (GSM8K,MultiArith), logical reasoning and commonsense tasks, achieving substantialaccuracy gains compared to static prompting baselines. By integrating guidedprompts, intermediate validation, and self-corrective steps, our approachenables smaller models to achieve competitive performance with largercounterparts, such as GPT-4, while maintaining computational efficiency. Theframework achieves this without requiring fine-tuning or task-specific trainingdata, highlighting the untapped potential of iterative reasoning methods."}], "temperature": 0.1}}
{"custom_id": "283_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Kamesh R"}], "temperature": 0.1}}
{"custom_id": "284_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "284_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Complex logical reasoning tasks require a long sequence of reasoning, which alarge language model (LLM) with chain-of-thought prompting still falls short.To alleviate this issue, neurosymbolic approaches incorporate a symbolicsolver. Specifically, an LLM only translates a natural language problem into asatisfiability (SAT) problem that consists of first-order logic formulas, and asound symbolic solver returns a mathematically correct solution. However, wediscover that LLMs have difficulties to capture complex logical semanticshidden in the natural language during translation. To resolve this limitation,we propose a Compositional First-Order Logic Translation. An LLM first parses anatural language sentence into newly defined logical dependency structures thatconsist of an atomic subsentence and its dependents, then sequentiallytranslate the parsed subsentences. Since multiple logical dependency structuresand sequential translations are possible for a single sentence, we alsointroduce two Verification algorithms to ensure more reliable results. Weutilize an SAT solver to rigorously compare semantics of generated first-orderlogic formulas and select the most probable one. We evaluate the proposedmethod, dubbed CLOVER, on seven logical reasoning benchmarks and show that itoutperforms the previous neurosymbolic approaches and achieves newstate-of-the-art results."}], "temperature": 0.1}}
{"custom_id": "284_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hyun Ryu"}], "temperature": 0.1}}
{"custom_id": "285_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "HLM-Cite: Hybrid Language Model Workflow for Text-based Scientific Citation Prediction"}], "temperature": 0.1}}
{"custom_id": "285_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Citation networks are critical in modern science, and predicting whichprevious papers (candidates) will a new paper (query) cite is a criticalproblem. However, the roles of a paper's citations vary significantly, rangingfrom foundational knowledge basis to superficial contexts. Distinguishing theseroles requires a deeper understanding of the logical relationships amongpapers, beyond simple edges in citation networks. The emergence of LLMs withtextual reasoning capabilities offers new possibilities for discerning theserelationships, but there are two major challenges. First, in practice, a newpaper may select its citations from gigantic existing papers, where the textsexceed the context length of LLMs. Second, logical relationships between papersare implicit, and directly prompting an LLM to predict citations may result insurface-level textual similarities rather than the deeper logical reasoning. Inthis paper, we introduce the novel concept of core citation, which identifiesthe critical references that go beyond superficial mentions. Thereby, weelevate the citation prediction task from a simple binary classification todistinguishing core citations from both superficial citations andnon-citations. To address this, we propose $\\textbf{HLM-Cite}$, a$\\textbf{H}$ybrid $\\textbf{L}$anguage $\\textbf{M}$odel workflow for citationprediction, which combines embedding and generative LMs. We design a curriculumfinetune procedure to adapt a pretrained text embedding model to coarselyretrieve high-likelihood core citations from vast candidates and then design anLLM agentic workflow to rank the retrieved papers through one-shot reasoning,revealing the implicit relationships among papers. With the pipeline, we canscale the candidate sets to 100K papers. We evaluate HLM-Cite across 19scientific fields, demonstrating a 17.6% performance improvement comparing SOTAmethods."}], "temperature": 0.1}}
{"custom_id": "285_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Qianyue Hao"}], "temperature": 0.1}}
{"custom_id": "286_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Can Transformers Reason Logically? A Study in SAT Solving"}], "temperature": 0.1}}
{"custom_id": "286_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We formally study the logical reasoning capabilities of decoder-onlyTransformers in the context of the boolean satisfiability (SAT) problem. First,we prove by construction that decoder-only Transformers can decide 3-SAT, in anon-uniform model of computation, using backtracking and deduction viaChain-of-Thought (CoT). %We prove its correctness by showing trace equivalenceto the well-known DPLL SAT-solving algorithm. Second, we implement ourconstruction as a PyTorch model with a tool (PARAT) that we designed toempirically demonstrate its correctness and investigate its properties. Third,rather than \\textit{programming} a transformer to reason, we evaluateempirically whether it can be \\textit{trained} to do so by learning directlyfrom algorithmic traces (``reasoning paths'') from our theoreticalconstruction. The trained models demonstrate strong out-of-distributiongeneralization on problem sizes seen during training but has limited lengthgeneralization, which is consistent with the implications of our theoreticalresult"}], "temperature": 0.1}}
{"custom_id": "286_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Leyan Pan"}], "temperature": 0.1}}
{"custom_id": "287_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles"}], "temperature": 0.1}}
{"custom_id": "287_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "As the application of Large Language Models (LLMs) expands, the demand forreliable evaluations increases. Existing LLM evaluation benchmarks primarilyrely on static datasets, making it challenging to assess model performance indynamic interactions with users. Moreover, these benchmarks often depend onspecific background knowledge, complicating the measurement of a model'slogical reasoning capabilities. Other dynamic evaluation methods based onstrong models or manual efforts may introduce biases and incur high costs andtime demands, hindering large-scale application. To address these issues, wepropose TurtleBench. TurtleBench collects real user guesses from our onlineTurtle Soup Puzzle platform that we developed. This approach allows for therelatively dynamic generation of evaluation datasets, mitigating the risk ofmodel cheating while aligning assessments more closely with genuine user needsfor reasoning capabilities, thus enhancing the reliability of evaluations.TurtleBench includes 1,532 user guesses along with the correctness of guessesafter annotation. Using this dataset, we thoroughly evaluated nine of the mostadvanced LLMs available today. Notably, the OpenAI o1 series models did notachieve leading results in these evaluations. We propose several hypotheses forfurther research, such as \"the latent reasoning of o1 utilizes trivialChain-of-Thought (CoT) techniques\" and \"increasing CoT length not only providesreasoning benefits but also incurs noise costs.\""}], "temperature": 0.1}}
{"custom_id": "287_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Qingchen Yu"}], "temperature": 0.1}}
{"custom_id": "288_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models"}], "temperature": 0.1}}
{"custom_id": "288_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advancements in Large Language Models (LLMs) have sparked interest intheir formal reasoning capabilities, particularly in mathematics. The GSM8Kbenchmark is widely used to assess the mathematical reasoning of models ongrade-school-level questions. While the performance of LLMs on GSM8K hassignificantly improved in recent years, it remains unclear whether theirmathematical reasoning capabilities have genuinely advanced, raising questionsabout the reliability of the reported metrics. To address these concerns, weconduct a large-scale study on several SOTA open and closed models. To overcomethe limitations of existing evaluations, we introduce GSM-Symbolic, an improvedbenchmark created from symbolic templates that allow for the generation of adiverse set of questions. GSM-Symbolic enables more controllable evaluations,providing key insights and more reliable metrics for measuring the reasoningcapabilities of models.Our findings reveal that LLMs exhibit noticeablevariance when responding to different instantiations of the same question.Specifically, the performance of all models declines when only the numericalvalues in the question are altered in the GSM-Symbolic benchmark. Furthermore,we investigate the fragility of mathematical reasoning in these models and showthat their performance significantly deteriorates as the number of clauses in aquestion increases. We hypothesize that this decline is because current LLMscannot perform genuine logical reasoning; they replicate reasoning steps fromtheir training data. Adding a single clause that seems relevant to the questioncauses significant performance drops (up to 65%) across all state-of-the-artmodels, even though the clause doesn't contribute to the reasoning chain neededfor the final answer. Overall, our work offers a more nuanced understanding ofLLMs' capabilities and limitations in mathematical reasoning."}], "temperature": 0.1}}
{"custom_id": "288_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Iman Mirzadeh"}], "temperature": 0.1}}
{"custom_id": "289_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification"}], "temperature": 0.1}}
{"custom_id": "289_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Vision models excel in image classification but struggle to generalize tounseen data, such as classifying images from unseen domains or discoveringnovel categories. In this paper, we explore the relationship between logicalreasoning and deep learning generalization in visual classification. A logicalregularization termed L-Reg is derived which bridges a logical analysisframework to image classification. Our work reveals that L-Reg reduces thecomplexity of the model in terms of the feature distribution and classifierweights. Specifically, we unveil the interpretability brought by L-Reg, as itenables the model to extract the salient features, such as faces to persons,for classification. Theoretical analysis and experiments demonstrate that L-Regenhances generalization across various scenarios, including multi-domaingeneralization and generalized category discovery. In complex real-worldscenarios where images span unknown classes and unseen domains, L-Regconsistently improves generalization, highlighting its practical efficacy."}], "temperature": 0.1}}
{"custom_id": "289_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhaorui Tan"}], "temperature": 0.1}}
{"custom_id": "290_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Latent Feature Mining for Predictive Model Enhancement with Large Language Models"}], "temperature": 0.1}}
{"custom_id": "290_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Predictive modeling often faces challenges due to limited data availabilityand quality, especially in domains where collected features are weaklycorrelated with outcomes and where additional feature collection is constrainedby ethical or practical difficulties. Traditional machine learning (ML) modelsstruggle to incorporate unobserved yet critical factors. In this work, weintroduce an effective approach to formulate latent feature mining astext-to-text propositional logical reasoning. We propose FLAME (Faithful LatentFeature Mining for Predictive Model Enhancement), a framework that leverageslarge language models (LLMs) to augment observed features with latent featuresand enhance the predictive power of ML models in downstream tasks. Ourframework is generalizable across various domains with necessarydomain-specific adaptation, as it is designed to incorporate contextualinformation unique to each area, ensuring effective transfer to different areasfacing similar data availability challenges. We validate our framework with twocase studies: (1) the criminal justice system, a domain characterized bylimited and ethically challenging data collection; (2) the healthcare domain,where patient privacy concerns and the complexity of medical data limitcomprehensive feature collection. Our results show that inferred latentfeatures align well with ground truth labels and significantly enhance thedownstream classifier."}], "temperature": 0.1}}
{"custom_id": "290_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Bingxuan Li"}], "temperature": 0.1}}
{"custom_id": "291_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Deliberate Reasoning in Language Models as Structure-Aware Planning with an Accurate World Model"}], "temperature": 0.1}}
{"custom_id": "291_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Enhancing the reasoning capabilities of language models (LMs) remains a keychallenge, especially for tasks that require complex, multi-stepdecision-making where existing Chain-of-Thought (CoT) approaches struggle withconsistency and verification. In this paper, we propose a novel reasoningframework, referred to as Structure-aware Planning with an Accurate World Model(SWAP), that integrates structured knowledge representation with learnedplanning. Unlike prior methods that rely purely on natural language reasoning,SWAP leverages entailment graphs to encode structured dependencies and enablesymbolic verification of intermediate steps. To systematically construct andupdate the graph, SWAP employs a policy model to propose candidate expansionsand a world model to predict structural updates. To improve accuracy, the worldmodel generates multiple alternative updates, and a discriminator re-ranks thembased on plausibility. To encourage diverse exploration, we introduceDiversity-based Modelling (DM), which samples candidates from the remainingprobability mass after removing previously sampled candidates from the originalpolicy distribution. Additionally, SWAP improves the discrimination accuracythrough Contrastive Ranking (CR), which directly compares candidates withinprompts and incorporates meta-knowledge to improve ranking quality. We evaluateSWAP across diverse reasoning-intensive benchmarks including math reasoning,logical reasoning, and coding tasks. Extensive experiments demonstrate thatSWAP significantly improves upon the base models and consistently outperformsexisting reasoning methods."}], "temperature": 0.1}}
{"custom_id": "291_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Siheng Xiong"}], "temperature": 0.1}}
{"custom_id": "292_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "GraphIC: A Graph-Based In-Context Example Retrieval Model for Multi-Step Reasoning"}], "temperature": 0.1}}
{"custom_id": "292_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In-context learning (ICL) enhances large language models (LLMs) byincorporating demonstration examples, yet its effectiveness heavily depends onthe quality of selected examples. Current methods typically use text embeddingsto measure semantic similarity, which often introduces bias in multi-stepreasoning tasks. This occurs because text embeddings contain irrelevantsemantic information and lack deeper reasoning structures. To address this, wepropose GraphIC, a graph-based retrieval model that leverages reasoning-awarerepresentation and specialized similarity metric for in-context exampleretrieval. GraphIC first constructs thought graphs-directed, node-attributedgraphs that explicitly model reasoning steps and their dependencies-forcandidate examples and queries. This approach filters out superficial semanticswhile preserving essential reasoning processes. Next, GraphIC retrievesexamples using a novel similarity metric tailored for these graphs, capturingsequential reasoning patterns and asymmetry between examples. Comprehensiveevaluations across mathematical reasoning, code generation, and logicalreasoning tasks demonstrate that GraphIC outperforms 10 baseline methods. Ourresults highlight the importance of reasoning-aware retrieval in ICL, offeringa robust solution for enhancing LLM performance in multi-step reasoningscenarios."}], "temperature": 0.1}}
{"custom_id": "292_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jiale Fu"}], "temperature": 0.1}}
{"custom_id": "293_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "BabelBench: An Omni Benchmark for Code-Driven Analysis of Multimodal and Multistructured Data"}], "temperature": 0.1}}
{"custom_id": "293_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have become increasingly pivotal across variousdomains, especially in handling complex data types. This includes structureddata processing, as exemplified by ChartQA and ChatGPT-Ada, and multimodalunstructured data processing as seen in Visual Question Answering (VQA). Theseareas have attracted significant attention from both industry and academia.Despite this, there remains a lack of unified evaluation methodologies forthese diverse data handling scenarios. In response, we introduce BabelBench, aninnovative benchmark framework that evaluates the proficiency of LLMs inmanaging multimodal multistructured data with code execution. BabelBenchincorporates a dataset comprising 247 meticulously curated problems thatchallenge the models with tasks in perception, commonsense reasoning, logicalreasoning, and so on. Besides the basic capabilities of multimodalunderstanding, structured data processing as well as code generation, thesetasks demand advanced capabilities in exploration, planning, reasoning anddebugging. Our experimental findings on BabelBench indicate that evencutting-edge models like ChatGPT 4 exhibit substantial room for improvement.The insights derived from our comprehensive analysis offer valuable guidancefor future research within the community. The benchmark data can be found athttps://github.com/FFD8FFE/babelbench."}], "temperature": 0.1}}
{"custom_id": "293_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xuwu Wang"}], "temperature": 0.1}}
{"custom_id": "294_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Wait, but Tylenol is Acetaminophen... Investigating and Improving Language Models' Ability to Resist Requests for Misinformation"}], "temperature": 0.1}}
{"custom_id": "294_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Background: Large language models (LLMs) are trained to follow directions,but this introduces a vulnerability to blindly comply with user requests evenif they generate wrong information. In medicine, this could accelerate thegeneration of misinformation that impacts human well-being.  Objectives/Methods: We analyzed compliance to requests to generate misleadingcontent about medications in settings where models know the request isillogical. We investigated whether in-context directions and instruction-tuningof LLMs to prioritize logical reasoning over compliance reduced misinformationrisk.  Results: While all frontier LLMs complied with misinformation requests, bothprompt-based and parameter-based approaches can improve the detection of logicflaws in requests and prevent the dissemination of medical misinformation.  Conclusion: Shifting LLMs to prioritize logic over compliance could reducerisks of exploitation for medical misinformation."}], "temperature": 0.1}}
{"custom_id": "294_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Shan Chen"}], "temperature": 0.1}}
{"custom_id": "295_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models"}], "temperature": 0.1}}
{"custom_id": "295_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities acrossvarious tasks but their performance in complex logical reasoning tasks remainsunsatisfactory. Although some prompting methods, such as Chain-of-Thought, canimprove the reasoning ability of LLMs to some extent, they suffer from anunfaithful issue where derived conclusions may not align with the generatedreasoning chain. To address this issue, some studies employ the approach ofpropositional logic to further enhance logical reasoning abilities of LLMs.However, the potential omissions in the extraction of logical expressions inthese methods can cause information loss in the logical reasoning process,thereby generating incorrect results. To this end, we propose Logic-of-Thought(LoT) prompting which employs propositional logic to generate expanded logicalinformation descriptions and utilizes them as an additional augmentation tooriginal contexts, thereby ensuring information completeness and enhancinglogical reasoning ability. LoT is orthogonal to existing prompting methods andcan be seamlessly integrated with them. Extensive experiments demonstrate thatLoT boosts the performance of various prompting methods with a striking marginacross five logical reasoning tasks. In particular, LoT enhancesChain-of-Thought's performance on the ReClor dataset by +4.35%, improvesChain-of-Thought with Self-Consistency's performance on the RuleTaker datasetby +3.52%, and boosts performance of Tree-of-Thoughts on the ProofWriterdataset by +8%."}], "temperature": 0.1}}
{"custom_id": "295_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Tongxuan Liu"}], "temperature": 0.1}}
{"custom_id": "296_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification"}], "temperature": 0.1}}
{"custom_id": "296_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning is a fundamental task in natural language processing thatpresents significant challenges to Large Language Models (LLMs). The inherentcharacteristics of logical reasoning makes it well-suited for symbolicrepresentations such as first-order logic (FOL). Research in symbolic logicalreasoning explored FOL generation using state-of-the-art LLMs (i.e., GPT-4) toproduce FOL translations of natural language (NL) statements, but errors intranslation are usually not the focus. We address this by categorizing thetranslation errors in FOL statements generated by LLMs. To make progresstowards improving the quality of FOL translations for smaller language modelssuch as LLaMA-2 13B and Mistral 7B, we create ProofFOL, a high-qualityFOL-annotated subset of ProofWriter dataset using GPT-4o. The models fine-tunedon this silver standard data achieve a significant gain in performance whencompared to larger language models such as LLaMA-2 70B. In addition toimproving the model using large data, we also tackle the issue of data scarcityand introduce an incremental framework encompassing of data augmentation andverification steps. In the augmentation process, a single pair of (premises,conclusion) is split into multiple new instances based on the predicates andFOLs. This data is used for fine-tuning, and the inference on this modelgenerates FOLs with fewer errors over the model trained on the original data.Our investigation on the translation errors leads to generation of aperturbation dataset, which is used to train a verifier that corrects potentialsyntactic and semantic FOL translation errors. We demonstrate an efficientmethod for making the most of a limited existing human-annotated dataset. Ourresults show state-of-the-art performance for ProofWriter and ProntoQA datasetsusing ProofFOL on LLaMA-2 and Mistral models."}], "temperature": 0.1}}
{"custom_id": "296_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ramya Keerthy Thatikonda"}], "temperature": 0.1}}
{"custom_id": "297_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LTNtorch: PyTorch Implementation of Logic Tensor Networks"}], "temperature": 0.1}}
{"custom_id": "297_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logic Tensor Networks (LTN) is a Neuro-Symbolic framework that effectivelyincorporates deep learning and logical reasoning. In particular, LTN allowsdefining a logical knowledge base and using it as the objective of a neuralmodel. This makes learning by logical reasoning possible as the parameters ofthe model are optimized by minimizing a loss function composed of a set oflogical formulas expressing facts about the learning task. The framework learnsvia gradient-descent optimization. Fuzzy logic, a relaxation of classical logicpermitting continuous truth values in the interval [0,1], makes this learningpossible. Specifically, the training of an LTN consists of three steps.Firstly, (1) the training data is used to ground the formulas. Then, (2) theformulas are evaluated, and the loss function is computed. Lastly, (3) thegradients are back-propagated through the logical computational graph, and theweights of the neural model are changed so the knowledge base is maximallysatisfied. LTNtorch is the fully documented and tested PyTorch implementationof Logic Tensor Networks. This paper presents the formalization of LTN and howLTNtorch implements it. Moreover, it provides a basic binary classificationexample."}], "temperature": 0.1}}
{"custom_id": "297_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Tommaso Carraro"}], "temperature": 0.1}}
{"custom_id": "298_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Lost in the Logic: An Evaluation of Large Language Models' Reasoning Capabilities on LSAT Logic Games"}], "temperature": 0.1}}
{"custom_id": "298_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In this thesis, I evaluate the performance of Large Language Models (LLMs) onthe Law School Admissions Test (LSAT), specifically the Logic Games section ofthe test. I focus on this section because it presents a complex logicalreasoning task and thus is a valuable source of data for evaluating how modern,increasingly capable LLMs can handle hard logical reasoning tasks. I constructa dataset of LSAT logic games and their associated metadata, and extensivelyevaluate LLMs' performance in a Chain-of-Thought prompting setting. Given theweak performance in this setting, I explore other prompting frameworks on asmaller subset of the dataset, adapting ideas from Reflexion to this task. Thisresults in a substantially improved accuracy of 70 percent for GPT-4 and 46percent for GPT-3.5 on this data subset, highlighting the capacity of LLMs torevise their logical errors, despite initially weak performance. Finally, Ianalyze the types of logic games that models perform better or worse on, aswell as the types of logical errors I observe from human annotation, providingdetailed insights on the logical reasoning capabilities of LLMs."}], "temperature": 0.1}}
{"custom_id": "298_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Saumya Malik"}], "temperature": 0.1}}
{"custom_id": "299_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Thought-Path Contrastive Learning via Premise-Oriented Data Augmentation for Logical Reading Comprehension"}], "temperature": 0.1}}
{"custom_id": "299_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reading comprehension is a challenging task that entails grasping theunderlying semantics of text and applying reasoning to deduce the correctanswer. Prior researches have primarily focused on enhancing logical reasoningcapabilities through Chain-of-Thought (CoT) or data augmentation. However,previous work constructing chain-of-thought rationales concentrates solely onanalyzing correct options, neglecting the incorrect alternatives. Addtionally,earlier efforts on data augmentation by altering contexts rely on rule-basedmethods, which result in generated contexts that lack diversity and coherence.To address these issues, we propose a Premise-Oriented Data Augmentation (PODA)framework. This framework can generate CoT rationales including analyses forboth correct and incorrect options, while constructing diverse and high-qualitycounterfactual contexts from incorrect candidate options. We integratesummarizing premises and identifying premises for each option into rationales.Subsequently, we employ multi-step prompts with identified premises toconstruct counterfactual context. To facilitate the model's capabilities tobetter differentiate the reasoning process associated with each option, weintroduce a novel thought-path contrastive learning method that comparesreasoning paths between the original and counterfactual samples. Experimentalresults on three representative LLMs demonstrate that our method can improvethe baselines substantially across two challenging logical reasoning benchmarks(ReClor and LogiQA 2.0). The data and code are released athttps://github.com/lalalamdbf/TPReasoner."}], "temperature": 0.1}}
{"custom_id": "299_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chenxu Wang"}], "temperature": 0.1}}
{"custom_id": "300_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "GroupDebate: Enhancing the Efficiency of Multi-Agent Debate Using Group Discussion"}], "temperature": 0.1}}
{"custom_id": "300_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In recent years, Large Language Models (LLMs) have demonstrated remarkablecapabilities across diverse NLP tasks. Extensive research has explored how toenhance the logical reasoning abilities such as Chain-of-Thought,Chain-of-Thought with Self-Consistency, Tree-Of-Thoughts, and multi-agentdebates. In the context of multi-agent debates, significant performanceimprovements can be achieved with an increasing number of agents and debaterounds. However, the escalation in the number of agents and debate rounds candrastically raise the tokens cost of debates, thereby limiting the scalabilityof the multi-agent debate technique. To better harness the advantages ofmulti-agent debates in logical reasoning tasks, this paper proposes a method tosignificantly reduce token cost in multi-agent debates. This approach involvesdividing all agents into multiple debate groups, with agents engaging indebates within their respective groups and sharing interim debate resultsbetween groups. Comparative experiments across multiple datasets havedemonstrated that this method can reduce the total tokens by up to 51.7% duringdebates and while potentially enhancing accuracy by as much as 25%. Our methodsignificantly enhances the performance and efficiency of interactions in themulti-agent debate."}], "temperature": 0.1}}
{"custom_id": "300_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Tongxuan Liu"}], "temperature": 0.1}}
{"custom_id": "301_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning"}], "temperature": 0.1}}
{"custom_id": "301_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In this paper, we propose a new data synthesis method called\\textbf{LogicPro}, which leverages LeetCode-style algorithm\\underline{Pro}blems and their corresponding \\underline{Pro}gram solutions tosynthesize Complex \\underline{Logic}al Reasoning data in text format. First, wesynthesize complex reasoning problems through source algorithm problems andtest cases. Then, standard answers and intermediate variable outputs areobtained for each problem based on standard python solutions and test cases.Finally, with the guidance of code intermediate variables, we synthesize thetext reasoning process for each reasoning problems. Through this method, we cansynthesize data that is difficult, scalable, effective, and comes with goldenstandard answers and high-quality reasoning processes. As a result, with our540K synthesized dataset constructed solely from 2,360 algorithm problems, ourapproach  Code and data are publicly available athttps://github.com/jiangjin1999/LogicPro achieves significant improvements inmultiple models for the datasets \\textit{BBH$^{27}$}, \\textit{LogicBench},\\textit{DROP}, \\textit{AR-LSAT}, and \\textit{GSM8K}, etc. outperforming a widerange of existing reasoning datasets."}], "temperature": 0.1}}
{"custom_id": "301_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jin Jiang"}], "temperature": 0.1}}
{"custom_id": "302_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data"}], "temperature": 0.1}}
{"custom_id": "302_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Despite recent advances in training and prompting strategies for LargeLanguage Models (LLMs), these models continue to face challenges with complexlogical reasoning tasks that involve long reasoning chains. In this work, weexplore the potential and limitations of using graph-based synthetic reasoningdata as training signals to enhance LLMs' reasoning capabilities. Our extensiveexperiments, conducted on two established natural language reasoning tasks --inductive reasoning and spatial reasoning -- demonstrate that supervisedfine-tuning (SFT) with synthetic graph-based reasoning data effectivelyenhances LLMs' reasoning performance without compromising their effectivenesson other standard evaluation benchmarks."}], "temperature": 0.1}}
{"custom_id": "302_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jiaming Zhou"}], "temperature": 0.1}}
{"custom_id": "303_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "ProSLM : A Prolog Synergized Language Model for explainable Domain Specific Knowledge Based Question Answering"}], "temperature": 0.1}}
{"custom_id": "303_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neurosymbolic approaches can add robustness to opaque neural systems byincorporating explainable symbolic representations. However, previousapproaches have not used formal logic to contextualize queries to and validateoutputs of large language models (LLMs). We propose \\systemname{}, a novelneurosymbolic framework, to improve the robustness and reliability of LLMs inquestion-answering tasks. We provide \\systemname{} with a domain-specificknowledge base, a logical reasoning system, and an integration to an existingLLM. This framework has two capabilities (1) context gathering: generatingexplainable and relevant context for a given query, and (2) validation:confirming and validating the factual accuracy of a statement in accordancewith a knowledge base (KB). Our work opens a new area of neurosymbolicgenerative AI text validation and user personalization."}], "temperature": 0.1}}
{"custom_id": "303_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Priyesh Vakharia"}], "temperature": 0.1}}
{"custom_id": "304_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Video Token Sparsification for Efficient Multimodal LLMs in Autonomous Driving"}], "temperature": 0.1}}
{"custom_id": "304_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Multimodal large language models (MLLMs) have demonstrated remarkablepotential for enhancing scene understanding in autonomous driving systemsthrough powerful logical reasoning capabilities. However, the deployment ofthese models faces significant challenges due to their substantial parametersizes and computational demands, which often exceed the constraints of onboardcomputation. One major limitation arises from the large number of visual tokensrequired to capture fine-grained and long-context visual information, leadingto increased latency and memory consumption. To address this issue, we proposeVideo Token Sparsification (VTS), a novel approach that leverages the inherentredundancy in consecutive video frames to significantly reduce the total numberof visual tokens while preserving the most salient information. VTS employs alightweight CNN-based proposal model to adaptively identify key frames andprune less informative tokens, effectively mitigating hallucinations andincreasing inference throughput without compromising performance. We conductcomprehensive experiments on the DRAMA and LingoQA benchmarks, demonstratingthe effectiveness of VTS in achieving up to a 33\\% improvement in inferencethroughput and a 28\\% reduction in memory usage compared to the baselinewithout compromising performance."}], "temperature": 0.1}}
{"custom_id": "304_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yunsheng Ma"}], "temperature": 0.1}}
{"custom_id": "305_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower Dynamic Semantic Token Generator"}], "temperature": 0.1}}
{"custom_id": "305_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Owing to the unprecedented capability in semantic understanding and logicalreasoning, the pre-trained large language models (LLMs) have shown fantasticpotential in developing the next-generation recommender systems (RSs). However,the static index paradigm adopted by current methods greatly restricts theutilization of LLMs capacity for recommendation, leading to not only theinsufficient alignment between semantic and collaborative knowledge, but alsothe neglect of high-order user-item interaction patterns. In this paper, wepropose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RSwhich adopts dynamic semantic index paradigm, targeting at resolving the aboveproblems simultaneously. To be more specific, we for the first time contrive adynamic knowledge fusion framework which integrates a twin-tower semantic tokengenerator into the LLM-based recommender, hierarchically allocating meaningfulsemantic index for items and users, and accordingly predicting the semanticindex of target item. Furthermore, a dual-modality variational auto-encoder isproposed to facilitate multi-grained alignment between semantic andcollaborative knowledge. Eventually, a series of novel tuning tasks speciallycustomized for capturing high-order user-item interaction patterns are proposedto take advantages of user historical behavior. Extensive experiments acrossthree public datasets demonstrate the superiority of the proposed methodologyin developing LLM-based generative RSs. The proposed TTDS recommender achievesan average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric,compared with the leading baseline methods."}], "temperature": 0.1}}
{"custom_id": "305_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jun Yin"}], "temperature": 0.1}}
{"custom_id": "306_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "CauseJudger: Identifying the Cause with LLMs for Abductive Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "306_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have been utilized in solving diverse reasoningtasks, encompassing common sense, arithmetic and deduction tasks. However, withdifficulties of reversing thinking patterns and irrelevant premises, how todetermine the authenticity of the cause in abductive logical reasoning remainsunderexplored. Inspired by hypothesis and verification method andidentification of irrelevant information in human thinking process, we proposea new framework for LLMs abductive logical reasoning called CauseJudger (CJ),which identifies the authenticity of possible cause by transforming thinkingfrom reverse to forward and removing irrelevant information. In addition, weconstruct an abductive logical reasoning dataset for decision task calledCauseLogics, which contains 200,000 tasks of varying reasoning lengths. Ourexperiments show the efficiency of CJ with overall experiments and ablationexperiments as well as case studies on our dataset and reconstructed publicdataset. Notably, CJ's implementation is efficient, requiring only two calls toLLM. Its impact is profound: when using gpt-3.5, CJ achieves a maximumcorrectness improvement of 41% compared to Zero-Shot-CoT. Moreover, with gpt-4,CJ attains an accuracy exceeding 90% across all datasets."}], "temperature": 0.1}}
{"custom_id": "306_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jinwei He"}], "temperature": 0.1}}
{"custom_id": "307_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models"}], "temperature": 0.1}}
{"custom_id": "307_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Harnessing the robust capabilities of Large Language Models (LLMs) fornarrative generation, logical reasoning, and common-sense knowledgeintegration, this study delves into utilizing LLMs to enhance automatedradiology report generation (R2Gen). Despite the wealth of knowledge withinLLMs, efficiently triggering relevant knowledge within these large models forspecific tasks like R2Gen poses a critical research challenge. This paperpresents KARGEN, a Knowledge-enhanced Automated radiology Report GENerationframework based on LLMs. Utilizing a frozen LLM to generate reports, theframework integrates a knowledge graph to unlock chest disease-relatedknowledge within the LLM to enhance the clinical utility of generated reports.This is achieved by leveraging the knowledge graph to distill disease-relatedfeatures in a designed way. Since a radiology report encompasses both normaland disease-related findings, the extracted graph-enhanced disease-relatedfeatures are integrated with regional image features, attending to bothaspects. We explore two fusion methods to automatically prioritize and selectthe most relevant features. The fused features are employed by LLM to generatereports that are more sensitive to diseases and of improved quality. Ourapproach demonstrates promising results on the MIMIC-CXR and IU-Xray datasets."}], "temperature": 0.1}}
{"custom_id": "307_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yingshu Li"}], "temperature": 0.1}}
{"custom_id": "308_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Action is the primary key: a categorical framework for episode description and logical reasoning"}], "temperature": 0.1}}
{"custom_id": "308_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This research presents a computational framework for describing andrecognizing episodes and for logical reasoning. This framework, namedcognitive-logs, consists of a set of relational and graph databases.Cognitive-logs record knowledge, particularly in episodes that consist of\"actions\" represented by verbs in natural languages and \"participants\" whoperform the actions. These objects are connected by arrows (morphisms) thatlink each action to its participant and link cause to effect. Operations basedon category theory enable comparisons between episodes and deductiveinferences, including abstractions of stories. One of the goals of this studyis to develop a database-driven artificial intelligence. This artificialintelligence thinks like a human but possesses the accuracy and rigour of amachine. The vast capacities of databases (up to petabyte scales in currenttechnologies) enable the artificial intelligence to store a greater volume ofknowledge than neural-network based artificial intelligences. Cognitive-logsserve as a model of human cognition and designed with references to cognitivelinguistics. Cognitive-logs also have the potential to model various human mindactivities."}], "temperature": 0.1}}
{"custom_id": "308_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yoshiki Fukada"}], "temperature": 0.1}}
{"custom_id": "309_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Testing and Evaluation of Large Language Models: Correctness, Non-Toxicity, and Fairness"}], "temperature": 0.1}}
{"custom_id": "309_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs), such as ChatGPT, have rapidly penetrated intopeople's work and daily lives over the past few years, due to theirextraordinary conversational skills and intelligence. ChatGPT has become thefastest-growing software in terms of user numbers in human history and becomean important foundational model for the next generation of artificialintelligence applications. However, the generations of LLMs are not entirelyreliable, often producing content with factual errors, biases, and toxicity.Given their vast number of users and wide range of application scenarios, theseunreliable responses can lead to many serious negative impacts. This thesisintroduces the exploratory works in the field of language model reliabilityduring the PhD study, focusing on the correctness, non-toxicity, and fairnessof LLMs from both software testing and natural language processingperspectives. First, to measure the correctness of LLMs, we introduce twotesting frameworks, FactChecker and LogicAsker, to evaluate factual knowledgeand logical reasoning accuracy, respectively. Second, for the non-toxicity ofLLMs, we introduce two works for red-teaming LLMs. Third, to evaluate thefairness of LLMs, we introduce two evaluation frameworks, BiasAsker andXCulturalBench, to measure the social bias and cultural bias of LLMs,respectively."}], "temperature": 0.1}}
{"custom_id": "309_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Wenxuan Wang"}], "temperature": 0.1}}
{"custom_id": "310_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Story3D-Agent: Exploring 3D Storytelling Visualization with Large Language Models"}], "temperature": 0.1}}
{"custom_id": "310_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Traditional visual storytelling is complex, requiring specialized knowledgeand substantial resources, yet often constrained by human creativity andcreation precision. While Large Language Models (LLMs) enhance visualstorytelling, current approaches often limit themselves to 2D visuals oroversimplify stories through motion synthesis and behavioral simulation,failing to create comprehensive, multi-dimensional narratives. To this end, wepresent Story3D-Agent, a pioneering approach that leverages the capabilities ofLLMs to transform provided narratives into 3D-rendered visualizations. Byintegrating procedural modeling, our approach enables precise control overmulti-character actions and motions, as well as diverse decorative elements,ensuring the long-range and dynamic 3D representation. Furthermore, our methodsupports narrative extension through logical reasoning, ensuring that generatedcontent remains consistent with existing conditions. We have thoroughlyevaluated our Story3D-Agent to validate its effectiveness, offering a basicframework to advance 3D story representation."}], "temperature": 0.1}}
{"custom_id": "310_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yuzhou Huang"}], "temperature": 0.1}}
{"custom_id": "311_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SarcasmBench: Towards Evaluating Large Language Models on Sarcasm Understanding"}], "temperature": 0.1}}
{"custom_id": "311_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In the era of large language models (LLMs), the task of ``System I''~-~thefast, unconscious, and intuitive tasks, e.g., sentiment analysis, textclassification, etc., have been argued to be successfully solved. However,sarcasm, as a subtle linguistic phenomenon, often employs rhetorical deviceslike hyperbole and figuration to convey true sentiments and intentions,involving a higher level of abstraction than sentiment analysis. There isgrowing concern that the argument about LLMs' success may not be fully tenablewhen considering sarcasm understanding. To address this question, we selecteleven SOTA LLMs and eight SOTA pre-trained language models (PLMs) and presentcomprehensive evaluations on six widely used benchmark datasets throughdifferent prompting approaches, i.e., zero-shot input/output (IO) prompting,few-shot IO prompting, chain of thought (CoT) prompting. Our results highlightthree key findings: (1) current LLMs underperform supervised PLMs based sarcasmdetection baselines across six sarcasm benchmarks. This suggests thatsignificant efforts are still required to improve LLMs' understanding of humansarcasm. (2) GPT-4 consistently and significantly outperforms other LLMs acrossvarious prompting methods, with an average improvement of 14.0\\%$\\uparrow$.Claude 3 and ChatGPT demonstrate the next best performance after GPT-4. (3)Few-shot IO prompting method outperforms the other two methods: zero-shot IOand few-shot CoT. The reason is that sarcasm detection, being a holistic,intuitive, and non-rational cognitive process, is argued not to adhere tostep-by-step logical reasoning, making CoT less effective in understandingsarcasm compared to its effectiveness in mathematical reasoning tasks."}], "temperature": 0.1}}
{"custom_id": "311_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yazhou Zhang"}], "temperature": 0.1}}
{"custom_id": "312_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Exploring Reasoning Biases in Large Language Models Through Syllogism: Insights from the NeuBAROCO Dataset"}], "temperature": 0.1}}
{"custom_id": "312_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This paper explores the question of how accurately current large languagemodels can perform logical reasoning in natural language, with an emphasis onwhether these models exhibit reasoning biases similar to humans. Specifically,our study focuses on syllogistic reasoning, a form of deductive reasoningextensively studied in cognitive science as a natural form of human reasoning.We present a syllogism dataset called NeuBAROCO, which consists of syllogisticreasoning problems in English and Japanese. This dataset was originallydesigned for psychological experiments to assess human reasoning capabilitiesusing various forms of syllogisms. Our experiments with leading large languagemodels indicate that these models exhibit reasoning biases similar to humans,along with other error tendencies. Notably, there is significant room forimprovement in reasoning problems where the relationship between premises andhypotheses is neither entailment nor contradiction. We also presentexperimental results and in-depth analysis using a new Chain-of-Thoughtprompting method, which asks LLMs to translate syllogisms into abstract logicalexpressions and then explain their reasoning process. Our analysis using thismethod suggests that the primary limitations of LLMs lie in the reasoningprocess itself rather than the interpretation of syllogisms."}], "temperature": 0.1}}
{"custom_id": "312_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Kentaro Ozeki"}], "temperature": 0.1}}
{"custom_id": "313_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "PAGED: A Benchmark for Procedural Graphs Extraction from Documents"}], "temperature": 0.1}}
{"custom_id": "313_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Automatic extraction of procedural graphs from documents creates a low-costway for users to easily understand a complex procedure by skimming visualgraphs. Despite the progress in recent studies, it remains unanswered: whetherthe existing studies have well solved this task (Q1) and whether the emerginglarge language models (LLMs) can bring new opportunities to this task (Q2). Tothis end, we propose a new benchmark PAGED, equipped with a large high-qualitydataset and standard evaluations. It investigates five state-of-the-artbaselines, revealing that they fail to extract optimal procedural graphs wellbecause of their heavy reliance on hand-written rules and limited availabledata. We further involve three advanced LLMs in PAGED and enhance them with anovel self-refine strategy. The results point out the advantages of LLMs inidentifying textual elements and their gaps in building logical structures. Wehope PAGED can serve as a major landmark for automatic procedural graphextraction and the investigations in PAGED can offer insights into the researchon logic reasoning among non-sequential elements."}], "temperature": 0.1}}
{"custom_id": "313_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Weihong Du"}], "temperature": 0.1}}
{"custom_id": "314_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Lifelong Personalized Low-Rank Adaptation of Large Language Models for Recommendation"}], "temperature": 0.1}}
{"custom_id": "314_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We primarily focus on the field of large language models (LLMs) forrecommendation, which has been actively explored recently and poses asignificant challenge in effectively enhancing recommender systems with logicalreasoning abilities and open-world knowledge. Current mainstream efforts mainlycenter around injecting personalized information from recommendation modelsinto LLMs by customizing input templates or aligning representations betweensemantic and recommendation spaces at the prediction layer. However, they facethree significant limitations: (1) LoRA is mostly used as a core component inexisting works, but personalization is not well established in LoRA parametersas the LoRA matrix shared by every user may not cater to different users'characteristics, leading to suboptimal performance. (2) Although lifelongpersonalized behavior sequences are ideal for personalization, their use raiseseffectiveness and efficiency issues since LLMs require escalating training andinference time to extend text lengths. (3) Existing approaches aren't scalablefor large datasets due to training efficiency constraints. Thus, LLMs only seea small fraction of the datasets (e.g., less than 10%) instead of the wholedatasets, limiting their exposure to the full training space. To address theseproblems, we propose RecLoRA. This model incorporates a Personalized LoRAmodule that maintains independent LoRAs for different users and a Long-ShortModality Retriever that retrieves different history lengths for differentmodalities, significantly improving performance while adding minimal time cost.Furthermore, we design a Few2Many Learning Strategy, using a conventionalrecommendation model as a lens to magnify small training spaces to full spaces.Extensive experiments on public datasets demonstrate the efficacy of ourRecLoRA compared to existing baseline models."}], "temperature": 0.1}}
{"custom_id": "314_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jiachen Zhu"}], "temperature": 0.1}}
{"custom_id": "315_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Automated Theorem Provers Help Improve Large Language Model Reasoning"}], "temperature": 0.1}}
{"custom_id": "315_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In this paper we demonstrate how logic programming systems and Automatedfirst-order logic Theorem Provers (ATPs) can improve the accuracy of LargeLanguage Models (LLMs) for logical reasoning tasks where the baselineperformance is given by direct LLM solutions. We first evaluate LLM reasoningon steamroller problems using the PRONTOQA benchmark. We show how accuracy canbe improved with a neuro-symbolic architecture where the LLM acts solely as afront-end for translating a given problem into a formal logic language and anautomated reasoning engine is called for solving it. However, this approachcritically hinges on the correctness of the LLM translation. To assess thistranslation correctness, we secondly define a framework of syntactic andsemantic error categories. We implemented the framework and used it to identifyerrors that LLMs make in the benchmark domain. Based on these findings, wethirdly extended our method with capabilities for automatically correctingsyntactic and semantic errors. For semantic error correction we integratefirst-order logic ATPs, which is our main and novel contribution. Wedemonstrate that this approach reduces semantic errors significantly andfurther increases the accurracy of LLM logical reasoning."}], "temperature": 0.1}}
{"custom_id": "315_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Lachlan McGinness"}], "temperature": 0.1}}
{"custom_id": "316_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Leveraging Large Language Models with Chain-of-Thought and Prompt Engineering for Traffic Crash Severity Analysis and Inference"}], "temperature": 0.1}}
{"custom_id": "316_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Harnessing the power of Large Language Models (LLMs), this study explores theuse of three state-of-the-art LLMs, specifically GPT-3.5-turbo, LLaMA3-8B, andLLaMA3-70B, for crash severity inference, framing it as a classification task.We generate textual narratives from original traffic crash tabular data using apre-built template infused with domain knowledge. Additionally, we incorporatedChain-of-Thought (CoT) reasoning to guide the LLMs in analyzing the crashcauses and then inferring the severity. This study also examine the impact ofprompt engineering specifically designed for crash severity inference. The LLMswere tasked with crash severity inference to: (1) evaluate the models'capabilities in crash severity analysis, (2) assess the effectiveness of CoTand domain-informed prompt engineering, and (3) examine the reasoning abilitieswith the CoT framework. Our results showed that LLaMA3-70B consistentlyoutperformed the other models, particularly in zero-shot settings. The CoT andPrompt Engineering techniques significantly enhanced performance, improvinglogical reasoning and addressing alignment issues. Notably, the CoT offersvaluable insights into LLMs' reasoning processes, unleashing their capacity toconsider diverse factors such as environmental conditions, driver behavior, andvehicle characteristics in severity analysis and inference."}], "temperature": 0.1}}
{"custom_id": "316_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hao Zhen"}], "temperature": 0.1}}
{"custom_id": "317_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Coalitions of Large Language Models Increase the Robustness of AI Agents"}], "temperature": 0.1}}
{"custom_id": "317_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The emergence of Large Language Models (LLMs) have fundamentally altered theway we interact with digital systems and have led to the pursuit of LLM poweredAI agents to assist in daily workflows. LLMs, whilst powerful and capable ofdemonstrating some emergent properties, are not logical reasoners and oftenstruggle to perform well at all sub-tasks carried out by an AI agent to planand execute a workflow. While existing studies tackle this lack of proficiencyby generalised pretraining at a huge scale or by specialised fine-tuning fortool use, we assess if a system comprising of a coalition of pretrained LLMs,each exhibiting specialised performance at individual sub-tasks, can match theperformance of single model agents. The coalition of models approach showcasesits potential for building robustness and reducing the operational costs ofthese AI agents by leveraging traits exhibited by specific models. Our findingsdemonstrate that fine-tuning can be mitigated by considering a coalition ofpretrained models and believe that this approach can be applied to othernon-agentic systems which utilise LLMs."}], "temperature": 0.1}}
{"custom_id": "317_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Prattyush Mangal"}], "temperature": 0.1}}
{"custom_id": "318_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Leveraging Large Language Models (LLMs) for Traffic Management at Urban Intersections: The Case of Mixed Traffic Scenarios"}], "temperature": 0.1}}
{"custom_id": "318_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Urban traffic management faces significant challenges due to the dynamicenvironments, and traditional algorithms fail to quickly adapt to thisenvironment in real-time and predict possible conflicts. This study exploresthe ability of a Large Language Model (LLM), specifically, GPT-4o-mini toimprove traffic management at urban intersections. We recruited GPT-4o-mini toanalyze, predict position, detect and resolve the conflicts at an intersectionin real-time for various basic scenarios. The key findings of this study toinvestigate whether LLMs can logically reason and understand the scenarios toenhance the traffic efficiency and safety by providing real-time analysis. Thestudy highlights the potential of LLMs in urban traffic management creatingmore intelligent and more adaptive systems. Results showed the GPT-4o-mini waseffectively able to detect and resolve conflicts in heavy traffic, congestion,and mixed-speed conditions. The complex scenario of multiple intersections withobstacles and pedestrians saw successful conflict management as well. Resultsshow that the integration of LLMs promises to improve the effectiveness oftraffic control for safer and more efficient urban intersection management."}], "temperature": 0.1}}
{"custom_id": "318_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Sari Masri"}], "temperature": 0.1}}
{"custom_id": "319_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Deceptive AI systems that give explanations are more convincing than honest AI systems and can amplify belief in misinformation"}], "temperature": 0.1}}
{"custom_id": "319_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Advanced Artificial Intelligence (AI) systems, specifically large languagemodels (LLMs), have the capability to generate not just misinformation, butalso deceptive explanations that can justify and propagate false informationand erode trust in the truth. We examined the impact of deceptive AI generatedexplanations on individuals' beliefs in a pre-registered online experiment with23,840 observations from 1,192 participants. We found that in addition to beingmore persuasive than accurate and honest explanations, AI-generated deceptiveexplanations can significantly amplify belief in false news headlines andundermine true ones as compared to AI systems that simply classify the headlineincorrectly as being true/false. Moreover, our results show that personalfactors such as cognitive reflection and trust in AI do not necessarily protectindividuals from these effects caused by deceptive AI generated explanations.Instead, our results show that the logical validity of AI generated deceptiveexplanations, that is whether the explanation has a causal effect on thetruthfulness of the AI's classification, plays a critical role in counteringtheir persuasiveness - with logically invalid explanations being deemed lesscredible. This underscores the importance of teaching logical reasoning andcritical thinking skills to identify logically invalid arguments, fosteringgreater resilience against advanced AI-driven misinformation."}], "temperature": 0.1}}
{"custom_id": "319_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Valdemar Danry"}], "temperature": 0.1}}
{"custom_id": "320_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large Language Models over Factual Knowledge"}], "temperature": 0.1}}
{"custom_id": "320_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "While large language models (LLMs) have demonstrated impressive capabilitiesacross various natural language processing tasks by acquiring rich factualknowledge from their broad training data, their ability to synthesize andlogically reason with this knowledge in complex ways remains underexplored. Inthis work, we present a systematic evaluation of state-of-the-art LLMs' complexlogical reasoning abilities through a novel benchmark of automaticallygenerated complex reasoning questions over general domain and biomedicalknowledge graphs. Our extensive experiments, employing diverse in-contextlearning techniques, reveal that LLMs excel at reasoning over general worldknowledge but face significant challenges with specialized domain-specificknowledge. We find that prompting with explicit Chain-of-Thought demonstrationscan substantially improve LLM performance on complex logical reasoning taskswith diverse logical operations. Interestingly, our controlled evaluationsuncover an asymmetry where LLMs display proficiency at set union operations,but struggle considerably with set intersections - a key building block oflogical reasoning. To foster further work, we will publicly release ourevaluation benchmark and code."}], "temperature": 0.1}}
{"custom_id": "320_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Tianshi Zheng"}], "temperature": 0.1}}
{"custom_id": "321_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Take A Step Back: Rethinking the Two Stages in Visual Reasoning"}], "temperature": 0.1}}
{"custom_id": "321_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Visual reasoning, as a prominent research area, plays a crucial role in AI byfacilitating concept formation and interaction with the world. However, currentworks are usually carried out separately on small datasets thus lackinggeneralization ability. Through rigorous evaluation of diverse benchmarks, wedemonstrate the shortcomings of existing ad-hoc methods in achievingcross-domain reasoning and their tendency to data bias fitting. In this paper,we revisit visual reasoning with a two-stage perspective: (1) symbolization and(2) logical reasoning given symbols or their representations. We find that thereasoning stage is better at generalization than symbolization. Thus, it ismore efficient to implement symbolization via separated encoders for differentdata domains while using a shared reasoner. Given our findings, we establishdesign principles for visual reasoning frameworks following the separatedsymbolization and shared reasoning. The proposed two-stage framework achievesimpressive generalization ability on various visual reasoning tasks, includingpuzzles, physical prediction, and visual question answering (VQA), encompassingboth 2D and 3D modalities. We believe our insights will pave the way forgeneralizable visual reasoning."}], "temperature": 0.1}}
{"custom_id": "321_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mingyu Zhang"}], "temperature": 0.1}}
{"custom_id": "322_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logic Distillation: Learning from Code Function by Function for Planning and Decision-making"}], "temperature": 0.1}}
{"custom_id": "322_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have garnered increasing attention owing totheir powerful logical reasoning capabilities. Generally, larger LLMs (L-LLMs)that require paid interfaces exhibit significantly superior performancecompared to smaller LLMs (S-LLMs) that can be deployed on a variety of devices.Knowledge distillation (KD) aims to empower S-LLMs with the capabilities ofL-LLMs, while S-LLMs merely mimic the outputs of L-LLMs, failing to get thepowerful logical reasoning capabilities. Consequently, S-LLMs are helpless whenit comes to planning and decision-making tasks that require logical reasoningcapabilities. To tackle the identified challenges, we propose a novel frameworkcalled Logic Distillation (LD). Initially, LD employs L-LLMs to instantiatecomplex instructions into discrete functions and illustrates their usage toestablish a function base. Subsequently, based on the function base, LDfine-tunes S-LLMs to learn the logic employed by L-LLMs in planning anddecision-making. During testing, LD utilizes a retriever to identify thetop-$K$ relevant functions based on instructions and current states, which willbe selected and invoked by S-LLMs. Ultimately, S-LLMs yield planning anddecision-making outcomes, function by function. Relevant experimentsdemonstrate that with the assistance of LD, S-LLMs can achieve outstandingresults in planning and decision-making tasks, comparable to, or evensurpassing, those of L-LLMs."}], "temperature": 0.1}}
{"custom_id": "322_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Dong Chen"}], "temperature": 0.1}}
{"custom_id": "323_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Audio Entailment: Assessing Deductive Reasoning for Audio Understanding"}], "temperature": 0.1}}
{"custom_id": "323_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent literature uses language to build foundation models for audio. TheseAudio-Language Models (ALMs) are trained on a vast number of audio-text pairsand show remarkable performance in tasks including Text-to-Audio Retrieval,Captioning, and Question Answering. However, their ability to engage in morecomplex open-ended tasks, like Interactive Question-Answering, requiresproficiency in logical reasoning -- a skill not yet benchmarked. We introducethe novel task of Audio Entailment to evaluate an ALM's deductive reasoningability. This task assesses whether a text description (hypothesis) of audiocontent can be deduced from an audio recording (premise), with potentialconclusions being entailment, neutral, or contradiction, depending on thesufficiency of the evidence. We create two datasets for this task with audiorecordings sourced from two audio captioning datasets -- AudioCaps and Clotho-- and hypotheses generated using Large Language Models (LLMs). We benchmarkstate-of-the-art ALMs and find deficiencies in logical reasoning with bothzero-shot and linear probe evaluations. Finally, we propose\"caption-before-reason\", an intermediate step of captioning that improves thezero-shot and linear-probe performance of ALMs by an absolute 6% and 3%,respectively."}], "temperature": 0.1}}
{"custom_id": "323_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Soham Deshmukh"}], "temperature": 0.1}}
{"custom_id": "324_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Fusing LLMs and KGs for Formal Causal Reasoning behind Financial Risk Contagion"}], "temperature": 0.1}}
{"custom_id": "324_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Financial risks trend to spread from one entity to another, ultimatelyleading to systemic risks. The key to preventing such risks lies inunderstanding the causal chains behind risk contagion. Despite this, prevailingapproaches primarily emphasize identifying risks, overlooking the underlyingcausal analysis of risk. To address such an issue, we propose a Risk ContagionCausal Reasoning model called RC2R, which uses the logical reasoningcapabilities of large language models (LLMs) to dissect the causal mechanismsof risk contagion grounded in the factual and expert knowledge embedded withinfinancial knowledge graphs (KGs). At the data level, we utilize financial KGsto construct causal instructions, empowering LLMs to perform formal causalreasoning on risk propagation and tackle the \"causal parrot\" problem of LLMs.In terms of model architecture, we integrate a fusion module that aligns tokensand nodes across various granularities via multi-scale contrastive learning,followed by the amalgamation of textual and graph-structured data through softprompt with cross multi-head attention mechanisms. To quantify risk contagion,we introduce a risk pathway inference module for calculating risk scores foreach node in the graph. Finally, we visualize the risk contagion pathways andtheir intensities using Sankey diagrams, providing detailed causalexplanations. Comprehensive experiments on financial KGs and supply chaindatasets demonstrate that our model outperforms several state-of-the-art modelsin prediction performance and out-of-distribution (OOD) generalizationcapabilities. We will make our dataset and code publicly accessible toencourage further research and development in this field."}], "temperature": 0.1}}
{"custom_id": "324_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Guanyuan Yu"}], "temperature": 0.1}}
{"custom_id": "325_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SelfPiCo: Self-Guided Partial Code Execution with LLMs"}], "temperature": 0.1}}
{"custom_id": "325_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Code executability plays a vital role in software debugging and testing(e.g., detecting runtime exceptions or assertion violations). However, codeexecution, especially partial or arbitrary code execution, is a non-trivialtask due to missing definitions and complex third-party dependencies. To makepartial code (such as code snippets posted on the web or code fragments deepinside complex software projects) executable, the existing study has proposed amachine learning model to predict the undefined element types and inject thepre-defined dummy values into execution. However, the performance of their toolis limited due to its simply designed dummy values and the inability tocontinue learning. In this paper, we design and implement a novel framework,named SelfPiCo (Self Guided Partial Code Executor), to dynamically guidepartial code execution by incorporating the open-source LLM (i.e., Code Llama)within an interactive loop. Particularly, SelfPiCo leverages few-shotin-context learning and chain-of-thought reasoning to elicit human knowledgeand logical reasoning based on fine-tuning the Code Llama model. SelfPiCocontinuously learns from code execution results and refines its predictionsstep after step. Our evaluations demonstrate that SelfPiCo can execute 72.7%and 83.3% of all lines in the open-source code and Stack Overflow snippets,outperforming the most recent state-of-the-art Lexecutor by 37.9% and 33.5%,respectively. Moreover, SelfPiCo successfully detected 18 and 33 runtime typeerror issues by executing the partial code from eight GitHub software projectsand 43 Stack Overflow posts, demonstrating the practical usage and potentialapplication of our framework in practice."}], "temperature": 0.1}}
{"custom_id": "325_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhipeng Xue"}], "temperature": 0.1}}
{"custom_id": "326_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Networks of Networks: Complexity Class Principles Applied to Compound AI Systems Design"}], "temperature": 0.1}}
{"custom_id": "326_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "As practitioners seek to surpass the current reliability and quality frontierof monolithic models, Compound AI Systems consisting of many language modelinference calls are increasingly employed. In this work, we construct systems,which we call Networks of Networks (NoNs) organized around the distinctionbetween generating a proposed answer and verifying its correctness, afundamental concept in complexity theory that we show empirically extends toLanguage Models (LMs). We introduce a verifier-based judge NoN with Kgenerators, an instantiation of \"best-of-K\" or \"judge-based\" compound AIsystems. Through experiments on synthetic tasks such as prime factorization,and core benchmarks such as the MMLU, we demonstrate notable performance gains.For instance, in factoring products of two 3-digit primes, a simple NoNimproves accuracy from 3.7\\% to 36.6\\%. On MMLU, a verifier-based judgeconstruction with only 3 generators boosts accuracy over individual GPT-4-Turbocalls by 2.8\\%. Our analysis reveals that these gains are most pronounced indomains where verification is notably easier than generation--acharacterization which we believe subsumes many reasoning and proceduralknowledge tasks, but doesn't often hold for factual and declarativeknowledge-based settings. For mathematical and formal logic reasoning-basedsubjects of MMLU, we observe a 5-8\\% or higher gain, whilst no gain on otherssuch as geography and religion. We provide key takeaways for ML practitioners,including the importance of considering verification complexity, the impact ofwitness format on verifiability, and a simple test to determine the potentialbenefit of this NoN approach for a given problem distribution. This work aimsto inform future research and practice in the design of compound AI systems."}], "temperature": 0.1}}
{"custom_id": "326_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jared Quincy Davis"}], "temperature": 0.1}}
{"custom_id": "327_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought"}], "temperature": 0.1}}
{"custom_id": "327_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Since the launch of ChatGPT at the end of 2022, generative dialogue modelsrepresented by ChatGPT have quickly become essential tools in daily life. Asuser expectations increase, enhancing the capability of generative dialoguemodels to solve complex problems has become a focal point of current research.This paper delves into the effectiveness of the RAFT (Retrieval AugmentedFine-Tuning) method in improving the performance of Generative dialogue models.RAFT combines chain-of-thought with model supervised fine-tuning (SFT) andretrieval augmented generation (RAG), which significantly enhanced the model'sinformation extraction and logical reasoning abilities. We evaluated the RAFTmethod across multiple datasets and analysed its performance in variousreasoning tasks, including long-form QA and short-form QA tasks, tasks in bothChinese and English, and supportive and comparison reasoning tasks. Notably, itaddresses the gaps in previous research regarding long-form QA tasks andChinese datasets. Moreover, we also evaluate the benefit of thechain-of-thought (CoT) in the RAFT method. This work offers valuable insightsfor studies focused on enhancing the performance of generative dialogue models."}], "temperature": 0.1}}
{"custom_id": "327_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yuetong Zhao"}], "temperature": 0.1}}
{"custom_id": "328_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "An Explainable Fast Deep Neural Network for Emotion Recognition"}], "temperature": 0.1}}
{"custom_id": "328_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In the context of artificial intelligence, the inherent human attribute ofengaging in logical reasoning to facilitate decision-making is mirrored by theconcept of explainability, which pertains to the ability of a model to providea clear and interpretable account of how it arrived at a particular outcome.This study explores explainability techniques for binary deep neuralarchitectures in the framework of emotion classification through videoanalysis. We investigate the optimization of input features to binaryclassifiers for emotion recognition, with face landmarks detection using animproved version of the Integrated Gradients explainability method. The maincontribution of this paper consists in the employment of an innovativeexplainable artificial intelligence algorithm to understand the crucial faciallandmarks movements during emotional feeling, using this information also forimproving the performances of deep learning-based emotion classifiers. By meansof explainability, we can optimize the number and the position of the faciallandmarks used as input features for facial emotion recognition, lowering theimpact of noisy landmarks and thus increasing the accuracy of the developedmodels. In order to test the effectiveness of the proposed approach, weconsidered a set of deep binary models for emotion classification trainedinitially with a complete set of facial landmarks, which are progressivelyreduced based on a suitable optimization procedure. The obtained results provethe robustness of the proposed explainable approach in terms of understandingthe relevance of the different facial points for the different emotions, alsoimproving the classification accuracy and diminishing the computational cost."}], "temperature": 0.1}}
{"custom_id": "328_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Francesco Di Luzio"}], "temperature": 0.1}}
{"custom_id": "329_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Leveraging large language models for nano synthesis mechanism explanation: solid foundations or mere conjectures?"}], "temperature": 0.1}}
{"custom_id": "329_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "With the rapid development of artificial intelligence (AI), large languagemodels (LLMs) such as GPT-4 have garnered significant attention in thescientific community, demonstrating great potential in advancing scientificdiscovery. This progress raises a critical question: are these LLMswell-aligned with real-world physicochemical principles? Current evaluationstrategies largely emphasize fact-based knowledge, such as material propertyprediction or name recognition, but they often lack an understanding offundamental physicochemical mechanisms that require logical reasoning. Tobridge this gap, our study developed a benchmark consisting of 775multiple-choice questions focusing on the mechanisms of gold nanoparticlesynthesis. By reflecting on existing evaluation metrics, we question whether adirect true-or-false assessment merely suggests conjecture. Hence, we propose anovel evaluation metric, the confidence-based score (c-score), which probes theoutput logits to derive the precise probability for the correct answer. Basedon extensive experiments, our results show that in the context of goldnanoparticle synthesis, LLMs understand the underlying physicochemicalmechanisms rather than relying on conjecture. This study underscores thepotential of LLMs to grasp intrinsic scientific mechanisms and sets the stagefor developing more reliable and effective AI tools across various scientificdomains."}], "temperature": 0.1}}
{"custom_id": "329_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yingming Pu"}], "temperature": 0.1}}
{"custom_id": "330_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hypergraph Multi-modal Large Language Model: Exploiting EEG and Eye-tracking Modalities to Evaluate Heterogeneous Responses for Video Understanding"}], "temperature": 0.1}}
{"custom_id": "330_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Understanding of video creativity and content often varies among individuals,with differences in focal points and cognitive levels across different ages,experiences, and genders. There is currently a lack of research in this area,and most existing benchmarks suffer from several drawbacks: 1) a limited numberof modalities and answers with restrictive length; 2) the content and scenarioswithin the videos are excessively monotonous, transmitting allegories andemotions that are overly simplistic. To bridge the gap to real-worldapplications, we introduce a large-scale Subjective Response Indicators forAdvertisement Videos dataset, namely SRI-ADV. Specifically, we collected realchanges in Electroencephalographic (EEG) and eye-tracking regions fromdifferent demographics while they viewed identical video content. Utilizingthis multi-modal dataset, we developed tasks and protocols to analyze andevaluate the extent of cognitive understanding of video content among differentusers. Along with the dataset, we designed a Hypergraph Multi-modal LargeLanguage Model (HMLLM) to explore the associations among differentdemographics, video elements, EEG, and eye-tracking indicators. HMLLM couldbridge semantic gaps across rich modalities and integrate information beyonddifferent modalities to perform logical reasoning. Extensive experimentalevaluations on SRI-ADV and other additional video-based generative performancebenchmarks demonstrate the effectiveness of our method. The codes and datasetwill be released at https://github.com/mininglamp-MLLM/HMLLM."}], "temperature": 0.1}}
{"custom_id": "330_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Minghui Wu"}], "temperature": 0.1}}
{"custom_id": "331_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Analyzing Large language models chatbots: An experimental approach using a probability test"}], "temperature": 0.1}}
{"custom_id": "331_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This study consists of qualitative empirical research, conducted throughexploratory tests with two different Large Language Models (LLMs) chatbots:ChatGPT and Gemini. The methodological procedure involved exploratory testsbased on prompts designed with a probability question. The \"Linda Problem\",widely recognized in cognitive psychology, was used as a basis to create thetests, along with the development of a new problem specifically for thisexperiment, the \"Mary Problem\". The object of analysis is the dataset with theoutputs provided by each chatbot interaction. The purpose of the analysis is toverify whether the chatbots mainly employ logical reasoning that aligns withprobability theory or if they are more frequently affected by the stereotypicaltextual descriptions in the prompts. The findings provide insights about theapproach each chatbot employs in handling logic and textual constructions,suggesting that, while the analyzed chatbots perform satisfactorily on awell-known probabilistic problem, they exhibit significantly lower performanceon new tests that require direct application of probabilistic logic."}], "temperature": 0.1}}
{"custom_id": "331_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Melise Peruchini"}], "temperature": 0.1}}
{"custom_id": "332_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Why should we ever automate moral decision making?"}], "temperature": 0.1}}
{"custom_id": "332_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "While people generally trust AI to make decisions in various aspects of theirlives, concerns arise when AI is involved in decisions with significant moralimplications. The absence of a precise mathematical framework for moralreasoning intensifies these concerns, as ethics often defies simplisticmathematical models. Unlike fields such as logical reasoning, reasoning underuncertainty, and strategic decision-making, which have well-definedmathematical frameworks, moral reasoning lacks a broadly accepted framework.This absence raises questions about the confidence we can place in AI's moraldecision-making capabilities.  The environments in which AI systems are typically trained today seeminsufficiently rich for such a system to learn ethics from scratch, and even ifwe had an appropriate environment, it is unclear how we might bring about suchlearning. An alternative approach involves AI learning from human moraldecisions. This learning process can involve aggregating curated humanjudgments or demonstrations in specific domains, or leveraging a foundationmodel fed with a wide range of data. Still, concerns persist, given theimperfections in human moral decision making.  Given this, why should we ever automate moral decision making -- is it notbetter to leave all moral decision making to humans? This paper lays out anumber of reasons why we should expect AI systems to engage in decisions with amoral component, with brief discussions of the associated risks."}], "temperature": 0.1}}
{"custom_id": "332_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Vincent Conitzer"}], "temperature": 0.1}}
{"custom_id": "333_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "$R^2$-Guard: Robust Reasoning Enabled LLM Guardrail via Knowledge-Enhanced Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "333_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "As LLMs become increasingly prevalent across various applications, it iscritical to establish safety guardrails to moderate input/output content ofLLMs. Existing guardrail models treat various safety categories independentlyand fail to explicitly capture the intercorrelations among them. This has ledto limitations such as ineffectiveness due to inadequate training on long-taildata from correlated safety categories, susceptibility to jailbreaking attacks,and inflexibility regarding new safety categories. To address theselimitations, we propose $R^2$-Guard, a robust reasoning enabled LLM guardrailvia knowledge-enhanced logical reasoning. Specifically, $R^2$-Guard comprisestwo parts: data-driven category-specific learning and reasoning components. Thedata-driven guardrail models provide unsafety probabilities of moderatedcontent on different safety categories. We then encode safety knowledge amongdifferent categories as first-order logical rules and embed them into aprobabilistic graphic model (PGM) based reasoning component. The unsafetyprobabilities of different categories from data-driven guardrail models aresent to the reasoning component for final inference. We employ two types ofPGMs: Markov logic networks (MLNs) and probabilistic circuits (PCs), andoptimize PCs to achieve precision-efficiency balance via improved graphstructure. To further perform stress tests for guardrail models, we employ apairwise construction method to construct a new safety benchmark TwinSafety,which features principled categories. We demonstrate the effectiveness of$R^2$-Guard by comparisons with eight strong guardrail models on six safetybenchmarks, and demonstrate the robustness of $R^2$-Guard against four SOTAjailbreaking attacks. $R^2$-Guard significantly surpasses SOTA methodLlamaGuard by 30.2% on ToxicChat and by 59.5% against jailbreaking attacks."}], "temperature": 0.1}}
{"custom_id": "333_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mintong Kang"}], "temperature": 0.1}}
{"custom_id": "334_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LTLBench: Towards Benchmarks for Evaluating Temporal Logic Reasoning in Large Language Models"}], "temperature": 0.1}}
{"custom_id": "334_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Temporal reasoning (TR) is a critical component of artificial intelligence,encompassing understanding and processing temporal information andrelationships between events. To discover and study the TR ability in LargeLanguage Models (LLMs), various datasets have been constructed in differentways for evaluating various aspects of TR ability. Our work proposes a novelapproach to design and develop a pipeline for constructing datasets to evaluatethe TR ability of LLMs by leveraging random directed graph generation, LTLformula, and the NuSMV model checker. Based on the pipeline, we have alsoconstructed a dataset as a benchmark, namely LTLBench, consisting of 2,000 TRchallenges and evaluated six LLMs with it. Furthermore, we have conductedadditional experiments to discover the impact of increasing the number ofevents and formula operators on the complexity of TR problems and theperformance of LLMs. We have demonstrated that although LLMs exhibit somepromise in handling TR challenges, they still struggle with complex TR. Weexpect this work can offer insights into TR ability in LLMs while alsoproviding a valuable tool for future TR evaluations."}], "temperature": 0.1}}
{"custom_id": "334_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Weizhi Tang"}], "temperature": 0.1}}
{"custom_id": "335_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "ElecBench: a Power Dispatch Evaluation Benchmark for Large Language Models"}], "temperature": 0.1}}
{"custom_id": "335_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In response to the urgent demand for grid stability and the complexchallenges posed by renewable energy integration and electricity marketdynamics, the power sector increasingly seeks innovative technologicalsolutions. In this context, large language models (LLMs) have become a keytechnology to improve efficiency and promote intelligent progress in the powersector with their excellent natural language processing, logical reasoning, andgeneralization capabilities. Despite their potential, the absence of aperformance evaluation benchmark for LLM in the power sector has limited theeffective application of these technologies. Addressing this gap, our studyintroduces \"ElecBench\", an evaluation benchmark of LLMs within the powersector. ElecBench aims to overcome the shortcomings of existing evaluationbenchmarks by providing comprehensive coverage of sector-specific scenarios,deepening the testing of professional knowledge, and enhancing decision-makingprecision. The framework categorizes scenarios into general knowledge andprofessional business, further divided into six core performance metrics:factuality, logicality, stability, security, fairness, and expressiveness, andis subdivided into 24 sub-metrics, offering profound insights into thecapabilities and limitations of LLM applications in the power sector. To ensuretransparency, we have made the complete test set public, evaluating theperformance of eight LLMs across various scenarios and metrics. ElecBenchaspires to serve as the standard benchmark for LLM applications in the powersector, supporting continuous updates of scenarios, metrics, and models todrive technological progress and application."}], "temperature": 0.1}}
{"custom_id": "335_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xiyuan Zhou"}], "temperature": 0.1}}
{"custom_id": "336_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts"}], "temperature": 0.1}}
{"custom_id": "336_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We propose LogicVista, an evaluation benchmark that assesses the integratedlogical reasoning capabilities of multimodal large language models (MLLMs) inVisual contexts. Recent advancements in MLLMs have demonstrated variousfascinating abilities, from crafting poetry based on an image to performingmathematical reasoning. However, there is still a lack of systematic evaluationof MLLMs' proficiency in logical reasoning tasks, which are essential foractivities like navigation and puzzle-solving. Thus we evaluate general logicalcognition abilities across 5 logical reasoning tasks encompassing 9 differentcapabilities, using a sample of 448 multiple-choice questions. Each question isannotated with the correct answer and the human-written reasoning behind theselection, enabling both open-ended and multiple-choice evaluation. A total of8 MLLMs are comprehensively evaluated using LogicVista. Code and Data Availableat https://github.com/Yijia-Xiao/LogicVista."}], "temperature": 0.1}}
{"custom_id": "336_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yijia Xiao"}], "temperature": 0.1}}
{"custom_id": "337_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Combining Classical and Probabilistic Independence Reasoning to Verify the Security of Oblivious Algorithms (Extended Version)"}], "temperature": 0.1}}
{"custom_id": "337_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We consider the problem of how to verify the security of probabilisticoblivious algorithms formally and systematically. Unfortunately, prior programlogics fail to support a number of complexities that feature in the semanticsand invariant needed to verify the security of many practical probabilisticoblivious algorithms. We propose an approach based on reasoning over perfectlyoblivious approximations, using a program logic that combines both classicalHoare logic reasoning and probabilistic independence reasoning to support allthe needed features. We formalise and prove our new logic sound in Isabelle/HOLand apply our approach to formally verify the security of several challengingcase studies beyond the reach of prior methods for proving obliviousness."}], "temperature": 0.1}}
{"custom_id": "337_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Pengbo Yan"}], "temperature": 0.1}}
{"custom_id": "338_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "PUZZLES: A Benchmark for Neural Algorithmic Reasoning"}], "temperature": 0.1}}
{"custom_id": "338_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Algorithmic reasoning is a fundamental cognitive ability that plays a pivotalrole in problem-solving and decision-making processes. Reinforcement Learning(RL) has demonstrated remarkable proficiency in tasks such as motor control,handling perceptual input, and managing stochastic environments. Theseadvancements have been enabled in part by the availability of benchmarks. Inthis work we introduce PUZZLES, a benchmark based on Simon Tatham's PortablePuzzle Collection, aimed at fostering progress in algorithmic and logicalreasoning in RL. PUZZLES contains 40 diverse logic puzzles of adjustable sizesand varying levels of complexity; many puzzles also feature a diverse set ofadditional configuration parameters. The 40 puzzles provide detailedinformation on the strengths and generalization capabilities of RL agents.Furthermore, we evaluate various RL algorithms on PUZZLES, providing baselinecomparisons and demonstrating the potential for future research. All thesoftware, including the environment, is available athttps://github.com/ETH-DISCO/rlp."}], "temperature": 0.1}}
{"custom_id": "338_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Benjamin Estermann"}], "temperature": 0.1}}
{"custom_id": "339_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "FlowVQA: Mapping Multimodal Logic in Visual Question Answering with Flowcharts"}], "temperature": 0.1}}
{"custom_id": "339_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Existing benchmarks for visual question answering lack in visual groundingand complexity, particularly in evaluating spatial reasoning skills. Weintroduce FlowVQA, a novel benchmark aimed at assessing the capabilities ofvisual question-answering multimodal language models in reasoning withflowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated andhuman-verified flowchart images from three distinct content sources, along with22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,including information localization, decision-making, and logical progression.We conduct a thorough baseline evaluation on a suite of both open-source andproprietary multimodal language models using various strategies, followed by ananalysis of directional bias. The results underscore the benchmark's potentialas a vital tool for advancing the field of multimodal modeling, providing afocused and challenging environment for enhancing model performance in visualand logical reasoning tasks."}], "temperature": 0.1}}
{"custom_id": "339_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Shubhankar Singh"}], "temperature": 0.1}}
{"custom_id": "340_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "CharED: Character-wise Ensemble Decoding for Large Language Models"}], "temperature": 0.1}}
{"custom_id": "340_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have shown remarkable potential for problemsolving, with open source models achieving increasingly impressive performanceon benchmarks measuring areas from logical reasoning to mathematical ability.Ensembling models can further improve capabilities across a variety of domains.However, conventional methods of combining models at inference time such asshallow fusion necessitate a shared vocabulary and tokenization, andalternatives like fine-tuning for domain-specific performance are both timeconsuming and computationally expensive. We therefore present an inference-timeensembling algorithm aimed at \"averaging\" outputs from multiple LLMs andillustrate its improved performance across multiple domains compared to itsconstituent models alone. Character-wise ensemble decoding, CharED, finds themarginal distribution of each character for an individual model and performs aweighted average to generate an output, character by character. In coding,math, and toxicity benchmarks, we find our proposed model able to combinecomplimentary strengths of multiple LLMs, regardless of vocabulary,tokenization, or model size."}], "temperature": 0.1}}
{"custom_id": "340_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Kevin Gu"}], "temperature": 0.1}}
{"custom_id": "341_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic"}], "temperature": 0.1}}
{"custom_id": "341_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We introduce LLM-ARC, a neuro-symbolic framework designed to enhance thelogical reasoning capabilities of Large Language Models (LLMs), by combiningthem with an Automated Reasoning Critic (ARC). LLM-ARC employs an Actor-Criticmethod where the LLM Actor generates declarative logic programs along withtests for semantic correctness, while the Automated Reasoning Critic evaluatesthe code, runs the tests and provides feedback on test failures for iterativerefinement. Implemented using Answer Set Programming (ASP), LLM-ARC achieves anew state-of-the-art accuracy of 88.32% on the FOLIO benchmark which testscomplex logical reasoning capabilities. Our experiments demonstrate significantimprovements over LLM-only baselines, highlighting the importance of logic testgeneration and iterative self-refinement. We achieve our best result using afully automated self-supervised training loop where the Actor is trained onend-to-end dialog traces with Critic feedback. We discuss potentialenhancements and provide a detailed error analysis, showcasing the robustnessand efficacy of LLM-ARC for complex natural language reasoning tasks."}], "temperature": 0.1}}
{"custom_id": "341_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Aditya Kalyanpur"}], "temperature": 0.1}}
{"custom_id": "342_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Pathformer: Recursive Path Query Encoding for Complex Logical Query Answering"}], "temperature": 0.1}}
{"custom_id": "342_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Complex Logical Query Answering (CLQA) over incomplete knowledge graphs is achallenging task. Recently, Query Embedding (QE) methods are proposed to solveCLQA by performing multi-hop logical reasoning. However, most of them onlyconsider historical query context information while ignoring futureinformation, which leads to their failure to capture the complex dependenciesbehind the elements of a query. In recent years, the transformer architecturehas shown a strong ability to model long-range dependencies between words. Thebidirectional attention mechanism proposed by the transformer can solve thelimitation of these QE methods regarding query context. Still, as a sequencemodel, it is difficult for the transformer to model complex logical querieswith branch structure computation graphs directly. To this end, we propose aneural one-point embedding method called Pathformer based on the tree-likecomputation graph, i.e., query computation tree. Specifically, Pathformerdecomposes the query computation tree into path query sequences by branches andthen uses the transformer encoder to recursively encode these path querysequences to obtain the final query embedding. This allows Pathformer to fullyutilize future context information to explicitly model the complex interactionsbetween various parts of the path query. Experimental results show thatPathformer outperforms existing competitive neural QE methods, and we foundthat Pathformer has the potential to be applied to non-one-point embeddingspace."}], "temperature": 0.1}}
{"custom_id": "342_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chongzhi Zhang"}], "temperature": 0.1}}
{"custom_id": "343_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The neural correlates of logical-mathematical symbol systems processing resemble that of spatial cognition more than natural language processing"}], "temperature": 0.1}}
{"custom_id": "343_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The ability to manipulate logical-mathematical symbols (LMS), encompassingtasks such as calculation, reasoning, and programming, is a cognitive skillarguably unique to humans. Considering the relatively recent emergence of thisability in human evolutionary history, it has been suggested that LMSprocessing may build upon more fundamental cognitive systems, possibly throughneuronal recycling. Previous studies have pinpointed two primary candidates,natural language processing and spatial cognition. Existing comparisons betweenthese domains largely relied on task-level comparison, which may be confoundedby task idiosyncrasy. The present study instead compared the neural correlatesat the domain level with both automated meta-analysis and synthesized mapsbased on three representative LMS tasks, reasoning, calculation, and mentalprogramming. Our results revealed a more substantial cortical overlap betweenLMS processing and spatial cognition, in contrast to language processing.Furthermore, in regions activated by both spatial and language processing, themultivariate activation pattern for LMS processing exhibited greatermultivariate similarity to spatial cognition than to language processing. Ahierarchical clustering analysis further indicated that typical LMS tasks wereindistinguishable from spatial cognition tasks at the neural level, suggestingan inherent connection between these two cognitive processes. Taken together,our findings support the hypothesis that spatial cognition is likely the basisof LMS processing, which may shed light on the limitations of large languagemodels in logical reasoning, particularly those trained exclusively on textualdata without explicit emphasis on spatial content."}], "temperature": 0.1}}
{"custom_id": "343_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yuannan Li"}], "temperature": 0.1}}
{"custom_id": "344_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Program Synthesis Benchmark for Visual Programming in XLogoOnline Environment"}], "temperature": 0.1}}
{"custom_id": "344_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language and multimodal models have shown remarkable successes onvarious benchmarks focused on specific skills such as general-purposeprogramming, natural language understanding, math word problem-solving, andvisual question answering. However, it is unclear how well these models performon tasks that require a combination of these skills. In this paper, we curate anovel program synthesis benchmark based on the XLogoOnline visual programmingenvironment. The benchmark comprises 85 real-world tasks from the Mini-level ofthe XLogoOnline environment, each requiring a combination of different skillssuch as spatial planning, basic programming, and logical reasoning. Ourevaluation shows that current state-of-the-art models like GPT-4V andLlama3-70B struggle to solve these tasks, achieving only 20% and 2.35% successrates. Next, we develop a fine-tuning pipeline to boost the performance ofmodels by leveraging a large-scale synthetic training dataset with over 80000tasks. Moreover, we showcase how emulator-driven feedback can be used to designa curriculum over training data distribution. We showcase that a fine-tunedLlama3-8B drastically outperforms GPT-4V and Llama3-70B models, and provide anin-depth analysis of the models' expertise across different skill dimensions.We will publicly release the benchmark for future research on program synthesisin visual programming."}], "temperature": 0.1}}
{"custom_id": "344_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chao Wen"}], "temperature": 0.1}}
{"custom_id": "345_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "VideoVista: A Versatile Benchmark for Video Understanding and Reasoning"}], "temperature": 0.1}}
{"custom_id": "345_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Despite significant breakthroughs in video analysis driven by the rapiddevelopment of large multimodal models (LMMs), there remains a lack of aversatile evaluation benchmark to comprehensively assess these models'performance in video understanding and reasoning. To address this, we presentVideoVista, a video QA benchmark that integrates challenges across diversecontent categories, durations, and abilities. Specifically, VideoVistacomprises 25,000 questions derived from 3,400 videos spanning 14 categories(e.g., Howto, Film, and Entertainment) with durations ranging from a fewseconds to over 10 minutes. Besides, it encompasses 19 types of understandingtasks (e.g., anomaly detection, interaction understanding) and 8 reasoningtasks (e.g., logical reasoning, causal reasoning). To achieve this, we presentan automatic data construction framework, leveraging powerful GPT-4o alongsideadvanced analysis tools (e.g., video splitting, object segmenting, andtracking). We also utilize this framework to construct training data to enhancethe capabilities of video-related LMMs (Video-LMMs). Through a comprehensiveand quantitative evaluation of cutting-edge models, we reveal that: 1)Video-LMMs face difficulties in fine-grained video tasks involving temporallocation, object tracking, and anomaly detection; 2) Video-LMMs presentinferior logical and relation reasoning abilities; 3) Open-source Video-LMMs'performance is significantly lower than GPT-4o and Gemini-1.5, lagging by 20points. This highlights the crucial role VideoVista will play in advancing LMMsthat can accurately understand videos and perform precise reasoning."}], "temperature": 0.1}}
{"custom_id": "345_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yunxin Li"}], "temperature": 0.1}}
{"custom_id": "346_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars"}], "temperature": 0.1}}
{"custom_id": "346_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning remains a challenge for natural language processing, but itcan be improved by training language models to mimic theorem provers onprocedurally generated problems. Previous work used domain-specific proofgeneration algorithms, which biases reasoning toward specific proof traces andlimits auditability and extensibility. We present a simpler and more generaldeclarative framework with flexible context-sensitive rules binding multiplelanguages (specifically, simplified English and the TPTP theorem-provinglanguage). We construct first-order logic problems by selecting up to 32premises and one hypothesis. We demonstrate that using semantic constraintsduring generation and careful English verbalization of predicates enhanceslogical reasoning without hurting natural English tasks. We use relativelysmall DeBERTa-v3 models to achieve state-of-the-art accuracy on the FOLIOhuman-authored logic dataset, surpassing GPT-4 in accuracy with or without anexternal solver by 12%."}], "temperature": 0.1}}
{"custom_id": "346_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Damien Sileo"}], "temperature": 0.1}}
{"custom_id": "347_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "City-LEO: Toward Transparent City Management Using LLM with End-to-End Optimization"}], "temperature": 0.1}}
{"custom_id": "347_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Existing operations research (OR) models and tools play indispensable rolesin smart-city operations, yet their practical implementation is limited by thecomplexity of modeling and deficiencies in optimization proficiency. Togenerate more relevant and accurate solutions to users' requirements, wepropose a large language model (LLM)-based agent (\"City-LEO\") that enhances theefficiency and transparency of city management through conversationalinteractions. Specifically, to accommodate diverse users' requirements andenhance computational tractability, City-LEO leverages LLM's logical reasoningcapabilities on prior knowledge to scope down large-scale optimization problemsefficiently. In the human-like decision process, City-LEO also incorporatesEnd-to-end (E2E) model to synergize the prediction and optimization. The E2Eframework be conducive to coping with environmental uncertainties and involvingmore query-relevant features, and then facilitates transparent andinterpretable decision-making process. In case study, we employ City-LEO in theoperations management of e-bike sharing (EBS) system. The numerical resultsdemonstrate that City-LEO has superior performance when benchmarks against thefull-scale optimization problem. With less computational time, City-LEOgenerates more satisfactory and relevant solutions to the users' requirements,and achieves lower global suboptimality without significantly compromisingaccuracy. In a broader sense, our proposed agent offers promise to developLLM-embedded OR tools for smart-city operations management."}], "temperature": 0.1}}
{"custom_id": "347_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zihao Jiao"}], "temperature": 0.1}}
{"custom_id": "348_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Evaluating ChatGPT-4 Vision on Brazil's National Undergraduate Computer Science Exam"}], "temperature": 0.1}}
{"custom_id": "348_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The recent integration of visual capabilities into Large Language Models(LLMs) has the potential to play a pivotal role in science and technologyeducation, where visual elements such as diagrams, charts, and tables arecommonly used to improve the learning experience. This study investigates theperformance of ChatGPT-4 Vision, OpenAI's most advanced visual model at thetime the study was conducted, on the Bachelor in Computer Science section ofBrazil's 2021 National Undergraduate Exam (ENADE). By presenting the model withthe exam's open and multiple-choice questions in their original image formatand allowing for reassessment in response to differing answer keys, we wereable to evaluate the model's reasoning and self-reflecting capabilities in alarge-scale academic assessment involving textual and visual content. ChatGPT-4Vision significantly outperformed the average exam participant, positioningitself within the top 10 best score percentile. While it excelled in questionsthat incorporated visual elements, it also encountered challenges with questioninterpretation, logical reasoning, and visual acuity. The involvement of anindependent expert panel to review cases of disagreement between the model andthe answer key revealed some poorly constructed questions containing vague orambiguous statements, calling attention to the critical need for improvedquestion design in future exams. Our findings suggest that while ChatGPT-4Vision shows promise in multimodal academic evaluations, human oversightremains crucial for verifying the model's accuracy and ensuring the fairness ofhigh-stakes educational exams. The paper's research materials are publiclyavailable at https://github.com/nabormendonca/gpt-4v-enade-cs-2021."}], "temperature": 0.1}}
{"custom_id": "348_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Nabor C. Mendon莽a"}], "temperature": 0.1}}
{"custom_id": "349_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Improving Multi-hop Logical Reasoning in Knowledge Graphs with Context-Aware Query Representation Learning"}], "temperature": 0.1}}
{"custom_id": "349_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Multi-hop logical reasoning on knowledge graphs is a pivotal task in naturallanguage processing, with numerous approaches aiming to answer First-OrderLogic (FOL) queries. Recent geometry (e.g., box, cone) and probability (e.g.,beta distribution)-based methodologies have effectively addressed complex FOLqueries. However, a common challenge across these methods lies in determiningaccurate geometric bounds or probability parameters for these queries. Thechallenge arises because existing methods rely on linear sequential operationswithin their computation graphs, overlooking the logical structure of the queryand the relation-induced information that can be gleaned from the relations ofthe query, which we call the context of the query. To address the problem, wepropose a model-agnostic methodology that enhances the effectiveness ofexisting multi-hop logical reasoning approaches by fully integrating thecontext of the FOL query graph. Our approach distinctively discerns (1) thestructural context inherent to the query structure and (2) the relation-inducedcontext unique to each node in the query graph as delineated in thecorresponding knowledge graph. This dual-context paradigm helps nodes within aquery graph attain refined internal representations throughout the multi-hopreasoning steps. Through experiments on two datasets, our method consistentlyenhances the three multi-hop reasoning foundation models, achieving performanceimprovements of up to 19.5%. Our code is available athttps://github.com/kjh9503/caqr."}], "temperature": 0.1}}
{"custom_id": "349_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jeonghoon Kim"}], "temperature": 0.1}}
{"custom_id": "350_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Investigating and Addressing Hallucinations of LLMs in Tasks Involving Negation"}], "temperature": 0.1}}
{"custom_id": "350_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have achieved remarkable performance across awide variety of natural language tasks. However, they have been shown to sufferfrom a critical limitation pertinent to 'hallucination' in their output. Recentresearch has focused on investigating and addressing this problem for a varietyof tasks such as biography generation, question answering, abstractivesummarization, and dialogue generation. However, the crucial aspect pertainingto 'negation' has remained considerably underexplored. Negation is importantbecause it adds depth and nuance to the understanding of language and is alsocrucial for logical reasoning and inference. In this work, we address the abovelimitation and particularly focus on studying the impact of negation in LLMhallucinations. Specifically, we study four tasks with negation: 'false premisecompletion', 'constrained fact generation', 'multiple choice questionanswering', and 'fact generation'. We show that open-source state-of-the-artLLMs such as LLaMA-2-chat, Vicuna, and Orca-2 hallucinate considerably on allthese tasks involving negation which underlines a critical shortcoming of thesemodels. Addressing this problem, we further study numerous strategies tomitigate these hallucinations and demonstrate their impact."}], "temperature": 0.1}}
{"custom_id": "350_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neeraj Varshney"}], "temperature": 0.1}}
{"custom_id": "351_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LogiCode: an LLM-Driven Framework for Logical Anomaly Detection"}], "temperature": 0.1}}
{"custom_id": "351_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This paper presents LogiCode, a novel framework that leverages Large LanguageModels (LLMs) for identifying logical anomalies in industrial settings, movingbeyond traditional focus on structural inconsistencies. By harnessing LLMs forlogical reasoning, LogiCode autonomously generates Python codes to pinpointanomalies such as incorrect component quantities or missing elements, marking asignificant leap forward in anomaly detection technologies. A custom dataset\"LOCO-Annotations\" and a benchmark \"LogiBench\" are introduced to evaluate theLogiCode's performance across various metrics including binary classificationaccuracy, code generation success rate, and precision in reasoning. Findingsdemonstrate LogiCode's enhanced interpretability, significantly improving theaccuracy of logical anomaly detection and offering detailed explanations foridentified anomalies. This represents a notable shift towards more intelligent,LLM-driven approaches in industrial anomaly detection, promising substantialimpacts on industry-specific applications."}], "temperature": 0.1}}
{"custom_id": "351_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yiheng Zhang"}], "temperature": 0.1}}
{"custom_id": "352_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "On the Hardness of Probabilistic Neurosymbolic Learning"}], "temperature": 0.1}}
{"custom_id": "352_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The limitations of purely neural learning have sparked an interest inprobabilistic neurosymbolic models, which combine neural networks withprobabilistic logical reasoning. As these neurosymbolic models are trained withgradient descent, we study the complexity of differentiating probabilisticreasoning. We prove that although approximating these gradients is intractablein general, it becomes tractable during training. Furthermore, we introduceWeightME, an unbiased gradient estimator based on model sampling. Under mildassumptions, WeightME approximates the gradient with probabilistic guaranteesusing a logarithmic number of calls to a SAT solver. Lastly, we evaluate thenecessity of these guarantees on the gradient. Our experiments indicate thatthe existing biased approximations indeed struggle to optimize even when exactsolving is still feasible."}], "temperature": 0.1}}
{"custom_id": "352_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jaron Maene"}], "temperature": 0.1}}
{"custom_id": "353_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Bi-Chainer: Automated Large Language Models Reasoning with Bidirectional Chaining"}], "temperature": 0.1}}
{"custom_id": "353_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have shown human-like reasoning abilities butstill face challenges in solving complex logical problems. Existingunidirectional chaining methods, such as forward chaining and backwardchaining, suffer from issues like low prediction accuracy and efficiency. Toaddress these, we propose a bidirectional chaining method, Bi-Chainer, whichdynamically switches to depth-first reasoning in the opposite reasoningdirection when it encounters multiple branching options within the currentdirection. Thus, the intermediate reasoning results can be utilized as guidanceto facilitate the reasoning process. We show that Bi-Chainer achieves sizableaccuracy boots over unidirectional chaining frameworks on four challenginglogical reasoning datasets. Moreover, Bi-Chainer enhances the accuracy ofintermediate proof steps and reduces the average number of inference calls,resulting in more efficient and accurate reasoning."}], "temperature": 0.1}}
{"custom_id": "353_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Shuqi Liu"}], "temperature": 0.1}}
{"custom_id": "354_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities"}], "temperature": 0.1}}
{"custom_id": "354_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This study intends to systematically disentangle pure logic reasoning andtext understanding by investigating the contrast across abstract andcontextualized logical problems from a comprehensive set of domains. We explorewhether LLMs demonstrate genuine reasoning capabilities across various domainswhen the underlying logical structure remains constant. We focus on two mainquestions (1) Can abstract logical problems alone accurately benchmark an LLM'sreasoning ability in real-world scenarios, disentangled from contextual supportin practical settings? (2) Does fine-tuning LLMs on abstract logic problemgeneralize to contextualized logic problems and vice versa? To investigatethese questions, we focus on standard propositional logic, specificallypropositional deductive and abductive logic reasoning. In particular, weconstruct instantiated datasets for deductive and abductive reasoning with 4levels of difficulty, encompassing 12 distinct categories or domains based onthe categorization of Wikipedia. Our experiments aim to provide insights intodisentangling context in logical reasoning and the true reasoning capabilitiesof LLMs and their generalization potential. The code and dataset are availableat: https://github.com/agiresearch/ContextHub."}], "temperature": 0.1}}
{"custom_id": "354_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Wenyue Hua"}], "temperature": 0.1}}
{"custom_id": "355_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Explicitly Encoding Structural Symmetry is Key to Length Generalization in Arithmetic Tasks"}], "temperature": 0.1}}
{"custom_id": "355_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Despite the success of Transformers on language understanding, codegeneration, and logical reasoning, they still fail to generalize over length onbasic arithmetic tasks such as addition and multiplication. A major reasonbehind this failure is the vast difference in structure between numbers andtext; For example, the numbers are typically parsed from right to left, andthere is a correspondence between digits at the same position across differentnumbers. In contrast, for text, such symmetries are quite unnatural. In thiswork, we propose to encode these semantics explicitly into the model viamodified number formatting and custom positional encodings. Empirically, ourmethod allows a Transformer trained on numbers with at most 5-digits foraddition and multiplication to generalize up to 50-digit numbers, without usingadditional data for longer sequences. We further demonstrate that traditionalabsolute positional encodings (APE) fail to generalize to longer sequences,even when trained with augmented data that captures task symmetries. Toelucidate the importance of explicitly encoding structure, we prove thatexplicit incorporation of structure via positional encodings is necessary forout-of-distribution generalization. Finally, we pinpoint other challengesinherent to length generalization beyond capturing symmetries, in particularcomplexity of the underlying task, and propose changes in the trainingdistribution to address them."}], "temperature": 0.1}}
{"custom_id": "355_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mahdi Sabbaghi"}], "temperature": 0.1}}
{"custom_id": "356_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical Reasoning with Relation Network for Inductive Knowledge Graph Completion"}], "temperature": 0.1}}
{"custom_id": "356_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Inductive knowledge graph completion (KGC) aims to infer the missing relationfor a set of newly-coming entities that never appeared in the training set.Such a setting is more in line with reality, as real-world KGs are constantlyevolving and introducing new knowledge. Recent studies have shown promisingresults using message passing over subgraphs to embed newly-coming entities forinductive KGC. However, the inductive capability of these methods is usuallylimited by two key issues. (i) KGC always suffers from data sparsity, and thesituation is even exacerbated in inductive KGC where new entities often havefew or no connections to the original KG. (ii) Cold-start problem. It is overcoarse-grained for accurate KG reasoning to generate representations for newentities by gathering the local information from few neighbors. To this end, wepropose a novel iNfOmax RelAtion Network, namely NORAN, for inductive KGcompletion. It aims to mine latent relation patterns for inductive KGcompletion. Specifically, by centering on relations, NORAN provides a hyperview towards KG modeling, where the correlations between relations can benaturally captured as entity-independent logical evidence to conduct inductiveKGC. Extensive experiment results on five benchmarks show that our frameworksubstantially outperforms the state-of-the-art KGC methods."}], "temperature": 0.1}}
{"custom_id": "356_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Qinggang Zhang"}], "temperature": 0.1}}
{"custom_id": "357_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Synergistic Approach In Network Intrusion Detection By Neurosymbolic AI"}], "temperature": 0.1}}
{"custom_id": "357_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The prevailing approaches in Network Intrusion Detection Systems (NIDS) areoften hampered by issues such as high resource consumption, significantcomputational demands, and poor interpretability. Furthermore, these systemsgenerally struggle to identify novel, rapidly changing cyber threats. Thispaper delves into the potential of incorporating Neurosymbolic ArtificialIntelligence (NSAI) into NIDS, combining deep learning's data-driven strengthswith symbolic AI's logical reasoning to tackle the dynamic challenges incybersecurity, which also includes detailed NSAI techniques introduction forcyber professionals to explore the potential strengths of NSAI in NIDS. Theinclusion of NSAI in NIDS marks potential advancements in both the detectionand interpretation of intricate network threats, benefiting from the robustpattern recognition of neural networks and the interpretive prowess of symbolicreasoning. By analyzing network traffic data types and machine learningarchitectures, we illustrate NSAI's distinctive capability to offer moreprofound insights into network behavior, thereby improving both detectionperformance and the adaptability of the system. This merging of technologiesnot only enhances the functionality of traditional NIDS but also sets the stagefor future developments in building more resilient, interpretable, and dynamicdefense mechanisms against advanced cyber threats. The continued progress inthis area is poised to transform NIDS into a system that is both responsive toknown threats and anticipatory of emerging, unseen ones."}], "temperature": 0.1}}
{"custom_id": "357_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Alice Bizzarri"}], "temperature": 0.1}}
{"custom_id": "358_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Brainstorming Brings Power to Large Language Models of Knowledge Reasoning"}], "temperature": 0.1}}
{"custom_id": "358_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have demonstrated amazing capabilities inlanguage generation, text comprehension, and knowledge reasoning. While asingle powerful model can already handle multiple tasks, relying on a singleperspective can lead to biased and unstable results. Recent studies havefurther improved the model's reasoning ability on a wide range of tasks byintroducing multi-model collaboration. However, models with differentcapabilities may produce conflicting answers on the same problem, and how toreasonably obtain the correct answer from multiple candidate models has becomea challenging problem. In this paper, we propose the multi-model brainstormingbased on prompt. It incorporates different models into a group forbrainstorming, and after multiple rounds of reasoning elaboration andre-inference, a consensus answer is reached within the group. We conductedexperiments on three different types of datasets, and demonstrate that thebrainstorming can significantly improve the effectiveness in logical reasoningand fact extraction. Furthermore, we find that two small-parameter models canachieve accuracy approximating that of larger-parameter models throughbrainstorming, which provides a new solution for distributed deployment ofLLMs."}], "temperature": 0.1}}
{"custom_id": "358_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zining Qin"}], "temperature": 0.1}}
{"custom_id": "359_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Closer Look at Logical Reasoning with LLMs: The Choice of Tool Matters"}], "temperature": 0.1}}
{"custom_id": "359_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The emergence of Large Language Models (LLMs) has demonstrated promisingprogress in solving logical reasoning tasks effectively. Several recentapproaches have proposed to change the role of the LLM from the reasoner into atranslator between natural language statements and symbolic representationswhich are then sent to external symbolic solvers to resolve. This paradigm hasestablished the current state-of-the-art result in logical reasoning (i.e.,deductive reasoning). However, it remains unclear whether the variance inperformance of these approaches stems from the methodologies employed or thespecific symbolic solvers utilized. There is a lack of consistent comparisonbetween symbolic solvers and how they influence the overall reportedperformance. This is important, as each symbolic solver also has its own inputsymbolic language, presenting varying degrees of challenge in the translationprocess. To address this gap, we perform experiments on 3 deductive reasoningbenchmarks with LLMs augmented with widely used symbolic solvers: Z3, Pyke, andProver9. The tool-executable rates of symbolic translation generated bydifferent LLMs exhibit a near 50% performance variation. This highlights asignificant difference in performance rooted in very basic choices of tools.The almost linear correlation between the executable rate of translations andthe accuracy of the outcomes from Prover9 highlight a strong alignment betweenLLMs ability to translate into Prover9 symbolic language, and the correctnessof those translations."}], "temperature": 0.1}}
{"custom_id": "359_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Long Hei Matthew Lam"}], "temperature": 0.1}}
{"custom_id": "360_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Easy Problems That LLMs Get Wrong"}], "temperature": 0.1}}
{"custom_id": "360_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We introduce a comprehensive Linguistic Benchmark designed to evaluate thelimitations of Large Language Models (LLMs) in domains such as logicalreasoning, spatial intelligence, and linguistic understanding, among others.Through a series of straightforward questions, it uncovers the significantlimitations of well-regarded models to perform tasks that humans manage withease. It also highlights the potential of prompt engineering to mitigate someerrors and underscores the necessity for better training methodologies. Ourfindings stress the importance of grounding LLMs with human reasoning andcommon sense, emphasising the need for human-in-the-loop for enterpriseapplications. We hope this work paves the way for future research to enhancethe usefulness and reliability of new models."}], "temperature": 0.1}}
{"custom_id": "360_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Sean Williams"}], "temperature": 0.1}}
{"custom_id": "361_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "PathReasoner: Modeling Reasoning Path with Equivalent Extension for Logical Question Answering"}], "temperature": 0.1}}
{"custom_id": "361_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning task has attracted great interest since it was proposed.Faced with such a task, current competitive models, even large language models(e.g., ChatGPT and PaLM 2), still perform badly. Previous promising LMsstruggle in logical consistency modeling and logical structure perception. Tothis end, we model the logical reasoning task by transforming each logicalsample into reasoning paths and propose an architecture \\textbf{PathReasoner}.It addresses the task from the views of both data and model. To expand thediversity of the logical samples, we propose an atom extension strategysupported by equivalent logical formulas, to form new reasoning paths. From themodel perspective, we design a stack of transformer-style blocks. Inparticular, we propose a path-attention module to joint model in-atom andcross-atom relations with the high-order diffusion strategy. Experiments showthat PathReasoner achieves competitive performances on two logical reasoningbenchmarks and great generalization abilities."}], "temperature": 0.1}}
{"custom_id": "361_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Fangzhi Xu"}], "temperature": 0.1}}
{"custom_id": "362_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Is a 3D-Tokenized LLM the Key to Reliable Autonomous Driving?"}], "temperature": 0.1}}
{"custom_id": "362_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Rapid advancements in Autonomous Driving (AD) tasks turned a significantshift toward end-to-end fashion, particularly in the utilization ofvision-language models (VLMs) that integrate robust logical reasoning andcognitive abilities to enable comprehensive end-to-end planning. However, theseVLM-based approaches tend to integrate 2D vision tokenizers and a largelanguage model (LLM) for ego-car planning, which lack 3D geometric priors as acornerstone of reliable planning. Naturally, this observation raises a criticalconcern: Can a 2D-tokenized LLM accurately perceive the 3D environment? Ourevaluation of current VLM-based methods across 3D object detection, vectorizedmap construction, and environmental caption suggests that the answer is,unfortunately, NO. In other words, 2D-tokenized LLM fails to provide reliableautonomous driving. In response, we introduce DETR-style 3D perceptrons as 3Dtokenizers, which connect LLM with a one-layer linear projector. This simpleyet elegant strategy, termed Atlas, harnesses the inherent priors of the 3Dphysical world, enabling it to simultaneously process high-resolutionmulti-view images and employ spatiotemporal modeling. Despite its simplicity,Atlas demonstrates superior performance in both 3D detection and ego planningtasks on nuScenes dataset, proving that 3D-tokenized LLM is the key to reliableautonomous driving. The code and datasets will be released."}], "temperature": 0.1}}
{"custom_id": "362_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yifan Bai"}], "temperature": 0.1}}
{"custom_id": "363_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Faithful Logical Reasoning via Symbolic Chain-of-Thought"}], "temperature": 0.1}}
{"custom_id": "363_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "While the recent Chain-of-Thought (CoT) technique enhances the reasoningability of large language models (LLMs) with the theory of mind, it might stillstruggle in handling logical reasoning that relies much on symbolic expressionsand rigid deducing rules. To strengthen the logical reasoning capability ofLLMs, we propose a novel Symbolic Chain-of-Thought, namely SymbCoT, a fullyLLM-based framework that integrates symbolic expressions and logic rules withCoT prompting. Technically, building upon an LLM, SymbCoT 1) first translatesthe natural language context into the symbolic format, and then 2) derives astep-by-step plan to solve the problem with symbolic logical rules, 3) followedby a verifier to check the translation and reasoning chain. Via thoroughevaluations on 5 standard datasets with both First-Order Logic and ConstraintOptimization symbolic expressions, SymbCoT shows striking improvements over theCoT method consistently, meanwhile refreshing the current state-of-the-artperformances. We further demonstrate that our system advances in more faithful,flexible, and explainable logical reasoning. To our knowledge, this is thefirst to combine symbolic expressions and rules into CoT for logical reasoningwith LLMs. Code is open at https://github.com/Aiden0526/SymbCoT."}], "temperature": 0.1}}
{"custom_id": "363_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jundong Xu"}], "temperature": 0.1}}
{"custom_id": "364_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "STAR: A Benchmark for Situated Reasoning in Real-World Videos"}], "temperature": 0.1}}
{"custom_id": "364_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning in the real world is not divorced from situations. How to capturethe present knowledge from surrounding situations and perform reasoningaccordingly is crucial and challenging for machine intelligence. This paperintroduces a new benchmark that evaluates the situated reasoning ability viasituation abstraction and logic-grounded question answering for real-worldvideos, called Situated Reasoning in Real-World Videos (STAR Benchmark). Thisbenchmark is built upon the real-world videos associated with human actions orinteractions, which are naturally dynamic, compositional, and logical. Thedataset includes four types of questions, including interaction, sequence,prediction, and feasibility. We represent the situations in real-world videosby hyper-graphs connecting extracted atomic entities and relations (e.g.,actions, persons, objects, and relationships). Besides visual perception,situated reasoning also requires structured situation comprehension and logicalreasoning. Questions and answers are procedurally generated. The answeringlogic of each question is represented by a functional program based on asituation hyper-graph. We compare various existing video reasoning models andfind that they all struggle on this challenging situated reasoning task. Wefurther propose a diagnostic neuro-symbolic model that can disentangle visualperception, situation abstraction, language understanding, and functionalreasoning to understand the challenges of this benchmark."}], "temperature": 0.1}}
{"custom_id": "364_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Bo Wu"}], "temperature": 0.1}}
{"custom_id": "365_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "MathDivide: Improved mathematical reasoning by large language models"}], "temperature": 0.1}}
{"custom_id": "365_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models have been proven to be capable of handling complexlinguistic and cognitive tasks. Therefore their usage has been extended totasks requiring logical reasoning ability such as Mathematics. In this paper,we propose a prompting technique called MathDivide that breaks down themathematical problem into simpler subproblems. Each of the subproblems isformulated as an algebraic expression whose value is evaluated by the Pythoncode generated by the LLM for the corresponding algebraic expression. Thevalues fed to the Python code are the numerical values provided in the problemstatement. The solutions for the subproblems are composed together to obtainthe final answer for the problem statement. Finally, the final answer iscompared to the correct answer. If the final answer matches the correct answer,it is produced as output else a refinement prompt is fed to the LLM. Weexperiment with this prompting technique on both closed-source LLM models andopen-source LLM models using GSM8K dataset. The results obtained demonstratethat MathDivide was able to significantly outperform the leading promptingtechnique called Math-prompter."}], "temperature": 0.1}}
{"custom_id": "365_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Saksham Sahai Srivastava"}], "temperature": 0.1}}
{"custom_id": "366_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical Negation Augmenting and Debiasing for Prompt-based Methods"}], "temperature": 0.1}}
{"custom_id": "366_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Prompt-based methods have gained increasing attention on NLP and shownvalidity on many downstream tasks. Many works have focused on mining thesemethods' potential for knowledge extraction, but few explore their ability tomake logical reasoning. In this work, we focus on the effectiveness of theprompt-based methods on first-order logical reasoning and find that thebottleneck lies in logical negation. Based on our analysis, logical negationtends to result in spurious correlations to negative answers, whilepropositions without logical negation correlate to positive answers. To solvethe problem, we propose a simple but effective method, Negation Augmenting andNegation Debiasing (NAND), which introduces negative propositions toprompt-based methods without updating parameters. Specifically, these negativepropositions can counteract spurious correlations by providing \"not\" for allinstances so that models cannot make decisions only by whether expressionscontain a logical negation. Experiments on three datasets show that NAND notonly solves the problem of calibrating logical negation but also significantlyenhances prompt-based methods of logical reasoning without model retraining."}], "temperature": 0.1}}
{"custom_id": "366_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yitian Li"}], "temperature": 0.1}}
{"custom_id": "367_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Are Human Rules Necessary? Generating Reusable APIs with CoT Reasoning and In-Context Learning"}], "temperature": 0.1}}
{"custom_id": "367_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Inspired by the great potential of Large Language Models (LLMs) for solvingcomplex coding tasks, in this paper, we propose a novel approach, namedCode2API, to automatically perform APIzation for Stack Overflow code snippets.Code2API does not require additional model training or any manual craftingrules and can be easily deployed on personal computers without relying on otherexternal tools. Specifically, Code2API guides the LLMs through well-designedprompts to generate well-formed APIs for given code snippets. To elicitknowledge and logical reasoning from LLMs, we used chain-of-thought (CoT)reasoning and few-shot in-context learning, which can help the LLMs fullyunderstand the APIzation task and solve it step by step in a manner similar toa developer. Our evaluations show that Code2API achieves a remarkable accuracyin identifying method parameters (65%) and return statements (66%) equivalentto human-generated ones, surpassing the current state-of-the-art approach,APIzator, by 15.0% and 16.5% respectively. Moreover, compared with APIzator,our user study demonstrates that Code2API exhibits superior performance ingenerating meaningful method names, even surpassing the human-levelperformance, and developers are more willing to use APIs generated by ourapproach, highlighting the applicability of our tool in practice. Finally, wesuccessfully extend our framework to the Python dataset, achieving a comparableperformance with Java, which verifies the generalizability of our tool."}], "temperature": 0.1}}
{"custom_id": "367_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yubo Mai"}], "temperature": 0.1}}
{"custom_id": "368_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models"}], "temperature": 0.1}}
{"custom_id": "368_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have transformed the landscape of languageprocessing, yet struggle with significant challenges in terms of security,privacy, and the generation of seemingly coherent but factually inaccurateoutputs, commonly referred to as hallucinations. Among these challenges, oneparticularly pressing issue is Fact-Conflicting Hallucination (FCH), where LLMsgenerate content that directly contradicts established facts. Tackling FCHposes a formidable task due to two primary obstacles: Firstly, automating theconstruction and updating of benchmark datasets is challenging, as currentmethods rely on static benchmarks that don't cover the diverse range of FCHscenarios. Secondly, validating LLM outputs' reasoning process is inherentlycomplex, especially with intricate logical relations involved.  In addressing these obstacles, we propose an innovative approach leveraginglogic programming to enhance metamorphic testing for detecting Fact-ConflictingHallucinations (FCH). Our method gathers data from sources like Wikipedia,expands it with logical reasoning to create diverse test cases, assesses LLMsthrough structured prompts, and validates their coherence using semantic-awareassessment mechanisms. Our method generates test cases and detectshallucinations across six different LLMs spanning nine domains, revealinghallucination rates ranging from 24.7% to 59.8%. Key observations indicate thatLLMs encounter challenges, particularly with temporal concepts, handlingout-of-distribution knowledge, and exhibiting deficiencies in logical reasoningcapabilities. The outcomes underscore the efficacy of logic-based test casesgenerated by our tool in both triggering and identifying hallucinations. Thesefindings underscore the imperative for ongoing collaborative endeavors withinthe community to detect and address LLM hallucinations."}], "temperature": 0.1}}
{"custom_id": "368_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ningke Li"}], "temperature": 0.1}}
{"custom_id": "369_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SuperCLUE-Fin: Graded Fine-Grained Analysis of Chinese LLMs on Diverse Financial Tasks and Applications"}], "temperature": 0.1}}
{"custom_id": "369_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The SuperCLUE-Fin (SC-Fin) benchmark is a pioneering evaluation frameworktailored for Chinese-native financial large language models (FLMs). It assessesFLMs across six financial application domains and twenty-five specializedtasks, encompassing theoretical knowledge and practical applications such ascompliance, risk management, and investment analysis. Using multi-turn,open-ended conversations that mimic real-life scenarios, SC-Fin measures modelson a range of criteria, including accurate financial understanding, logicalreasoning, clarity, computational efficiency, business acumen, risk perception,and compliance with Chinese regulations.  In a rigorous evaluation involving over a thousand questions, SC-Finidentifies a performance hierarchy where domestic models like GLM-4 andMoonShot-v1-128k outperform others with an A-grade, highlighting the potentialfor further development in transforming theoretical knowledge into pragmaticfinancial solutions. This benchmark serves as a critical tool for refining FLMsin the Chinese context, directing improvements in financial knowledgedatabases, standardizing financial interpretations, and promoting models thatprioritize compliance, risk management, and secure practices.  We create a contextually relevant and comprehensive benchmark that drives thedevelopment of AI in the Chinese financial sector. SC-Fin facilitates theadvancement and responsible deployment of FLMs, offering valuable insights forenhancing model performance and usability for both individual and institutionalusers in the Chinese market..~\\footnote{Our benchmark can be found at\\url{https://www.CLUEbenchmarks.com}}."}], "temperature": 0.1}}
{"custom_id": "369_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Liang Xu"}], "temperature": 0.1}}
{"custom_id": "370_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Cantor: Inspiring Multimodal Chain-of-Thought of MLLM"}], "temperature": 0.1}}
{"custom_id": "370_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "With the advent of large language models(LLMs) enhanced by thechain-of-thought(CoT) methodology, visual reasoning problem is usuallydecomposed into manageable sub-tasks and tackled sequentially with variousexternal tools. However, such a paradigm faces the challenge of the potential\"determining hallucinations\" in decision-making due to insufficient visualinformation and the limitation of low-level perception tools that fail toprovide abstract summaries necessary for comprehensive reasoning. We argue thatconverging visual context acquisition and logical reasoning is pivotal fortackling visual reasoning tasks. This paper delves into the realm of multimodalCoT to solve intricate visual reasoning tasks with multimodal large languagemodels(MLLMs) and their cognitive capability. To this end, we propose aninnovative multimodal CoT framework, termed Cantor, characterized by aperception-decision architecture. Cantor first acts as a decision generator andintegrates visual inputs to analyze the image and problem, ensuring a closeralignment with the actual context. Furthermore, Cantor leverages the advancedcognitive functions of MLLMs to perform as multifaceted experts for derivinghigher-level information, enhancing the CoT generation process. Our extensiveexperiments demonstrate the efficacy of the proposed framework, showingsignificant improvements in multimodal CoT performance across two complexvisual reasoning datasets, without necessitating fine-tuning or ground-truthrationales. Project Page: https://ggg0919.github.io/cantor/ ."}], "temperature": 0.1}}
{"custom_id": "370_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Timin Gao"}], "temperature": 0.1}}
{"custom_id": "371_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models"}], "temperature": 0.1}}
{"custom_id": "371_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recently developed large language models (LLMs) have been shown to performremarkably well on a wide range of language understanding tasks. But, can theyreally \"reason\" over the natural language? This question has been receivingsignificant research attention and many reasoning skills such as commonsense,numerical, and qualitative have been studied. However, the crucial skillpertaining to 'logical reasoning' has remained underexplored. Existing workinvestigating this reasoning ability of LLMs has focused only on a couple ofinference rules (such as modus ponens and modus tollens) of propositional andfirst-order logic. Addressing the above limitation, we comprehensively evaluatethe logical reasoning ability of LLMs on 25 different reasoning patternsspanning over propositional, first-order, and non-monotonic logics. To enablesystematic evaluation, we introduce LogicBench, a natural languagequestion-answering dataset focusing on the use of a single inference rule. Weconduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini,Llama-2, and Mistral using chain-of-thought prompting. Experimental resultsshow that existing LLMs do not fare well on LogicBench; especially, theystruggle with instances involving complex reasoning and negations. Furthermore,they sometimes overlook contextual information necessary for reasoning toarrive at the correct conclusion. We believe that our work and findingsfacilitate future research for evaluating and enhancing the logical reasoningability of LLMs. Data and code are available athttps://github.com/Mihir3009/LogicBench."}], "temperature": 0.1}}
{"custom_id": "371_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mihir Parmar"}], "temperature": 0.1}}
{"custom_id": "372_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Aligning Knowledge Graphs Provided by Humans and Generated from Neural Networks in Specific Tasks"}], "temperature": 0.1}}
{"custom_id": "372_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This paper develops an innovative method that enables neural networks togenerate and utilize knowledge graphs, which describe their concept-levelknowledge and optimize network parameters through alignment with human-providedknowledge. This research addresses a gap where traditionally, network-generatedknowledge has been limited to applications in downstream symbolic analysis orenhancing network transparency. By integrating a novel autoencoder design withthe Vector Symbolic Architecture (VSA), we have introduced auxiliary tasks thatsupport end-to-end training. Our approach eschews traditional dependencies onontologies or word embedding models, mining concepts from neural networks anddirectly aligning them with human knowledge. Experiments show that our methodconsistently captures network-generated concepts that align closely with humanknowledge and can even uncover new, useful concepts not previously identifiedby humans. This plug-and-play strategy not only enhances the interpretabilityof neural networks but also facilitates the integration of symbolic logicalreasoning within these systems."}], "temperature": 0.1}}
{"custom_id": "372_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Tangrui Li"}], "temperature": 0.1}}
{"custom_id": "373_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "OWLOOP: Interfaces for Mapping OWL Axioms into OOP Hierarchies"}], "temperature": 0.1}}
{"custom_id": "373_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The paper tackles the issue of mapping logic axioms formalised in theOntology Web Language (OWL) within the Object-Oriented Programming (OOP)paradigm. The issues of mapping OWL axioms hierarchies and OOP objectshierarchies are due to OWL-based reasoning algorithms, which might change anOWL hierarchy at runtime; instead, OOP hierarchies are usually defined asstatic structures. Although programming paradigms based on reflection allowchanging the OOP hierarchies at runtime and mapping OWL axioms dynamically,there are no currently available mechanisms that do not limit the reasoningalgorithms. Thus, the factory-based paradigm is typically used since itdecouples the OWL and OOP hierarchies. However, the factory inhibits OOPpolymorphism and introduces a paradigm shift with respect to widely acceptedOOP paradigms. We present the OWLOOP API, which exploits the factory to notlimit reasoning algorithms, and it provides novel OOP interfaces concerning theaxioms in an ontology. OWLOOP is designed to limit the paradigm shift requiredfor using ontologies while improving, through OOP-like polymorphism, themodularity of software architectures that exploit logic reasoning. The paperdetails our OWL to OOP mapping mechanism, and it shows the benefits andlimitations of OWLOOP through examples concerning a robot in a smartenvironment."}], "temperature": 0.1}}
{"custom_id": "373_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Luca Buoncompagni"}], "temperature": 0.1}}
{"custom_id": "374_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues"}], "temperature": 0.1}}
{"custom_id": "374_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advancements in instruction-tuning datasets have predominantly focusedon specific tasks like mathematical or logical reasoning. There has been anotable gap in data designed for aligning language models to maintain topicrelevance in conversations - a critical aspect for deploying chatbots toproduction. We introduce the CantTalkAboutThis dataset to help language modelsremain focused on the subject at hand during task-oriented interactions. Itconsists of synthetic dialogues on a wide range of conversation topics fromdifferent domains. These dialogues are interspersed with distractor turns thatintentionally divert the chatbot from the predefined topic. Fine-tuninglanguage models on this dataset helps make them resilient to deviating from therole assigned and improves their ability to maintain topical coherence comparedto general-purpose instruction-tuned LLMs like GPT-4-turbo andMixtral-Instruct. Additionally, preliminary observations suggest that trainingmodels on this dataset also enhance their performance on fine-grainedinstruction following tasks, including safety alignment."}], "temperature": 0.1}}
{"custom_id": "374_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Makesh Narsimhan Sreedhar"}], "temperature": 0.1}}
{"custom_id": "375_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reason from Fallacy: Enhancing Large Language Models' Logical Reasoning through Logical Fallacy Understanding"}], "temperature": 0.1}}
{"custom_id": "375_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have demonstrated good performance in manyreasoning tasks, but they still struggle with some complicated reasoning tasksincluding logical reasoning. One non-negligible reason for LLMs' suboptimalperformance on logical reasoning is their overlooking of understanding logicalfallacies correctly. To evaluate LLMs' capability of logical fallacyunderstanding (LFU), we propose five concrete tasks from three cognitivedimensions of WHAT, WHY, and HOW in this paper. Towards these LFU tasks, wehave successfully constructed a new dataset LFUD based on GPT-4 accompanied bya little human effort. Our extensive experiments justify that our LFUD can beused not only to evaluate LLMs' LFU capability, but also to fine-tune LLMs toobtain significantly enhanced performance on logical reasoning."}], "temperature": 0.1}}
{"custom_id": "375_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yanda Li"}], "temperature": 0.1}}
{"custom_id": "376_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "I-Design: Personalized LLM Interior Designer"}], "temperature": 0.1}}
{"custom_id": "376_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Interior design allows us to be who we are and live how we want - each designis as unique as our distinct personality. However, it is not trivial fornon-professionals to express and materialize this since it requires aligningfunctional and visual expectations with the constraints of physical space; thisrenders interior design a luxury. To make it more accessible, we presentI-Design, a personalized interior designer that allows users to generate andvisualize their design goals through natural language communication. I-Designstarts with a team of large language model agents that engage in dialogues andlogical reasoning with one another, transforming textual user input intofeasible scene graph designs with relative object relationships. Subsequently,an effective placement algorithm determines optimal locations for each objectwithin the scene. The final design is then constructed in 3D by retrieving andintegrating assets from an existing object database. Additionally, we propose anew evaluation protocol that utilizes a vision-language model and complementsthe design pipeline. Extensive quantitative and qualitative experiments showthat I-Design outperforms existing methods in delivering high-quality 3D designsolutions and aligning with abstract concepts that match user input, showcasingits advantages across detailed 3D arrangement and conceptual fidelity."}], "temperature": 0.1}}
{"custom_id": "376_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ata 脟elen"}], "temperature": 0.1}}
{"custom_id": "377_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Advancing LLM Reasoning Generalists with Preference Trees"}], "temperature": 0.1}}
{"custom_id": "377_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We introduce Eurus, a suite of large language models (LLMs) optimized forreasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achievestate-of-the-art results among open-source models on a diverse set ofbenchmarks covering mathematics, code generation, and logical reasoningproblems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through acomprehensive benchmarking across 12 tests covering five tasks, and achieves a33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challengingbenchmarks, substantially outperforming existing open-source models by marginsmore than 13.3%. The strong performance of Eurus can be primarily attributed toUltraInteract, our newly-curated large-scale, high-quality alignment datasetspecifically designed for complex reasoning tasks. UltraInteract can be used inboth supervised fine-tuning and preference learning. For each instruction, itincludes a preference tree consisting of (1) reasoning chains with diverseplanning strategies in a unified format, (2) multi-turn interactiontrajectories with the environment and the critique, and (3) pairwise data tofacilitate preference learning. UltraInteract allows us to conduct an in-depthexploration of preference learning for reasoning tasks. Our investigationreveals that some well-established preference learning algorithms may be lesssuitable for reasoning tasks compared to their effectiveness in generalconversations. Inspired by this, we derive a novel reward modeling objectivewhich, together with UltraInteract, leads to a strong reward model."}], "temperature": 0.1}}
{"custom_id": "377_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Lifan Yuan"}], "temperature": 0.1}}
{"custom_id": "378_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation"}], "temperature": 0.1}}
{"custom_id": "378_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have achieved significant performance in variousnatural language reasoning tasks. However, they still struggle with performingfirst-order logic reasoning over formal logical theories expressed in naturallanguage. This is because the previous LLMs-based reasoning systems have thetheoretical incompleteness issue. As a result, it can only address a limitedset of simple reasoning problems, which significantly decreases theirgeneralization ability. To address this issue, we propose a novel framework,named Generalizable and Faithful Reasoner (GFaiR), which introduces theparadigm of resolution refutation. Resolution refutation has the capability tosolve all first-order logic reasoning problems by extending reasoning rules andemploying the principle of proof by contradiction, so our system's completenesscan be improved by introducing resolution refutation. Experimental resultsdemonstrate that our system outperforms previous works by achievingstate-of-the-art performances in complex scenarios while maintainingperformances in simple scenarios. Besides, we observe that GFaiR is faithful toits reasoning process."}], "temperature": 0.1}}
{"custom_id": "378_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhouhao Sun"}], "temperature": 0.1}}
{"custom_id": "379_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Language Model Guided Interpretable Video Action Reasoning"}], "temperature": 0.1}}
{"custom_id": "379_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "While neural networks have excelled in video action recognition tasks, theirblack-box nature often obscures the understanding of their decision-makingprocesses. Recent approaches used inherently interpretable models to analyzevideo actions in a manner akin to human reasoning. These models, however,usually fall short in performance compared to their black-box counterparts. Inthis work, we present a new framework named Language-guided InterpretableAction Recognition framework (LaIAR). LaIAR leverages knowledge from languagemodels to enhance both the recognition capabilities and the interpretability ofvideo models. In essence, we redefine the problem of understanding video modeldecisions as a task of aligning video and language models. Using the logicalreasoning captured by the language model, we steer the training of the videomodel. This integrated approach not only improves the video model'sadaptability to different domains but also boosts its overall performance.Extensive experiments on two complex video action datasets, Charades & CAD-120,validates the improved performance and interpretability of our LaIAR framework.The code of LaIAR is available at https://github.com/NingWang2049/LaIAR."}], "temperature": 0.1}}
{"custom_id": "379_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ning Wang"}], "temperature": 0.1}}
{"custom_id": "380_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Classifying Conspiratorial Narratives At Scale: False Alarms and Erroneous Connections"}], "temperature": 0.1}}
{"custom_id": "380_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Online discussions frequently involve conspiracy theories, which cancontribute to the proliferation of belief in them. However, not all discussionssurrounding conspiracy theories promote them, as some are intended to debunkthem. Existing research has relied on simple proxies or focused on aconstrained set of signals to identify conspiracy theories, which limits ourunderstanding of conspiratorial discussions across different topics and onlinecommunities. This work establishes a general scheme for classifying discussionsrelated to conspiracy theories based on authors' perspectives on the conspiracybelief, which can be expressed explicitly through narrative elements, such asthe agent, action, or objective, or implicitly through references to knowntheories, such as chemtrails or the New World Order. We leverage human-labeledground truth to train a BERT-based model for classifying online CTs, which wethen compared to the Generative Pre-trained Transformer machine (GPT) fordetecting online conspiratorial content. Despite GPT's known strengths in itsexpressiveness and contextual understanding, our study revealed significantflaws in its logical reasoning, while also demonstrating comparable strengthsfrom our classifiers. We present the first large-scale classification studyusing posts from the most active conspiracy-related Reddit forums and find thatonly one-third of the posts are classified as positive. This research shedslight on the potential applications of large language models in tasks demandingnuanced contextual comprehension."}], "temperature": 0.1}}
{"custom_id": "380_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ahmad Diab"}], "temperature": 0.1}}
{"custom_id": "381_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Capability for Large Vision-Language Models"}], "temperature": 0.1}}
{"custom_id": "381_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This paper presents ConvBench, a novel multi-turn conversation evaluationbenchmark tailored for Large Vision-Language Models (LVLMs). Unlike existingbenchmarks that assess individual capabilities in single-turn dialogues,ConvBench adopts a three-level multimodal capability hierarchy, mimicking humancognitive processes by stacking up perception, reasoning, and creativity. Eachlevel focuses on a distinct capability, mirroring the cognitive progressionfrom basic perception to logical reasoning and ultimately to advancedcreativity. ConvBench comprises 577 meticulously curated multi-turnconversations encompassing 215 tasks reflective of real-world demands.Automatic evaluations quantify response performance at each turn and overallconversation level. Leveraging the capability hierarchy, ConvBench enablesprecise attribution of conversation mistakes to specific levels. Experimentalresults reveal a performance gap between multi-modal models, including GPT4-V,and human performance in multi-turn conversations. Additionally, weakfine-grained perception in multi-modal models contributes to reasoning andcreation failures. ConvBench serves as a catalyst for further research aimed atenhancing visual dialogues."}], "temperature": 0.1}}
{"custom_id": "381_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Shuo Liu"}], "temperature": 0.1}}
{"custom_id": "382_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LLMSense: Harnessing LLMs for High-level Reasoning Over Spatiotemporal Sensor Traces"}], "temperature": 0.1}}
{"custom_id": "382_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Most studies on machine learning in sensing systems focus on low-levelperception tasks that process raw sensory data within a short time window.However, many practical applications, such as human routine modeling andoccupancy tracking, require high-level reasoning abilities to comprehendconcepts and make inferences based on long-term sensor traces. Existing machinelearning-based approaches for handling such complex tasks struggle togeneralize due to the limited training samples and the high dimensionality ofsensor traces, necessitating the integration of human knowledge for designingfirst-principle models or logic reasoning methods. We pose a fundamentalquestion: Can we harness the reasoning capabilities and world knowledge ofLarge Language Models (LLMs) to recognize complex events from long-termspatiotemporal sensor traces? To answer this question, we design an effectiveprompting framework for LLMs on high-level reasoning tasks, which can handletraces from the raw sensor data as well as the low-level perception results. Wealso design two strategies to enhance performance with long sensor traces,including summarization before reasoning and selective inclusion of historicaltraces. Our framework can be implemented in an edge-cloud setup, running smallLLMs on the edge for data summarization and performing high-level reasoning onthe cloud for privacy preservation. The results show that LLMSense can achieveover 80\\% accuracy on two high-level reasoning tasks such as dementia diagnosiswith behavior traces and occupancy tracking with environmental sensor traces.This paper provides a few insights and guidelines for leveraging LLM forhigh-level reasoning on sensor traces and highlights several directions forfuture work."}], "temperature": 0.1}}
{"custom_id": "382_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xiaomin Ouyang"}], "temperature": 0.1}}
{"custom_id": "383_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Can Language Models Pretend Solvers? Logic Code Simulation with LLMs"}], "temperature": 0.1}}
{"custom_id": "383_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Transformer-based large language models (LLMs) have demonstrated significantpotential in addressing logic problems. capitalizing on the great capabilitiesof LLMs for code-related activities, several frameworks leveraging logicalsolvers for logic reasoning have been proposed recently. While existingresearch predominantly focuses on viewing LLMs as natural language logicsolvers or translators, their roles as logic code interpreters and executorshave received limited attention. This study delves into a novel aspect, namelylogic code simulation, which forces LLMs to emulate logical solvers inpredicting the results of logical programs. To further investigate this noveltask, we formulate our three research questions: Can LLMs efficiently simulatethe outputs of logic codes? What strength arises along with logic codesimulation? And what pitfalls? To address these inquiries, we curate threenovel datasets tailored for the logic code simulation task and undertakethorough experiments to establish the baseline performance of LLMs in codesimulation. Subsequently, we introduce a pioneering LLM-based code simulationtechnique, Dual Chains of Logic (DCoL). This technique advocates a dual-paththinking approach for LLMs, which has demonstrated state-of-the-art performancecompared to other LLM prompt strategies, achieving a notable improvement inaccuracy by 7.06% with GPT-4-Turbo."}], "temperature": 0.1}}
{"custom_id": "383_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Minyu Chen"}], "temperature": 0.1}}
{"custom_id": "384_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "CoNVOI: Context-aware Navigation using Vision Language Models in Outdoor and Indoor Environments"}], "temperature": 0.1}}
{"custom_id": "384_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We present ConVOI, a novel method for autonomous robot navigation inreal-world indoor and outdoor environments using Vision Language Models (VLMs).We employ VLMs in two ways: first, we leverage their zero-shot imageclassification capability to identify the context or scenario (e.g., indoorcorridor, outdoor terrain, crosswalk, etc) of the robot's surroundings, andformulate context-based navigation behaviors as simple text prompts (e.g.``stay on the pavement\"). Second, we utilize their state-of-the-art semanticunderstanding and logical reasoning capabilities to compute a suitabletrajectory given the identified context. To this end, we propose a novelmulti-modal visual marking approach to annotate the obstacle-free regions inthe RGB image used as input to the VLM with numbers, by correlating it with alocal occupancy map of the environment. The marked numbers ground imagelocations in the real-world, direct the VLM's attention solely to navigablelocations, and elucidate the spatial relationships between them and terrainsdepicted in the image to the VLM. Next, we query the VLM to select numbers onthe marked image that satisfy the context-based behavior text prompt, andconstruct a reference path using the selected numbers. Finally, we propose amethod to extrapolate the reference trajectory when the robot's environmentalcontext has not changed to prevent unnecessary VLM queries. We use thereference trajectory to guide a motion planner, and demonstrate that it leadsto human-like behaviors (e.g. not cutting through a group of people, usingcrosswalks, etc.) in various real-world indoor and outdoor scenarios."}], "temperature": 0.1}}
{"custom_id": "384_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Adarsh Jagan Sathyamoorthy"}], "temperature": 0.1}}
{"custom_id": "385_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LeanReasoner: Boosting Complex Logical Reasoning with Lean"}], "temperature": 0.1}}
{"custom_id": "385_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) often struggle with complex logical reasoningdue to logical inconsistencies and the inherent difficulty of such reasoning.We use Lean, a theorem proving framework, to address these challenges. Byformalizing logical reasoning problems into theorems within Lean, we can solvethem by proving or disproving the corresponding theorems. This method reducesthe risk of logical inconsistencies with the help of Lean's symbolic solver. Italso enhances our ability to treat complex reasoning tasks by using Lean'sextensive library of theorem proofs. Our method achieves state-of-the-artperformance on the FOLIO dataset and achieves performance near this level onProofWriter. Notably, these results were accomplished by fine-tuning on fewerthan 100 in-domain samples for each dataset."}], "temperature": 0.1}}
{"custom_id": "385_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Dongwei Jiang"}], "temperature": 0.1}}
{"custom_id": "386_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits"}], "temperature": 0.1}}
{"custom_id": "386_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Conformal prediction has shown spurring performance in constructingstatistically rigorous prediction sets for arbitrary black-box machine learningmodels, assuming the data is exchangeable. However, even small adversarialperturbations during the inference can violate the exchangeability assumption,challenge the coverage guarantees, and result in a subsequent decline inempirical coverage. In this work, we propose a certifiably robustlearning-reasoning conformal prediction framework (COLEP) via probabilisticcircuits, which comprise a data-driven learning component that trainsstatistical models to learn different semantic concepts, and a reasoningcomponent that encodes knowledge and characterizes the relationships among thetrained models for logic reasoning. To achieve exact and efficient reasoning,we employ probabilistic circuits (PCs) within the reasoning component.Theoretically, we provide end-to-end certification of prediction coverage forCOLEP in the presence of bounded adversarial perturbations. We also providecertified coverage considering the finite size of the calibration set.Furthermore, we prove that COLEP achieves higher prediction coverage andaccuracy over a single model as long as the utilities of knowledge models arenon-trivial. Empirically, we show the validity and tightness of our certifiedcoverage, demonstrating the robust conformal prediction of COLEP on variousdatasets, including GTSRB, CIFAR10, and AwA2. We show that COLEP achieves up to12% improvement in certified coverage on GTSRB, 9% on CIFAR-10, and 14% onAwA2."}], "temperature": 0.1}}
{"custom_id": "386_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mintong Kang"}], "temperature": 0.1}}
{"custom_id": "387_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning in Transformers -- Mitigating Spurious Correlations and Reasoning Shortcuts"}], "temperature": 0.1}}
{"custom_id": "387_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Transformer language models are neural networks used for a wide variety oftasks concerning natural language, including some that also require logicalreasoning. However, a transformer model may easily learn spurious patterns inthe data, short-circuiting actual reasoning. In this paper we investigate towhat extent transformers can be trained to a) approximate reasoning inpropositional logic while b) avoiding known reasoning shortcuts via spuriouscorrelations in the training data. To do so, we use a dataset with knownspurious correlation between truth and e.g. the number of rules in the problem.We augment the data with proofs, and train two models: a generativetransformer, WP-BART, trained on problems and their whole proofs, and aneuro-symbolic model, SIP-BART, trained on individual proof steps and combiningthe generative transformer model BART with a symbolic proof checker. We findthat SIP-BART succeeds in avoiding reasoning shortcuts, while WP-BART does not.For SIP-BART, we then identify a few remaining reasoning errors, not previouslydescribed in the literature, arising from using a pre-trained language model.These are qualitatively analysed to create a taxonomy of four different typesof additional pitfalls."}], "temperature": 0.1}}
{"custom_id": "387_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Daniel Enstr枚m"}], "temperature": 0.1}}
{"custom_id": "388_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations"}], "temperature": 0.1}}
{"custom_id": "388_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This article explores the dynamic influence of computational entities basedon multi-agent systems theory (SMA) combined with large language models (LLM),which are characterized by their ability to simulate complex humaninteractions, as a possibility to revolutionize human user interaction from theuse of specialized artificial agents to support everything from operationalorganizational processes to strategic decision making based on appliedknowledge and human orchestration. Previous investigations reveal that thereare limitations, particularly in the autonomous approach of artificial agents,especially when dealing with new challenges and pragmatic tasks such asinducing logical reasoning and problem solving. It is also considered thattraditional techniques, such as the stimulation of chains of thoughts, requireexplicit human guidance. In our approach we employ agents developed from largelanguage models (LLM), each with distinct prototyping that considers behavioralelements, driven by strategies that stimulate the generation of knowledge basedon the use case proposed in the scenario (role-play) business, using adiscussion approach between agents (guided conversation). We demonstrate thepotential of developing agents useful for organizational strategies, based onmulti-agent system theories (SMA) and innovative uses based on large languagemodels (LLM based), offering a differentiated and adaptable experiment todifferent applications, complexities, domains, and capabilities from LLM."}], "temperature": 0.1}}
{"custom_id": "388_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Carlos Jose Xavier Cruz"}], "temperature": 0.1}}
{"custom_id": "389_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Learning Guided Automated Reasoning: A Brief Survey"}], "temperature": 0.1}}
{"custom_id": "389_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Automated theorem provers and formal proof assistants are general reasoningsystems that are in theory capable of proving arbitrarily hard theorems, thussolving arbitrary problems reducible to mathematics and logical reasoning. Inpractice, such systems however face large combinatorial explosion, andtherefore include many heuristics and choice points that considerably influencetheir performance. This is an opportunity for trained machine learningpredictors, which can guide the work of such reasoning systems. Conversely,deductive search supported by the notion of logically valid proof allows one totrain machine learning systems on large reasoning corpora. Such bodies of proofare usually correct by construction and when combined with more and moreprecise trained guidance they can be boostrapped into very large corpora, withincreasingly long reasoning chains and possibly novel proof ideas. In thispaper we provide an overview of several automated reasoning and theorem provingdomains and the learning and AI methods that have been so far developed forthem. These include premise selection, proof guidance in several settings, AIsystems and feedback loops iterating between reasoning and learning, andsymbolic classification problems."}], "temperature": 0.1}}
{"custom_id": "389_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Lasse Blaauwbroek"}], "temperature": 0.1}}
{"custom_id": "390_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments"}], "temperature": 0.1}}
{"custom_id": "390_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The integration of learning and reasoning is high on the research agenda inAI. Nevertheless, there is only a little attention to use existing backgroundknowledge for reasoning about partially observed scenes to answer questionsabout the scene. Yet, we as humans use such knowledge frequently to inferplausible answers to visual questions (by eliminating all inconsistent ones).Such knowledge often comes in the form of constraints about objects and ittends to be highly domain or environment-specific. We contribute a novelbenchmark called CLEVR-POC for reasoning-intensive visual question answering(VQA) in partially observable environments under constraints. In CLEVR-POC,knowledge in the form of logical constraints needs to be leveraged to generateplausible answers to questions about a hidden object in a given partial scene.For instance, if one has the knowledge that all cups are colored either red,green or blue and that there is only one green cup, it becomes possible todeduce the color of an occluded cup as either red or blue, provided that allother cups, including the green one, are observed. Through experiments, weobserve that the low performance of pre-trained vision language models likeCLIP (~ 22%) and a large language model (LLM) like GPT-4 (~ 46%) on CLEVR-POCascertains the necessity for frameworks that can handle reasoning-intensivetasks where environment-specific background knowledge is available and crucial.Furthermore, our demonstration illustrates that a neuro-symbolic model, whichintegrates an LLM like GPT-4 with a visual perception network and a formallogical reasoner, exhibits exceptional performance on CLEVR-POC."}], "temperature": 0.1}}
{"custom_id": "390_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Savitha Sam Abraham"}], "temperature": 0.1}}
{"custom_id": "391_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Fuzzy Datalog$^\\exists$ over Arbitrary t-Norms"}], "temperature": 0.1}}
{"custom_id": "391_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "One of the main challenges in the area of Neuro-Symbolic AI is to performlogical reasoning in the presence of both neural and symbolic data. Thisrequires combining heterogeneous data sources such as knowledge graphs, neuralmodel predictions, structured databases, crowd-sourced data, and many more. Toallow for such reasoning, we generalise the standard rule-based languageDatalog with existential rules (commonly referred to as tuple-generatingdependencies) to the fuzzy setting, by allowing for arbitrary t-norms in theplace of classical conjunctions in rule bodies. The resulting formalism allowsus to perform reasoning about data associated with degrees of uncertainty whilepreserving computational complexity results and the applicability of reasoningtechniques established for the standard Datalog setting. In particular, weprovide fuzzy extensions of Datalog chases which produce fuzzy universal modelsand we exploit them to show that in important fragments of the language,reasoning has the same complexity as in the classical setting."}], "temperature": 0.1}}
{"custom_id": "391_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Matthias Lanzinger"}], "temperature": 0.1}}
{"custom_id": "392_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "AS-ES Learning: Towards Efficient CoT Learning in Small Models"}], "temperature": 0.1}}
{"custom_id": "392_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chain-of-Thought (CoT) serves as a critical emerging ability in LLMs,especially when it comes to logical reasoning. Attempts have been made toinduce such ability in small models as well by distilling from the data withCoT generated by Large Language Models (LLMs). However, existing methods oftensimply generate and incorporate more data from LLMs and fail to note theimportance of efficiently utilizing existing CoT data. We here propose a newtraining paradigm AS-ES (Abstractive Segments - Extractive Segments) learning,which exploits the inherent information in CoT for iterative generation.Experiments show that our methods surpass the direct seq2seq training onCoT-extensive tasks like MWP and PET summarization, without data augmentationor altering the model itself. Furthermore, we explore the reason behind theinefficiency of small models in learning CoT and provide an explanation of whyAS-ES learning works, giving insights into the underlying mechanism of CoT."}], "temperature": 0.1}}
{"custom_id": "392_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Nuwa Xi"}], "temperature": 0.1}}
{"custom_id": "393_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Enhancing Multi-Domain Automatic Short Answer Grading through an Explainable Neuro-Symbolic Pipeline"}], "temperature": 0.1}}
{"custom_id": "393_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Grading short answer questions automatically with interpretable reasoningbehind the grading decision is a challenging goal for current transformerapproaches. Justification cue detection, in combination with logical reasoners,has shown a promising direction for neuro-symbolic architectures in ASAG. But,one of the main challenges is the requirement of annotated justification cuesin the students' responses, which only exist for a few ASAG datasets. Toovercome this challenge, we contribute (1) a weakly supervised annotationprocedure for justification cues in ASAG datasets, and (2) a neuro-symbolicmodel for explainable ASAG based on justification cues. Our approach improvesupon the RMSE by 0.24 to 0.3 compared to the state-of-the-art on the ShortAnswer Feedback dataset in a bilingual, multi-domain, and multi-questiontraining setup. This result shows that our approach provides a promisingdirection for generating high-quality grades and accompanying explanations forfuture research in ASAG and educational NLP."}], "temperature": 0.1}}
{"custom_id": "393_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Felix K眉nnecke"}], "temperature": 0.1}}
{"custom_id": "394_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Balancing Exploration and Exploitation in LLM using Soft RLLF for Enhanced Negation Understanding"}], "temperature": 0.1}}
{"custom_id": "394_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Finetuning approaches in NLP often focus on exploitation rather thanexploration, which may lead to suboptimal models. Given the vast search spaceof natural language, this limited exploration can restrict their performance incomplex, high-stakes domains, where accurate negation understanding and logicalreasoning abilities are crucial. To address this issue, we leverageReinforcement Learning from Logical Feedback (RLLF) to create an effectivebalance between exploration and exploitation in LLMs. Our approach employs anappropriate benchmark dataset for training and evaluation, highlighting theimportance of exploration in enhancing negation understanding capabilities. Wecompare the performance of our RLLF-enhanced LLMs with baseline models trainedwithout RLLF, demonstrating the value of this balanced approach. Furthermore,we showcase the potential of our method in legal AI applications by employingtransfer learning and evaluating its impact on negation understanding. Ourexperimental results exhibit the effectiveness of balancing exploration andexploitation with RLLF in improving LLMs' negation capabilities. This hasimplications for the development of more accurate, reliable, and logicallyconsistent language models in high-stakes domains."}], "temperature": 0.1}}
{"custom_id": "394_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ha-Thanh Nguyen"}], "temperature": 0.1}}
{"custom_id": "395_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Towards Generalist Prompting for Large Language Models by Mental Models"}], "temperature": 0.1}}
{"custom_id": "395_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have demonstrated impressive performance on manytasks. However, to achieve optimal performance, specially designed promptingmethods are still needed. These methods either rely on task-specific few-shotexamples that require a certain level of domain knowledge, or are designed tobe simple but only perform well on a few types of tasks. In this work, weattempt to introduce the concept of generalist prompting, which operates on thedesign principle of achieving optimal or near-optimal performance on a widerange of tasks while eliminating the need for manual selection andcustomization of prompts tailored to specific problems. Furthermore, we proposeMeMo (Mental Models), an innovative prompting method that is simple-designedyet effectively fulfills the criteria of generalist prompting. MeMo distillsthe cores of various prompting methods into individual mental models and allowsLLMs to autonomously select the most suitable mental models for the problem,achieving or being near to the state-of-the-art results on diverse tasks suchas STEM, logical reasoning, and commonsense reasoning in zero-shot settings. Wehope that the insights presented herein will stimulate further exploration ofgeneralist prompting methods for LLMs."}], "temperature": 0.1}}
{"custom_id": "395_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Haoxiang Guan"}], "temperature": 0.1}}
{"custom_id": "396_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Enhanced User Interaction in Operating Systems through Machine Learning Language Models"}], "temperature": 0.1}}
{"custom_id": "396_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "With the large language model showing human-like logical reasoning andunderstanding ability, whether agents based on the large language model cansimulate the interaction behavior of real users, so as to build a reliablevirtual recommendation A/B test scene to help the application of recommendationresearch is an urgent, important and economic value problem. The combination ofinteraction design and machine learning can provide a more efficient andpersonalized user experience for products and services. This personalizedservice can meet the specific needs of users and improve user satisfaction andloyalty. Second, the interactive system can understand the user's views andneeds for the product by providing a good user interface and interactiveexperience, and then use machine learning algorithms to improve and optimizethe product. This iterative optimization process can continuously improve thequality and performance of the product to meet the changing needs of users. Atthe same time, designers need to consider how these algorithms and tools can becombined with interactive systems to provide a good user experience. This paperexplores the potential applications of large language models, machine learningand interaction design for user interaction in recommendation systems andoperating systems. By integrating these technologies, more intelligent andpersonalized services can be provided to meet user needs and promote continuousimprovement and optimization of products. This is of great value for bothrecommendation research and user experience applications."}], "temperature": 0.1}}
{"custom_id": "396_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chenwei Zhang"}], "temperature": 0.1}}
{"custom_id": "397_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge"}], "temperature": 0.1}}
{"custom_id": "397_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have recently showcased remarkablegeneralizability in various domains. Despite their extensive knowledge, LLMsstill face challenges in efficiently utilizing encoded knowledge to developaccurate and logical reasoning processes. To mitigate this problem, weintroduced Hint-before-Solving Prompting (HSP), which guides the model togenerate hints (e.g., specific knowledge or key ideas) for solving the problemand then generate solutions containing intermediate reasoning steps. Since HSPis orthogonal to prompting methods (e.g., Chain-of-Thought (CoT)), we appliedHSP to CoT, Least-to-Most, Plan-and-Solve, and Standard promptings. The resultsof extensive experiments on 6 reasoning benchmarks and 4 open-source LLMsdemonstrate that HSP can effectively improve the accuracy of reasoning tasks:(1) By applying high-quality hint-enhanced HSP to CoT prompting,Llama2-70B-Chat shows an improvement of 9.7. (2) Beyond exploring training-freeLLM capabilities, we built the HSPMATH dataset based on HSP and fine-tunedLlemma-7B, reaching 64.3 accuracy, surpassing GPT-3.5 and WizardMath-13B. Wemake our code and dataset publicly available at\\url{https://github.com/jinlanfu/HSP}."}], "temperature": 0.1}}
{"custom_id": "397_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jinlan Fu"}], "temperature": 0.1}}
{"custom_id": "398_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "398_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Information retrieval is a rapidly evolving field. However it still facessignificant limitations in the scientific and industrial vast amounts ofinformation, such as semantic divergence and vocabulary gaps in sparseretrieval, low precision and lack of interpretability in semantic search, orhallucination and outdated information in generative models. In this paper, weintroduce a two-block approach to tackle these hurdles for long documents. Thefirst block enhances language understanding in sparse retrieval by queryexpansion to retrieve relevant documents. The second block deepens the resultby providing comprehensive and informative answers to the complex questionusing only the information spread in the long document, enabling bidirectionalengagement. At various stages of the pipeline, intermediate results arepresented to users to facilitate understanding of the system's reasoning. Webelieve this bidirectional approach brings significant advancements in terms oftransparency, logical thinking, and comprehensive understanding in the field ofscientific information retrieval."}], "temperature": 0.1}}
{"custom_id": "398_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Lo茂c Rakotoson"}], "temperature": 0.1}}
{"custom_id": "399_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Simple and Effective Transfer Learning for Neuro-Symbolic Integration"}], "temperature": 0.1}}
{"custom_id": "399_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Deep Learning (DL) techniques have achieved remarkable successes in recentyears. However, their ability to generalize and execute reasoning tasks remainsa challenge. A potential solution to this issue is Neuro-Symbolic Integration(NeSy), where neural approaches are combined with symbolic reasoning. Most ofthese methods exploit a neural network to map perceptions to symbols and alogical reasoner to predict the output of the downstream task. These methodsexhibit superior generalization capacity compared to fully neuralarchitectures. However, they suffer from several issues, including slowconvergence, learning difficulties with complex perception tasks, andconvergence to local minima. This paper proposes a simple yet effective methodto ameliorate these problems. The key idea involves pretraining a neural modelon the downstream task. Then, a NeSy model is trained on the same task viatransfer learning, where the weights of the perceptual part are injected fromthe pretrained network. The key observation of our work is that the neuralnetwork fails to generalize only at the level of the symbolic part while beingperfectly capable of learning the mapping from perceptions to symbols. We havetested our training strategy on various SOTA NeSy methods and datasets,demonstrating consistent improvements in the aforementioned problems."}], "temperature": 0.1}}
{"custom_id": "399_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Alessandro Daniele"}], "temperature": 0.1}}
{"custom_id": "400_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning Algorithmically in Graph Neural Networks"}], "temperature": 0.1}}
{"custom_id": "400_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The development of artificial intelligence systems with advanced reasoningcapabilities represents a persistent and long-standing research question.Traditionally, the primary strategy to address this challenge involved theadoption of symbolic approaches, where knowledge was explicitly represented bymeans of symbols and explicitly programmed rules. However, with the advent ofmachine learning, there has been a paradigm shift towards systems that canautonomously learn from data, requiring minimal human guidance. In light ofthis shift, in latest years, there has been increasing interest and efforts atendowing neural networks with the ability to reason, bridging the gap betweendata-driven learning and logical reasoning. Within this context, NeuralAlgorithmic Reasoning (NAR) stands out as a promising research field, aiming tointegrate the structured and rule-based reasoning of algorithms with theadaptive learning capabilities of neural networks, typically by tasking neuralmodels to mimic classical algorithms. In this dissertation, we providetheoretical and practical contributions to this area of research. We explorethe connections between neural networks and tropical algebra, deriving powerfularchitectures that are aligned with algorithm execution. Furthermore, wediscuss and show the ability of such neural reasoners to learn and manipulatecomplex algorithmic and combinatorial optimization concepts, such as theprinciple of strong duality. Finally, in our empirical efforts, we validate thereal-world utility of NAR networks across different practical scenarios. Thisincludes tasks as diverse as planning problems, large-scale edge classificationtasks and the learning of polynomial-time approximate algorithms for NP-hardcombinatorial problems. Through this exploration, we aim to showcase thepotential integrating algorithmic reasoning in machine learning models."}], "temperature": 0.1}}
{"custom_id": "400_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Danilo Numeroso"}], "temperature": 0.1}}
{"custom_id": "401_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large Language Models"}], "temperature": 0.1}}
{"custom_id": "401_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Modern large language models (LLMs) should generally benefit individuals fromvarious cultural backgrounds around the world. However, most recent advancedgenerative evaluation benchmarks tailed for LLMs mainly focus on English. Tothis end, we introduce OMGEval, the first Open-source Multilingual Generativetest set that can assess the capability of LLMs in different languages. Foreach language, OMGEval provides 804 open-ended questions, covering a wide rangeof important capabilities of LLMs, such as general knowledge, logicalreasoning, and so on. Each question is rigorously verified by human annotators.Notably, to sufficiently reflect the compatibility of LLMs in differentcultural backgrounds, we perform localization for each non-English language.Specifically, the current version of OMGEval includes 5 languages (i.e., Zh,Ru, Fr, Es, Ar). Following AlpacaEval, we employ GPT-4 as the adjudicator toautomatically score different model outputs, which is shown closely related tohuman evaluation. We evaluate several representative multilingual LLMs on theproposed OMGEval, which we believe will provide a valuable reference for thecommunity to further understand and improve the multilingual capability ofLLMs. OMGEval is available at https://github.com/blcuicall/OMGEval."}], "temperature": 0.1}}
{"custom_id": "401_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yang Liu"}], "temperature": 0.1}}
{"custom_id": "402_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Neuro-Symbolic Approach to Multi-Agent RL for Interpretability and Probabilistic Decision Making"}], "temperature": 0.1}}
{"custom_id": "402_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Multi-agent reinforcement learning (MARL) is well-suited for runtimedecision-making in optimizing the performance of systems where multiple agentscoexist and compete for shared resources. However, applying common deeplearning-based MARL solutions to real-world problems suffers from issues ofinterpretability, sample efficiency, partial observability, etc. To addressthese challenges, we present an event-driven formulation, where decision-makingis handled by distributed co-operative MARL agents using neuro-symbolicmethods. The recently introduced neuro-symbolic Logical Neural Networks (LNN)framework serves as a function approximator for the RL, to train a rules-basedpolicy that is both logical and interpretable by construction. To enabledecision-making under uncertainty and partial observability, we developed anovel probabilistic neuro-symbolic framework, Probabilistic Logical NeuralNetworks (PLNN), which combines the capabilities of logical reasoning withprobabilistic graphical models. In PLNN, the upward/downward inferencestrategy, inherited from LNN, is coupled with belief bounds by setting theactivation function for the logical operator associated with each neuralnetwork node to a probability-respecting generalization of the Fr\\'echetinequalities. These PLNN nodes form the unifying element that combinesprobabilistic logic and Bayes Nets, permitting inference for variables withunobserved states. We demonstrate our contributions by addressing key MARLchallenges for power sharing in a system-on-chip application."}], "temperature": 0.1}}
{"custom_id": "402_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chitra Subramanian"}], "temperature": 0.1}}
{"custom_id": "403_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models"}], "temperature": 0.1}}
{"custom_id": "403_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We introduce Generalized Instruction Tuning (called GLAN), a general andscalable method for instruction tuning of Large Language Models (LLMs). Unlikeprior work that relies on seed examples or existing datasets to constructinstruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy ofhuman knowledge and capabilities as input and generates large-scale syntheticinstruction data across all disciplines. Specifically, inspired by thesystematic structure in human education system, we build the taxonomy bydecomposing human knowledge and capabilities to various fields, sub-fields andultimately, distinct disciplines semi-automatically, facilitated by LLMs.Subsequently, we generate a comprehensive list of subjects for every disciplineand proceed to design a syllabus tailored to each subject, again utilizingLLMs. With the fine-grained key concepts detailed in every class session of thesyllabus, we are able to generate diverse instructions with a broad coverageacross the entire spectrum of human knowledge and skills. Extensive experimentson large language models (e.g., Mistral) demonstrate that GLAN excels inmultiple dimensions from mathematical reasoning, coding, academic exams,logical reasoning to general instruction following without using task-specifictraining data of these tasks. In addition, GLAN allows for easy customizationand new fields or skills can be added by simply incorporating a new node intoour taxonomy."}], "temperature": 0.1}}
{"custom_id": "403_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Haoran Li"}], "temperature": 0.1}}
{"custom_id": "404_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Conditional Logical Message Passing Transformer for Complex Query Answering"}], "temperature": 0.1}}
{"custom_id": "404_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Complex Query Answering (CQA) over Knowledge Graphs (KGs) is a challengingtask. Given that KGs are usually incomplete, neural models are proposed tosolve CQA by performing multi-hop logical reasoning. However, most of themcannot perform well on both one-hop and multi-hop queries simultaneously.Recent work proposes a logical message passing mechanism based on thepre-trained neural link predictors. While effective on both one-hop andmulti-hop queries, it ignores the difference between the constant and variablenodes in a query graph. In addition, during the node embedding update stage,this mechanism cannot dynamically measure the importance of different messages,and whether it can capture the implicit logical dependencies related to a nodeand received messages remains unclear. In this paper, we propose ConditionalLogical Message Passing Transformer (CLMPT), which considers the differencebetween constants and variables in the case of using pre-trained neural linkpredictors and performs message passing conditionally on the node type. Weempirically verified that this approach can reduce computational costs withoutaffecting performance. Furthermore, CLMPT uses the transformer to aggregatereceived messages and update the corresponding node embedding. Through theself-attention mechanism, CLMPT can assign adaptive weights to elements in aninput set consisting of received messages and the corresponding node andexplicitly model logical dependencies between various elements. Experimentalresults show that CLMPT is a new state-of-the-art neural CQA model.https://github.com/qianlima-lab/CLMPT."}], "temperature": 0.1}}
{"custom_id": "404_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chongzhi Zhang"}], "temperature": 0.1}}
{"custom_id": "405_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations"}], "temperature": 0.1}}
{"custom_id": "405_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "As Large Language Models (LLMs) are integrated into critical real-worldapplications, their strategic and logical reasoning abilities are increasinglycrucial. This paper evaluates LLMs' reasoning abilities in competitiveenvironments through game-theoretic tasks, e.g., board and card games thatrequire pure logic and strategic reasoning to compete with opponents. We firstpropose GTBench, a language-driven environment composing 10 widely recognizedtasks, across a comprehensive game taxonomy: complete versus incompleteinformation, dynamic versus static, and probabilistic versus deterministicscenarios. Then, we (1) Characterize the game-theoretic reasoning of LLMs; and(2) Perform LLM-vs.-LLM competitions as reasoning evaluation. We observe that(1) LLMs have distinct behaviors regarding various gaming scenarios; forexample, LLMs fail in complete and deterministic games yet they are competitivein probabilistic gaming scenarios; (2) Most open-source LLMs, e.g.,CodeLlama-34b-Instruct and Llama-2-70b-chat, are less competitive thancommercial LLMs, e.g., GPT-4, in complex games, yet the recently releasedLlama-3-70b-Instruct makes up for this shortcoming. In addition,code-pretraining greatly benefits strategic reasoning, while advanced reasoningmethods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not alwayshelp. We further characterize the game-theoretic properties of LLMs, such asequilibrium and Pareto Efficiency in repeated games. Detailed error profilesare provided for a better understanding of LLMs' behavior. We hope our researchprovides standardized protocols and serves as a foundation to spur furtherexplorations in the strategic reasoning of LLMs."}], "temperature": 0.1}}
{"custom_id": "405_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jinhao Duan"}], "temperature": 0.1}}
{"custom_id": "406_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Do Large Language Models Understand Logic or Just Mimick Context?"}], "temperature": 0.1}}
{"custom_id": "406_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Over the past few years, the abilities of large language models (LLMs) havereceived extensive attention, which have performed exceptionally well incomplicated scenarios such as logical reasoning and symbolic inference. Asignificant factor contributing to this progress is the benefit of in-contextlearning and few-shot prompting. However, the reasons behind the success ofsuch models using contextual reasoning have not been fully explored. Do LLMshave understand logical rules to draw inferences, or do they ``guess'' theanswers by learning a type of probabilistic mapping through context? This paperinvestigates the reasoning capabilities of LLMs on two logical reasoningdatasets by using counterfactual methods to replace context text and modifylogical concepts. Based on our analysis, it is found that LLMs do not trulyunderstand logical rules; rather, in-context learning has simply enhanced thelikelihood of these models arriving at the correct answers. If one alterscertain words in the context text or changes the concepts of logical terms, theoutputs of LLMs can be significantly disrupted, leading to counter-intuitiveresponses. This work provides critical insights into the limitations of LLMs,underscoring the need for more robust mechanisms to ensure reliable logicalreasoning in LLMs."}], "temperature": 0.1}}
{"custom_id": "406_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Junbing Yan"}], "temperature": 0.1}}
{"custom_id": "407_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "DiLA: Enhancing LLM Tool Learning with Differential Logic Layer"}], "temperature": 0.1}}
{"custom_id": "407_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Considering the challenges faced by large language models (LLMs) in logicalreasoning and planning, prior efforts have sought to augment LLMs with accessto external solvers. While progress has been made on simple reasoning problems,solving classical constraint satisfaction problems, such as the BooleanSatisfiability Problem (SAT) and Graph Coloring Problem (GCP), remainsdifficult for off-the-shelf solvers due to their intricate expressions andexponential search spaces. In this paper, we propose a novel differential logiclayer-aided language modeling (DiLA) approach, where logical constraints areintegrated into the forward and backward passes of a network layer, to provideanother option for LLM tool learning. In DiLA, LLM aims to transform thelanguage description to logic constraints and identify initial solutions of thehighest quality, while the differential logic layer focuses on iterativelyrefining the LLM-prompted solution. Leveraging the logic layer as a bridge,DiLA enhances the logical reasoning ability of LLMs on a range of reasoningproblems encoded by Boolean variables, guaranteeing the efficiency andcorrectness of the solution process. We evaluate the performance of DiLA on twoclassic reasoning problems and empirically demonstrate its consistentoutperformance against existing prompt-based and solver-aided approaches."}], "temperature": 0.1}}
{"custom_id": "407_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yu Zhang"}], "temperature": 0.1}}
{"custom_id": "408_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs"}], "temperature": 0.1}}
{"custom_id": "408_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have achieved impressive human-like performanceacross various reasoning tasks. However, their mastery of underlyinginferential rules still falls short of human capabilities. To investigate this,we propose a logic scaffolding inferential rule generation framework, toconstruct an inferential rule base, ULogic, comprising both primitive andcompositional rules across five domains. Our analysis of GPT-series models overa rule subset reveals significant gaps in LLMs' logic understanding compared tohuman performance, especially in compositional and structural complex ruleswith certain bias patterns. We further distill these rules into a smaller-scaleinference engine for flexible rule generation and enhancing downstreamreasoning. Through a multi-judger evaluation, our inference engine proveseffective in generating accurate, complex and abstract conclusions andpremises, and improve various commonsense reasoning tasks. Overall, our worksheds light on LLMs' limitations in grasping inferential rule and suggests waysto enhance their logical reasoning abilities~\\footnote{Code and data areavailable at \\url{https://github.com/SiyuanWangw/ULogic}.}."}], "temperature": 0.1}}
{"custom_id": "408_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Siyuan Wang"}], "temperature": 0.1}}
{"custom_id": "409_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Puzzle Solving using Reasoning of Large Language Models: A Survey"}], "temperature": 0.1}}
{"custom_id": "409_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Exploring the capabilities of Large Language Models (LLMs) in puzzle solvingunveils critical insights into their potential and challenges in AI, marking asignificant step towards understanding their applicability in complex reasoningtasks. This survey leverages a unique taxonomy -- dividing puzzles intorule-based and rule-less categories -- to critically assess LLMs throughvarious methodologies, including prompting techniques, neuro-symbolicapproaches, and fine-tuning. Through a critical review of relevant datasets andbenchmarks, we assess LLMs' performance, identifying significant challenges incomplex puzzle scenarios. Our findings highlight the disparity between LLMcapabilities and human-like reasoning, particularly in those requiring advancedlogical inference. The survey underscores the necessity for novel strategiesand richer datasets to advance LLMs' puzzle-solving proficiency and contributeto AI's logical reasoning and creative problem-solving advancements."}], "temperature": 0.1}}
{"custom_id": "409_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Panagiotis Giadikiaroglou"}], "temperature": 0.1}}
{"custom_id": "410_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models"}], "temperature": 0.1}}
{"custom_id": "410_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Memory Editing (ME) has emerged as an efficient method to modify erroneousfacts or inject new facts into Large Language Models (LLMs). Two mainstream MEmethods exist: parameter-modifying ME and parameter-preserving ME (integratingextra modules while preserving original parameters). Regrettably, previousstudies on ME evaluation have two critical limitations: (i) evaluating LLMswith single edit only, neglecting the need for continuous editing, and (ii)evaluations focusing solely on basic factual triples, overlooking broader LLMcapabilities like logical reasoning and reading understanding. This studyaddresses these limitations with contributions threefold: (i) We explore how MEaffects a wide range of fundamental capabilities of LLMs under sequentialediting. Experimental results reveal an intriguing phenomenon: Mostparameter-modifying ME consistently degrade performance across all tasks aftera few sequential edits. In contrast, parameter-preserving ME effectivelymaintains LLMs' fundamental capabilities but struggles to accurately recalledited knowledge presented in a different format. (ii) We extend our evaluationto different editing settings, such as layers to edit, model size, instructiontuning, etc. Experimental findings indicate several strategies that canpotentially mitigate the adverse effects of ME. (iii) We further explain whyparameter-modifying ME damages LLMs from three dimensions: parameter changesafter editing, language modeling capability, and the in-context learningcapability. Our in-depth study advocates more careful use of ME in real-worldscenarios."}], "temperature": 0.1}}
{"custom_id": "410_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zihao Lin"}], "temperature": 0.1}}
{"custom_id": "411_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LogicPrpBank: A Corpus for Logical Implication and Equivalence"}], "temperature": 0.1}}
{"custom_id": "411_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logic reasoning has been critically needed in problem-solving anddecision-making. Although Language Models (LMs) have demonstrated capabilitiesof handling multiple reasoning tasks (e.g., commonsense reasoning), theirability to reason complex mathematical problems, specifically propositionallogic, remains largely underexplored. This lack of exploration can beattributed to the limited availability of annotated corpora. Here, we present awell-labeled propositional logic corpus, LogicPrpBank, containing 7093Propositional Logic Statements (PLSs) across six mathematical subjects, tostudy a brand-new task of reasoning logical implication and equivalence. Webenchmark LogicPrpBank with widely-used LMs to show that our corpus offers auseful resource for this challenging task and there is ample room for modelimprovement."}], "temperature": 0.1}}
{"custom_id": "411_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhexiong Liu"}], "temperature": 0.1}}
{"custom_id": "412_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Beyond LLMs: Advancing the Landscape of Complex Reasoning"}], "temperature": 0.1}}
{"custom_id": "412_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Since the advent of Large Language Models a few years ago, they have oftenbeen considered the de facto solution for many AI problems. However, inaddition to the many deficiencies of LLMs that prevent them from broad industryadoption, such as reliability, cost, and speed, there is a whole class ofcommon real world problems that Large Language Models perform poorly on,namely, constraint satisfaction and optimization problems. These problems areubiquitous and current solutions are highly specialized and expensive toimplement. At Elemental Cognition, we developed our EC AI platform which takesa neuro-symbolic approach to solving constraint satisfaction and optimizationproblems. The platform employs, at its core, a precise and high performancelogical reasoning engine, and leverages LLMs for knowledge acquisition and userinteraction. This platform supports developers in specifying application logicin natural and concise language while generating application user interfaces tointeract with users effectively. We evaluated LLMs against systems built on theEC AI platform in three domains and found the EC AI systems to significantlyoutperform LLMs on constructing valid and optimal solutions, on validatingproposed solutions, and on repairing invalid solutions."}], "temperature": 0.1}}
{"custom_id": "412_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jennifer Chu-Carroll"}], "temperature": 0.1}}
{"custom_id": "413_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs"}], "temperature": 0.1}}
{"custom_id": "413_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Vision language models (VLMs) have shown impressive capabilities across avariety of tasks, from logical reasoning to visual understanding. This opensthe door to richer interaction with the world, for example robotic control.However, VLMs produce only textual outputs, while robotic control and otherspatial tasks require outputting continuous coordinates, actions, ortrajectories. How can we enable VLMs to handle such settings withoutfine-tuning on task-specific data?  In this paper, we propose a novel visual prompting approach for VLMs that wecall Prompting with Iterative Visual Optimization (PIVOT), which casts tasks asiterative visual question answering. In each iteration, the image is annotatedwith a visual representation of proposals that the VLM can refer to (e.g.,candidate robot actions, localizations, or trajectories). The VLM then selectsthe best ones for the task. These proposals are iteratively refined, allowingthe VLM to eventually zero in on the best available answer. We investigatePIVOT on real-world robotic navigation, real-world manipulation from images,instruction following in simulation, and additional spatial inference taskssuch as localization. We find, perhaps surprisingly, that our approach enableszero-shot control of robotic systems without any robot training data,navigation in a variety of environments, and other capabilities. Althoughcurrent performance is far from perfect, our work highlights potentials andlimitations of this new regime and shows a promising approach forInternet-Scale VLMs in robotic and spatial reasoning domains. Website:pivot-prompt.github.io and HuggingFace:https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo."}], "temperature": 0.1}}
{"custom_id": "413_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Soroush Nasiriany"}], "temperature": 0.1}}
{"custom_id": "414_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "TIC: Translate-Infer-Compile for accurate \"text to plan\" using LLMs and Logical Representations"}], "temperature": 0.1}}
{"custom_id": "414_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We study the problem of generating plans for given natural language planningtask requests. On one hand, LLMs excel at natural language processing but donot perform well on planning. On the other hand, classical planning tools excelat planning tasks but require input in a structured language such as thePlanning Domain Definition Language (PDDL). We leverage the strengths of boththe techniques by using an LLM for generating the PDDL representation (taskPDDL) of planning task requests followed by using a classical planner forcomputing a plan. Unlike previous approaches that use LLMs for generating taskPDDLs directly, our approach comprises of (a) translate: using an LLM only forgenerating a logically interpretable intermediate representation of naturallanguage task description, (b) infer: deriving additional logically dependentinformation from the intermediate representation using a logic reasoner(currently, Answer Set Programming solver), and (c) compile: generating thetarget task PDDL from the base and inferred information. We observe that usingan LLM to only output the intermediate representation significantly reduces LLMerrors. Consequently, TIC approach achieves, for at least one LLM, highaccuracy on task PDDL generation for all seven domains of our evaluationdataset."}], "temperature": 0.1}}
{"custom_id": "414_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Sudhir Agarwal"}], "temperature": 0.1}}
{"custom_id": "415_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The Quantified Boolean Bayesian Network: Theory and Experiments with a Logical Graphical Model"}], "temperature": 0.1}}
{"custom_id": "415_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This paper introduces the Quantified Boolean Bayesian Network (QBBN), whichprovides a unified view of logical and probabilistic reasoning. The QBBN ismeant to address a central problem with the Large Language Model (LLM), whichhas become extremely popular in Information Retrieval, which is that the LLMhallucinates. A Bayesian Network, by construction, cannot hallucinate, becauseit can only return answers that it can explain. We show how a Bayesian Networkover an unbounded number of boolean variables can be configured to representthe logical reasoning underlying human language. We do this by creating akey-value version of the First-Order Calculus, for which we can proveconsistency and completeness. We show that the model is trivially trained overfully observed data, but that inference is non-trivial. Exact inference in aBayesian Network is intractable (i.e. $\\Omega(2^N)$ for $N$ variables). Forinference, we investigate the use of Loopy Belief Propagation (LBP), which isnot guaranteed to converge, but which has been shown to often converge inpractice. Our experiments show that LBP indeed does converge very reliably, andour analysis shows that a round of LBP takes time $O(N2^n)$, where $N$ boundsthe number of variables considered, and $n$ bounds the number of incomingconnections to any factor, and further improvements may be possible. Ournetwork is specifically designed to alternate between AND and OR gates in aBoolean Algebra, which connects more closely to logical reasoning, allowing acompleteness proof for an expanded version of our network, and also allowsinference to follow specific but adequate pathways, that turn out to be fast."}], "temperature": 0.1}}
{"custom_id": "415_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Gregory Coppola"}], "temperature": 0.1}}
{"custom_id": "416_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs"}], "temperature": 0.1}}
{"custom_id": "416_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The evolution of Large Language Models (LLMs) has showcased remarkablecapacities for logical reasoning and natural language comprehension. Thesecapabilities can be leveraged in solutions that semantically and textuallymodel complex problems. In this paper, we present our efforts towardconstructing a framework that can serve as an intermediary between a user andtheir user interface (UI), enabling dynamic and real-time interactions. Weemploy a system that stands upon textual semantic mappings of UI components, inthe form of annotations. These mappings are stored, parsed, and scaled in acustom data structure, supplementary to an agent-based prompting backendengine. Employing textual semantic mappings allows each component to not onlyexplain its role to the engine but also provide expectations. By comprehendingthe needs of both the user and the components, our LLM engine can classify themost appropriate application, extract relevant parameters, and subsequentlyexecute precise predictions of the user's expected actions. Such an integrationevolves static user interfaces into highly dynamic and adaptable solutions,introducing a new frontier of intelligent and responsive user experiences."}], "temperature": 0.1}}
{"custom_id": "416_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Syed Mekael Wasti"}], "temperature": 0.1}}
{"custom_id": "417_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Symbol Correctness in Deep Neural Networks Containing Symbolic Layers"}], "temperature": 0.1}}
{"custom_id": "417_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "To handle AI tasks that combine perception and logical reasoning, recent workintroduces Neurosymbolic Deep Neural Networks (NS-DNNs), which contain -- inaddition to traditional neural layers -- symbolic layers: symbolic expressions(e.g., SAT formulas, logic programs) that are evaluated by symbolic solversduring inference. We identify and formalize an intuitive, high-level principlethat can guide the design and analysis of NS-DNNs: symbol correctness, thecorrectness of the intermediate symbols predicted by the neural layers withrespect to a (generally unknown) ground-truth symbolic representation of theinput data. We demonstrate that symbol correctness is a necessary property forNS-DNN explainability and transfer learning (despite being in generalimpossible to train for). Moreover, we show that the framework of symbolcorrectness provides a precise way to reason and communicate about modelbehavior at neural-symbolic boundaries, and gives insight into the fundamentaltradeoffs faced by NS-DNN training algorithms. In doing so, we both identifysignificant points of ambiguity in prior work, and provide a framework tosupport further NS-DNN developments."}], "temperature": 0.1}}
{"custom_id": "417_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Aaron Bembenek"}], "temperature": 0.1}}
{"custom_id": "418_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Understanding Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation"}], "temperature": 0.1}}
{"custom_id": "418_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Pre-trained language models (LMs) are able to perform complex reasoningwithout explicit fine-tuning. To understand how pre-training with a next-tokenprediction objective contributes to the emergence of such reasoning capability,we propose that we can view an LM as deriving new conclusions by aggregatingindirect reasoning paths seen at pre-training time. We found this perspectiveeffective in two important cases of reasoning: logic reasoning with knowledgegraphs (KGs) and chain-of-thought (CoT) reasoning. More specifically, weformalize the reasoning paths as random walk paths on the knowledge/reasoninggraphs. Analyses of learned LM distributions suggest that a weighted sum ofrelevant random walk path probabilities is a reasonable way to explain how LMsreason. Experiments and analysis on multiple KG and CoT datasets reveal theeffect of training on random walk paths and suggest that augmenting unlabeledrandom walk reasoning paths can improve real-world multi-step reasoningperformance. code: https://github.com/WANGXinyiLinda/LM_random_walk"}], "temperature": 0.1}}
{"custom_id": "418_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xinyi Wang"}], "temperature": 0.1}}
{"custom_id": "419_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models"}], "temperature": 0.1}}
{"custom_id": "419_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recently, large language models (LLMs) have shown remarkable capabilitiesincluding understanding context, engaging in logical reasoning, and generatingresponses. However, this is achieved at the expense of stringent computationaland memory requirements, hindering their ability to effectively support longinput sequences. This survey provides an inclusive review of the recenttechniques and methods devised to extend the sequence length in LLMs, therebyenhancing their capacity for long-context understanding. In particular, wereview and categorize a wide range of techniques including architecturalmodifications, such as modified positional encoding and altered attentionmechanisms, which are designed to enhance the processing of longer sequenceswhile avoiding a proportional increase in computational requirements. Thediverse methodologies investigated in this study can be leveraged acrossdifferent phases of LLMs, i.e., training, fine-tuning and inference. Thisenables LLMs to efficiently process extended sequences. The limitations of thecurrent methodologies is discussed in the last section along with thesuggestions for future research directions, underscoring the importance ofsequence length in the continued advancement of LLMs."}], "temperature": 0.1}}
{"custom_id": "419_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xindi Wang"}], "temperature": 0.1}}
{"custom_id": "420_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Revisiting Document-Level Relation Extraction with Context-Guided Link Prediction"}], "temperature": 0.1}}
{"custom_id": "420_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Document-level relation extraction (DocRE) poses the challenge of identifyingrelationships between entities within a document as opposed to the traditionalRE setting where a single sentence is input. Existing approaches rely onlogical reasoning or contextual cues from entities. This paper reframesdocument-level RE as link prediction over a knowledge graph with distinctbenefits: 1) Our approach combines entity context with document-derived logicalreasoning, enhancing link prediction quality. 2) Predicted links betweenentities offer interpretability, elucidating employed reasoning. We evaluateour approach on three benchmark datasets: DocRED, ReDocRED, and DWIE. Theresults indicate that our proposed method outperforms the state-of-the-artmodels and suggests that incorporating context-based link prediction techniquescan enhance the performance of document-level relation extraction models."}], "temperature": 0.1}}
{"custom_id": "420_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Monika Jain"}], "temperature": 0.1}}
{"custom_id": "421_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The Radiation Oncology NLP Database"}], "temperature": 0.1}}
{"custom_id": "421_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We present the Radiation Oncology NLP Database (ROND), the first dedicatedNatural Language Processing (NLP) dataset for radiation oncology, an importantmedical specialty that has received limited attention from the NLP community inthe past. With the advent of Artificial General Intelligence (AGI), there is anincreasing need for specialized datasets and benchmarks to facilitate researchand development. ROND is specifically designed to address this gap in thedomain of radiation oncology, a field that offers many opportunities for NLPexploration. It encompasses various NLP tasks including Logic Reasoning, TextClassification, Named Entity Recognition (NER), Question Answering (QA), TextSummarization, and Patient-Clinician Conversations, each with a distinct focuson radiation oncology concepts and application cases. In addition, we havedeveloped an instruction-tuning dataset consisting of over 20k instructionpairs (based on ROND) and trained a large language model, CancerChat. Thisserves to demonstrate the potential of instruction-tuning large language modelswithin a highly-specialized medical domain. The evaluation results in thisstudy could serve as baseline results for future research. ROND aims tostimulate advancements in radiation oncology and clinical NLP by offering aplatform for testing and improving algorithms and models in a domain-specificcontext. The ROND dataset is a joint effort of multiple U.S. healthinstitutions. The data is available athttps://github.com/zl-liu/Radiation-Oncology-NLP-Database."}], "temperature": 0.1}}
{"custom_id": "421_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhengliang Liu"}], "temperature": 0.1}}
{"custom_id": "422_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LangBridge: Multilingual Reasoning Without Multilingual Supervision"}], "temperature": 0.1}}
{"custom_id": "422_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We introduce LangBridge, a zero-shot approach to adapt language models formultilingual reasoning tasks without multilingual supervision. LangBridgeoperates by bridging two models, each specialized in different aspects: (1) onespecialized in understanding multiple languages (e.g., mT5 encoder) and (2) onespecialized in reasoning (e.g., MetaMath). LangBridge connects the two modelsby introducing minimal trainable parameters between them. Despite utilizingonly English data for training, LangBridge considerably enhances theperformance of language models on low-resource languages across mathematicalreasoning, code completion, logical reasoning, and commonsense reasoning. Ouranalysis suggests that the efficacy of LangBridge stems from thelanguage-agnostic characteristics of multilingual representations. We publiclyrelease our code and models."}], "temperature": 0.1}}
{"custom_id": "422_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Dongkeun Yoon"}], "temperature": 0.1}}
{"custom_id": "423_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Formal Logic Enabled Personalized Federated Learning Through Property Inference"}], "temperature": 0.1}}
{"custom_id": "423_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advancements in federated learning (FL) have greatly facilitated thedevelopment of decentralized collaborative applications, particularly in thedomain of Artificial Intelligence of Things (AIoT). However, a critical aspectmissing from the current research landscape is the ability to enabledata-driven client models with symbolic reasoning capabilities. Specifically,the inherent heterogeneity of participating client devices poses a significantchallenge, as each client exhibits unique logic reasoning properties. Failingto consider these device-specific specifications can result in criticalproperties being missed in the client predictions, leading to suboptimalperformance. In this work, we propose a new training paradigm that leveragestemporal logic reasoning to address this issue. Our approach involves enhancingthe training process by incorporating mechanically generated logic expressionsfor each FL client. Additionally, we introduce the concept of aggregationclusters and develop a partitioning algorithm to effectively group clientsbased on the alignment of their temporal reasoning properties. We evaluate theproposed method on two tasks: a real-world traffic volume prediction taskconsisting of sensory data from fifteen states and a smart city multi-taskprediction utilizing synthetic data. The evaluation results exhibit clearimprovements, with performance accuracy improved by up to 54% across allsequential prediction models."}], "temperature": 0.1}}
{"custom_id": "423_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ziyan An"}], "temperature": 0.1}}
{"custom_id": "424_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "DCR: Divide-and-Conquer Reasoning for Multi-choice Question Answering with LLMs"}], "temperature": 0.1}}
{"custom_id": "424_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have shown impressive performance in reasoningbenchmarks with the emergence of Chain-of-Thought (CoT), particularly inmulti-choice question (MCQ). However, current works equally resolve questionsregardless of the problem-solving difficulty, leading to an excessive focus onsimple items while insufficient attention on intricate ones. To address thischallenge, we propose a simple yet effective strategy, Divide and ConquerReasoning (DCR), to enhance the reasoning capability of LLMs for MCQs, asinspired by human beings using heuristics to first categorize tasks and thenhandle them separately. In particular, we first categorize questions into twosubsets based on confidence score ($\\mathcal{CS}$), which is estimated bystatistical frequency of generated answers. Subsequently, we propose FilterChoices based Reasoning (FCR) to improve model performance on MCQs with low($\\mathcal{CS}$). Our experiments demonstrate that the proposed strategy onlycosts 85% of SOTA, while still achieves average accuracy improvement of 1.56%across nine datasets including arithmetic, commonsense, and logic reasoningtasks. The code is at \\url{https://github.com/AiMijie/Divide-and-Conquer}"}], "temperature": 0.1}}
{"custom_id": "424_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zijie Meng"}], "temperature": 0.1}}
{"custom_id": "425_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "RePLan: Robotic Replanning with Perception and Language Models"}], "temperature": 0.1}}
{"custom_id": "425_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Advancements in large language models (LLMs) have demonstrated theirpotential in facilitating high-level reasoning, logical reasoning and roboticsplanning. Recently, LLMs have also been able to generate reward functions forlow-level robot actions, effectively bridging the interface between high-levelplanning and low-level robot control. However, the challenge remains that evenwith syntactically correct plans, robots can still fail to achieve theirintended goals due to imperfect plans or unexpected environmental issues. Toovercome this, Vision Language Models (VLMs) have shown remarkable success intasks such as visual question answering. Leveraging the capabilities of VLMs,we present a novel framework called Robotic Replanning with Perception andLanguage Models (RePLan) that enables online replanning capabilities forlong-horizon tasks. This framework utilizes the physical grounding provided bya VLM's understanding of the world's state to adapt robot actions when theinitial plan fails to achieve the desired goal. We developed a Reasoning andControl (RC) benchmark with eight long-horizon tasks to test our approach. Wefind that RePLan enables a robot to successfully adapt to unforeseen obstacleswhile accomplishing open-ended, long-horizon goals, where baseline modelscannot, and can be readily applied to real robots. Find more information athttps://replan-lm.github.io/replan.github.io/"}], "temperature": 0.1}}
{"custom_id": "425_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Marta Skreta"}], "temperature": 0.1}}
{"custom_id": "426_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LLM4EDA: Emerging Progress in Large Language Models for Electronic Design Automation"}], "temperature": 0.1}}
{"custom_id": "426_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Driven by Moore's Law, the complexity and scale of modern chip design areincreasing rapidly. Electronic Design Automation (EDA) has been widely appliedto address the challenges encountered in the full chip design process. However,the evolution of very large-scale integrated circuits has made chip designtime-consuming and resource-intensive, requiring substantial prior expertknowledge. Additionally, intermediate human control activities are crucial forseeking optimal solutions. In system design stage, circuits are usuallyrepresented with Hardware Description Language (HDL) as a textual format.Recently, Large Language Models (LLMs) have demonstrated their capability incontext understanding, logic reasoning and answer generation. Since circuit canbe represented with HDL in a textual format, it is reasonable to questionwhether LLMs can be leveraged in the EDA field to achieve fully automated chipdesign and generate circuits with improved power, performance, and area (PPA).In this paper, we present a systematic study on the application of LLMs in theEDA field, categorizing it into the following cases: 1) assistant chatbot, 2)HDL and script generation, and 3) HDL verification and analysis. Additionally,we highlight the future research direction, focusing on applying LLMs in logicsynthesis, physical design, multi-modal feature extraction and alignment ofcircuits. We collect relevant papers up-to-date in this field via the followinglink: https://github.com/Thinklab-SJTU/Awesome-LLM4EDA."}], "temperature": 0.1}}
{"custom_id": "426_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ruizhe Zhong"}], "temperature": 0.1}}
{"custom_id": "427_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Detection-based Intermediate Supervision for Visual Question Answering"}], "temperature": 0.1}}
{"custom_id": "427_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recently, neural module networks (NMNs) have yielded ongoing success inanswering compositional visual questions, especially those involving multi-hopvisual and logical reasoning. NMNs decompose the complex question into severalsub-tasks using instance-modules from the reasoning paths of that question andthen exploit intermediate supervisions to guide answer prediction, therebyimproving inference interpretability. However, their performance may behindered due to sketchy modeling of intermediate supervisions. For instance,(1) a prior assumption that each instance-module refers to only one groundedobject yet overlooks other potentially associated grounded objects, impedingfull cross-modal alignment learning; (2) IoU-based intermediate supervisionsmay introduce noise signals as the bounding box overlap issue might guide themodel's focus towards irrelevant objects. To address these issues, a novelmethod, \\textbf{\\underline{D}}etection-based \\textbf{\\underline{I}}ntermediate\\textbf{\\underline{S}}upervision (DIS), is proposed, which adopts a generativedetection framework to facilitate multiple grounding supervisions via sequencegeneration. As such, DIS offers more comprehensive and accurate intermediatesupervisions, thereby boosting answer prediction performance. Furthermore, byconsidering intermediate results, DIS enhances the consistency in answeringcompositional questions and their sub-questions.Extensive experimentsdemonstrate the superiority of our proposed DIS, showcasing both improvedaccuracy and state-of-the-art reasoning consistency compared to priorapproaches."}], "temperature": 0.1}}
{"custom_id": "427_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yuhang Liu"}], "temperature": 0.1}}
{"custom_id": "428_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "TEILP: Time Prediction over Knowledge Graphs via Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "428_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Conventional embedding-based models approach event time prediction intemporal knowledge graphs (TKGs) as a ranking problem. However, they often fallshort in capturing essential temporal relationships such as order and distance.In this paper, we propose TEILP, a logical reasoning framework that naturallyintegrates such temporal elements into knowledge graph predictions. We firstconvert TKGs into a temporal event knowledge graph (TEKG) which has a moreexplicit representation of time in term of nodes of the graph. The TEKG equipsus to develop a differentiable random walk approach to time prediction.Finally, we introduce conditional probability density functions, associatedwith the logical rules involving the query interval, using which we arrive atthe time prediction. We compare TEILP with state-of-the-art methods on fivebenchmark datasets. We show that our model achieves a significant improvementover baselines while providing interpretable explanations. In particular, weconsider several scenarios where training samples are limited, event types areimbalanced, and forecasting the time of future events based on only past eventsis desired. In all these cases, TEILP outperforms state-of-the-art methods interms of robustness."}], "temperature": 0.1}}
{"custom_id": "428_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Siheng Xiong"}], "temperature": 0.1}}
{"custom_id": "429_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation"}], "temperature": 0.1}}
{"custom_id": "429_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Abductive reasoning is the process of making educated guesses to provideexplanations for observations. Although many applications require the use ofknowledge for explanations, the utilization of abductive reasoning inconjunction with structured knowledge, such as a knowledge graph, remainslargely unexplored. To fill this gap, this paper introduces the task of complexlogical hypothesis generation, as an initial step towards abductive logicalreasoning with KG. In this task, we aim to generate a complex logicalhypothesis so that it can explain a set of observations. We find that thesupervised trained generative model can generate logical hypotheses that arestructurally closer to the reference hypothesis. However, when generalized tounseen observations, this training objective does not guarantee betterhypothesis generation. To address this, we introduce the Reinforcement Learningfrom Knowledge Graph (RLF-KG) method, which minimizes differences betweenobservations and conclusions drawn from generated hypotheses according to theKG. Experiments show that, with RLF-KG's assistance, the generated hypothesesprovide better explanations, and achieve state-of-the-art results on threewidely used KGs."}], "temperature": 0.1}}
{"custom_id": "429_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jiaxin Bai"}], "temperature": 0.1}}
{"custom_id": "430_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Understanding Inter-Session Intentions via Complex Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "430_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Understanding user intentions is essential for improving productrecommendations, navigation suggestions, and query reformulations. However,user intentions can be intricate, involving multiple sessions and attributerequirements connected by logical operators such as And, Or, and Not. Forinstance, a user may search for Nike or Adidas running shoes across varioussessions, with a preference for purple. In another example, a user may havepurchased a mattress in a previous session and is now looking for a matchingbed frame without intending to buy another mattress. Existing research onsession understanding has not adequately addressed making product or attributerecommendations for such complex intentions. In this paper, we present the taskof logical session complex query answering (LS-CQA), where sessions are treatedas hyperedges of items, and we frame the problem of complex intentionunderstanding as an LS-CQA task on an aggregated hypergraph of sessions, items,and attributes. This is a unique complex query answering task with sessions asordered hyperedges. We also introduce a new model, the Logical Session GraphTransformer (LSGT), which captures interactions among items across differentsessions and their logical connections using a transformer structure. Weanalyze the expressiveness of LSGT and prove the permutation invariance of theinputs for the logical operators. By evaluating LSGT on three datasets, wedemonstrate that it achieves state-of-the-art results."}], "temperature": 0.1}}
{"custom_id": "430_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jiaxin Bai"}], "temperature": 0.1}}
{"custom_id": "431_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Empowering Few-Shot Recommender Systems with Large Language Models -- Enhanced Representations"}], "temperature": 0.1}}
{"custom_id": "431_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recommender systems utilizing explicit feedback have witnessed significantadvancements and widespread applications over the past years. However,generating recommendations in few-shot scenarios remains a persistentchallenge. Recently, large language models (LLMs) have emerged as a promisingsolution for addressing natural language processing (NLP) tasks, therebyoffering novel insights into tackling the few-shot scenarios encountered byexplicit feedback-based recommender systems. To bridge recommender systems andLLMs, we devise a prompting template that generates user and itemrepresentations based on explicit feedback. Subsequently, we integrate theseLLM-processed representations into various recommendation models to evaluatetheir significance across diverse recommendation tasks. Our ablationexperiments and case study analysis collectively demonstrate the effectivenessof LLMs in processing explicit feedback, highlighting that LLMs equipped withgenerative and logical reasoning capabilities can effectively serve as acomponent of recommender systems to enhance their performance in few-shotscenarios. Furthermore, the broad adaptability of LLMs augments thegeneralization potential of recommender models, despite certain inherentconstraints. We anticipate that our study can inspire researchers to delvedeeper into the multifaceted dimensions of LLMs's involvement in recommendersystems and contribute to the advancement of the explicit feedback-basedrecommender systems field."}], "temperature": 0.1}}
{"custom_id": "431_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhoumeng Wang"}], "temperature": 0.1}}
{"custom_id": "432_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models"}], "temperature": 0.1}}
{"custom_id": "432_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning is central to complex human activities, such as thinking,debating, and planning; it is also a central component of many AI systems aswell. In this paper, we investigate the extent to which encoder-onlytransformer language models (LMs) can reason according to logical rules. We askwhether those LMs can deduce theorems in propositional calculus and first-orderlogic; if their relative success in these problems reflects general logicalcapabilities; and which layers contribute the most to the task. First, we showfor several encoder-only LMs that they can be trained, to a reasonable degree,to determine logical validity on various datasets. Next, by cross-probingfine-tuned models on these datasets, we show that LMs have difficulty intransferring their putative logical reasoning ability, which suggests that theymay have learned dataset-specific features, instead of a general capability.Finally, we conduct a layerwise probing experiment, which shows that thehypothesis classification task is mostly solved through higher layers."}], "temperature": 0.1}}
{"custom_id": "432_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Paulo Pirozelli"}], "temperature": 0.1}}
{"custom_id": "433_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The Good, The Bad, and Why: Unveiling Emotions in Generative AI"}], "temperature": 0.1}}
{"custom_id": "433_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Emotion significantly impacts our daily behaviors and interactions. Whilerecent generative AI models, such as large language models, have shownimpressive performance in various tasks, it remains unclear whether they trulycomprehend emotions. This paper aims to address this gap by incorporatingpsychological theories to gain a holistic understanding of emotions ingenerative AI models. Specifically, we propose three approaches: 1)EmotionPrompt to enhance AI model performance, 2) EmotionAttack to impair AImodel performance, and 3) EmotionDecode to explain the effects of emotionalstimuli, both benign and malignant. Through extensive experiments involvinglanguage and multi-modal models on semantic understanding, logical reasoning,and generation tasks, we demonstrate that both textual and visual EmotionPromptcan boost the performance of AI models while EmotionAttack can hinder it.Additionally, EmotionDecode reveals that AI models can comprehend emotionalstimuli akin to the mechanism of dopamine in the human brain. Our work heraldsa novel avenue for exploring psychology to enhance our understanding ofgenerative AI models."}], "temperature": 0.1}}
{"custom_id": "433_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Cheng Li"}], "temperature": 0.1}}
{"custom_id": "434_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent"}], "temperature": 0.1}}
{"custom_id": "434_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) face challenges in solving complex mathematicalproblems that require comprehensive capacities to parse the statements,associate domain knowledge, perform compound logical reasoning, and integratethe intermediate rationales. Tackling all these problems once could be arduousfor LLMs, thus leading to confusion in generation. In this work, we explore thepotential of enhancing LLMs with agents by meticulous decomposition andmodeling of mathematical reasoning process. Specifically, we propose a formaldescription of the mathematical solving and extend LLMs with an agent-basedzero-shot framework named$\\bf{P}$lanner-$\\bf{R}$easoner-$\\bf{E}$xecutor-$\\bf{R}$eflector (PRER). Wefurther provide and implement two MathAgents that define the logical forms andinherent relations via a pool of actions in different grains and orientations:MathAgent-M adapts its actions to LLMs, while MathAgent-H aligns withhumankind. Experiments on miniF2F and MATH have demonstrated the effectivenessof PRER and proposed MathAgents, achieving an increase of$12.3\\%$($53.9\\%\\xrightarrow{}66.2\\%$) on the MiniF2F, $9.2\\%$($49.8\\%\\xrightarrow{}59.0\\%$) on MATH, and$13.2\\%$($23.2\\%\\xrightarrow{}35.4\\%$) for level-5 problems of MATH againstGPT-4. Further analytical results provide more insightful perspectives onexploiting the behaviors of LLMs as agents."}], "temperature": 0.1}}
{"custom_id": "434_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Haoran Liao"}], "temperature": 0.1}}
{"custom_id": "435_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Assessing SATNet's Ability to Solve the Symbol Grounding Problem"}], "temperature": 0.1}}
{"custom_id": "435_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SATNet is an award-winning MAXSAT solver that can be used to infer logicalrules and integrated as a differentiable layer in a deep neural network. It hadbeen shown to solve Sudoku puzzles visually from examples of puzzle digitimages, and was heralded as an impressive achievement towards the longstandingAI goal of combining pattern recognition with logical reasoning. In this paper,we clarify SATNet's capabilities by showing that in the absence of intermediatelabels that identify individual Sudoku digit images with their logicalrepresentations, SATNet completely fails at visual Sudoku (0% test accuracy).More generally, the failure can be pinpointed to its inability to learn toassign symbols to perceptual phenomena, also known as the symbol groundingproblem, which has long been thought to be a prerequisite for intelligentagents to perform real-world logical reasoning. We propose an MNIST based testas an easy instance of the symbol grounding problem that can serve as a sanitycheck for differentiable symbolic solvers in general. Naive applications ofSATNet on this test lead to performance worse than that of models withoutlogical reasoning capabilities. We report on the causes of SATNet's failure andhow to prevent them."}], "temperature": 0.1}}
{"custom_id": "435_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Oscar Chang"}], "temperature": 0.1}}
{"custom_id": "436_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Model Enhanced Multi-Agent Systems for 6G Communications"}], "temperature": 0.1}}
{"custom_id": "436_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The rapid development of the Large Language Model (LLM) presents hugeopportunities for 6G communications, e.g., network optimization and managementby allowing users to input task requirements to LLMs by nature language.However, directly applying native LLMs in 6G encounters various challenges,such as a lack of private communication data and knowledge, limited logicalreasoning, evaluation, and refinement abilities. Integrating LLMs with thecapabilities of retrieval, planning, memory, evaluation and reflection inagents can greatly enhance the potential of LLMs for 6G communications. To thisend, we propose a multi-agent system with customized communication knowledgeand tools for solving communication related tasks using natural language,comprising three components: (1) Multi-agent Data Retrieval (MDR), whichemploys the condensate and inference agents to refine and summarizecommunication knowledge from the knowledge base, expanding the knowledgeboundaries of LLMs in 6G communications; (2) Multi-agent Collaborative Planning(MCP), which utilizes multiple planning agents to generate feasible solutionsfor the communication related task from different perspectives based on theretrieved knowledge; (3) Multi-agent Evaluation and Reflecxion (MER), whichutilizes the evaluation agent to assess the solutions, and applies thereflexion agent and refinement agent to provide improvement suggestions forcurrent solutions. Finally, we validate the effectiveness of the proposedmulti-agent system by designing a semantic communication system, as a casestudy of 6G communications."}], "temperature": 0.1}}
{"custom_id": "436_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Feibo Jiang"}], "temperature": 0.1}}
{"custom_id": "437_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Physics Lab Inside Your Head: Quantum Thought Experiments as an Educational Tool"}], "temperature": 0.1}}
{"custom_id": "437_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Thought experiments are where logical reasoning meets storytelling,catalysing progress in quantum science and technology. Schr\\\"odinger's famouscat brought quantum science to the public consciousness, while Deutsch'sthought experiment to test the many-worlds and Copenhagen interpretationsinvolved the first conception of a quantum computer. I will show how presentingthought experiments using quantum circuits can demystify apparent quantumparadoxes, and provide fun, conceptually important activities for learners toimplement themselves on near-term quantum devices. Additionally, I will explainhow thought experiments can be used as a first introduction to quantum, andoutline a workshop based on the \"quantum bomb tester\" for school students asyoung as 11. This paper draws upon my experience in developing and deliveringquantum computing workshops in Oxford, and in creating a quantum paradoxescontent series with IBM Quantum of videos, blogs and code tutorials."}], "temperature": 0.1}}
{"custom_id": "437_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Maria Violaris"}], "temperature": 0.1}}
{"custom_id": "438_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models are Complex Table Parsers"}], "temperature": 0.1}}
{"custom_id": "438_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "With the Generative Pre-trained Transformer 3.5 (GPT-3.5) exhibitingremarkable reasoning and comprehension abilities in Natural Language Processing(NLP), most Question Answering (QA) research has primarily centered aroundgeneral QA tasks based on GPT, neglecting the specific challenges posed byComplex Table QA. In this paper, we propose to incorporate GPT-3.5 to addresssuch challenges, in which complex tables are reconstructed into tuples andspecific prompt designs are employed for dialogues. Specifically, we encodeeach cell's hierarchical structure, position information, and content as atuple. By enhancing the prompt template with an explanatory description of themeaning of each tuple and the logical reasoning process of the task, weeffectively improve the hierarchical structure awareness capability of GPT-3.5to better parse the complex tables. Extensive experiments and results onComplex Table QA datasets, i.e., the open-domain dataset HiTAB and the aviationdomain dataset AIT-QA show that our approach significantly outperforms previouswork on both datasets, leading to state-of-the-art (SOTA) performance."}], "temperature": 0.1}}
{"custom_id": "438_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Bowen Zhao"}], "temperature": 0.1}}
{"custom_id": "439_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SGLang: Efficient Execution of Structured Language Model Programs"}], "temperature": 0.1}}
{"custom_id": "439_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) are increasingly used for complex tasks thatrequire multiple generation calls, advanced prompting techniques, control flow,and structured inputs/outputs. However, efficient systems are lacking forprogramming and executing these applications. We introduce SGLang, a system forefficient execution of complex language model programs. SGLang consists of afrontend language and a runtime. The frontend simplifies programming withprimitives for generation and parallelism control. The runtime acceleratesexecution with novel optimizations like RadixAttention for KV cache reuse andcompressed finite state machines for faster structured output decoding.Experiments show that SGLang achieves up to 6.4x higher throughput compared tostate-of-the-art inference systems on various large language and multi-modalmodels on tasks including agent control, logical reasoning, few-shot learningbenchmarks, JSON decoding, retrieval-augmented generation pipelines, andmulti-turn chat. The code is publicly available athttps://github.com/sgl-project/sglang"}], "temperature": 0.1}}
{"custom_id": "439_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Lianmin Zheng"}], "temperature": 0.1}}
{"custom_id": "440_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Exploring the Reversal Curse and Other Deductive Logical Reasoning in BERT and GPT-Based Large Language Models"}], "temperature": 0.1}}
{"custom_id": "440_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The term \"Reversal Curse\" refers to the scenario where auto-regressivedecoder large language models (LLMs), such as ChatGPT, trained on \"A is B\" failto learn \"B is A,\" assuming that B and A are distinct and can be uniquelyidentified from each other, demonstrating a basic failure of logical deduction.This raises a red flag in the use of GPT models for certain general tasks suchas constructing knowledge graphs, considering their adherence to this symmetricprinciple. In our study, we examined a bidirectional LLM, BERT, and found thatit is immune to the reversal curse. Driven by ongoing efforts to constructbiomedical knowledge graphs with LLMs, we also embarked on evaluating morecomplex but essential deductive reasoning capabilities. This process includedfirst training encoder and decoder language models to master the intersectionand union operations on two sets and then moving on to assess their capabilityto infer different combinations of union and intersection operations on threenewly created sets. The findings showed that while both encoder and decoderlanguage models, trained for tasks involving two sets (union/intersection),were proficient in such scenarios, they encountered difficulties when dealingwith operations that included three sets (various combinations of union andintersection). Our research highlights the distinct characteristics of encoderand decoder models in simple and complex logical reasoning. In practice, thechoice between BERT and GPT should be guided by the specific requirements andnature of the task at hand, leveraging their respective strengths inbidirectional context comprehension and sequence prediction."}], "temperature": 0.1}}
{"custom_id": "440_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Da Wu"}], "temperature": 0.1}}
{"custom_id": "441_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation"}], "temperature": 0.1}}
{"custom_id": "441_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chain-of-Thought (CoT) guides large language models (LLMs) to reasonstep-by-step, and can motivate their logical reasoning ability. While effectivefor logical tasks, CoT is not conducive to creative problem-solving which oftenrequires out-of-box thoughts and is crucial for innovation advancements. Inthis paper, we explore the Leap-of-Thought (LoT) abilities within LLMs -- anon-sequential, creative paradigm involving strong associations and knowledgeleaps. To this end, we study LLMs on the popular Oogiri game which needsparticipants to have good creativity and strong associative thinking forresponding unexpectedly and humorously to the given image, text, or both, andthus is suitable for LoT study. Then to investigate LLMs' LoT ability in theOogiri game, we first build a multimodal and multilingual Oogiri-GO datasetwhich contains over 130,000 samples from the Oogiri game, and observe theinsufficient LoT ability or failures of most existing LLMs on the Oogiri game.Accordingly, we introduce a creative Leap-of-Thought (CLoT) paradigm to improveLLM's LoT ability. CLoT first formulates the Oogiri-GO dataset intoLoT-oriented instruction tuning data to train pretrained LLM for achievingcertain LoT humor generation and discrimination abilities. Then CLoT designs anexplorative self-refinement that encourages the LLM to generate more creativeLoT data via exploring parallels between seemingly unrelated concepts andselects high-quality data to train itself for self-refinement. CLoT not onlyexcels in humor generation in the Oogiri game but also boosts creativeabilities in various tasks like cloud guessing game and divergent associationtask. These findings advance our understanding and offer a pathway to improveLLMs' creative capacities for innovative applications across domains. Thedataset, code, and models will be released online.https://zhongshsh.github.io/CLoT/."}], "temperature": 0.1}}
{"custom_id": "441_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Shanshan Zhong"}], "temperature": 0.1}}
{"custom_id": "442_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games"}], "temperature": 0.1}}
{"custom_id": "442_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In this study, we explore the application of Large Language Models (LLMs) in\\textit{Jubensha}, a Chinese detective role-playing game and a novel area inArtificial Intelligence (AI) driven gaming. We introduce the first datasetspecifically for Jubensha, including character scripts and game rules, tofoster AI agent development in this complex narrative environment. Our workalso presents a unique multi-agent interaction framework using LLMs, allowingAI agents to autonomously engage in this game. To evaluate the gamingperformance of these AI agents, we developed novel methods measuring theirmastery of case information and reasoning skills. Furthermore, we incorporatedthe latest advancements in in-context learning to improve the agents'performance in information gathering, murderer identification, and logicalreasoning. The experimental results validate the effectiveness of our proposedmethods. This work aims to offer a novel perspective on understanding LLMcapabilities and establish a new benchmark for evaluating large languagemodel-based agents."}], "temperature": 0.1}}
{"custom_id": "442_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Dekun Wu"}], "temperature": 0.1}}
{"custom_id": "443_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Generation of Explanations for Logic Reasoning"}], "temperature": 0.1}}
{"custom_id": "443_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This thesis delves into a fortiori arguments in deductive reasoning,underscoring their relevance in various domains such as law, philosophy, andartificial intelligence. The research is centred on employing GPT-3.5-turbo toautomate the analysis of these arguments, with a focus on understandingintricate reasoning processes, generating clear and coherent explanations, andcreating novel arguments. The methodology encompasses a series of tasksincluding detailed reasoning, interpretation, and the augmentation of afortiori arguments. It involves meticulously identifying these arguments indiverse contexts, differentiating comparative elements, and categorizing thembased on their logical structure.  Extensive experiments reveals the challenges encountered by GPT-3.5-turbo inaccurately detecting and classifying a fortiori arguments. Nevertheless, themodel demonstrates a performance that rivals specialized models, particularlyin extracting key components and interpreting underlying properties. Theintegration of external information into the model's processing significantlyelevates the quality of the generated explanations. Additionally, the modelexhibits a noteworthy capability in augmenting arguments, thus contributing tothe enrichment of the data set.  Despite facing certain limitations, this thesis makes significantcontributions to the fields of artificial intelligence and logical reasoning.It introduces novel methodologies, establishes a rigorous evaluation framework,and provides deep insights that set the stage for future advancements inautomated logical reasoning. The findings and methodologies presented hereinnot only underscore the potential of AI in complex reasoning tasks but alsohighlight areas for future research and development."}], "temperature": 0.1}}
{"custom_id": "443_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yanyi Pu"}], "temperature": 0.1}}
{"custom_id": "444_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Enhancing Logical Reasoning in Large Language Models to Facilitate Legal Applications"}], "temperature": 0.1}}
{"custom_id": "444_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Language serves as a vehicle for conveying thought, enabling communicationamong individuals. The ability to distinguish between diverse concepts,identify fairness and injustice, and comprehend a range of legal notionsfundamentally relies on logical reasoning. Large Language Models (LLMs) attemptto emulate human language understanding and generation, but their competency inlogical reasoning remains limited. This paper seeks to address thephilosophical question: How can we effectively teach logical reasoning to LLMswhile maintaining a deep understanding of the intricate relationship betweenlanguage and logic? By focusing on bolstering LLMs' capabilities in logicalreasoning, we aim to expand their applicability in law and otherlogic-intensive disciplines. To this end, we propose a Reinforcement Learningfrom Logical Feedback (RLLF) approach, which serves as a potential frameworkfor refining LLMs' reasoning capacities. Through RLLF and a revised evaluationmethodology, we explore new avenues for research in this domain and contributeto the development of LLMs capable of handling complex legal reasoning taskswhile acknowledging the fundamental connection between language and logic."}], "temperature": 0.1}}
{"custom_id": "444_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ha-Thanh Nguyen"}], "temperature": 0.1}}
{"custom_id": "445_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "De-fine: Decomposing and Refining Visual Programs with Auto-Feedback"}], "temperature": 0.1}}
{"custom_id": "445_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Visual programming, a modular and generalizable paradigm, integratesdifferent modules and Python operators to solve various vision-language tasks.Unlike end-to-end models that need task-specific data, it advances inperforming visual processing and reasoning in an unsupervised manner. Currentvisual programming methods generate programs in a single pass for each taskwhere the ability to evaluate and optimize based on feedback, unfortunately, islacking, which consequentially limits their effectiveness for complex,multi-step problems. Drawing inspiration from benders decomposition, weintroduce De-fine, a training-free framework that automatically decomposescomplex tasks into simpler subtasks and refines programs through auto-feedback.This model-agnostic approach can improve logical reasoning performance byintegrating the strengths of multiple models. Our experiments across variousvisual tasks show that De-fine creates more robust programs. Moreover, viewingeach feedback module as an independent agent will yield fresh prospects for thefield of agent research."}], "temperature": 0.1}}
{"custom_id": "445_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Minghe Gao"}], "temperature": 0.1}}
{"custom_id": "446_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "WatME: Towards Lossless Watermarking Through Lexical Redundancy"}], "temperature": 0.1}}
{"custom_id": "446_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Text watermarking has emerged as a pivotal technique for identifyingmachine-generated text. However, existing methods often rely on arbitraryvocabulary partitioning during decoding to embed watermarks, which compromisesthe availability of suitable tokens and significantly degrades the quality ofresponses. This study assesses the impact of watermarking on differentcapabilities of large language models (LLMs) from a cognitive science lens. Ourfinding highlights a significant disparity; knowledge recall and logicalreasoning are more adversely affected than language generation. These resultssuggest a more profound effect of watermarking on LLMs than previouslyunderstood. To address these challenges, we introduce Watermarking with MutualExclusion (WatME), a novel approach leveraging linguistic prior knowledge ofinherent lexical redundancy in LLM vocabularies to seamlessly integratewatermarks. Specifically, WatME dynamically optimizes token usage during thedecoding process by applying a mutually exclusive rule to the identifiedlexical redundancies. This strategy effectively prevents the unavailability ofappropriate tokens and preserves the expressive power of LLMs. We provide boththeoretical analysis and empirical evidence showing that WatME effectivelypreserves the diverse capabilities of LLMs while ensuring watermarkdetectability."}], "temperature": 0.1}}
{"custom_id": "446_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Liang Chen"}], "temperature": 0.1}}
{"custom_id": "447_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "FollowEval: A Multi-Dimensional Benchmark for Assessing the Instruction-Following Capability of Large Language Models"}], "temperature": 0.1}}
{"custom_id": "447_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The effective assessment of the instruction-following ability of largelanguage models (LLMs) is of paramount importance. A model that cannot adhereto human instructions might be not able to provide reliable and helpfulresponses. In pursuit of this goal, various benchmarks have been constructed toevaluate the instruction-following capacity of these models. However, thesebenchmarks are limited to a single language and are constructed using automatedapproaches, which restricts their applicability and the quality of the testexamples they contain. To bridge this gap, we introduce the FollowEvalbenchmark in this paper. This benchmark is composed of instances in bothEnglish and Chinese, and all test examples are crafted by human experts.Furthermore, the FollowEval benchmark is designed to assess LLMs across fivecritical dimensions of instruction following: string manipulation, commonsensereasoning, logical reasoning, spatial reasoning, and response constraints. Toenhance the complexity and present a sufficient challenge, each test example isdesigned to evaluate more than one dimension. We have evaluated various LLMsusing the FollowEval benchmark and found that their performance significantlylags behind that of humans. This highlights the considerable room forimprovement in the instruction-following ability of these models."}], "temperature": 0.1}}
{"custom_id": "447_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yimin Jing"}], "temperature": 0.1}}
{"custom_id": "448_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neural-Logic Human-Object Interaction Detection"}], "temperature": 0.1}}
{"custom_id": "448_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The interaction decoder utilized in prevalent Transformer-based HOI detectorstypically accepts pre-composed human-object pairs as inputs. Though achievingremarkable performance, such paradigm lacks feasibility and cannot explorenovel combinations over entities during decoding. We present L OGIC HOI, a newHOI detector that leverages neural-logic reasoning and Transformer to inferfeasible interactions between entities. Specifically, we modify theself-attention mechanism in vanilla Transformer, enabling it to reason over the<human, action, object> triplet and constitute novel interactions. Meanwhile,such reasoning process is guided by two crucial properties for understandingHOI: affordances (the potential actions an object can facilitate) and proxemics(the spatial relations between humans and objects). We formulate these twoproperties in first-order logic and ground them into continuous space toconstrain the learning process of our approach, leading to improved performanceand zero-shot generalization capabilities. We evaluate L OGIC HOI on V-COCO andHICO-DET under both normal and zero-shot setups, achieving significantimprovements over existing methods."}], "temperature": 0.1}}
{"custom_id": "448_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Liulei Li"}], "temperature": 0.1}}
{"custom_id": "449_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "449_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning has been an ongoing pursuit in the field of AI. Despitesignificant advancements made by large language models (LLMs), they stillstruggle with complex logical reasoning problems. To enhance reasoningperformance, one promising direction is scalable oversight, which requires LLMsto identify their own errors and then improve by themselves. Variousself-verification methods have been proposed in pursuit of this goal.Nevertheless, whether existing models understand their own errors well is stillunder investigation. In this paper, we take a closer look at theself-verification abilities of LLMs in the context of logical reasoning,focusing on their ability to identify logical fallacies accurately. Weintroduce a dataset, FALLACIES, containing 232 types of reasoning fallaciescategorized in a hierarchical taxonomy. By conducting exhaustive experiments onFALLACIES, we obtain comprehensive and detailed analyses of a series of modelson their verification abilities. Our main findings suggest that existing LLMscould struggle to identify fallacious reasoning steps accurately and may fallshort of guaranteeing the validity of self-verification methods. Drawing fromthese observations, we offer suggestions for future research and practicalapplications of self-verification methods."}], "temperature": 0.1}}
{"custom_id": "449_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ruixin Hong"}], "temperature": 0.1}}
{"custom_id": "450_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study"}], "temperature": 0.1}}
{"custom_id": "450_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have shown remarkable proficiency in languageunderstanding and have been successfully applied to a variety of real-worldtasks through task-specific fine-tuning or prompt engineering. Despite theseadvancements, it remains an open question whether LLMs are fundamentallycapable of reasoning and planning, or if they primarily rely on recalling andsynthesizing information from their training data. In our research, weintroduce a novel task -- Minesweeper -- specifically designed in a formatunfamiliar to LLMs and absent from their training datasets. This taskchallenges LLMs to identify the locations of mines based on numerical cluesprovided by adjacent opened cells. Successfully completing this task requiresan understanding of each cell's state, discerning spatial relationships betweenthe clues and mines, and strategizing actions based on logical deductions drawnfrom the arrangement of the cells. Our experiments, including trials with theadvanced GPT-4 model, indicate that while LLMs possess the foundationalabilities required for this task, they struggle to integrate these into acoherent, multi-step logical reasoning process needed to solve Minesweeper.These findings highlight the need for further research to understand the natureof reasoning capabilities in LLMs under similar circumstances, and to explorepathways towards more sophisticated AI reasoning and planning models."}], "temperature": 0.1}}
{"custom_id": "450_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yinghao Li"}], "temperature": 0.1}}
{"custom_id": "451_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "From Complex to Simple: Unraveling the Cognitive Tree for Reasoning with Small Language Models"}], "temperature": 0.1}}
{"custom_id": "451_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning is a distinctive human capacity, enabling us to address complexproblems by breaking them down into a series of manageable cognitive steps.Yet, complex logical reasoning is still cumbersome for language models. Basedon the dual process theory in cognitive science, we are the first to unravelthe cognitive reasoning abilities of language models. Our framework employs aniterative methodology to construct a Cognitive Tree (CogTree). The root node ofthis tree represents the initial query, while the leaf nodes consist ofstraightforward questions that can be answered directly. This constructioninvolves two main components: the implicit extraction module (referred to asthe intuitive system) and the explicit reasoning module (referred to as thereflective system). The intuitive system rapidly generates multiple responsesby utilizing in-context examples, while the reflective system scores theseresponses using comparative learning. The scores guide the intuitive system inits subsequent generation step. Our experimental results on two popular andchallenging reasoning tasks indicate that it is possible to achieve aperformance level comparable to that of GPT-3.5 (with 175B parameters), using asignificantly smaller language model that contains fewer parameters (<=7B) than5% of GPT-3.5."}], "temperature": 0.1}}
{"custom_id": "451_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Junbing Yan"}], "temperature": 0.1}}
{"custom_id": "452_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Language Models can be Logical Solvers"}], "temperature": 0.1}}
{"custom_id": "452_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning is a fundamental aspect of human intelligence and a keycomponent of tasks like problem-solving and decision-making. Recentadvancements have enabled Large Language Models (LLMs) to potentially exhibitreasoning capabilities, but complex logical reasoning remains a challenge. Thestate-of-the-art, solver-augmented language models, use LLMs to parse naturallanguage logical questions into symbolic representations first and then adoptexternal logical solvers to take in the symbolic representations and output theanswers. Despite their impressive performance, any parsing errors willinevitably result in the failure of the execution of the external logicalsolver and no answer to the logical questions. In this paper, we introduceLoGiPT, a novel language model that directly emulates the reasoning processesof logical solvers and bypasses the parsing errors by learning to strictadherence to solver syntax and grammar. LoGiPT is fine-tuned on a newlyconstructed instruction-tuning dataset derived from revealing and refining theinvisible reasoning process of deductive solvers. Experimental results on twopublic deductive reasoning datasets demonstrate that LoGiPT outperformsstate-of-the-art solver-augmented LMs and few-shot prompting methods oncompetitive LLMs like ChatGPT or GPT-4."}], "temperature": 0.1}}
{"custom_id": "452_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jiazhan Feng"}], "temperature": 0.1}}
{"custom_id": "453_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Let's Reinforce Step by Step"}], "temperature": 0.1}}
{"custom_id": "453_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "While recent advances have boosted LM proficiency in linguistic benchmarks,LMs consistently struggle to reason correctly on complex tasks likemathematics. We turn to Reinforcement Learning from Human Feedback (RLHF) as amethod with which to shape model reasoning processes. In particular, we exploretwo reward schemes, outcome-supervised reward models (ORMs) andprocess-supervised reward models (PRMs), to optimize for logical reasoning. Ourresults show that the fine-grained reward provided by PRM-based methodsenhances accuracy on simple mathematical reasoning (GSM8K) while, unexpectedly,reducing performance in complex tasks (MATH). Furthermore, we show the criticalrole reward aggregation functions play in model performance. Providingpromising avenues for future research, our study underscores the need forfurther exploration into fine-grained reward modeling for more reliablelanguage models."}], "temperature": 0.1}}
{"custom_id": "453_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Sarah Pan"}], "temperature": 0.1}}
{"custom_id": "454_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chain of Images for Intuitively Reasoning"}], "temperature": 0.1}}
{"custom_id": "454_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The human brain is naturally equipped to comprehend and interpret visualinformation rapidly. When confronted with complex problems or concepts, we useflowcharts, sketches, and diagrams to aid our thought process. Leveraging thisinherent ability can significantly enhance logical reasoning. However, currentLarge Language Models (LLMs) do not utilize such visual intuition to help theirthinking. Even the most advanced version language models (e.g., GPT-4V andLLaVA) merely align images into textual space, which means their reasoningprocesses remain purely verbal. To mitigate such limitations, we present aChain of Images (CoI) approach, which can convert complex language reasoningproblems to simple pattern recognition by generating a series of images asintermediate representations. Furthermore, we have developed a CoI evaluationdataset encompassing 15 distinct domains where images can intuitively aidproblem-solving. Based on this dataset, we aim to construct a benchmark toassess the capability of future multimodal large-scale models to leverageimages for reasoning. In supporting our CoI reasoning, we introduce a symbolicmultimodal large language model (SyMLLM) that generates images strictly basedon language instructions and accepts both text and image as input. Experimentson Geometry, Chess and Common Sense tasks sourced from the CoI evaluationdataset show that CoI improves performance significantly over the pure-languageChain of Thoughts (CoT) baselines. The code is available athttps://github.com/GraphPKU/CoI."}], "temperature": 0.1}}
{"custom_id": "454_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Fanxu Meng"}], "temperature": 0.1}}
{"custom_id": "455_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "COOL: A Constraint Object-Oriented Logic Programming Language and its Neural-Symbolic Compilation System"}], "temperature": 0.1}}
{"custom_id": "455_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This paper explores the integration of neural networks with logicprogramming, addressing the longstanding challenges of combining thegeneralization and learning capabilities of neural networks with the precisionof symbolic logic. Traditional attempts at this integration have been hamperedby difficulties in initial data acquisition, the reliability of undertrainednetworks, and the complexity of reusing and augmenting trained models. Toovercome these issues, we introduce the COOL (Constraint Object-Oriented Logic)programming language, an innovative approach that seamlessly combines logicalreasoning with neural network technologies. COOL is engineered to autonomouslyhandle data collection, mitigating the need for user-supplied initial data. Itincorporates user prompts into the coding process to reduce the risks ofundertraining and enhances the interaction among models throughout theirlifecycle to promote the reuse and augmentation of networks. Furthermore, thefoundational principles and algorithms in COOL's design and its compilationsystem could provide valuable insights for future developments in programminglanguages and neural network architectures."}], "temperature": 0.1}}
{"custom_id": "455_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jipeng Han"}], "temperature": 0.1}}
{"custom_id": "456_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Equivariance Is Not All You Need: Characterizing the Utility of Equivariant Graph Neural Networks for Particle Physics Tasks"}], "temperature": 0.1}}
{"custom_id": "456_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Incorporating inductive biases into ML models is an active area of MLresearch, especially when ML models are applied to data about the physicalworld. Equivariant Graph Neural Networks (GNNs) have recently become a popularmethod for learning from physics data because they directly incorporate thesymmetries of the underlying physical system. Drawing from the relevantliterature around group equivariant networks, this paper presents acomprehensive evaluation of the proposed benefits of equivariant GNNs by usingreal-world particle physics reconstruction tasks as an evaluation test-bed. Wedemonstrate that many of the theoretical benefits generally associated withequivariant networks may not hold for realistic systems and introducecompelling directions for future research that will benefit both the scientifictheory of ML and physics applications."}], "temperature": 0.1}}
{"custom_id": "456_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Savannah Thais"}], "temperature": 0.1}}
{"custom_id": "457_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Rule Learning as Machine Translation using the Atomic Knowledge Bank"}], "temperature": 0.1}}
{"custom_id": "457_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Machine learning models, and in particular language models, are being appliedto various tasks that require reasoning. While such models are good atcapturing patterns their ability to reason in a trustable and controlled manneris frequently questioned. On the other hand, logic-based rule systems allow forcontrolled inspection and already established verification methods. However itis well-known that creating such systems manually is time-consuming and proneto errors. We explore the capability of transformers to translate sentencesexpressing rules in natural language into logical rules. We see reasoners asthe most reliable tools for performing logical reasoning and focus ontranslating language into the format expected by such tools. We performexperiments using the DKET dataset from the literature and create a dataset forlanguage to logic translation based on the Atomic knowledge bank."}], "temperature": 0.1}}
{"custom_id": "457_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Kristoffer 脝s酶y"}], "temperature": 0.1}}
{"custom_id": "458_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions"}], "temperature": 0.1}}
{"custom_id": "458_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advances in Large Language Models (LLMs) have presented newopportunities for integrating Artificial General Intelligence (AGI) intobiological research and education. This study evaluated the capabilities ofleading LLMs, including GPT-4, GPT-3.5, PaLM2, Claude2, and SenseNova, inanswering conceptual biology questions. The models were tested on a108-question multiple-choice exam covering biology topics in molecular biology,biological techniques, metabolic engineering, and synthetic biology. Among themodels, GPT-4 achieved the highest average score of 90 and demonstrated thegreatest consistency across trials with different prompts. The resultsindicated GPT-4's proficiency in logical reasoning and its potential to aidbiology research through capabilities like data analysis, hypothesisgeneration, and knowledge integration. However, further development andvalidation are still required before the promise of LLMs in acceleratingbiological discovery can be realized."}], "temperature": 0.1}}
{"custom_id": "458_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xinyu Gong"}], "temperature": 0.1}}
{"custom_id": "459_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LLM4Drive: A Survey of Large Language Models for Autonomous Driving"}], "temperature": 0.1}}
{"custom_id": "459_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Autonomous driving technology, a catalyst for revolutionizing transportationand urban mobility, has the tend to transition from rule-based systems todata-driven strategies. Traditional module-based systems are constrained bycumulative errors among cascaded modules and inflexible pre-set rules. Incontrast, end-to-end autonomous driving systems have the potential to avoiderror accumulation due to their fully data-driven training process, althoughthey often lack transparency due to their \"black box\" nature, complicating thevalidation and traceability of decisions. Recently, large language models(LLMs) have demonstrated abilities including understanding context, logicalreasoning, and generating answers. A natural thought is to utilize theseabilities to empower autonomous driving. By combining LLM with foundationvision models, it could open the door to open-world understanding, reasoning,and few-shot learning, which current autonomous driving systems are lacking. Inthis paper, we systematically review a research line about \\textit{LargeLanguage Models for Autonomous Driving (LLM4AD)}. This study evaluates thecurrent state of technological advancements, distinctly outlining the principalchallenges and prospective directions for the field. For the convenience ofresearchers in academia and industry, we provide real-time updates on thelatest advances in the field as well as relevant open-source resources via thedesignated link: https://github.com/Thinklab-SJTU/Awesome-LLM4AD."}], "temperature": 0.1}}
{"custom_id": "459_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhenjie Yang"}], "temperature": 0.1}}
{"custom_id": "460_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis"}], "temperature": 0.1}}
{"custom_id": "460_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advances in prompt engineering enable large language models (LLMs) tosolve multi-hop logical reasoning problems with impressive accuracy. However,there is little existing work investigating the robustness of LLMs withfew-shot prompting techniques. Therefore, we introduce a systematic approach totest the robustness of LLMs in multi-hop reasoning tasks via domain-agnosticperturbations. We include perturbations at multiple levels of abstractions(e.g. lexical perturbations such as typos, and semantic perturbations such asthe inclusion of intermediate reasoning steps in the questions) to conductbehavioral analysis on the LLMs. Throughout our experiments, we find thatmodels are more sensitive to certain perturbations such as replacing words withtheir synonyms. We also demonstrate that increasing the proportion of perturbedexemplars in the prompts improves the robustness of few-shot prompting methods."}], "temperature": 0.1}}
{"custom_id": "460_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hongyi Zheng"}], "temperature": 0.1}}
{"custom_id": "461_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy"}], "temperature": 0.1}}
{"custom_id": "461_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advances in large language models (LLMs) have revolutionized thelandscape of reasoning tasks. To enhance the capabilities of LLMs to emulatehuman reasoning, prior studies have focused on modeling reasoning steps usingvarious thought structures like chains, trees, or graphs. However, LLM-basedreasoning still encounters the following challenges: (1) Limited adaptabilityof preset structures to diverse tasks; (2) Insufficient precision in exploitingknown conditions to derive new ones; and (3) Inadequate consideration ofhistorical reasoning experiences for subsequent reasoning steps. To this end,we propose DetermLR, a novel perspective that rethinks the reasoning process asan evolution from indeterminacy to determinacy. First, we categorize knownconditions into two types: determinate and indeterminate premises This providesan oveall direction for the reasoning process and guides LLMs in convertingindeterminate data into progressively determinate insights. Subsequently, weleverage quantitative measurements to prioritize more relevant premises toexplore new insights. Furthermore, we automate the storage and extraction ofavailable premises and reasoning paths with reasoning memory, preservinghistorical reasoning details for subsequent reasoning steps. Comprehensiveexperimental results demonstrate that DetermLR surpasses all baselines onvarious logical reasoning benchmarks: LogiQA, ProofWriter, FOLIO, PrOntoQA, andLogicalDeduction. Compared to previous multi-step reasoning methods, DetermLRachieves higher accuracy with fewer reasoning steps, highlighting its superiorefficiency and effectiveness in solving logical reasoning tasks."}], "temperature": 0.1}}
{"custom_id": "461_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hongda Sun"}], "temperature": 0.1}}
{"custom_id": "462_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Borhan: A Novel System for Prioritized Default Logic"}], "temperature": 0.1}}
{"custom_id": "462_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Prioritized Default Logic presents an optimal solution for addressingreal-world problems characterized by incomplete information and the need toestablish preferences among diverse scenarios. Although it has reached greatsuccess in the theoretical aspect, its practical implementation has receivedless attention. In this article, we introduce Borhan, a system designed andcreated for prioritized default logic reasoning. To create an effective system,we have refined existing default logic definitions, including the extensionconcept, and introduced novel concepts. In addition to its theoretical merits,Borhan proves its practical utility by efficiently addressing a range ofprioritized default logic problems. In addition, one of the advantages of oursystem is its ability to both store and report the explanation path for anyinferred triple, enhancing transparency and interpretability. Borhan is offeredas an open-source system, implemented in Python, and even offers a simplifiedJava version as a plugin for the Protege ontology editor. Borhan thusrepresents a significant step forward in bridging the gap between thetheoretical foundations of default logic and its real-world applications."}], "temperature": 0.1}}
{"custom_id": "462_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Alireza Shahbazi"}], "temperature": 0.1}}
{"custom_id": "463_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "POE: Process of Elimination for Multiple Choice Reasoning"}], "temperature": 0.1}}
{"custom_id": "463_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Language models (LMs) are capable of conducting in-context learning formultiple choice reasoning tasks, but the options in these tasks are treatedequally. As humans often first eliminate wrong options before picking the finalcorrect answer, we argue a similar two-step strategy can make LMs better atthese tasks. To this end, we present the Process of Elimination (POE), atwo-step scoring method. In the first step, POE scores each option, andeliminates seemingly wrong options. In the second step, POE masks these wrongoptions, and makes the final prediction from the remaining options. Zero-shotexperiments on 8 reasoning tasks illustrate the effectiveness of POE, and afollowing analysis finds our method to be especially performant on logicalreasoning tasks. We further analyze the effect of masks, and show that POEapplies to few-shot settings and large language models (LLMs) like ChatGPT."}], "temperature": 0.1}}
{"custom_id": "463_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chenkai Ma"}], "temperature": 0.1}}
{"custom_id": "464_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Breaking the Language Barrier: Improving Cross-Lingual Reasoning with Structured Self-Attention"}], "temperature": 0.1}}
{"custom_id": "464_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In this work, we study whether multilingual language models (MultiLMs) cantransfer logical reasoning abilities to other languages when they arefine-tuned for reasoning in a different language. We evaluate the cross-lingualreasoning abilities of MultiLMs in two schemes: (1) where the language of thecontext and the question remain the same in the new languages that are tested(i.e., the reasoning is still monolingual, but the model must transfer thelearned reasoning ability across languages), and (2) where the language of thecontext and the question is different (which we term code-switched reasoning).On two logical reasoning datasets, RuleTaker and LeapOfThought, we demonstratethat although MultiLMs can transfer reasoning ability across languages in amonolingual setting, they struggle to transfer reasoning abilities in acode-switched setting. Following this observation, we propose a novel attentionmechanism that uses a dedicated set of parameters to encourage cross-lingualattention in code-switched sequences, which improves the reasoning performanceby up to 14% and 4% on the RuleTaker and LeapOfThought datasets, respectively."}], "temperature": 0.1}}
{"custom_id": "464_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Negar Foroutan"}], "temperature": 0.1}}
{"custom_id": "465_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers"}], "temperature": 0.1}}
{"custom_id": "465_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning, i.e., deductively inferring the truth value of aconclusion from a set of premises, is an important task for artificialintelligence with wide potential impacts on science, mathematics, and society.While many prompting-based strategies have been proposed to enable LargeLanguage Models (LLMs) to do such reasoning more effectively, they still appearunsatisfactory, often failing in subtle and unpredictable ways. In this work,we investigate the validity of instead reformulating such tasks as modularneurosymbolic programming, which we call LINC: Logical Inference viaNeurosymbolic Computation. In LINC, the LLM acts as a semantic parser,translating premises and conclusions from natural language to expressions infirst-order logic. These expressions are then offloaded to an external theoremprover, which symbolically performs deductive inference. Leveraging thisapproach, we observe significant performance gains on FOLIO and a balancedsubset of ProofWriter for three different models in nearly all experimentalconditions we evaluate. On ProofWriter, augmenting the comparatively smallopen-source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5and GPT-4 with Chain-of-Thought (CoT) prompting by an absolute 38% and 10%,respectively. When used with GPT-4, LINC scores 26% higher than CoT onProofWriter while performing comparatively on FOLIO. Further analysis revealsthat although both methods on average succeed roughly equally often on thisdataset, they exhibit distinct and complementary failure modes. We thus providepromising evidence for how logical reasoning over natural language can betackled through jointly leveraging LLMs alongside symbolic provers. Allcorresponding code is publicly available at https://github.com/benlipkin/linc"}], "temperature": 0.1}}
{"custom_id": "465_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Theo X. Olausson"}], "temperature": 0.1}}
{"custom_id": "466_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism"}], "temperature": 0.1}}
{"custom_id": "466_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) take advantage of step-by-step reasoninginstructions, e.g., chain-of-thought (CoT) prompting. Building on this, theirability to perform CoT-style reasoning robustly is of interest from a probingperspective. In this study, we inspect the step-by-step reasoning ability ofLLMs with a focus on negation, which is a core linguistic phenomenon that isdifficult to process. In particular, we introduce several controlled settings(e.g., reasoning in case of fictional entities) to evaluate the logicalreasoning abilities of the models. We observed that dozens of modern LLMs werenot robust against lexical negation (e.g., plausible ->implausible) whenperforming CoT-style reasoning, and the results highlight unique limitations ineach LLM family."}], "temperature": 0.1}}
{"custom_id": "466_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mengyu Ye"}], "temperature": 0.1}}
{"custom_id": "467_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts"}], "temperature": 0.1}}
{"custom_id": "467_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "As large language models (LLMs) have shown effectiveness with differentprompting methods, such as Chain of Thought, Program of Thought, we find thatthese methods have formed a great complementarity to each other on mathreasoning tasks. In this work, we propose XoT, an integrated problem solvingframework by prompting LLMs with diverse reasoning thoughts. For each question,XoT always begins with selecting the most suitable method then executes eachmethod iteratively. Within each iteration, XoT actively checks the validity ofthe generated answer and incorporates the feedback from external executors,allowing it to dynamically switch among different prompting methods. Throughextensive experiments on 10 popular math reasoning datasets, we demonstrate theeffectiveness of our proposed approach and thoroughly analyze the strengths ofeach module. Moreover, empirical results suggest that our framework isorthogonal to recent work that makes improvements on single reasoning methodsand can further generalise to logical reasoning domain. By allowing methodswitching, XoT provides a fresh perspective on the collaborative integration ofdiverse reasoning thoughts in a unified framework. The code is available athttps://github.com/tengxiaoliu/XoT."}], "temperature": 0.1}}
{"custom_id": "467_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Tengxiao Liu"}], "temperature": 0.1}}
{"custom_id": "468_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "DetectGPT-SC: Improving Detection of Text Generated by Large Language Models through Self-Consistency with Masked Predictions"}], "temperature": 0.1}}
{"custom_id": "468_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "General large language models (LLMs) such as ChatGPT have shown remarkablesuccess, but it has also raised concerns among people about the misuse ofAI-generated texts. Therefore, an important question is how to detect whetherthe texts are generated by ChatGPT or by humans. Existing detectors are builton the assumption that there is a distribution gap between human-generated andAI-generated texts. These gaps are typically identified using statisticalinformation or classifiers. In contrast to prior research methods, we find thatlarge language models such as ChatGPT exhibit strong self-consistency in textgeneration and continuation. Self-consistency capitalizes on the intuition thatAI-generated texts can still be reasoned with by large language models usingthe same logical reasoning when portions of the texts are masked, which differsfrom human-generated texts. Using this observation, we subsequently proposed anew method for AI-generated texts detection based on self-consistency withmasked predictions to determine whether a text is generated by LLMs. Thismethod, which we call DetectGPT-SC. We conducted a series of experiments toevaluate the performance of DetectGPT-SC. In these experiments, we employedvarious mask scheme, zero-shot, and simple prompt for completing masked textsand self-consistency predictions. The results indicate that DetectGPT-SCoutperforms the current state-of-the-art across different tasks."}], "temperature": 0.1}}
{"custom_id": "468_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Rongsheng Wang"}], "temperature": 0.1}}
{"custom_id": "469_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Retrieval-Augmented Neural Response Generation Using Logical Reasoning and Relevance Scoring"}], "temperature": 0.1}}
{"custom_id": "469_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Constructing responses in task-oriented dialogue systems typically relies oninformation sources such the current dialogue state or external databases. Thispaper presents a novel approach to knowledge-grounded response generation thatcombines retrieval-augmented language models with logical reasoning. Theapproach revolves around a knowledge graph representing the current dialoguestate and background information, and proceeds in three steps. The knowledgegraph is first enriched with logically derived facts inferred usingprobabilistic logical programming. A neural model is then employed at each turnto score the conversational relevance of each node and edge of this extendedgraph. Finally, the elements with highest relevance scores are converted to anatural language form, and are integrated into the prompt for the neuralconversational model employed to generate the system response.  We investigate the benefits of the proposed approach on two datasets (KVRETand GraphWOZ) along with a human evaluation. Experimental results show that thecombination of (probabilistic) logical reasoning with conversational relevancescoring does increase both the factuality and fluency of the responses."}], "temperature": 0.1}}
{"custom_id": "469_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Nicholas Thomas Walker"}], "temperature": 0.1}}
{"custom_id": "470_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Improving Large Language Models in Event Relation Logical Prediction"}], "temperature": 0.1}}
{"custom_id": "470_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Event relations are crucial for narrative understanding and reasoning.Governed by nuanced logic, event relation extraction (ERE) is a challengingtask that demands thorough semantic understanding and rigorous logicalreasoning. In this paper, we conduct an in-depth investigation tosystematically explore the capability of LLMs in understanding and applyingevent relation logic. More in detail, we first investigate the deficiencies ofLLMs in logical reasoning across different tasks. Our study reveals that LLMsare not logically consistent reasoners, which results in their suboptimalperformance on tasks that need rigorous reasoning. To address this, we explorethree different approaches to endow LLMs with event relation logic, and thusenable them to generate more coherent answers across various scenarios. Basedon our approach, we also contribute a synthesized dataset (LLM-ERL) involvinghigh-order reasoning for evaluation and fine-tuning. Extensive quantitative andqualitative analyses on different tasks also validate the effectiveness of ourapproaches and provide insights for solving practical tasks with LLMs in futurework. Codes are available at https://github.com/chenmeiqii/Teach-LLM-LR."}], "temperature": 0.1}}
{"custom_id": "470_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Meiqi Chen"}], "temperature": 0.1}}
{"custom_id": "471_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The potential of large language models for improving probability learning: A study on ChatGPT3.5 and first-year computer engineering students"}], "temperature": 0.1}}
{"custom_id": "471_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In this paper, we assess the efficacy of ChatGPT (version Feb 2023), alarge-scale language model, in solving probability problems typically presentedin introductory computer engineering exams. Our study comprised a set of 23probability exercises administered to students at Rey Juan Carlos University(URJC) in Madrid. The responses produced by ChatGPT were evaluated by a groupof five statistics professors, who assessed them qualitatively and assignedgrades based on the same criteria used for students. Our results indicate thatChatGPT surpasses the average student in terms of phrasing, organization, andlogical reasoning. The model's performance remained consistent for both theSpanish and English versions of the exercises. However, ChatGPT encountereddifficulties in executing basic numerical operations. Our experimentsdemonstrate that requesting ChatGPT to provide the solution in the form of an Rscript proved to be an effective approach for overcoming these limitations. Insummary, our results indicate that ChatGPT surpasses the average student insolving probability problems commonly presented in introductory computerengineering exams. Nonetheless, the model exhibits limitations in reasoningaround certain probability concepts. The model's ability to deliverhigh-quality explanations and illustrate solutions in any programming language,coupled with its performance in solving probability exercises, suggests thatlarge language models have the potential to serve as learning assistants."}], "temperature": 0.1}}
{"custom_id": "471_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Angel Udias"}], "temperature": 0.1}}
{"custom_id": "472_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Empower Nested Boolean Logic via Self-Supervised Curriculum Learning"}], "temperature": 0.1}}
{"custom_id": "472_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Beyond the great cognitive powers showcased by language models, it is crucialto scrutinize whether their reasoning capabilities stem from stronggeneralization or merely exposure to relevant data. As opposed to constructingincreasingly complex logic, this paper probes into the boolean logic, the rootcapability of a logical reasoner. We find that any pre-trained language modelseven including large language models only behave like a random selector in theface of multi-nested boolean logic, a task that humans can handle with ease. Toempower language models with this fundamental capability, this paper proposes anew self-supervised learning method \\textit{Curriculum Logical Reasoning}(\\textsc{Clr}), where we augment the training data with nested boolean logicchain step-by-step, and program the training from simpler logical patternsgradually to harder ones. This new training paradigm allows language models toeffectively generalize to much harder and longer-hop logic, which can hardly belearned through naive training. Furthermore, we show that boolean logic is agreat foundation for improving the subsequent general logical tasks."}], "temperature": 0.1}}
{"custom_id": "472_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hongqiu Wu"}], "temperature": 0.1}}
{"custom_id": "473_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers"}], "temperature": 0.1}}
{"custom_id": "473_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In recent years, many interpretability methods have been proposed to helpinterpret the internal states of Transformer-models, at different levels ofprecision and complexity. Here, to analyze encoder-decoder Transformers, wepropose a simple, new method: DecoderLens. Inspired by the LogitLens (fordecoder-only Transformers), this method involves allowing the decoder tocross-attend representations of intermediate encoder layers instead of usingthe final encoder output, as is normally done in encoder-decoder models. Themethod thus maps previously uninterpretable vector representations tohuman-interpretable sequences of words or symbols. We report results from theDecoderLens applied to models trained on question answering, logical reasoning,speech recognition and machine translation. The DecoderLens reveals severalspecific subtasks that are solved at low or intermediate layers, shedding newlight on the information flow inside the encoder component of this importantclass of models."}], "temperature": 0.1}}
{"custom_id": "473_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Anna Langedijk"}], "temperature": 0.1}}
{"custom_id": "474_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Learning Reliable Logical Rules with SATNet"}], "temperature": 0.1}}
{"custom_id": "474_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Bridging logical reasoning and deep learning is crucial for advanced AIsystems. In this work, we present a new framework that addresses this goal bygenerating interpretable and verifiable logical rules through differentiablelearning, without relying on pre-specified logical structures. Our approachbuilds upon SATNet, a differentiable MaxSAT solver that learns the underlyingrules from input-output examples. Despite its efficacy, the learned weights inSATNet are not straightforwardly interpretable, failing to producehuman-readable rules. To address this, we propose a novel specification methodcalled \"maximum equality\", which enables the interchangeability between thelearned weights of SATNet and a set of propositional logical rules in weightedMaxSAT form. With the decoded weighted MaxSAT formula, we further introduceseveral effective verification techniques to validate it against the groundtruth rules. Experiments on stream transformations and Sudoku problems showthat our decoded rules are highly reliable: using exact solvers on them couldachieve 100% accuracy, whereas the original SATNet fails to give correctsolutions in many cases. Furthermore, we formally verify that our decodedlogical rules are functionally equivalent to the ground truth ones."}], "temperature": 0.1}}
{"custom_id": "474_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhaoyu Li"}], "temperature": 0.1}}
{"custom_id": "475_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models"}], "temperature": 0.1}}
{"custom_id": "475_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning is fundamental for humans yet presents a substantialchallenge in the domain of Artificial Intelligence. Initially, researchers usedKnowledge Representation and Reasoning (KR) systems that did not scale andrequired non-trivial manual effort. Recently, the emergence of large languagemodels (LLMs) has demonstrated the ability to overcome various limitations offormal Knowledge Representation (KR) systems. Consequently, there's a growinginterest in using LLMs for logical reasoning via natural language. This workstrives to understand the proficiency of LLMs in logical reasoning by offeringa brief review of the latest progress in this area; with a focus on the logicalreasoning datasets, tasks, and the methods adopted to utilize LLMs forreasoning. To offer a thorough analysis, we have compiled a benchmark titledLogiGLUE. This includes 24 varied datasets encompassing deductive, abductive,and inductive reasoning. Utilizing LogiGLUE as a foundation, we have trained aninstruction fine-tuned language model, resulting in LogiT5. We studysingle-task training, multi-task training, and \"chain-of-thought\" knowledgedistillation fine-tuning technique to assess the performance of model acrossthe different logical reasoning categories. We also assess various LLMs usingLogiGLUE, and the findings indicate that LLMs excel most in abductivereasoning, followed by deductive reasoning, while they are least effective atinductive reasoning. We aim to shed light on the capabilities and potentialpathways for enhancing logical reasoning proficiency in LLMs, paving the wayfor more advanced and nuanced developments in this critical field."}], "temperature": 0.1}}
{"custom_id": "475_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Man Luo"}], "temperature": 0.1}}
{"custom_id": "476_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "ReAcTable: Enhancing ReAct for Table Question Answering"}], "temperature": 0.1}}
{"custom_id": "476_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Table Question Answering (TQA) presents a substantial challenge at theintersection of natural language processing and data analytics. This taskinvolves answering natural language (NL) questions on top of tabular data,demanding proficiency in logical reasoning, understanding of data semantics,and fundamental analytical capabilities. Due to its significance, a substantialvolume of research has been dedicated to exploring a wide range of strategiesaimed at tackling this challenge including approaches that leverage LargeLanguage Models (LLMs) through in-context learning or Chain-of-Thought (CoT)prompting as well as approaches that train and fine-tune custom models.  Nonetheless, a conspicuous gap exists in the research landscape, where thereis limited exploration of how innovative foundational research, whichintegrates incremental reasoning with external tools in the context of LLMs, asexemplified by the ReAct paradigm, could potentially bring advantages to theTQA task. In this paper, we aim to fill this gap, by introducing ReAcTable(ReAct for Table Question Answering tasks), a framework inspired by the ReActparadigm that is carefully enhanced to address the challenges uniquelyappearing in TQA tasks such as interpreting complex data semantics, dealingwith errors generated by inconsistent data and generating intricate datatransformations. ReAcTable relies on external tools such as SQL and Python codeexecutors, to progressively enhance the data by generating intermediate datarepresentations, ultimately transforming it into a more accessible format foranswering the questions with greater ease. We demonstrate that ReAcTableachieves remarkable performance even when compared to fine-tuned approaches. Inparticular, it outperforms the best prior result on the WikiTQ benchmark,achieving an accuracy of 68.0% without requiring training a new model orfine-tuning."}], "temperature": 0.1}}
{"custom_id": "476_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yunjia Zhang"}], "temperature": 0.1}}
{"custom_id": "477_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LOGICSEG: Parsing Visual Semantics with Neural Logic Learning and Reasoning"}], "temperature": 0.1}}
{"custom_id": "477_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Current high-performance semantic segmentation models are purely data-drivensub-symbolic approaches and blind to the structured nature of the visual world.This is in stark contrast to human cognition which abstracts visual perceptionsat multiple levels and conducts symbolic reasoning with such structuredabstraction. To fill these fundamental gaps, we devise LOGICSEG, a holisticvisual semantic parser that integrates neural inductive learning and logicreasoning with both rich data and symbolic knowledge. In particular, thesemantic concepts of interest are structured as a hierarchy, from which a setof constraints are derived for describing the symbolic relations and formalizedas first-order logic rules. After fuzzy logic-based continuous relaxation,logical formulae are grounded onto data and neural computational graphs, henceenabling logic-induced network training. During inference, logical constraintsare packaged into an iterative process and injected into the network in a formof several matrix multiplications, so as to achieve hierarchy-coherentprediction with logic reasoning. These designs together make LOGICSEG a generaland compact neural-logic machine that is readily integrated into existingsegmentation models. Extensive experiments over four datasets with varioussegmentation models and backbones verify the effectiveness and generality ofLOGICSEG. We believe this study opens a new avenue for visual semantic parsing."}], "temperature": 0.1}}
{"custom_id": "477_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Liulei Li"}], "temperature": 0.1}}
{"custom_id": "478_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Walking-by-Logic: Signal Temporal Logic-Guided Model Predictive Control for Bipedal Locomotion Resilient to External Perturbations"}], "temperature": 0.1}}
{"custom_id": "478_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This study proposes a novel planning framework based on a model predictivecontrol formulation that incorporates signal temporal logic (STL)specifications for task completion guarantees and robustness quantification.This marks the first-ever study to apply STL-guided trajectory optimization forbipedal locomotion push recovery, where the robot experiences unexpecteddisturbances. Existing recovery strategies often struggle with complex tasklogic reasoning and locomotion robustness evaluation, making them susceptibleto failures caused by inappropriate recovery strategies or insufficientrobustness. To address this issue, the STL-guided framework generates optimaland safe recovery trajectories that simultaneously satisfy the taskspecification and maximize the locomotion robustness. Our framework outperformsa state-of-the-art locomotion controller in a high-fidelity dynamic simulation,especially in scenarios involving crossed-leg maneuvers. Furthermore, itdemonstrates versatility in tasks such as locomotion on stepping stones, wherethe robot must select from a set of disjointed footholds to maneuversuccessfully."}], "temperature": 0.1}}
{"custom_id": "478_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhaoyuan Gu"}], "temperature": 0.1}}
{"custom_id": "479_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Do PLMs Know and Understand Ontological Knowledge?"}], "temperature": 0.1}}
{"custom_id": "479_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ontological knowledge, which comprises classes and properties and theirrelationships, is integral to world knowledge. It is significant to explorewhether Pretrained Language Models (PLMs) know and understand such knowledge.However, existing PLM-probing studies focus mainly on factual knowledge,lacking a systematic probing of ontological knowledge. In this paper, we focuson probing whether PLMs store ontological knowledge and have a semanticunderstanding of the knowledge rather than rote memorization of the surfaceform. To probe whether PLMs know ontological knowledge, we investigate how wellPLMs memorize: (1) types of entities; (2) hierarchical relationships amongclasses and properties, e.g., Person is a subclass of Animal and Member ofSports Team is a subproperty of Member of ; (3) domain and range constraints ofproperties, e.g., the subject of Member of Sports Team should be a Person andthe object should be a Sports Team. To further probe whether PLMs trulyunderstand ontological knowledge beyond memorization, we comprehensively studywhether they can reliably perform logical reasoning with given knowledgeaccording to ontological entailment rules. Our probing results show that PLMscan memorize certain ontological knowledge and utilize implicit knowledge inreasoning. However, both the memorizing and reasoning performances are lessthan perfect, indicating incomplete knowledge and understanding."}], "temperature": 0.1}}
{"custom_id": "479_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Weiqi Wu"}], "temperature": 0.1}}
{"custom_id": "480_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "On the Potential of CLIP for Compositional Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "480_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In this paper we explore the possibility of using OpenAI's CLIP to performlogically coherent grounded visual reasoning. To that end, we formalize ourterms and give a geometric analysis of how embeddings in CLIP's latent spacewould need to be configured in order for the system to be logically coherent.Our main conclusion is that, as usually configured, CLIP cannot perform suchreasoning."}], "temperature": 0.1}}
{"custom_id": "480_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Justin Brody"}], "temperature": 0.1}}
{"custom_id": "481_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Human Comprehensible Active Learning of Genome-Scale Metabolic Networks"}], "temperature": 0.1}}
{"custom_id": "481_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "An important application of Synthetic Biology is the engineering of the hostcell system to yield useful products. However, an increase in the scale of thehost system leads to huge design space and requires a large number ofvalidation trials with high experimental costs. A comprehensible machinelearning approach that efficiently explores the hypothesis space and guidesexperimental design is urgently needed for the Design-Build-Test-Learn (DBTL)cycle of the host cell system. We introduce a novel machine learning frameworkILP-iML1515 based on Inductive Logic Programming (ILP) that performs abductivelogical reasoning and actively learns from training examples. In contrast tonumerical models, ILP-iML1515 is built on comprehensible logicalrepresentations of a genome-scale metabolic model and can update the model bylearning new logical structures from auxotrophic mutant trials. The ILP-iML1515framework 1) allows high-throughput simulations and 2) actively selectsexperiments that reduce the experimental cost of learning gene functions incomparison to randomly selected experiments."}], "temperature": 0.1}}
{"custom_id": "481_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Lun Ai"}], "temperature": 0.1}}
{"custom_id": "482_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "How susceptible are LLMs to Logical Fallacies?"}], "temperature": 0.1}}
{"custom_id": "482_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This paper investigates the rational thinking capability of Large LanguageModels (LLMs) in multi-round argumentative debates by exploring the impact offallacious arguments on their logical reasoning performance. More specifically,we present Logic Competence Measurement Benchmark (LOGICOM), a diagnosticbenchmark to assess the robustness of LLMs against logical fallacies. LOGICOMinvolves two agents: a persuader and a debater engaging in a multi-round debateon a controversial topic, where the persuader tries to convince the debater ofthe correctness of its claim. First, LOGICOM assesses the potential of LLMs tochange their opinions through reasoning. Then, it evaluates the debater'sperformance in logical reasoning by contrasting the scenario where thepersuader employs logical fallacies against one where logical reasoning isused. We use this benchmark to evaluate the performance of GPT-3.5 and GPT-4using a dataset containing controversial topics, claims, and reasons supportingthem. Our findings indicate that both GPT-3.5 and GPT-4 can adjust theiropinion through reasoning. However, when presented with logical fallacies,GPT-3.5 and GPT-4 are erroneously convinced 41% and 69% more often,respectively, compared to when logical reasoning is used. Finally, we introducea new dataset containing over 5k pairs of logical vs. fallacious arguments. Thesource code and dataset of this work are made publicly available."}], "temperature": 0.1}}
{"custom_id": "482_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Amirreza Payandeh"}], "temperature": 0.1}}
{"custom_id": "483_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought"}], "temperature": 0.1}}
{"custom_id": "483_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent advancements in large-scale models, such as GPT-4, have showcasedremarkable capabilities in addressing standard queries. However, when facingcomplex problems that require multi-step logical reasoning, their accuracydramatically decreases. Current research has explored the realm of\\textit{prompting engineering} to bolster the inferential capacities of thesemodels. Our paper unveils a pioneering prompting technique, dubbed\\textit{Graph of Thoughts (GoT)}. Through testing on a trio of escalatingchallenges: the 24-point game, resolution of high-degree polynomial equations,and derivation of formulas for recursive sequences, our method outperformedGPT-4, achieving accuracy improvements of $89.7\\%$, $86\\%$, and $56\\%$ for eachrespective task. Moreover, when juxtaposed with the state-of-the-art (SOTA)prompting method, \\textit{Tree of Thought (ToT)}, our approach registered anaverage accuracy boost of $23\\%$, $24\\%$, and $15\\%$."}], "temperature": 0.1}}
{"custom_id": "483_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Bin Lei"}], "temperature": 0.1}}
{"custom_id": "484_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Why Not? Explaining Missing Entailments with Evee (Technical Report)"}], "temperature": 0.1}}
{"custom_id": "484_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Understanding logical entailments derived by a description logic reasoner isnot always straight-forward for ontology users. For this reason, variousmethods for explaining entailments using justifications and proofs have beendeveloped and implemented as plug-ins for the ontology editor Prot\\'eg\\'e.However, when the user expects a missing consequence to hold, it is equallyimportant to explain why it does not follow from the ontology. In this paper,we describe a new version of $\\rm E{\\scriptsize VEE}$, a Prot\\'eg\\'e pluginthat now also provides explanations for missing consequences, via existing andnew techniques based on abduction and counterexamples."}], "temperature": 0.1}}
{"custom_id": "484_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Christian Alrabbaa"}], "temperature": 0.1}}
{"custom_id": "485_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Thinking Like an Expert:Multimodal Hypergraph-of-Thought (HoT) Reasoning to boost Foundation Modals"}], "temperature": 0.1}}
{"custom_id": "485_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning ability is one of the most crucial capabilities of a foundationmodel, signifying its capacity to address complex reasoning tasks.Chain-of-Thought (CoT) technique is widely regarded as one of the effectivemethods for enhancing the reasoning ability of foundation models and hasgarnered significant attention. However, the reasoning process of CoT islinear, step-by-step, similar to personal logical reasoning, suitable forsolving general and slightly complicated problems. On the contrary, thethinking pattern of an expert owns two prominent characteristics that cannot behandled appropriately in CoT, i.e., high-order multi-hop reasoning andmultimodal comparative judgement. Therefore, the core motivation of this paperis transcending CoT to construct a reasoning paradigm that can think like anexpert. The hyperedge of a hypergraph could connect various vertices, making itnaturally suitable for modelling high-order relationships. Inspired by this,this paper innovatively proposes a multimodal Hypergraph-of-Thought (HoT)reasoning paradigm, which enables the foundation models to possess theexpert-level ability of high-order multi-hop reasoning and multimodalcomparative judgement. Specifically, a textual hypergraph-of-thought isconstructed utilizing triple as the primary thought to model higher-orderrelationships, and a hyperedge-of-thought is generated through multi-hopwalking paths to achieve multi-hop inference. Furthermore, we devise a visualhypergraph-of-thought to interact with the textual hypergraph-of-thought viaCross-modal Co-Attention Graph Learning for multimodal comparativeverification. Experimentations on the ScienceQA benchmark demonstrate theproposed HoT-based T5 outperforms CoT-based GPT3.5 and chatGPT, which is on parwith CoT-based GPT4 with a lower model size."}], "temperature": 0.1}}
{"custom_id": "485_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Fanglong Yao"}], "temperature": 0.1}}
{"custom_id": "486_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neuro-Symbolic RDF and Description Logic Reasoners: The State-Of-The-Art and Challenges"}], "temperature": 0.1}}
{"custom_id": "486_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ontologies are used in various domains, with RDF and OWL being prominentstandards for ontology development. RDF is favored for its simplicity andflexibility, while OWL enables detailed domain knowledge representation.However, as ontologies grow larger and more expressive, reasoning complexityincreases, and traditional reasoners struggle to perform efficiently. Despiteoptimization efforts, scalability remains an issue. Additionally, advancementsin automated knowledge base construction have created large and expressiveontologies that are often noisy and inconsistent, posing further challenges forconventional reasoners. To address these challenges, researchers have exploredneuro-symbolic approaches that combine neural networks' learning capabilitieswith symbolic systems' reasoning abilities. In this chapter,we provide anoverview of the existing literature in the field of neuro-symbolic deductivereasoning supported by RDF(S), the description logics EL and ALC, and OWL 2 RL,discussing the techniques employed, the tasks they address, and other relevantefforts in this area."}], "temperature": 0.1}}
{"custom_id": "486_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Gunjan Singh"}], "temperature": 0.1}}
{"custom_id": "487_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Structural Embeddings of Tools for Large Language Models"}], "temperature": 0.1}}
{"custom_id": "487_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "It is evident that the current state of Large Language Models (LLMs)necessitates the incorporation of external tools. The lack of straightforwardalgebraic and logical reasoning is well documented and prompted researchers todevelop frameworks which allow LLMs to operate via external tools. Theontological nature of tool utilization for a specific task can be wellformulated with a Directed Acyclic Graph (DAG). The central aim of the paper isto highlight the importance of graph based approaches to LLM-tool interactionin near future. We propose an exemplary framework to guide the orchestration ofexponentially increasing numbers of external tools with LLMs,where objectivesand functionalities of tools are graph encoded hierarchically. Assuming thattextual segments of a Chain-of-Thought (CoT) can be imagined as a tool asdefined here, the graph based framework can pave new avenues in that particulardirection as well."}], "temperature": 0.1}}
{"custom_id": "487_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Eren Unlu"}], "temperature": 0.1}}
{"custom_id": "488_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A system of inference based on proof search: an extended abstract"}], "temperature": 0.1}}
{"custom_id": "488_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Gentzen designed his natural deduction proof system to ``come as close aspossible to actual reasoning.'' Indeed, natural deduction proofs closelyresemble the static structure of logical reasoning in mathematical arguments.However, different features of inference are compelling to capture when onewants to support the process of searching for proofs. PSF (Proof SearchFramework) attempts to capture these features naturally and directly. Thedesign and metatheory of PSF are presented, and its ability to specify a rangeof proof systems for classical, intuitionistic, and linear logic isillustrated."}], "temperature": 0.1}}
{"custom_id": "488_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Dale Miller"}], "temperature": 0.1}}
{"custom_id": "489_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Invalid Logic, Equivalent Gains: The Bizarreness of Reasoning in Language Model Prompting"}], "temperature": 0.1}}
{"custom_id": "489_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Language models can be prompted to reason through problems in a manner thatsignificantly improves performance. However, \\textit{why} such promptingimproves performance is unclear. Recent work showed that using logically\\textit{invalid} Chain-of-Thought (CoT) prompting improves performance almostas much as logically \\textit{valid} CoT prompting, and that editing CoT promptsto replace problem-specific information with abstract information orout-of-distribution information typically doesn't harm performance. Criticshave responded that these findings are based on too few and too easily solvedtasks to draw meaningful conclusions. To resolve this dispute, we test whetherlogically invalid CoT prompts offer the same level of performance gains aslogically valid prompts on the hardest tasks in the BIG-Bench benchmark, termedBIG-Bench Hard (BBH). We find that the logically \\textit{invalid} reasoningprompts do indeed achieve similar performance gains on BBH tasks as logicallyvalid reasoning prompts. We also discover that some CoT prompts used byprevious works contain logical errors. This suggests that covariates beyondlogically valid reasoning are responsible for performance improvements."}], "temperature": 0.1}}
{"custom_id": "489_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Rylan Schaeffer"}], "temperature": 0.1}}
{"custom_id": "490_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "COLLIE: Systematic Construction of Constrained Text Generation Tasks"}], "temperature": 0.1}}
{"custom_id": "490_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Text generation under constraints have seen increasing interests in naturallanguage processing, especially with the rapidly improving capabilities oflarge language models. However, existing benchmarks for constrained generationusually focus on fixed constraint types (e.g.,generate a sentence containingcertain words) that have proved to be easy for state-of-the-art models likeGPT-4. We present COLLIE, a grammar-based framework that allows thespecification of rich, compositional constraints with diverse generation levels(word, sentence, paragraph, passage) and modeling challenges (e.g.,languageunderstanding, logical reasoning, counting, semantic planning). We also developtools for automatic extraction of task instances given a constraint structureand a raw text corpus. Using COLLIE, we compile the COLLIE-v1 dataset with 2080instances comprising 13 constraint structures. We perform systematicexperiments across five state-of-the-art instruction-tuned language models andanalyze their performances to reveal shortcomings. COLLIE is designed to beextensible and lightweight, and we hope the community finds it useful todevelop more complex constraints and evaluations in the future."}], "temperature": 0.1}}
{"custom_id": "490_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Shunyu Yao"}], "temperature": 0.1}}
{"custom_id": "491_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neurosymbolic AI for Reasoning on Biomedical Knowledge Graphs"}], "temperature": 0.1}}
{"custom_id": "491_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Biomedical datasets are often modeled as knowledge graphs (KGs) because theycapture the multi-relational, heterogeneous, and dynamic natures of biomedicalsystems. KG completion (KGC), can, therefore, help researchers make predictionsto inform tasks like drug repositioning. While previous approaches for KGC wereeither rule-based or embedding-based, hybrid approaches based on neurosymbolicartificial intelligence are becoming more popular. Many of these methodspossess unique characteristics which make them even better suited towardbiomedical challenges. Here, we survey such approaches with an emphasis ontheir utilities and prospective benefits for biomedicine."}], "temperature": 0.1}}
{"custom_id": "491_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Lauren Nicole DeLong"}], "temperature": 0.1}}
{"custom_id": "492_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "$\\text{EFO}_{k}$-CQA: Towards Knowledge Graph Complex Query Answering beyond Set Operation"}], "temperature": 0.1}}
{"custom_id": "492_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "To answer complex queries on knowledge graphs, logical reasoning overincomplete knowledge is required due to the open-world assumption.Learning-based methods are essential because they are capable of generalizingover unobserved knowledge. Therefore, an appropriate dataset is fundamental toboth obtaining and evaluating such methods under this paradigm. In this paper,we propose a comprehensive framework for data generation, model training, andmethod evaluation that covers the combinatorial space of ExistentialFirst-order Queries with multiple variables ($\\text{EFO}_{k}$). Thecombinatorial query space in our framework significantly extends those definedby set operations in the existing literature. Additionally, we construct adataset, $\\text{EFO}_{k}$-CQA, with 741 types of query for empiricalevaluation, and our benchmark results provide new insights into how queryhardness affects the results. Furthermore, we demonstrate that the existingdataset construction process is systematically biased that hinders theappropriate development of query-answering methods, highlighting the importanceof our work. Our code and data are providedin~\\url{https://github.com/HKUST-KnowComp/EFOK-CQA}."}], "temperature": 0.1}}
{"custom_id": "492_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hang Yin"}], "temperature": 0.1}}
{"custom_id": "493_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neural-Symbolic Recommendation with Graph-Enhanced Information"}], "temperature": 0.1}}
{"custom_id": "493_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The recommendation system is not only a problem of inductive statistics fromdata but also a cognitive task that requires reasoning ability. The mostadvanced graph neural networks have been widely used in recommendation systemsbecause they can capture implicit structured information from graph-structureddata. However, like most neural network algorithms, they only learn matchingpatterns from a perception perspective. Some researchers use user behavior forlogic reasoning to achieve recommendation prediction from the perspective ofcognitive reasoning, but this kind of reasoning is a local one and ignoresimplicit information on a global scale. In this work, we combine the advantagesof graph neural networks and propositional logic operations to construct aneuro-symbolic recommendation model with both global implicit reasoning abilityand local explicit logic reasoning ability. We first build an item-item graphbased on the principle of adjacent interaction and use graph neural networks tocapture implicit information in global data. Then we transform user behaviorinto propositional logic expressions to achieve recommendations from theperspective of cognitive reasoning. Extensive experiments on five publicdatasets show that our proposed model outperforms several state-of-the-artmethods, source code is avaliable at [https://github.com/hanzo2020/GNNLR]."}], "temperature": 0.1}}
{"custom_id": "493_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Bang Chen"}], "temperature": 0.1}}
{"custom_id": "494_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Counterfactual Collaborative Reasoning"}], "temperature": 0.1}}
{"custom_id": "494_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Causal reasoning and logical reasoning are two important types of reasoningabilities for human intelligence. However, their relationship has not beenextensively explored under machine intelligence context. In this paper, weexplore how the two reasoning abilities can be jointly modeled to enhance bothaccuracy and explainability of machine learning models. More specifically, byintegrating two important types of reasoning ability -- counterfactualreasoning and (neural) logical reasoning -- we propose CounterfactualCollaborative Reasoning (CCR), which conducts counterfactual logic reasoning toimprove the performance. In particular, we use recommender system as an exampleto show how CCR alleviate data scarcity, improve accuracy and enhancetransparency. Technically, we leverage counterfactual reasoning to generate\"difficult\" counterfactual training examples for data augmentation, which --together with the original training examples -- can enhance the modelperformance. Since the augmented data is model irrelevant, they can be used toenhance any model, enabling the wide applicability of the technique. Besides,most of the existing data augmentation methods focus on \"implicit dataaugmentation\" over users' implicit feedback, while our framework conducts\"explicit data augmentation\" over users explicit feedback based oncounterfactual logic reasoning. Experiments on three real-world datasets showthat CCR achieves better performance than non-augmented models and implicitlyaugmented models, and also improves model transparency by generatingcounterfactual explanations."}], "temperature": 0.1}}
{"custom_id": "494_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jianchao Ji"}], "temperature": 0.1}}
{"custom_id": "495_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "What is the Title of this Paper? Solving logic puzzles using algorithms"}], "temperature": 0.1}}
{"custom_id": "495_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This work delves into the realm of logic puzzles by focusing on the Knightand Knave problems popularized by Raymond Smullyan in his book series \"What isthe Name of This Book?\". The puzzles revolve around characters known as Knights(truth-tellers) and Knaves (liars), challenging solvers to determine the trueidentity of each person based on their statements. This paper explores theutilization of Python algorithms to automate the process of solving thesepuzzles, offering a computational approach that enhances efficiency andaccessibility. In this work, we aim to develop a Python algorithm capable ofparsing and analyzing the statements provided in the Knight and Knave puzzles.A logical reasoning framework is integrated within the algorithm to deduce theidentities of the characters based on their statements. The algorithm processesthe input statements, create a knowledge base, and make deductions followingthe rules of Knight and Knave logic. The developed algorithm is thoroughlytested on various instances of Knight and Knave puzzles, comparing its resultsto known solutions and manual approaches. We further expand the scope of theproblem by introducing a Normal (who can sometimes lie and sometimes say thetruth)."}], "temperature": 0.1}}
{"custom_id": "495_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ujaan Rakshit"}], "temperature": 0.1}}
{"custom_id": "496_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Exploring & Exploiting High-Order Graph Structure for Sparse Knowledge Graph Completion"}], "temperature": 0.1}}
{"custom_id": "496_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Sparse knowledge graph (KG) scenarios pose a challenge for previous KnowledgeGraph Completion (KGC) methods, that is, the completion performance decreasesrapidly with the increase of graph sparsity. This problem is also exacerbatedbecause of the widespread existence of sparse KGs in practical applications. Toalleviate this challenge, we present a novel framework, LR-GCN, that is able toautomatically capture valuable long-range dependency among entities tosupplement insufficient structure features and distill logical reasoningknowledge for sparse KGC. The proposed approach comprises two main components:a GNN-based predictor and a reasoning path distiller. The reasoning pathdistiller explores high-order graph structures such as reasoning paths andencodes them as rich-semantic edges, explicitly compositing long-rangedependencies into the predictor. This step also plays an essential role indensifying KGs, effectively alleviating the sparse issue. Furthermore, the pathdistiller further distills logical reasoning knowledge from these minedreasoning paths into the predictor. These two components are jointly optimizedusing a well-designed variational EM algorithm. Extensive experiments andanalyses on four sparse benchmarks demonstrate the effectiveness of ourproposed method."}], "temperature": 0.1}}
{"custom_id": "496_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Tao He"}], "temperature": 0.1}}
{"custom_id": "497_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "IDOL: Indicator-oriented Logic Pre-training for Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "497_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In the field of machine reading comprehension (MRC), existing systems havesurpassed the average performance of human beings in many tasks like SQuAD.However, there is still a long way to go when it comes to logical reasoning.Although some methods for it have been put forward, they either are designed ina quite complicated way or rely too much on external structures. In this paper,we proposed IDOL (InDicator-Oriented Logic Pre-training), an easy-to-understandbut highly effective further pre-training task which logically strengthens thepre-trained models with the help of 6 types of logical indicators and alogically rich dataset LGP (LoGic Pre-training). IDOL achieves state-of-the-artperformance on ReClor and LogiQA, the two most representative benchmarks inlogical reasoning MRC, and is proven to be capable of generalizing to differentpre-trained models and other types of MRC benchmarks like RACE and SQuAD 2.0while keeping competitive general language understanding ability throughtesting on tasks in GLUE. Besides, at the beginning of the era of largelanguage models, we take several of them like ChatGPT into comparison and findthat IDOL still shows its advantage."}], "temperature": 0.1}}
{"custom_id": "497_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zihang Xu"}], "temperature": 0.1}}
{"custom_id": "498_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning Ability and Human-like Biases"}], "temperature": 0.1}}
{"custom_id": "498_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This paper investigates whether current large language models exhibit biasesin logical reasoning, similar to humans. Specifically, we focus on syllogisticreasoning, a well-studied form of inference in the cognitive science of humandeduction. To facilitate our analysis, we introduce a dataset called NeuBAROCO,originally designed for psychological experiments that assess human logicalabilities in syllogistic reasoning. The dataset consists of syllogisticinferences in both English and Japanese. We examine three types of biasesobserved in human syllogistic reasoning: belief biases, conversion errors, andatmosphere effects. Our findings demonstrate that current large language modelsstruggle more with problems involving these three types of biases."}], "temperature": 0.1}}
{"custom_id": "498_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Risako Ando"}], "temperature": 0.1}}
{"custom_id": "499_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension"}], "temperature": 0.1}}
{"custom_id": "499_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Machine reading comprehension (MRC) poses new challenges over logicalreasoning, which aims to understand the implicit logical relations entailed inthe given contexts and perform inference over them. Due to the complexity oflogic, logical relations exist at different granularity levels. However, mostexisting methods of logical reasoning individually focus on either entity-awareor discourse-based information but ignore the hierarchical relations that mayeven have mutual effects. In this paper, we propose a holistic graph network(HGN) which deals with context at both discourse level and word level, as thebasis for logical reasoning, to provide a more fine-grained relationextraction. Specifically, node-level and type-level relations, which can beinterpreted as bridges in the reasoning process, are modeled by a hierarchicalinteraction mechanism to improve the interpretation of MRC systems.Experimental results on logical reasoning QA datasets (ReClor and LogiQA) andnatural language inference datasets (SNLI and ANLI) show the effectiveness andgeneralization of our method, and in-depth analysis verifies its capability tounderstand complex logical relations."}], "temperature": 0.1}}
{"custom_id": "499_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jialin Chen"}], "temperature": 0.1}}
{"custom_id": "500_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Language to Rewards for Robotic Skill Synthesis"}], "temperature": 0.1}}
{"custom_id": "500_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have demonstrated exciting progress in acquiringdiverse new capabilities through in-context learning, ranging from logicalreasoning to code-writing. Robotics researchers have also explored using LLMsto advance the capabilities of robotic control. However, since low-level robotactions are hardware-dependent and underrepresented in LLM training corpora,existing efforts in applying LLMs to robotics have largely treated LLMs assemantic planners or relied on human-engineered control primitives to interfacewith the robot. On the other hand, reward functions are shown to be flexiblerepresentations that can be optimized for control policies to achieve diversetasks, while their semantic richness makes them suitable to be specified byLLMs. In this work, we introduce a new paradigm that harnesses this realizationby utilizing LLMs to define reward parameters that can be optimized andaccomplish variety of robotic tasks. Using reward as the intermediate interfacegenerated by LLMs, we can effectively bridge the gap between high-levellanguage instructions or corrections to low-level robot actions. Meanwhile,combining this with a real-time optimizer, MuJoCo MPC, empowers an interactivebehavior creation experience where users can immediately observe the resultsand provide feedback to the system. To systematically evaluate the performanceof our proposed method, we designed a total of 17 tasks for a simulatedquadruped robot and a dexterous manipulator robot. We demonstrate that ourproposed method reliably tackles 90% of the designed tasks, while a baselineusing primitive skills as the interface with Code-as-policies achieves 50% ofthe tasks. We further validated our method on a real robot arm where complexmanipulation skills such as non-prehensile pushing emerge through ourinteractive system."}], "temperature": 0.1}}
{"custom_id": "500_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Wenhao Yu"}], "temperature": 0.1}}
{"custom_id": "501_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models as Tax Attorneys: A Case Study in Legal Capabilities Emergence"}], "temperature": 0.1}}
{"custom_id": "501_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Better understanding of Large Language Models' (LLMs) legal analysisabilities can contribute to improving the efficiency of legal services,governing artificial intelligence, and leveraging LLMs to identifyinconsistencies in law. This paper explores LLM capabilities in applying taxlaw. We choose this area of law because it has a structure that allows us toset up automated validation pipelines across thousands of examples, requireslogical reasoning and maths skills, and enables us to test LLM capabilities ina manner relevant to real-world economic lives of citizens and companies. Ourexperiments demonstrate emerging legal understanding capabilities, withimproved performance in each subsequent OpenAI model release. We experimentwith retrieving and utilising the relevant legal authority to assess the impactof providing additional legal context to LLMs. Few-shot prompting, presentingexamples of question-answer pairs, is also found to significantly enhance theperformance of the most advanced model, GPT-4. The findings indicate that LLMs,particularly when combined with prompting enhancements and the correct legaltexts, can perform at high levels of accuracy but not yet at expert tax lawyerlevels. As LLMs continue to advance, their ability to reason about lawautonomously could have significant implications for the legal profession andAI governance."}], "temperature": 0.1}}
{"custom_id": "501_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "John J. Nay"}], "temperature": 0.1}}
{"custom_id": "502_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Human-in-the-Loop through Chain-of-Thought"}], "temperature": 0.1}}
{"custom_id": "502_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "While the emergence of powerful language models along with Chain-of-thoughtprompting has made automation more and more omnipresent, it sometimesdemonstrates its weakness in long-term or multi-step logical reasoning. Forexample, users don't always get desirable answers for complex mathematicalproblems without human involvement. Against this background, we present theManual Correction System (MCS) -- a human-in-the-loop system enhanced byChain-of-Thought prompting, which explores how manual correction of sub-logicsin rationales can improve LLM's reasoning performance. Moving one step forward,considering a system with human-in-the-loop involves more than having humansimprove performance but also controlling the cost. Therefore, we post aCost-utility Analysis Model for Human-in-the-Loop systems (CAMLOP) based onclassical economics theory to analyze, quantify and balance the utility and thecorresponding cost. We conduct experiments of MCS and CAMLOP with twelvedatasets. A significant advantage w.r.t cost and utility proves its superiorityover strong baselines."}], "temperature": 0.1}}
{"custom_id": "502_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zefan Cai"}], "temperature": 0.1}}
{"custom_id": "503_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Deductive Verification of Chain-of-Thought Reasoning"}], "temperature": 0.1}}
{"custom_id": "503_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) significantly benefit from Chain-of-Thought(CoT) prompting in performing various reasoning tasks. While CoT allows modelsto produce more comprehensive reasoning processes, its emphasis on intermediatereasoning steps can inadvertently introduce hallucinations and accumulatederrors, thereby limiting models' ability to solve complex reasoning tasks.Inspired by how humans engage in careful and meticulous deductive logicalreasoning processes to solve tasks, we seek to enable language models toperform explicit and rigorous deductive reasoning, and also ensure thetrustworthiness of their reasoning process through self-verification. However,directly verifying the validity of an entire deductive reasoning process ischallenging, even with advanced models like ChatGPT. In light of this, wepropose to decompose a reasoning verification process into a series ofstep-by-step subprocesses, each only receiving their necessary context andpremises. To facilitate this procedure, we propose Natural Program, a naturallanguage-based deductive reasoning format. Our approach enables models togenerate precise reasoning steps where subsequent steps are more rigorouslygrounded on prior steps. It also empowers language models to carry outreasoning self-verification in a step-by-step manner. By integrating thisverification process into each deductive reasoning stage, we significantlyenhance the rigor and trustfulness of generated reasoning steps. Along thisprocess, we also improve the answer correctness on complex reasoning tasks.Code will be released at https://github.com/lz1oceani/verify_cot."}], "temperature": 0.1}}
{"custom_id": "503_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhan Ling"}], "temperature": 0.1}}
{"custom_id": "504_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "ChatGPT is a Remarkable Tool -- For Experts"}], "temperature": 0.1}}
{"custom_id": "504_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This paper investigates the capabilities of ChatGPT as an automated assistantin diverse domains, including scientific writing, mathematics, education,programming, and healthcare. We explore the potential of ChatGPT to enhanceproductivity, streamline problem-solving processes, and improve writing style.Furthermore, we highlight the potential risks associated with excessivereliance on ChatGPT in these fields. These limitations encompass factors likeincorrect and fictitious responses, inaccuracies in code, limited logicalreasoning abilities, overconfidence, and critical ethical concerns ofcopyrights and privacy violation. We outline areas and objectives where ChatGPTproves beneficial, applications where it should be used judiciously, andscenarios where its reliability may be limited. In light of observedlimitations, and given that the tool's fundamental errors may pose a specialchallenge for non-experts, ChatGPT should be used with a strategic methodology.By drawing from comprehensive experimental studies, we offer methods and flowcharts for effectively using ChatGPT. Our recommendations emphasize iterativeinteraction with ChatGPT and independent verification of its outputs.Considering the importance of utilizing ChatGPT judiciously and with expertise,we recommend its usage for experts who are well-versed in the respectivedomains."}], "temperature": 0.1}}
{"custom_id": "504_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Amos Azaria"}], "temperature": 0.1}}
{"custom_id": "505_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Knowledge-based Reasoning and Learning under Partial Observability in Ad Hoc Teamwork"}], "temperature": 0.1}}
{"custom_id": "505_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ad hoc teamwork refers to the problem of enabling an agent to collaboratewith teammates without prior coordination. Data-driven methods represent thestate of the art in ad hoc teamwork. They use a large labeled dataset of priorobservations to model the behavior of other agent types and to determine the adhoc agent's behavior. These methods are computationally expensive, lacktransparency, and make it difficult to adapt to previously unseen changes,e.g., in team composition. Our recent work introduced an architecture thatdetermined an ad hoc agent's behavior based on non-monotonic logical reasoningwith prior commonsense domain knowledge and predictive models of other agents'behavior that were learned from limited examples. In this paper, wesubstantially expand the architecture's capabilities to support: (a) onlineselection, adaptation, and learning of the models that predict the otheragents' behavior; and (b) collaboration with teammates in the presence ofpartial observability and limited communication. We illustrate andexperimentally evaluate the capabilities of our architecture in two simulatedmultiagent benchmark domains for ad hoc teamwork: Fort Attack and Half FieldOffense. We show that the performance of our architecture is comparable orbetter than state of the art data-driven baselines in both simple and complexscenarios, particularly in the presence of limited training data, partialobservability, and changes in team composition."}], "temperature": 0.1}}
{"custom_id": "505_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hasra Dodampegama"}], "temperature": 0.1}}
{"custom_id": "506_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "InDL: A New Dataset and Benchmark for In-Diagram Logic Interpretation based on Visual Illusion"}], "temperature": 0.1}}
{"custom_id": "506_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This paper introduces a novel approach to evaluating deep learning models'capacity for in-diagram logic interpretation. Leveraging the intriguing realmof visual illusions, we establish a unique dataset, InDL, designed torigorously test and benchmark these models. Deep learning has witnessedremarkable progress in domains such as computer vision and natural languageprocessing. However, models often stumble in tasks requiring logical reasoningdue to their inherent 'black box' characteristics, which obscure thedecision-making process. Our work presents a new lens to understand thesemodels better by focusing on their handling of visual illusions -- a complexinterplay of perception and logic. We utilize six classic geometric opticalillusions to create a comparative framework between human and machine visualperception. This methodology offers a quantifiable measure to rank models,elucidating potential weaknesses and providing actionable insights for modelimprovements. Our experimental results affirm the efficacy of our benchmarkingstrategy, demonstrating its ability to effectively rank models based on theirlogic interpretation ability. As part of our commitment to reproducibleresearch, the source code and datasets will be made publicly available athttps://github.com/rabbit-magic-wh/InDL"}], "temperature": 0.1}}
{"custom_id": "506_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Haobo Yang"}], "temperature": 0.1}}
{"custom_id": "507_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Synthesizing a Progression of Subtasks for Block-Based Visual Programming Tasks"}], "temperature": 0.1}}
{"custom_id": "507_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Block-based visual programming environments play an increasingly importantrole in introducing computing concepts to K-12 students. In recent years, theyhave also gained popularity in neuro-symbolic AI, serving as a benchmark toevaluate general problem-solving and logical reasoning skills. The open-endedand conceptual nature of these visual programming tasks make them challenging,both for state-of-the-art AI agents as well as for novice programmers. Anatural approach to providing assistance for problem-solving is breaking down acomplex task into a progression of simpler subtasks; however, this is nottrivial given that the solution codes are typically nested and have non-linearexecution behavior. In this paper, we formalize the problem of synthesizingsuch a progression for a given reference block-based visual programming task.We propose a novel synthesis algorithm that generates a progression of subtasksthat are high-quality, well-spaced in terms of their complexity, and solvingthis progression leads to solving the reference task. We show the utility ofour synthesis algorithm in improving the efficacy of AI agents (in this case,neural program synthesizers) for solving tasks in the Karel programmingenvironment. Then, we conduct a user study to demonstrate that our synthesizedprogression of subtasks can assist a novice programmer in solving tasks in theHour of Code: Maze Challenge by Code-dot-org."}], "temperature": 0.1}}
{"custom_id": "507_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Alperen Tercan"}], "temperature": 0.1}}
{"custom_id": "508_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Counterfactual reasoning: Testing language models' understanding of hypothetical scenarios"}], "temperature": 0.1}}
{"custom_id": "508_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Current pre-trained language models have enabled remarkable improvements indownstream tasks, but it remains difficult to distinguish effects ofstatistical correlation from more systematic logical reasoning grounded on theunderstanding of real world. We tease these factors apart by leveragingcounterfactual conditionals, which force language models to predict unusualconsequences based on hypothetical propositions. We introduce a set of testsfrom psycholinguistic experiments, as well as larger-scale controlled datasets,to probe counterfactual predictions from five pre-trained language models. Wefind that models are consistently able to override real-world knowledge incounterfactual scenarios, and that this effect is more robust in case ofstronger baseline world knowledge -- however, we also find that for most modelsthis effect appears largely to be driven by simple lexical cues. When wemitigate effects of both world knowledge and lexical cues to test knowledge oflinguistic nuances of counterfactuals, we find that only GPT-3 showssensitivity to these nuances, though this sensitivity is also non-triviallyimpacted by lexical associative factors."}], "temperature": 0.1}}
{"custom_id": "508_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jiaxuan Li"}], "temperature": 0.1}}
{"custom_id": "509_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Not wacky vs. definitely wacky: A study of scalar adverbs in pretrained language models"}], "temperature": 0.1}}
{"custom_id": "509_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Vector space models of word meaning all share the assumption that wordsoccurring in similar contexts have similar meanings. In such models, words thatare similar in their topical associations but differ in their logical forcetend to emerge as semantically close, creating well-known challenges for NLPapplications that involve logical reasoning. Modern pretrained language models,such as BERT, RoBERTa and GPT-3 hold the promise of performing better onlogical tasks than classic static word embeddings. However, reports are mixedabout their success. In the current paper, we advance this discussion through asystematic study of scalar adverbs, an under-explored class of words withstrong logical force. Using three different tasks, involving both naturalisticsocial media data and constructed examples, we investigate the extent to whichBERT, RoBERTa, GPT-2 and GPT-3 exhibit general, human-like, knowledge of thesecommon words. We ask: 1) Do the models distinguish amongst the three semanticcategories of MODALITY, FREQUENCY and DEGREE? 2) Do they have implicitrepresentations of full scales from maximally negative to maximally positive?3) How do word frequency and contextual factors impact model performance? Wefind that despite capturing some aspects of logical meaning, the models fallfar short of human performance."}], "temperature": 0.1}}
{"custom_id": "509_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Isabelle Lorge"}], "temperature": 0.1}}
{"custom_id": "510_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "DeepGate2: Functionality-Aware Circuit Representation Learning"}], "temperature": 0.1}}
{"custom_id": "510_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Circuit representation learning aims to obtain neural representations ofcircuit elements and has emerged as a promising research direction that can beapplied to various EDA and logic reasoning tasks. Existing solutions, such asDeepGate, have the potential to embed both circuit structural information andfunctional behavior. However, their capabilities are limited due to weaksupervision or flawed model design, resulting in unsatisfactory performance indownstream tasks. In this paper, we introduce DeepGate2, a novelfunctionality-aware learning framework that significantly improves upon theoriginal DeepGate solution in terms of both learning effectiveness andefficiency. Our approach involves using pairwise truth table differencesbetween sampled logic gates as training supervision, along with a well-designedand scalable loss function that explicitly considers circuit functionality.Additionally, we consider inherent circuit characteristics and design anefficient one-round graph neural network (GNN), resulting in an order ofmagnitude faster learning speed than the original DeepGate solution.Experimental results demonstrate significant improvements in two practicaldownstream tasks: logic synthesis and Boolean satisfiability solving. The codeis available at https://github.com/cure-lab/DeepGate2"}], "temperature": 0.1}}
{"custom_id": "510_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhengyuan Shi"}], "temperature": 0.1}}
{"custom_id": "511_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Deduction under Perturbed Evidence: Probing Student Simulation Capabilities of Large Language Models"}], "temperature": 0.1}}
{"custom_id": "511_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We explore whether Large Language Models (LLMs) are capable of logicalreasoning with distorted facts, which we call Deduction under PerturbedEvidence (DUPE). DUPE presents a unique challenge to LLMs since they typicallyrely on their parameters, which encode mostly accurate information, to reasonand make inferences. However, in DUPE, LLMs must reason over manipulated orfalsified evidence present in their prompts, which can result in falseconclusions that are valid only under the manipulated evidence. Our goal withDUPE is to determine whether LLMs can arrive at these false conclusions andidentify whether the dominant factor influencing the deduction process is theencoded data in the parameters or the manipulated evidence in the prompts. Toevaluate the DUPE capabilities of LLMs, we create a DUPEd version of theStrategyQA dataset, where facts are manipulated to reverse the answer to thequestion. Our findings show that even the most advanced GPT models struggle toreason on manipulated facts - showcasing poor DUPE skills - with accuracydropping by 45% compared to the original dataset. We also investigate promptsettings inspired from student simulation models, which mitigate the accuracydrop to some extent. Our findings have practical implications for understandingthe performance of LLMs in real-world applications such as student simulationmodels that involve reasoning over inaccurate information."}], "temperature": 0.1}}
{"custom_id": "511_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Shashank Sonkar"}], "temperature": 0.1}}
{"custom_id": "512_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Query Structure Modeling for Inductive Logical Reasoning Over Knowledge Graphs"}], "temperature": 0.1}}
{"custom_id": "512_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning over incomplete knowledge graphs to answer complex logicalqueries is a challenging task. With the emergence of new entities and relationsin constantly evolving KGs, inductive logical reasoning over KGs has become acrucial problem. However, previous PLMs-based methods struggle to model thelogical structures of complex queries, which limits their ability to generalizewithin the same structure. In this paper, we propose a structure-modeledtextual encoding framework for inductive logical reasoning over KGs. It encodeslinearized query structures and entities using pre-trained language models tofind answers. For structure modeling of complex queries, we design stepwiseinstructions that implicitly prompt PLMs on the execution order of geometricoperations in each query. We further separately model different geometricoperations (i.e., projection, intersection, and union) on the representationspace using a pre-trained encoder with additional attention and maxout layersto enhance structured modeling. We conduct experiments on two inductive logicalreasoning datasets and three transductive datasets. The results demonstrate theeffectiveness of our method on logical reasoning over KGs in both inductive andtransductive settings."}], "temperature": 0.1}}
{"custom_id": "512_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Siyuan Wang"}], "temperature": 0.1}}
{"custom_id": "513_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "513_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models (LLMs) have shown human-like reasoning abilities butstill struggle with complex logical problems. This paper introduces a novelframework, Logic-LM, which integrates LLMs with symbolic solvers to improvelogical problem-solving. Our method first utilizes LLMs to translate a naturallanguage problem into a symbolic formulation. Afterward, a deterministicsymbolic solver performs inference on the formulated problem. We also introducea self-refinement module, which utilizes the symbolic solver's error messagesto revise symbolic formalizations. We demonstrate Logic-LM's effectiveness onfive logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO,LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significantperformance boost of 39.2% over using LLM alone with standard prompting and18.4% over LLM with chain-of-thought prompting. Our findings suggest thatLogic-LM, by combining LLMs with symbolic logic, offers a promising avenue forfaithful logical reasoning. Code and data are publicly available athttps://github.com/teacherpeterpan/Logic-LLM."}], "temperature": 0.1}}
{"custom_id": "513_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Liangming Pan"}], "temperature": 0.1}}
{"custom_id": "514_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Simple Generative Model of Logical Reasoning and Statistical Learning"}], "temperature": 0.1}}
{"custom_id": "514_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Statistical learning and logical reasoning are two major fields of AIexpected to be unified for human-like machine intelligence. Most existing workconsiders how to combine existing logical and statistical systems. However,there is no theory of inference so far explaining how basic approaches tostatistical learning and logical reasoning stem from a common principle.Inspired by the fact that much empirical work in neuroscience suggests Bayesian(or probabilistic generative) approaches to brain function including learningand reasoning, we here propose a simple Bayesian model of logical reasoning andstatistical learning. The theory is statistically correct as it satisfiesKolmogorov's axioms, is consistent with both Fenstad's representation theoremand maximum likelihood estimation and performs exact Bayesian inference with alinear-time complexity. The theory is logically correct as it is a data-drivengeneralisation of uncertain reasoning from consistency, possibility,inconsistency and impossibility. The theory is correct in terms of machinelearning as its solution to generation and prediction tasks on the MNISTdataset is not only empirically reasonable but also theoretically correctagainst the K nearest neighbour method. We simply model how data causessymbolic knowledge in terms of its satisfiability in formal logic. Symbolicreasoning emerges as a result of the process of going the causality forwardsand backwards. The forward and backward processes correspond to aninterpretation and inverse interpretation in formal logic, respectively. Theinverse interpretation differentiates our work from the mainstream oftenreferred to as inverse entailment, inverse deduction or inverse resolution. Theperspective gives new insights into learning and reasoning towards human-likemachine intelligence."}], "temperature": 0.1}}
{"custom_id": "514_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hiroyuki Kido"}], "temperature": 0.1}}
{"custom_id": "515_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Knowledge Authoring for Rules and Actions"}], "temperature": 0.1}}
{"custom_id": "515_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Knowledge representation and reasoning (KRR) systems describe and reason withcomplex concepts and relations in the form of facts and rules. Unfortunately,wide deployment of KRR systems runs into the problem that domain experts havegreat difficulty constructing correct logical representations of their domainknowledge. Knowledge engineers can help with this construction process, butthere is a deficit of such specialists. The earlier Knowledge Authoring LogicMachine (KALM) based on Controlled Natural Language (CNL) was shown to havevery high accuracy for authoring facts and questions. More recently, KALMFL, asuccessor of KALM, replaced CNL with factual English, which is much lessrestrictive and requires very little training from users. However, KALMFL haslimitations in representing certain types of knowledge, such as authoring rulesfor multi-step reasoning or understanding actions with timestamps. To addressthese limitations, we propose KALMRA to enable authoring of rules and actions.Our evaluation using the UTI guidelines benchmark shows that KALMRA achieves ahigh level of correctness (100%) on rule authoring. When used for authoring andreasoning with actions, KALMRA achieves more than 99.3% correctness on the bAbIbenchmark, demonstrating its effectiveness in more sophisticated KRR jobs.Finally, we illustrate the logical reasoning capabilities of KALMRA by drawingattention to the problems faced by the recently made famous AI, ChatGPT."}], "temperature": 0.1}}
{"custom_id": "515_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yuheng Wang"}], "temperature": 0.1}}
{"custom_id": "516_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Scalable Coupling of Deep Learning with Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "516_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In the ongoing quest for hybridizing discrete reasoning with neural nets,there is an increasing interest in neural architectures that can learn how tosolve discrete reasoning or optimization problems from natural inputs. In thispaper, we introduce a scalable neural architecture and loss function dedicatedto learning the constraints and criteria of NP-hard reasoning problemsexpressed as discrete Graphical Models. Our loss function solves one of themain limitations of Besag's pseudo-loglikelihood, enabling learning of highenergies. We empirically show it is able to efficiently learn how to solveNP-hard reasoning problems from natural inputs as the symbolic, visual ormany-solutions Sudoku problems as well as the energy optimization formulationof the protein design problem, providing data efficiency, interpretability, and\\textit{a posteriori} control over predictions."}], "temperature": 0.1}}
{"custom_id": "516_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Marianne Defresne"}], "temperature": 0.1}}
{"custom_id": "517_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting"}], "temperature": 0.1}}
{"custom_id": "517_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) demonstrate impressive multilingual capability,but their performance varies substantially across different languages. In thiswork, we introduce a simple yet effective method, called cross-lingual-thoughtprompting (XLT), to systematically improve the multilingual capability of LLMs.Specifically, XLT is a generic template prompt that stimulates cross-lingualand logical reasoning skills to enhance task performance across languages. Weconduct comprehensive evaluations on 7 typical benchmarks related to reasoning,understanding, and generation tasks, covering both high-resource andlow-resource languages. Experimental results show that XLT not only remarkablyenhances the performance of various multilingual tasks but also significantlyreduces the gap between the average performance and the best performance ofeach task in different languages. Notably, XLT brings over 10 points of averageimprovement in arithmetic reasoning and open-domain question-answering tasks."}], "temperature": 0.1}}
{"custom_id": "517_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Haoyang Huang"}], "temperature": 0.1}}
{"custom_id": "518_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Wasserstein-Fisher-Rao Embedding: Logical Query Embeddings with Local Comparison and Global Transport"}], "temperature": 0.1}}
{"custom_id": "518_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Answering complex queries on knowledge graphs is important but particularlychallenging because of the data incompleteness. Query embedding methods addressthis issue by learning-based models and simulating logical reasoning with setoperators. Previous works focus on specific forms of embeddings, but scoringfunctions between embeddings are underexplored. In contrast to existing scoringfunctions motivated by local comparison or global transport, this workinvestigates the local and global trade-off with unbalanced optimal transporttheory. Specifically, we embed sets as bounded measures in $\\real$ endowed witha scoring function motivated by the Wasserstein-Fisher-Rao metric. Such adesign also facilitates closed-form set operators in the embedding space.Moreover, we introduce a convolution-based algorithm for linear timecomputation and a block-diagonal kernel to enforce the trade-off. Results showthat WFRE can outperform existing query embedding methods on standard datasets,evaluation sets with combinatorially complex queries, and hierarchicalknowledge graphs. Ablation study shows that finding a better local and globaltrade-off is essential for performance improvement."}], "temperature": 0.1}}
{"custom_id": "518_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zihao Wang"}], "temperature": 0.1}}
{"custom_id": "519_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming"}], "temperature": 0.1}}
{"custom_id": "519_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Pre-trained large language models (LMs) struggle to perform logical reasoningreliably despite advances in scale and compositionality. In this work, wetackle this challenge through the lens of symbolic programming. We proposeDSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMsgovern the perception of factual knowledge, and a symbolic module performsdeductive reasoning. In contrast to works that rely on hand-crafted logicrules, our differentiable symbolic reasoning framework efficiently learnsweighted rules and applies semantic loss to further improve LMs. DSR-LM isscalable, interpretable, and allows easy integration of prior knowledge,thereby supporting extensive symbolic programming to robustly derive a logicalconclusion. The results of our experiments suggest that DSR-LM improves thelogical reasoning abilities of pre-trained language models, resulting in asignificant increase in accuracy of over 20% on deductive reasoning benchmarks.Furthermore, DSR-LM outperforms a variety of competitive baselines when facedwith systematic changes in sequence length."}], "temperature": 0.1}}
{"custom_id": "519_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hanlin Zhang"}], "temperature": 0.1}}
{"custom_id": "520_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Tackling Universal Properties of Minimal Trap Spaces of Boolean Networks"}], "temperature": 0.1}}
{"custom_id": "520_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Minimal trap spaces (MTSs) capture subspaces in which the Boolean dynamics istrapped, whatever the update mode. They correspond to the attractors of themost permissive mode. Due to their versatility, the computation of MTSs hasrecently gained traction, essentially by focusing on their enumeration. In thispaper, we address the logical reasoning on universal properties of MTSs in thescope of two problems: the reprogramming of Boolean networks for identifyingthe permanent freeze of Boolean variables that enforce a given property on allthe MTSs, and the synthesis of Boolean networks from universal properties ontheir MTSs. Both problems reduce to solving the satisfiability of quantifiedpropositional logic formula with 3 levels of quantifiers($\\exists\\forall\\exists$). In this paper, we introduce a Counter-Example GuidedRefinement Abstraction (CEGAR) to efficiently solve these problems by couplingthe resolution of two simpler formulas. We provide a prototype relying onAnswer-Set Programming for each formula and show its tractability on a widerange of Boolean models of biological networks."}], "temperature": 0.1}}
{"custom_id": "520_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Sara Riva"}], "temperature": 0.1}}
{"custom_id": "521_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text"}], "temperature": 0.1}}
{"custom_id": "521_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Pretrained Vision-Language Models (VLMs) have achieved remarkable performancein image retrieval from text. However, their performance drops drastically whenconfronted with linguistically complex texts that they struggle to comprehend.Inspired by the Divide-and-Conquer algorithm and dual-process theory, in thispaper, we regard linguistically complex texts as compound proposition textscomposed of multiple simple proposition sentences and propose an end-to-endNeural Divide-and-Conquer Reasoning framework, dubbed NDCR. It contains threemain components: 1) Divide: a proposition generator divides the compoundproposition text into simple proposition sentences and produces theircorresponding representations, 2) Conquer: a pretrained VLMs-basedvisual-linguistic interactor achieves the interaction between decomposedproposition sentences and images, 3) Combine: a neural-symbolic reasonercombines the above reasoning states to obtain the final solution via a neurallogic reasoning approach. According to the dual-process theory, thevisual-linguistic interactor and neural-symbolic reasoner could be regarded asanalogical reasoning System 1 and logical reasoning System 2. We conductextensive experiments on a challenging image retrieval from contextualdescriptions data set. Experimental results and analyses indicate NDCRsignificantly improves performance in the complex image-text reasoning problem.Code link: https://github.com/YunxinLi/NDCR."}], "temperature": 0.1}}
{"custom_id": "521_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yunxin Li"}], "temperature": 0.1}}
{"custom_id": "522_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The Dark Side of Explanations: Poisoning Recommender Systems with Counterfactual Examples"}], "temperature": 0.1}}
{"custom_id": "522_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Deep learning-based recommender systems have become an integral part ofseveral online platforms. However, their black-box nature emphasizes the needfor explainable artificial intelligence (XAI) approaches to providehuman-understandable reasons why a specific item gets recommended to a givenuser. One such method is counterfactual explanation (CF). While CFs can behighly beneficial for users and system designers, malicious actors may alsoexploit these explanations to undermine the system's security. In this work, wepropose H-CARS, a novel strategy to poison recommender systems via CFs.Specifically, we first train a logical-reasoning-based surrogate model ontraining data derived from counterfactual explanations. By reversing thelearning process of the recommendation model, we thus develop a proficientgreedy algorithm to generate fabricated user profiles and their associatedinteraction records for the aforementioned surrogate model. Our experiments,which employ a well-known CF generation method and are conducted on twodistinct datasets, show that H-CARS yields significant and successful attackperformance."}], "temperature": 0.1}}
{"custom_id": "522_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ziheng Chen"}], "temperature": 0.1}}
{"custom_id": "523_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Sequential Recommendation with Probabilistic Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "523_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Deep learning and symbolic learning are two frequently employed methods inSequential Recommendation (SR). Recent neural-symbolic SR models demonstratetheir potential to enable SR to be equipped with concurrent perception andcognition capacities. However, neural-symbolic SR remains a challenging problemdue to open issues like representing users and items in logical reasoning. Inthis paper, we combine the Deep Neural Network (DNN) SR models with logicalreasoning and propose a general framework named Sequential Recommendation withProbabilistic Logical Reasoning (short for SR-PLR). This framework allowsSR-PLR to benefit from both similarity matching and logical reasoning bydisentangling feature embedding and logic embedding in the DNN andprobabilistic logic network. To better capture the uncertainty and evolution ofuser tastes, SR-PLR embeds users and items with a probabilistic method andconducts probabilistic logical reasoning on users' interaction patterns. Thenthe feature and logic representations learned from the DNN and logic networkare concatenated to make the prediction. Finally, experiments on varioussequential recommendation models demonstrate the effectiveness of the SR-PLR."}], "temperature": 0.1}}
{"custom_id": "523_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Huanhuan Yuan"}], "temperature": 0.1}}
{"custom_id": "524_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT"}], "temperature": 0.1}}
{"custom_id": "524_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) such as ChatGPT have recently demonstratedsignificant potential in mathematical abilities, providing valuable reasoningparadigm consistent with human natural language. However, LLMs currently havedifficulty in bridging perception, language understanding and reasoningcapabilities due to incompatibility of the underlying information flow amongthem, making it challenging to accomplish tasks autonomously. On the otherhand, abductive learning (ABL) frameworks for integrating the two abilities ofperception and reasoning has seen significant success in inverse deciphermentof incomplete facts, but it is limited by the lack of semantic understanding oflogical reasoning rules and the dependence on complicated domain knowledgerepresentation. This paper presents a novel method (ChatABL) for integratingLLMs into the ABL framework, aiming at unifying the three abilities in a moreuser-friendly and understandable manner. The proposed method uses the strengthsof LLMs' understanding and logical reasoning to correct the incomplete logicalfacts for optimizing the performance of perceptual module, by summarizing andreorganizing reasoning rules represented in natural language format. Similarly,perceptual module provides necessary reasoning examples for LLMs in naturallanguage format. The variable-length handwritten equation deciphering task, anabstract expression of the Mayan calendar decoding, is used as a testbed todemonstrate that ChatABL has reasoning ability beyond most existingstate-of-the-art methods, which has been well supported by comparative studies.To our best knowledge, the proposed ChatABL is the first attempt to explore anew pattern for further approaching human-level cognitive ability via naturallanguage interaction with ChatGPT."}], "temperature": 0.1}}
{"custom_id": "524_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Tianyang Zhong"}], "temperature": 0.1}}
{"custom_id": "525_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LeafAI: query generator for clinical cohort discovery rivaling a human programmer"}], "temperature": 0.1}}
{"custom_id": "525_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Objective: Identifying study-eligible patients within clinical databases is acritical step in clinical research. However, accurate query design typicallyrequires extensive technical and biomedical expertise. We sought to create asystem capable of generating data model-agnostic queries while also providingnovel logical reasoning capabilities for complex clinical trial eligibilitycriteria.  Materials and Methods: The task of query creation from eligibility criteriarequires solving several text-processing problems, including named entityrecognition and relation extraction, sequence-to-sequence transformation,normalization, and reasoning. We incorporated hybrid deep learning andrule-based modules for these, as well as a knowledge base of the UnifiedMedical Language System (UMLS) and linked ontologies. To enable data-modelagnostic query creation, we introduce a novel method for tagging databaseschema elements using UMLS concepts. To evaluate our system, called LeafAI, wecompared the capability of LeafAI to a human database programmer to identifypatients who had been enrolled in 8 clinical trials conducted at ourinstitution. We measured performance by the number of actual enrolled patientsmatched by generated queries.  Results: LeafAI matched a mean 43% of enrolled patients with 27,225 eligibleacross 8 clinical trials, compared to 27% matched and 14,587 eligible inqueries by a human database programmer. The human programmer spent 26 totalhours crafting queries compared to several minutes by LeafAI.  Conclusions: Our work contributes a state-of-the-art data model-agnosticquery generation system capable of conditional reasoning using a knowledgebase. We demonstrate that LeafAI can rival an experienced human programmer infinding patients eligible for clinical trials."}], "temperature": 0.1}}
{"custom_id": "525_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Nicholas J Dobbins"}], "temperature": 0.1}}
{"custom_id": "526_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Make flows small again: revisiting the flow framework"}], "temperature": 0.1}}
{"custom_id": "526_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We present a new flow framework for separation logic reasoning about programsthat manipulate general graphs. The framework overcomes problems in earlierdevelopments: it is based on standard fixed point theory, guarantees leastflows, rules out vanishing flows, and has an easy to understand notion offootprint as needed for soundness of the frame rule. In addition, we presentalgorithms for automating the frame rule, which we evaluate on graph updatesextracted from linearizability proofs for concurrent data structures. Theevaluation demonstrates that our algorithms help to automate key aspects ofthese proofs that have previously relied on user guidance or heuristics."}], "temperature": 0.1}}
{"custom_id": "526_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Roland Meyer"}], "temperature": 0.1}}
{"custom_id": "527_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Scallop: A Language for Neurosymbolic Programming"}], "temperature": 0.1}}
{"custom_id": "527_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We present Scallop, a language which combines the benefits of deep learningand logical reasoning. Scallop enables users to write a wide range ofneurosymbolic applications and train them in a data- and compute-efficientmanner. It achieves these goals through three key features: 1) a flexiblesymbolic representation that is based on the relational data model; 2) adeclarative logic programming language that is based on Datalog and supportsrecursion, aggregation, and negation; and 3) a framework for automatic andefficient differentiable reasoning that is based on the theory of provenancesemirings. We evaluate Scallop on a suite of eight neurosymbolic applicationsfrom the literature. Our evaluation demonstrates that Scallop is capable ofexpressing algorithmic reasoning in diverse and challenging AI tasks, providesa succinct interface for machine learning programmers to integrate logicaldomain knowledge, and yields solutions that are comparable or superior tostate-of-the-art models in terms of accuracy. Furthermore, Scallop's solutionsoutperform these models in aspects such as runtime and data efficiency,interpretability, and generalizability."}], "temperature": 0.1}}
{"custom_id": "527_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ziyang Li"}], "temperature": 0.1}}
{"custom_id": "528_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4"}], "temperature": 0.1}}
{"custom_id": "528_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Harnessing logical reasoning ability is a comprehensive natural languageunderstanding endeavor. With the release of Generative Pretrained Transformer 4(GPT-4), highlighted as \"advanced\" at reasoning tasks, we are eager to learnthe GPT-4 performance on various logical reasoning tasks. This report analysesmultiple logical reasoning datasets, with popular benchmarks like LogiQA andReClor, and newly-released datasets like AR-LSAT. We test the multi-choicereading comprehension and natural language inference tasks with benchmarksrequiring logical reasoning. We further construct a logical reasoningout-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4.We also make a performance comparison between ChatGPT and GPT-4. Experimentresults show that ChatGPT performs significantly better than the RoBERTafine-tuning method on most logical reasoning benchmarks. With early access tothe GPT-4 API we are able to conduct intense experiments on the GPT-4 model.The results show GPT-4 yields even higher performance on most logical reasoningdatasets. Among benchmarks, ChatGPT and GPT-4 do relatively well on well-knowndatasets like LogiQA and ReClor. However, the performance drops significantlywhen handling newly released and out-of-distribution datasets. Logicalreasoning remains challenging for ChatGPT and GPT-4, especially onout-of-distribution and natural language inference datasets. We release theprompt-style logical reasoning datasets as a benchmark suite and name itLogiEval."}], "temperature": 0.1}}
{"custom_id": "528_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hanmeng Liu"}], "temperature": 0.1}}
{"custom_id": "529_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Natural Language Reasoning, A Survey"}], "temperature": 0.1}}
{"custom_id": "529_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This survey paper proposes a clearer view of natural language reasoning inthe field of Natural Language Processing (NLP), both conceptually andpractically. Conceptually, we provide a distinct definition for naturallanguage reasoning in NLP, based on both philosophy and NLP scenarios, discusswhat types of tasks require reasoning, and introduce a taxonomy of reasoning.Practically, we conduct a comprehensive literature review on natural languagereasoning in NLP, mainly covering classical logical reasoning, natural languageinference, multi-hop question answering, and commonsense reasoning. The paperalso identifies and views backward reasoning, a powerful paradigm formulti-step reasoning, and introduces defeasible reasoning as one of the mostimportant future directions in natural language reasoning research. We focus onsingle-modality unstructured natural language text, excluding neuro-symbolictechniques and mathematical reasoning."}], "temperature": 0.1}}
{"custom_id": "529_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Fei Yu"}], "temperature": 0.1}}
{"custom_id": "530_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neural Graph Reasoning: Complex Logical Query Answering Meets Graph Databases"}], "temperature": 0.1}}
{"custom_id": "530_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Complex logical query answering (CLQA) is a recently emerged task of graphmachine learning that goes beyond simple one-hop link prediction and solves afar more complex task of multi-hop logical reasoning over massive, potentiallyincomplete graphs in a latent space. The task received a significant tractionin the community; numerous works expanded the field along theoretical andpractical axes to tackle different types of complex queries and graphmodalities with efficient systems. In this paper, we provide a holistic surveyof CLQA with a detailed taxonomy studying the field from multiple angles,including graph types (modality, reasoning domain, background semantics),modeling aspects (encoder, processor, decoder), supported queries (operators,patterns, projected variables), datasets, evaluation metrics, and applications.  Refining the CLQA task, we introduce the concept of Neural Graph Databases(NGDBs). Extending the idea of graph databases (graph DBs), NGDB consists of aNeural Graph Storage and a Neural Graph Engine. Inside Neural Graph Storage, wedesign a graph store, a feature store, and further embed information in alatent embedding store using an encoder. Given a query, Neural Query Enginelearns how to perform query planning and execution in order to efficientlyretrieve the correct results by interacting with the Neural Graph Storage.Compared with traditional graph DBs, NGDBs allow for a flexible and unifiedmodeling of features in diverse modalities using the embedding store. Moreover,when the graph is incomplete, they can provide robust retrieval of answerswhich a normal graph DB cannot recover. Finally, we point out promisingdirections, unsolved problems and applications of NGDB for future research."}], "temperature": 0.1}}
{"custom_id": "530_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hongyu Ren"}], "temperature": 0.1}}
{"custom_id": "531_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Soy: An Efficient MILP Solver for Piecewise-Affine Systems"}], "temperature": 0.1}}
{"custom_id": "531_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Piecewise-affine (PWA) systems are widely used for modeling and control ofrobotics problems including modeling contact dynamics. A common approach is toencode the control problem of the PWA system as a Mixed-Integer Convex Program(MICP), which can be solved by general-purpose off-the-shelf MICP solvers. Tomitigate the scalability challenge of solving these MICP problems, existingwork focuses on devising efficient and strong formulations of the problems,while less effort has been spent on exploiting their specific structure todevelop specialized solvers. The latter is the theme of our work. We focus onefficiently handling one-hot constraints, which are particularly relevant whenencoding PWA dynamics. We have implemented our techniques in a tool, Soy, whichorganically integrates logical reasoning, arithmetic reasoning, and stochasticlocal search. For a set of PWA control benchmarks, Soy solves more problems,faster, than two state-of-the-art MICP solvers."}], "temperature": 0.1}}
{"custom_id": "531_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Haoze Wu"}], "temperature": 0.1}}
{"custom_id": "532_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Weakly Supervised Knowledge Transfer with Probabilistic Logical Reasoning for Object Detection"}], "temperature": 0.1}}
{"custom_id": "532_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Training object detection models usually requires instance-level annotations,such as the positions and labels of all objects present in each image. Suchsupervision is unfortunately not always available and, more often, onlyimage-level information is provided, also known as weak supervision. Recentworks have addressed this limitation by leveraging knowledge from a richlyannotated domain. However, the scope of weak supervision supported by theseapproaches has been very restrictive, preventing them to use all availableinformation. In this work, we propose ProbKT, a framework based onprobabilistic logical reasoning that allows to train object detection modelswith arbitrary types of weak supervision. We empirically show on differentdatasets that using all available information is beneficial as our ProbKT leadsto significant improvement on target domain and better generalization comparedto existing baselines. We also showcase the ability of our approach to handlecomplex logic statements as supervision signal."}], "temperature": 0.1}}
{"custom_id": "532_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Martijn Oldenhof"}], "temperature": 0.1}}
{"custom_id": "533_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Attribution-Scores and Causal Counterfactuals as Explanations in Artificial Intelligence"}], "temperature": 0.1}}
{"custom_id": "533_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In this expository article we highlight the relevance of explanations forartificial intelligence, in general, and for the newer developments in {\\emexplainable AI}, referring to origins and connections of and among differentapproaches. We describe in simple terms, explanations in data management andmachine learning that are based on attribution-scores, and counterfactuals asfound in the area of causality. We elaborate on the importance of logicalreasoning when dealing with counterfactuals, and their use for scorecomputation."}], "temperature": 0.1}}
{"custom_id": "533_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Leopoldo Bertossi"}], "temperature": 0.1}}
{"custom_id": "534_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Domain Specific Question Answering Over Knowledge Graphs Using Logical Programming and Large Language Models"}], "temperature": 0.1}}
{"custom_id": "534_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Answering questions over domain-specific graphs requires a tailored approachdue to the limited number of relations and the specific nature of the domain.Our approach integrates classic logical programming languages into largelanguage models (LLMs), enabling the utilization of logical reasoningcapabilities to tackle the KGQA task. By representing the questions as Prologqueries, which are readable and near close to natural language inrepresentation, we facilitate the generation of programmatically derivedanswers. To validate the effectiveness of our approach, we evaluate it using awell-known benchmark dataset, MetaQA. Our experimental results demonstrate thatour method achieves accurate identification of correct answer entities for alltest questions, even when trained on a small fraction of annotated data.Overall, our work presents a promising approach to addressing questionanswering over domain-specific graphs, offering an explainable and robustsolution by incorporating logical programming languages."}], "temperature": 0.1}}
{"custom_id": "534_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Navid Madani"}], "temperature": 0.1}}
{"custom_id": "535_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Learning Language Representations with Logical Inductive Bias"}], "temperature": 0.1}}
{"custom_id": "535_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Transformer architectures have achieved great success in solving naturallanguage tasks, which learn strong language representations from large-scaleunlabeled texts. In this paper, we seek to go further beyond and explore a newlogical inductive bias for better language representation learning. Logicreasoning is known as a formal methodology to reach answers from givenknowledge and facts. Inspired by such a view, we develop a novel neuralarchitecture named FOLNet (First-Order Logic Network), to encode this newinductive bias. We construct a set of neural logic operators as learnable Hornclauses, which are further forward-chained into a fully differentiable neuralarchitecture (FOLNet). Interestingly, we find that the self-attention module intransformers can be composed by two of our neural logic operators, whichprobably explains their strong reasoning performance. Our proposed FOLNet hasthe same input and output interfaces as other pretrained models and thus couldbe pretrained/finetuned by using similar losses. It also allows FOLNet to beused in a plug-and-play manner when replacing other pretrained models. With ourlogical inductive bias, the same set of ``logic deduction skills'' learnedthrough pretraining are expected to be equally capable of solving diversedownstream tasks. For this reason, FOLNet learns language representations thathave much stronger transfer capabilities. Experimental results on severallanguage understanding tasks show that our pretrained FOLNet model outperformsthe existing strong transformer-based approaches."}], "temperature": 0.1}}
{"custom_id": "535_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jianshu Chen"}], "temperature": 0.1}}
{"custom_id": "536_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"}], "temperature": 0.1}}
{"custom_id": "536_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Pretrained Foundation Models (PFMs) are regarded as the foundation forvarious downstream tasks with different data modalities. A PFM (e.g., BERT,ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonableparameter initialization for a wide range of downstream applications. BERTlearns bidirectional encoder representations from Transformers, which aretrained on large datasets as contextual language models. Similarly, thegenerative pretrained transformer (GPT) method employs Transformers as thefeature extractor and is trained using an autoregressive paradigm on largedatasets. Recently, ChatGPT shows promising success on large language models,which applies an autoregressive language model with zero shot or few shotprompting. The remarkable achievements of PFM have brought significantbreakthroughs to various fields of AI. Numerous studies have proposed differentmethods, raising the demand for an updated survey. This study provides acomprehensive review of recent research advancements, challenges, andopportunities for PFMs in text, image, graph, as well as other data modalities.The review covers the basic components and existing pretraining methods used innatural language processing, computer vision, and graph learning. Additionally,it explores advanced PFMs used for different data modalities and unified PFMsthat consider data quality and quantity. The review also discusses researchrelated to the fundamentals of PFMs, such as model efficiency and compression,security, and privacy. Finally, the study provides key implications, futureresearch directions, challenges, and open problems in the field of PFMs.Overall, this survey aims to shed light on the research of the PFMs onscalability, security, logical reasoning ability, cross-domain learningability, and the user-friendly interactive ability for artificial generalintelligence."}], "temperature": 0.1}}
{"custom_id": "536_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ce Zhou"}], "temperature": 0.1}}
{"custom_id": "537_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models"}], "temperature": 0.1}}
{"custom_id": "537_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have recently demonstrated their potential inclinical applications, providing valuable medical knowledge and advice. Forexample, a large dialog LLM like ChatGPT has successfully passed part of the USmedical licensing exam. However, LLMs currently have difficulty processingimages, making it challenging to interpret information from medical images,which are rich in information that supports clinical decisions. On the otherhand, computer-aided diagnosis (CAD) networks for medical images have seensignificant success in the medical field by using advanced deep-learningalgorithms to support clinical decision-making. This paper presents a methodfor integrating LLMs into medical-image CAD networks. The proposed frameworkuses LLMs to enhance the output of multiple CAD networks, such as diagnosisnetworks, lesion segmentation networks, and report generation networks, bysummarizing and reorganizing the information presented in natural language textformat. The goal is to merge the strengths of LLMs' medical domain knowledgeand logical reasoning with the vision understanding capability of existingmedical-image CAD models to create a more user-friendly and understandablesystem for patients compared to conventional CAD systems. In the future, LLM'smedical knowledge can be also used to improve the performance of vision-basedmedical-image CAD models."}], "temperature": 0.1}}
{"custom_id": "537_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Sheng Wang"}], "temperature": 0.1}}
{"custom_id": "538_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Unifying Structure Reasoning and Language Model Pre-training for Complex Reasoning"}], "temperature": 0.1}}
{"custom_id": "538_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent pre-trained language models (PLMs) equipped with foundation reasoningskills have shown remarkable performance on downstream complex tasks. However,the significant structure reasoning skill has been rarely studied, whichinvolves modeling implicit structure information within the text and performingexplicit logical reasoning over them to deduce the conclusion. This paperproposes a unified learning framework that combines explicit structurereasoning and language pre-training to endow PLMs with the structure reasoningskill. It first identifies several elementary structures within contexts toconstruct structured queries and performs step-by-step reasoning along thequeries to identify the answer entity. The fusion of textual semantics andstructure reasoning is achieved by using contextual representations learned byPLMs to initialize the representation space of structures, and performingstepwise reasoning on this semantic representation space. Experimental resultson four datasets demonstrate that the proposed model achieves significantimprovements in complex reasoning tasks involving diverse structures, and showstransferability to downstream tasks with limited training data andeffectiveness for complex reasoning of KGs modality."}], "temperature": 0.1}}
{"custom_id": "538_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Siyuan Wang"}], "temperature": 0.1}}
{"custom_id": "539_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical Message Passing Networks with One-hop Inference on Atomic Formulas"}], "temperature": 0.1}}
{"custom_id": "539_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Complex Query Answering (CQA) over Knowledge Graphs (KGs) has attracted a lotof attention to potentially support many applications. Given that KGs areusually incomplete, neural models are proposed to answer the logical queries byparameterizing set operators with complex neural networks. However, suchmethods usually train neural set operators with a large number of entity andrelation embeddings from the zero, where whether and how the embeddings or theneural set operators contribute to the performance remains not clear. In thispaper, we propose a simple framework for complex query answering thatdecomposes the KG embeddings from neural set operators. We propose to representthe complex queries into the query graph. On top of the query graph, we proposethe Logical Message Passing Neural Network (LMPNN) that connects the localone-hop inferences on atomic formulas to the global logical reasoning forcomplex query answering. We leverage existing effective KG embeddings toconduct one-hop inferences on atomic formulas, the results of which areregarded as the messages passed in LMPNN. The reasoning process over theoverall logical formulas is turned into the forward pass of LMPNN thatincrementally aggregates local information to finally predict the answers'embeddings. The complex logical inference across different types of querieswill then be learned from training examples based on the LMPNN architecture.Theoretically, our query-graph represenation is more general than theprevailing operator-tree formulation, so our approach applies to a broaderrange of complex KG queries. Empirically, our approach yields the newstate-of-the-art neural CQA model. Our research bridges the gap between complexKG query answering tasks and the long-standing achievements of knowledge graphrepresentation learning."}], "temperature": 0.1}}
{"custom_id": "539_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zihao Wang"}], "temperature": 0.1}}
{"custom_id": "540_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A separation logic for sequences in pointer programs and its decidability"}], "temperature": 0.1}}
{"custom_id": "540_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Separation logic and its variants can describe various properties on pointerprograms. However, when it comes to properties on sequences, one may find ithard to formalize. To deal with properties on variable-length sequences andmultilevel data structures, we propose sequence-heap separation logic whichintegrates sequences into logical reasoning on heap-manipulated programs.Quantifiers over sequence variables and singleton heap storing sequence(sequence singleton heap) are new members in our logic. Further, we study thesatisfiability problem of two fragments. The propositional fragment ofsequence-heap separation logic is decidable, and the fragment with 2alternations on program variables and 1 alternation on sequence variables isundecidable. In addition, we explore boundaries between decidable andundecidable fragments of the logic with prenex normal form."}], "temperature": 0.1}}
{"custom_id": "540_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Tianyue Cao"}], "temperature": 0.1}}
{"custom_id": "541_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mind Reasoning Manners: Enhancing Type Perception for Generalized Zero-shot Logical Reasoning over Text"}], "temperature": 0.1}}
{"custom_id": "541_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning task involves diverse types of complex reasoning over text,based on the form of multiple-choice question answering. Given the context,question and a set of options as the input, previous methods achieve superiorperformances on the full-data setting. However, the current benchmark datasethas the ideal assumption that the reasoning type distribution on the trainsplit is close to the test split, which is inconsistent with many realapplication scenarios. To address it, there remain two problems to be studied:(1) How is the zero-shot capability of the models (train on seen types and teston unseen types)? (2) How to enhance the perception of reasoning types for themodels? For problem 1, we propose a new benchmark for generalized zero-shotlogical reasoning, named ZsLR. It includes six splits based on the three typesampling strategies. For problem 2, a type-aware model TaCo is proposed. Itutilizes both the heuristic input reconstruction and the contrastive learningto improve the type perception in the global representation. Extensiveexperiments on both the zero-shot and full-data settings prove the superiorityof TaCo over the state-of-the-art methods. Also, we experiment and verify thegeneralization capability of TaCo on other logical reasoning dataset."}], "temperature": 0.1}}
{"custom_id": "541_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Fangzhi Xu"}], "temperature": 0.1}}
{"custom_id": "542_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LAMBADA: Backward Chaining for Automated Reasoning in Natural Language"}], "temperature": 0.1}}
{"custom_id": "542_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Remarkable progress has been made on automated reasoning with natural text,by using Language Models (LMs) and methods such as Chain-of-Thought andSelection-Inference. These techniques search for proofs in the forwarddirection from axioms to the conclusion, which suffers from a combinatorialexplosion of the search space, and thus high failure rates for problemsrequiring longer chains of reasoning. The classical automated reasoningliterature has shown that reasoning in the backward direction (i.e. from theintended conclusion to supporting axioms) is significantly more efficient atproof-finding. Importing this intuition into the LM setting, we develop aBackward Chaining algorithm, called LAMBADA, that decomposes reasoning intofour sub-modules. These sub-modules are simply implemented by few-shot promptedLM inference. We show that LAMBADA achieves sizable accuracy boosts overstate-of-the-art forward reasoning methods on challenging logical reasoningdatasets, particularly when deep and accurate proof chains are required."}], "temperature": 0.1}}
{"custom_id": "542_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mehran Kazemi"}], "temperature": 0.1}}
{"custom_id": "543_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large Language Models are Better Reasoners with Self-Verification"}], "temperature": 0.1}}
{"custom_id": "543_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recently, with the chain of thought (CoT) prompting, large language models(LLMs), e.g., GPT-3, have shown strong reasoning ability in several naturallanguage processing tasks such as arithmetic, commonsense, and logicalreasoning. However, LLMs with CoT require multi-step prompting and multi-tokenprediction, which is highly sensitive to individual mistakes and vulnerable toerror accumulation. The above issues make the LLMs need the ability to verifythe answers. In fact, after inferring conclusions in some thinking decisiontasks, people often check them by re-verifying steps to avoid some mistakes. Inthis paper, we propose and prove that LLMs also have similar self-verificationabilities. We take the conclusion obtained by CoT as one of the conditions forsolving the original problem. By performing a backward verification of theanswers that LLM deduced for itself, we can obtain interpretable answervalidation scores to select the candidate answer with the highest score.Experimental results demonstrate that the proposed method can improve thereasoning performance on various arithmetic, commonsense, and logical reasoningdatasets. Our code is publicly available at:https://github.com/WENGSYX/Self-Verification."}], "temperature": 0.1}}
{"custom_id": "543_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yixuan Weng"}], "temperature": 0.1}}
{"custom_id": "544_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "544_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning of text is an important ability that requires understandingthe information present in the text, their interconnections, and then reasoningthrough them to infer new conclusions. Prior works on improving the logicalreasoning ability of language models require complex processing of trainingdata (e.g., aligning symbolic knowledge to text), yielding task-specific dataaugmentation solutions that restrict the learning of general logical reasoningskills. In this work, we propose APOLLO, an adaptively pretrained languagemodel that has improved logical reasoning abilities. We select a subset ofWikipedia, based on a set of logical inference keywords, for continuedpretraining of a language model. We use two self-supervised loss functions: amodified masked language modeling loss where only specific parts-of-speechwords, that would likely require more reasoning than basic languageunderstanding, are masked, and a sentence-level classification loss thatteaches the model to distinguish between entailment and contradiction types ofsentences. The proposed training paradigm is both simple and independent oftask formats. We demonstrate the effectiveness of APOLLO by comparing it withprior baselines on two logical reasoning datasets. APOLLO performs comparablyon ReClor and outperforms baselines on LogiQA. The code base has been madepublicly available."}], "temperature": 0.1}}
{"custom_id": "544_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Soumya Sanyal"}], "temperature": 0.1}}
{"custom_id": "545_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning"}], "temperature": 0.1}}
{"custom_id": "545_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Generating a Chain of Thought (CoT) has been shown to consistently improvelarge language model (LLM) performance on a wide range of NLP tasks. However,prior work has mainly focused on logical reasoning tasks (e.g. arithmetic,commonsense QA); it remains unclear whether improvements hold for more diversetypes of reasoning, especially in socially situated contexts. Concretely, weperform a controlled evaluation of zero-shot CoT across two socially sensitivedomains: harmful questions and stereotype benchmarks. We find that zero-shotCoT reasoning in sensitive domains significantly increases a model's likelihoodto produce harmful or undesirable output, with trends holding across differentprompt formats and model variants. Furthermore, we show that harmful CoTsincrease with model size, but decrease with improved instruction following. Ourwork suggests that zero-shot CoT should be used with caution on sociallyimportant tasks, especially when marginalized groups or sensitive topics areinvolved."}], "temperature": 0.1}}
{"custom_id": "545_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Omar Shaikh"}], "temperature": 0.1}}
{"custom_id": "546_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Towards High-Order Complementary Recommendation via Logical Reasoning Network"}], "temperature": 0.1}}
{"custom_id": "546_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Complementary recommendation gains increasing attention in e-commerce sinceit expedites the process of finding frequently-bought-with products for usersin their shopping journey. Therefore, learning the product representation thatcan reflect this complementary relationship plays a central role in modernrecommender systems. In this work, we propose a logical reasoning network,LOGIREC, to effectively learn embeddings of products as well as varioustransformations (projection, intersection, negation) between them. LOGIREC iscapable of capturing the asymmetric complementary relationship between productsand seamlessly extending to high-order recommendations where more comprehensiveand meaningful complementary relationship is learned for a query set ofproducts. Finally, we further propose a hybrid network that is jointlyoptimized for learning a more generic product representation. We demonstratethe effectiveness of our LOGIREC on multiple public real-world datasets interms of various ranking-based metrics under both low-order and high-orderrecommendation scenarios."}], "temperature": 0.1}}
{"custom_id": "546_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Longfeng Wu"}], "temperature": 0.1}}
{"custom_id": "547_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Counterfactual reasoning: Do language models need world knowledge for causal understanding?"}], "temperature": 0.1}}
{"custom_id": "547_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Current pre-trained language models have enabled remarkable improvements indownstream tasks, but it remains difficult to distinguish effects ofstatistical correlation from more systematic logical reasoning grounded onunderstanding of the real world. In this paper we tease these factors apart byleveraging counterfactual conditionals, which force language models to predictunusual consequences based on hypothetical propositions. We introduce a set oftests drawn from psycholinguistic experiments, as well as larger-scalecontrolled datasets, to probe counterfactual predictions from a variety ofpopular pre-trained language models. We find that models are consistently ableto override real-world knowledge in counterfactual scenarios, and that thiseffect is more robust in case of stronger baseline world knowledge -- however,we also find that for most models this effect appears largely to be driven bysimple lexical cues. When we mitigate effects of both world knowledge andlexical cues to test knowledge of linguistic nuances of counterfactuals, wefind that only GPT-3 shows sensitivity to these nuances, though thissensitivity is also non-trivially impacted by lexical associative factors."}], "temperature": 0.1}}
{"custom_id": "547_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jiaxuan Li"}], "temperature": 0.1}}
{"custom_id": "548_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression"}], "temperature": 0.1}}
{"custom_id": "548_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Geometry problem solving is a well-recognized testbed for evaluating thehigh-level multi-modal reasoning capability of deep models. In most existingworks, two main geometry problems: calculation and proving, are usually treatedas two specific tasks, hindering a deep model to unify its reasoning capabilityon multiple math tasks. However, in essence, these two tasks have similarproblem representations and overlapped math knowledge which can improve theunderstanding and reasoning ability of a deep model on both two tasks.Therefore, we construct a large-scale Unified Geometry problem benchmark,UniGeo, which contains 4,998 calculation problems and 9,543 proving problems.Each proving problem is annotated with a multi-step proof with reasons andmathematical expressions. The proof can be easily reformulated as a provingsequence that shares the same formats with the annotated program sequence forcalculation problems. Naturally, we also present a unified multi-task GeometricTransformer framework, Geoformer, to tackle calculation and proving problemssimultaneously in the form of sequence generation, which finally shows thereasoning ability can be improved on both two tasks by unifying formulation.Furthermore, we propose a Mathematical Expression Pretraining (MEP) method thataims to predict the mathematical expressions in the problem solution, thusimproving the Geoformer model. Experiments on the UniGeo demonstrate that ourproposed Geoformer obtains state-of-the-art performance by outperformingtask-specific model NGS with over 5.6% and 3.2% accuracies on calculation andproving problems, respectively."}], "temperature": 0.1}}
{"custom_id": "548_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jiaqi Chen"}], "temperature": 0.1}}
{"custom_id": "549_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Weisfeiler and Leman Go Relational"}], "temperature": 0.1}}
{"custom_id": "549_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Knowledge graphs, modeling multi-relational data, improve numerousapplications such as question answering or graph logical reasoning. Many graphneural networks for such data emerged recently, often outperforming shallowarchitectures. However, the design of such multi-relational graph neuralnetworks is ad-hoc, driven mainly by intuition and empirical insights. Up tonow, their expressivity, their relation to each other, and their (practical)learning performance is poorly understood. Here, we initiate the study ofderiving a more principled understanding of multi-relational graph neuralnetworks. Namely, we investigate the limitations in the expressive power of thewell-known Relational GCN and Compositional GCN architectures and shed somelight on their practical learning performance. By aligning both architectureswith a suitable version of the Weisfeiler-Leman test, we establish under whichconditions both models have the same expressive power in distinguishingnon-isomorphic (multi-relational) graphs or vertices with different structuralroles. Further, by leveraging recent progress in designing expressive graphneural networks, we introduce the $k$-RN architecture that provably overcomesthe expressiveness limitations of the above two architectures. Empirically, weconfirm our theoretical findings in a vertex classification setting over smalland large multi-relational graphs."}], "temperature": 0.1}}
{"custom_id": "549_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Pablo Barcelo"}], "temperature": 0.1}}
{"custom_id": "550_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neuro-Symbolic Spatio-Temporal Reasoning"}], "temperature": 0.1}}
{"custom_id": "550_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Knowledge about space and time is necessary to solve problems in the physicalworld: An AI agent situated in the physical world and interacting with objectsoften needs to reason about positions of and relations between objects; and assoon as the agent plans its actions to solve a task, it needs to consider thetemporal aspect (e.g., what actions to perform over time). Spatio-temporalknowledge, however, is required beyond interacting with the physical world, andis also often transferred to the abstract world of concepts through analogiesand metaphors (e.g., \"a threat that is hanging over our heads\"). As spatial andtemporal reasoning is ubiquitous, different attempts have been made tointegrate this into AI systems. In the area of knowledge representation,spatial and temporal reasoning has been largely limited to modeling objects andrelations and developing reasoning methods to verify statements about objectsand relations. On the other hand, neural network researchers have tried toteach models to learn spatial relations from data with limited reasoningcapabilities. Bridging the gap between these two approaches in a mutuallybeneficial way could allow us to tackle many complex real-world problems, suchas natural language processing, visual question answering, and semantic imagesegmentation. In this chapter, we view this integration problem from theperspective of Neuro-Symbolic AI. Specifically, we propose a synergy betweenlogical reasoning and machine learning that will be grounded on spatial andtemporal knowledge. Describing some successful applications, remainingchallenges, and evaluation datasets pertaining to this direction is the maintopic of this contribution."}], "temperature": 0.1}}
{"custom_id": "550_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jae Hee Lee"}], "temperature": 0.1}}
{"custom_id": "551_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "NQE: N-ary Query Embedding for Complex Query Answering over Hyper-Relational Knowledge Graphs"}], "temperature": 0.1}}
{"custom_id": "551_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Complex query answering (CQA) is an essential task for multi-hop and logicalreasoning on knowledge graphs (KGs). Currently, most approaches are limited toqueries among binary relational facts and pay less attention to n-ary facts(n>=2) containing more than two entities, which are more prevalent in the realworld. Moreover, previous CQA methods can only make predictions for a few giventypes of queries and cannot be flexibly extended to more complex logicalqueries, which significantly limits their applications. To overcome thesechallenges, in this work, we propose a novel N-ary Query Embedding (NQE) modelfor CQA over hyper-relational knowledge graphs (HKGs), which include massiven-ary facts. The NQE utilizes a dual-heterogeneous Transformer encoder andfuzzy logic theory to satisfy all n-ary FOL queries, including existentialquantifiers, conjunction, disjunction, and negation. We also propose a parallelprocessing algorithm that can train or predict arbitrary n-ary FOL queries in asingle batch, regardless of the kind of each query, with good flexibility andextensibility. In addition, we generate a new CQA dataset WD50K-NFOL, includingdiverse n-ary FOL queries over WD50K. Experimental results on WD50K-NFOL andother standard CQA datasets show that NQE is the state-of-the-art CQA methodover HKGs with good generalization capability. Our code and dataset arepublicly available."}], "temperature": 0.1}}
{"custom_id": "551_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Haoran Luo"}], "temperature": 0.1}}
{"custom_id": "552_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical Tasks for Measuring Extrapolation and Rule Comprehension"}], "temperature": 0.1}}
{"custom_id": "552_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning is essential in a variety of human activities. Arepresentative example of a logical task is mathematics. Recent large-scalemodels trained on large datasets have been successful in various fields, buttheir reasoning ability in arithmetic tasks is limited, which we reproduceexperimentally. Here, we recast this limitation as not unique to mathematicsbut common to tasks that require logical operations. We then propose a new setof tasks, termed logical tasks, which will be the next challenge to address.This higher point of view helps the development of inductive biases that havebroad impact beyond the solution of individual tasks. We define andcharacterize logical tasks and discuss system requirements for their solution.Furthermore, we discuss the relevance of logical tasks to concepts such asextrapolation, explainability, and inductive bias. Finally, we providedirections for solving logical tasks."}], "temperature": 0.1}}
{"custom_id": "552_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ippei Fujisawa"}], "temperature": 0.1}}
{"custom_id": "553_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Evident: a Development Methodology and a Knowledge Base Topology for Data Mining, Machine Learning and General Knowledge Management"}], "temperature": 0.1}}
{"custom_id": "553_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Software has been developed for knowledge discovery, prediction andmanagement for over 30 years. However, there are still unresolved pain pointswhen using existing project development and artifact management methodologies.Historically, there has been a lack of applicable methodologies. Further,methodologies that have been applied, such as Agile, have several limitationsincluding scientific unfalsifiability that reduce their applicability. Evident,a development methodology rooted in the philosophy of logical reasoning andEKB, a knowledge base topology, are proposed. Many pain points in data mining,machine learning and general knowledge management are alleviated conceptually.Evident can be extended potentially to accelerate philosophical exploration,science discovery, education as well as knowledge sharing & retention acrossthe globe. EKB offers one solution of storing information as knowledge, agranular level above data. Related topics in computer history, softwareengineering, database, sensor, philosophy, and project & organization &military managements are also discussed."}], "temperature": 0.1}}
{"custom_id": "553_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mingwu"}], "temperature": 0.1}}
{"custom_id": "554_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zero-Shot Classification by Logical Reasoning on Natural Language Explanations"}], "temperature": 0.1}}
{"custom_id": "554_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Humans can classify data of an unseen category by reasoning on its languageexplanations. This ability is owing to the compositional nature of language: wecan combine previously seen attributes to describe the new category. Forexample, we might describe a sage thrasher as \"it has a slim straightrelatively short bill, yellow eyes and a long tail\", so that others can usetheir knowledge of attributes \"slim straight relatively short bill\", \"yelloweyes\" and \"long tail\" to recognize a sage thrasher. Inspired by thisobservation, in this work we tackle zero-shot classification task by logicallyparsing and reasoning on natural language expla-nations. To this end, wepropose the framework CLORE (Classification by LOgical Reasoning onExplanations). While previous methods usually regard textual information asimplicit features, CLORE parses explanations into logical structures and thenexplicitly reasons along thess structures on the input to produce aclassification score. Experimental results on explanation-based zero-shotclassification benchmarks demonstrate that CLORE is superior to baselines,which we further show mainly comes from higher scores on tasks requiring morelogical reasoning. We also demonstrate that our framework can be extended tozero-shot classification on visual modality. Alongside classificationdecisions, CLORE can provide the logical parsing and reasoning process as aclear form of rationale. Through empirical analysis we demonstrate that CLOREis also less affected by linguistic biases than baselines."}], "temperature": 0.1}}
{"custom_id": "554_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chi Han"}], "temperature": 0.1}}
{"custom_id": "555_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Commentary: Is the moon there if nobody looks -- Bell inequalities and physical reality"}], "temperature": 0.1}}
{"custom_id": "555_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Marian Kupczynski(MK)is the author of a controversial paper published (2020)in the journal Frontiers in Physics. The work is built around a mathematicalclaim by MK which is actually false, and MK's logical reasoning around hisclaim is also incorrect. The same claim was made by him in several other recentpapers published in other journals. A proof that the claimed result is false isthe main content of our present \"Comment\". It is purely a mathematicalcounter-example to a mathematical claim in a number of MK's papers."}], "temperature": 0.1}}
{"custom_id": "555_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Richard D. Gill"}], "temperature": 0.1}}
{"custom_id": "556_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure"}], "temperature": 0.1}}
{"custom_id": "556_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In this paper, we propose a comprehensive benchmark to investigate models'logical reasoning capabilities in complex real-life scenarios. Currentexplanation datasets often employ synthetic data with simple reasoningstructures. Therefore, it cannot express more complex reasoning processes, suchas the rebuttal to a reasoning step and the degree of certainty of theevidence. To this end, we propose a comprehensive logical reasoning explanationform. Based on the multi-hop chain of reasoning, the explanation form includesthree main components: (1) The condition of rebuttal that the reasoning nodecan be challenged; (2) Logical formulae that uncover the internal texture ofreasoning nodes; (3) Reasoning strength indicated by degrees of certainty. Thefine-grained structure conforms to the real logical reasoning scenario, betterfitting the human cognitive process but, simultaneously, is more challengingfor the current models. We evaluate the current best models' performance onthis new explanation form. The experimental results show that generatingreasoning graphs remains a challenging task for current models, even with thehelp of giant pre-trained language models."}], "temperature": 0.1}}
{"custom_id": "556_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yinya Huang"}], "temperature": 0.1}}
{"custom_id": "557_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Investigating the Robustness of Natural Language Generation from Logical Forms via Counterfactual Samples"}], "temperature": 0.1}}
{"custom_id": "557_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The aim of Logic2Text is to generate controllable and faithful textsconditioned on tables and logical forms, which not only requires a deepunderstanding of the tables and logical forms, but also warrants symbolicreasoning over the tables. State-of-the-art methods based on pre-trained modelshave achieved remarkable performance on the standard test dataset. However, wequestion whether these methods really learn how to perform logical reasoning,rather than just relying on the spurious correlations between the headers ofthe tables and operators of the logical form. To verify this hypothesis, wemanually construct a set of counterfactual samples, which modify the originallogical forms to generate counterfactual logical forms with rarely co-occurredtable headers and logical operators. SOTA methods give much worse results onthese counterfactual samples compared with the results on the original testdataset, which verifies our hypothesis. To deal with this problem, we firstlyanalyze this bias from a causal perspective, based on which we propose twoapproaches to reduce the model's reliance on the shortcut. The first oneincorporates the hierarchical structure of the logical forms into the model.The second one exploits automatically generated counterfactual data fortraining. Automatic and manual experimental results on the original testdataset and the counterfactual dataset show that our method is effective toalleviate the spurious correlation. Our work points out the weakness ofprevious methods and takes a further step toward developing Logic2Text modelswith real logical reasoning ability."}], "temperature": 0.1}}
{"custom_id": "557_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chengyuan Liu"}], "temperature": 0.1}}
{"custom_id": "558_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Inductive Logical Query Answering in Knowledge Graphs"}], "temperature": 0.1}}
{"custom_id": "558_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Formulating and answering logical queries is a standard communicationinterface for knowledge graphs (KGs). Alleviating the notorious incompletenessof real-world KGs, neural methods achieved impressive results in linkprediction and complex query answering tasks by learning representations ofentities, relations, and queries. Still, most existing query answering methodsrely on transductive entity embeddings and cannot generalize to KGs containingnew entities without retraining the entity embeddings. In this work, we studythe inductive query answering task where inference is performed on a graphcontaining new entities with queries over both seen and unseen entities. Tothis end, we devise two mechanisms leveraging inductive node and relationalstructure representations powered by graph neural networks (GNNs).Experimentally, we show that inductive models are able to perform logicalreasoning at inference time over unseen nodes generalizing to graphs up to 500%larger than training ones. Exploring the efficiency--effectiveness trade-off,we find the inductive relational structure representation method generallyachieves higher performance, while the inductive node representation method isable to answer complex queries in the inference-only regime without anytraining on queries and scales to graphs of millions of nodes. Code isavailable at https://github.com/DeepGraphLearning/InductiveQE."}], "temperature": 0.1}}
{"custom_id": "558_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mikhail Galkin"}], "temperature": 0.1}}
{"custom_id": "559_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Join-Chain Network: A Logical Reasoning View of the Multi-head Attention in Transformer"}], "temperature": 0.1}}
{"custom_id": "559_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Developing neural architectures that are capable of logical reasoning hasbecome increasingly important for a wide range of applications (e.g., naturallanguage processing). Towards this grand objective, we propose a symbolicreasoning architecture that chains many join operators together to model outputlogical expressions. In particular, we demonstrate that such an ensemble ofjoin-chains can express a broad subset of ''tree-structured'' first-orderlogical expressions, named FOET, which is particularly useful for modelingnatural languages. To endow it with differentiable learning capability, weclosely examine various neural operators for approximating the symbolicjoin-chains. Interestingly, we find that the widely used multi-headself-attention module in transformer can be understood as a special neuraloperator that implements the union bound of the join operator in probabilisticpredicate space. Our analysis not only provides a new perspective on themechanism of the pretrained models such as BERT for natural languageunderstanding but also suggests several important future improvementdirections."}], "temperature": 0.1}}
{"custom_id": "559_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jianyi Zhang"}], "temperature": 0.1}}
{"custom_id": "560_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neural Methods for Logical Reasoning Over Knowledge Graphs"}], "temperature": 0.1}}
{"custom_id": "560_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning is a fundamental problem for computers and deeply studied inArtificial Intelligence. In this paper, we specifically focus on answeringmulti-hop logical queries on Knowledge Graphs (KGs). This is a complicated taskbecause, in real-world scenarios, the graphs tend to be large and incomplete.Most previous works have been unable to create models that accept fullFirst-Order Logical (FOL) queries, which include negative queries, and haveonly been able to process a limited set of query structures. Additionally, mostmethods present logic operators that can only perform the logical operationthey are made for. We introduce a set of models that use Neural Networks tocreate one-point vector embeddings to answer the queries. The versatility ofneural networks allows the framework to handle FOL queries with Conjunction($\\wedge$), Disjunction ($\\vee$) and Negation ($\\neg$) operators. Wedemonstrate experimentally the performance of our model through extensiveexperimentation on well-known benchmarking datasets. Besides having moreversatile operators, the models achieve a 10\\% relative increase over the bestperforming state of the art and more than 30\\% over the original method basedon single-point vector embeddings."}], "temperature": 0.1}}
{"custom_id": "560_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Alfonso Amayuelas"}], "temperature": 0.1}}
{"custom_id": "561_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Towards Human-Compatible XAI: Explaining Data Differentials with Concept Induction over Background Knowledge"}], "temperature": 0.1}}
{"custom_id": "561_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Concept induction, which is based on formal logical reasoning overdescription logics, has been used in ontology engineering in order to createontology (TBox) axioms from the base data (ABox) graph. In this paper, we showthat it can also be used to explain data differentials, for example in thecontext of Explainable AI (XAI), and we show that it can in fact be done in away that is meaningful to a human observer. Our approach utilizes a large classhierarchy, curated from the Wikipedia category hierarchy, as backgroundknowledge."}], "temperature": 0.1}}
{"custom_id": "561_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Cara Widmer"}], "temperature": 0.1}}
{"custom_id": "562_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Phases of methodological research in biostatistics - building the evidence base for new methods"}], "temperature": 0.1}}
{"custom_id": "562_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Although the biostatistical scientific literature publishes new methods at avery high rate, many of these developments are not trustworthy enough to beadopted by the scientific community. We propose a framework to think about howa piece of methodological work contributes to the evidence base for a method.Similarly to the well-known phases of clinical research in drug development, wedefine four phases of methodological research. These four phases cover (I)providing logical reasoning and proofs, (II) providing empirical evidence,first in a narrow target setting, then (III) in an extended range of settingsand for various outcomes, accompanied by appropriate application examples, and(IV) investigations that establish a method as sufficiently well-understood toknow when it is preferred over others and when it is not. We provide basicdefinitions of the four phases but acknowledge that more work is needed tofacilitate unambiguous classification of studies into phases. Methodologicaldevelopments that have undergone all four proposed phases are still rare, butwe give two examples with references. Our concept rebalances the emphasis tostudies in phase III and IV, i.e., carefully planned methods comparison studiesand studies that explore the empirical properties of existing methods in awider range of problems."}], "temperature": 0.1}}
{"custom_id": "562_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Georg Heinze"}], "temperature": 0.1}}
{"custom_id": "563_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Time-aware Self-Attention Meets Logic Reasoning in Recommender Systems"}], "temperature": 0.1}}
{"custom_id": "563_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "At the age of big data, recommender systems have shown remarkable success asa key means of information filtering in our daily life. Recent years havewitnessed the technical development of recommender systems, from perceptionlearning to cognition reasoning which intuitively build the task ofrecommendation as the procedure of logical reasoning and have achievesignificant improvement. However, the logical statement in reasoning implicitlyadmits irrelevance of ordering, even does not consider time information whichplays an important role in many recommendation tasks. Furthermore,recommendation model incorporated with temporal context would tend to beself-attentive, i.e., automatically focus more (less) on the relevance(irrelevance), respectively.  To address these issues, in this paper, we propose a Time-awareSelf-Attention with Neural Collaborative Reasoning (TiSANCR) basedrecommendation model, which integrates temporal patterns and self-attentionmechanism into reasoning-based recommendation. Specially, temporal patternsrepresented by relative time, provide context and auxiliary information tocharacterize the user's preference in recommendation, while self-attention isleveraged to distill informative patterns and suppress irrelevances. Therefore,the fusion of self-attentive temporal information provides deeperrepresentation of user's preference. Extensive experiments on benchmarkdatasets demonstrate that the proposed TiSANCR achieves significant improvementand consistently outperforms the state-of-the-art recommendation methods."}], "temperature": 0.1}}
{"custom_id": "563_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhijian Luo"}], "temperature": 0.1}}
{"custom_id": "564_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Knowledge-based and Data-driven Reasoning and Learning for Ad Hoc Teamwork"}], "temperature": 0.1}}
{"custom_id": "564_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We present an architecture for ad hoc teamwork, which refers to collaborationin a team of agents without prior coordination. State of the art methods forthis problem often include a data-driven component that uses a long history ofprior observations to model the behaviour of other agents (or agent types) andto determine the ad hoc agent's behaviour. In many practical domains, it ischallenging to find large training datasets, and necessary to understand andincrementally extend the existing models to account for changes in teamcomposition or domain attributes. Our architecture combines the principles ofknowledge-based and data-driven reasoning and learning. Specifically, we enablean ad hoc agent to perform non-monotonic logical reasoning with priorcommonsense domain knowledge and incrementally-updated simple predictive modelsof other agents' behaviour. We use the benchmark simulated multi-agentcollaboration domain Fort Attack to demonstrate that our architecture supportsadaptation to unforeseen changes, incremental learning and revision of modelsof other agents' behaviour from limited samples, transparency in the ad hocagent's decision making, and better performance than a data-driven baseline."}], "temperature": 0.1}}
{"custom_id": "564_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hasra Dodampegama"}], "temperature": 0.1}}
{"custom_id": "565_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Emotion Recognition in Conversation using Probabilistic Soft Logic"}], "temperature": 0.1}}
{"custom_id": "565_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Creating agents that can both appropriately respond to conversations andunderstand complex human linguistic tendencies and social cues has been a longstanding challenge in the NLP community. A recent pillar of research revolvesaround emotion recognition in conversation (ERC); a sub-field of emotionrecognition that focuses on conversations or dialogues that contain two or moreutterances. In this work, we explore an approach to ERC that exploits the useof neural embeddings along with complex structures in dialogues. We implementour approach in a framework called Probabilistic Soft Logic (PSL), adeclarative templating language that uses first-order like logical rules, thatwhen combined with data, define a particular class of graphical model.Additionally, PSL provides functionality for the incorporation of results fromneural models into PSL models. This allows our model to take advantage ofadvanced neural methods, such as sentence embeddings, and logical reasoningover the structure of a dialogue. We compare our method with state-of-the-artpurely neural ERC systems, and see almost a 20% improvement. With theseresults, we provide an extensive qualitative and quantitative analysis over theDailyDialog conversation dataset."}], "temperature": 0.1}}
{"custom_id": "565_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Eriq Augustine"}], "temperature": 0.1}}
{"custom_id": "566_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Learning Symmetric Rules with SATNet"}], "temperature": 0.1}}
{"custom_id": "566_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SATNet is a differentiable constraint solver with a custom backpropagationalgorithm, which can be used as a layer in a deep-learning system. It is apromising proposal for bridging deep learning and logical reasoning. In fact,SATNet has been successfully applied to learn, among others, the rules of acomplex logical puzzle, such as Sudoku, just from input and output pairs whereinputs are given as images. In this paper, we show how to improve the learningof SATNet by exploiting symmetries in the target rules of a given but unknownlogical puzzle or more generally a logical formula. We present SymSATNet, avariant of SATNet that translates the given symmetries of the target rules to acondition on the parameters of SATNet and requires that the parameters shouldhave a particular parametric form that guarantees the condition. Therequirement dramatically reduces the number of parameters to learn for therules with enough symmetries, and makes the parameter learning of SymSATNetmuch easier than that of SATNet. We also describe a technique for automaticallydiscovering symmetries of the target rules from examples. Our experiments withSudoku and Rubik's cube show the substantial improvement of SymSATNet over thebaseline SATNet."}], "temperature": 0.1}}
{"custom_id": "566_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Sangho Lim"}], "temperature": 0.1}}
{"custom_id": "567_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Towards Unifying Perceptual Reasoning and Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "567_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "An increasing number of scientific experiments support the view of perceptionas Bayesian inference, which is rooted in Helmholtz's view of perception asunconscious inference. Recent study of logic presents a view of logicalreasoning as Bayesian inference. In this paper, we give a simple probabilisticmodel that is applicable to both perceptual reasoning and logical reasoning. Weshow that the model unifies the two essential processes common in perceptualand logical systems: on the one hand, the process by which perceptual andlogical knowledge is derived from another knowledge, and on the other hand, theprocess by which such knowledge is derived from data. We fully characterise themodel in terms of logical consequence relations."}], "temperature": 0.1}}
{"custom_id": "567_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hiroyuki Kido"}], "temperature": 0.1}}
{"custom_id": "568_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "On the Eve of True Explainability for OWL Ontologies: Description Logic Proofs with Evee and Evonne (Extended Version)"}], "temperature": 0.1}}
{"custom_id": "568_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "When working with description logic ontologies, understanding entailmentsderived by a description logic reasoner is not always straightforward. So far,the standard ontology editor Prot\\'eg\\'e offers two services to help:(black-box) justifications for OWL 2 DL ontologies, and (glass-box) proofs forlightweight OWL EL ontologies, where the latter exploits the proof facilitiesof reasoner ELK. Since justifications are often insufficient in explaininginferences, there is thus only little tool support for explaining inferences inmore expressive DLs. In this paper, we introduce EVEE-LIBS, a Java library forcomputing proofs for DLs up to ALCH, and EVEE-PROTEGE, a collection ofProt\\'eg\\'e plugins for displaying those proofs in Prot\\'eg\\'e. We also give ashort glimpse of the latest version of EVONNE, a more advanced standaloneapplication for displaying and interacting with proofs computed with EVEE-LIBS."}], "temperature": 0.1}}
{"custom_id": "568_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Christian Alrabbaa"}], "temperature": 0.1}}
{"custom_id": "569_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Semantic Probabilistic Layers for Neuro-Symbolic Learning"}], "temperature": 0.1}}
{"custom_id": "569_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We design a predictive layer for structured-output prediction (SOP) that canbe plugged into any neural network guaranteeing its predictions are consistentwith a set of predefined symbolic constraints. Our Semantic Probabilistic Layer(SPL) can model intricate correlations, and hard constraints, over a structuredoutput space all while being amenable to end-to-end learning via maximumlikelihood. SPLs combine exact probabilistic inference with logical reasoningin a clean and modular way, learning complex distributions and restrictingtheir support to solutions of the constraint. As such, they can faithfully, andefficiently, model complex SOP tasks beyond the reach of alternativeneuro-symbolic approaches. We empirically demonstrate that SPLs outperformthese competitors in terms of accuracy on challenging SOP tasks includinghierarchical multi-label classification, pathfinding and preference learning,while retaining perfect constraint satisfaction."}], "temperature": 0.1}}
{"custom_id": "569_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Kareem Ahmed"}], "temperature": 0.1}}
{"custom_id": "570_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "TAR: Neural Logical Reasoning across TBox and ABox"}], "temperature": 0.1}}
{"custom_id": "570_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Many ontologies, i.e., Description Logic (DL) knowledge bases, have beendeveloped to provide rich knowledge about various domains. An ontology consistsof an ABox, i.e., assertion axioms between two entities or between a conceptand an entity, and a TBox, i.e., terminology axioms between two concepts.Neural logical reasoning (NLR) is a fundamental task to explore such knowledgebases, which aims at answering multi-hop queries with logical operations basedon distributed representations of queries and answers. While previous NLRmethods can give specific entity-level answers, i.e., ABox answers, they arenot able to provide descriptive concept-level answers, i.e., TBox answers,where each concept is a description of a set of entities. In other words,previous NLR methods only reason over the ABox of an ontology while ignoringthe TBox. In particular, providing TBox answers enables inferring theexplanations of each query with descriptive concepts, which make answerscomprehensible to users and are of great usefulness in the field of appliedontology. In this work, we formulate the problem of neural logical reasoningacross TBox and ABox (TA-NLR), solving which needs to address challenges inincorporating, representing, and operating on concepts. We propose an originalsolution named TAR for TA-NLR. Firstly, we incorporate description logic basedontological axioms to provide the source of concepts. Then, we representconcepts and queries as fuzzy sets, i.e., sets whose elements have degrees ofmembership, to bridge concepts and queries with entities. Moreover, we designoperators involving concepts on top of fuzzy set representation of concepts andqueries for optimization and inference. Extensive experimental results on tworeal-world datasets demonstrate the effectiveness of TAR for TA-NLR."}], "temperature": 0.1}}
{"custom_id": "570_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhenwei Tang"}], "temperature": 0.1}}
{"custom_id": "571_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning over Logically Interacted Conditions for Question Answering"}], "temperature": 0.1}}
{"custom_id": "571_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Some questions have multiple answers that are not equally correct, i.e.answers are different under different conditions. Conditions are used todistinguish answers as well as to provide additional information to supportthem. In this paper, we study a more challenging task where answers areconstrained by a list of conditions that logically interact, which requiresperforming logical reasoning over the conditions to determine the correctnessof the answers. Even more challenging, we only provide evidences for a subsetof the conditions, so some questions may not have deterministic answers. Insuch cases, models are asked to find probable answers and identify conditionsthat need to be satisfied to make the answers correct. We propose a new model,TReasoner, for this challenging reasoning task. TReasoner consists of anentailment module, a reasoning module, and a generation module (if the answersare free-form text spans). TReasoner achieves state-of-the-art performance ontwo benchmark conditional QA datasets, outperforming the previousstate-of-the-art by 3-10 points."}], "temperature": 0.1}}
{"custom_id": "571_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Haitian Sun"}], "temperature": 0.1}}
{"custom_id": "572_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "RobustLR: Evaluating Robustness to Logical Perturbation in Deductive Reasoning"}], "temperature": 0.1}}
{"custom_id": "572_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Transformers have been shown to be able to perform deductive reasoning on alogical rulebase containing rules and statements written in English naturallanguage. While the progress is promising, it is currently unclear if thesemodels indeed perform logical reasoning by understanding the underlying logicalsemantics in the language. To this end, we propose RobustLR, a suite ofevaluation datasets that evaluate the robustness of these models to minimallogical edits in rulebases and some standard logical equivalence conditions. Inour experiments with RoBERTa and T5, we find that the models trained in priorworks do not perform consistently on the different perturbations in RobustLR,thus showing that the models are not robust to the proposed logicalperturbations. Further, we find that the models find it especially hard tolearn logical negation and disjunction operators. Overall, using our evaluationsets, we demonstrate some shortcomings of the deductive reasoning-basedlanguage models, which can eventually help towards designing better models forlogical reasoning over natural language. All the datasets and code base havebeen made publicly available."}], "temperature": 0.1}}
{"custom_id": "572_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Soumya Sanyal"}], "temperature": 0.1}}
{"custom_id": "573_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "On the Paradox of Learning to Reason from Data"}], "temperature": 0.1}}
{"custom_id": "573_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning is needed in a wide range of NLP tasks. Can a BERT model betrained end-to-end to solve logical reasoning problems presented in naturallanguage? We attempt to answer this question in a confined problem space wherethere exists a set of parameters that perfectly simulates logical reasoning. Wemake observations that seem to contradict each other: BERT attains near-perfectaccuracy on in-distribution test examples while failing to generalize to otherdata distributions over the exact same problem space. Our study provides anexplanation for this paradox: instead of learning to emulate the correctreasoning function, BERT has in fact learned statistical features thatinherently exist in logical reasoning problems. We also show that it isinfeasible to jointly remove statistical features from data, illustrating thedifficulty of learning to reason in general. Our result naturally extends toother neural models and unveils the fundamental difference between learning toreason and learning to achieve high performance on NLP benchmarks usingstatistical features."}], "temperature": 0.1}}
{"custom_id": "573_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Honghua Zhang"}], "temperature": 0.1}}
{"custom_id": "574_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical Reasoning with Span-Level Predictions for Interpretable and Robust NLI Models"}], "temperature": 0.1}}
{"custom_id": "574_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Current Natural Language Inference (NLI) models achieve impressive results,sometimes outperforming humans when evaluating on in-distribution test sets.However, as these models are known to learn from annotation artefacts anddataset biases, it is unclear to what extent the models are learning the taskof NLI instead of learning from shallow heuristics in their training data. Weaddress this issue by introducing a logical reasoning framework for NLI,creating highly transparent model decisions that are based on logical rules.Unlike prior work, we show that improved interpretability can be achievedwithout decreasing the predictive accuracy. We almost fully retain performanceon SNLI, while also identifying the exact hypothesis spans that are responsiblefor each model prediction. Using the e-SNLI human explanations, we verify thatour model makes sensible decisions at a span level, despite not using any spanlabels during training. We can further improve model performance and span-leveldecisions by using the e-SNLI explanations during training. Finally, our modelis more robust in a reduced data setting. When training with only 1,000examples, out-of-distribution performance improves on the MNLI matched andmismatched validation sets by 13% and 16% relative to the baseline. Trainingwith fewer observations yields further improvements, both in-distribution andout-of-distribution."}], "temperature": 0.1}}
{"custom_id": "574_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Joe Stacey"}], "temperature": 0.1}}
{"custom_id": "575_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Consistency of UML class, object and statechart diagrams using ontology reasoners"}], "temperature": 0.1}}
{"custom_id": "575_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We propose an automatic approach to analyze the consistency andsatisfiability of Unified Modeling Language UML models containing multipleclass, object and statechart diagrams using logic reasoners for the WebOntology Language OWL 2. We describe how to translate UML models in OWL 2 andwe present a tool chain implementing this translation that can be used with anystandard compliant UML modeling tool. The proposed approach is limited inscope, but is fully automatic and does not require any expertise about OWL 2and its reasoners from the designer."}], "temperature": 0.1}}
{"custom_id": "575_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ali Hanzala Khan"}], "temperature": 0.1}}
{"custom_id": "576_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "576_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large language models (LLMs) have been shown to be capable of impressivefew-shot generalisation to new tasks. However, they still tend to performpoorly on multi-step logical reasoning problems. Here we carry out acomprehensive evaluation of LLMs on 50 tasks that probe different aspects oflogical reasoning. We show that language models tend to perform fairly well atsingle step inference or entailment tasks, but struggle to chain togethermultiple reasoning steps to solve more complex problems. In light of this, wepropose a Selection-Inference (SI) framework that exploits pre-trained LLMs asgeneral processing modules, and alternates between selection and inference togenerate a series of interpretable, casual reasoning steps leading to the finalanswer. We show that a 7B parameter LLM used within the SI framework in a5-shot generalisation setting, with no fine-tuning, yields a performanceimprovement of over 100% compared to an equivalent vanilla baseline on a suiteof 10 logical reasoning tasks. The same model in the same setting evenoutperforms a significantly larger 280B parameter baseline on the same suite oftasks. Moreover, answers produced by the SI framework are accompanied by acausal natural-language-based reasoning trace, which has important implicationsfor the safety and trustworthiness of the system."}], "temperature": 0.1}}
{"custom_id": "576_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Antonia Creswell"}], "temperature": 0.1}}
{"custom_id": "577_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LogiGAN: Learning Logical Reasoning via Adversarial Pre-training"}], "temperature": 0.1}}
{"custom_id": "577_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We present LogiGAN, an unsupervised adversarial pre-training framework forimproving logical reasoning abilities of language models. Upon automaticidentifying logical reasoning phenomena in massive text corpus via detectionheuristics, we train language models to predict the masked-out logicalstatements. Inspired by the facilitation effect of reflective thinking in humanlearning, we analogically simulate the learning-thinking process with anadversarial Generator-Verifier architecture to assist logic learning. LogiGANimplements a novel sequential GAN approach that (a) circumvents thenon-differentiable challenge of the sequential GAN by leveraging the Generatoras a sentence-level generative likelihood scorer with a learning objective ofreaching scoring consensus with the Verifier; (b) is computationally feasiblefor large-scale pre-training with arbitrary target length. Both base and largesize language models pre-trained with LogiGAN demonstrate obvious performanceimprovement on 12 datasets requiring general reasoning abilities, revealing thefundamental role of logic in broad reasoning, as well as the effectiveness ofLogiGAN. Ablation studies on LogiGAN components reveal the relativeorthogonality between linguistic and logic abilities and suggest thatreflective thinking's facilitation effect might also generalize to machinelearning."}], "temperature": 0.1}}
{"custom_id": "577_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xinyu Pi"}], "temperature": 0.1}}
{"custom_id": "578_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Graph Neural Networks for Propositional Model Counting"}], "temperature": 0.1}}
{"custom_id": "578_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Graph Neural Networks (GNNs) have been recently leveraged to solve severallogical reasoning tasks. Nevertheless, counting problems such as propositionalmodel counting (#SAT) are still mostly approached with traditional solvers.Here we tackle this gap by presenting an architecture based on the GNNframework for belief propagation (BP) of Kuch et al., extended withself-attentive GNN and trained to approximately solve the #SAT problem. We rana thorough experimental investigation, showing that our model, trained on asmall set of random Boolean formulae, is able to scale effectively to muchlarger problem sizes, with comparable or better performances of state of theart approximate solvers. Moreover, we show that it can be efficientlyfine-tuned to provide good generalization results on different formulaedistributions, such as those coming from SAT-encoded combinatorial problems."}], "temperature": 0.1}}
{"custom_id": "578_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Gaia Saveri"}], "temperature": 0.1}}
{"custom_id": "579_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logiformer: A Two-Branch Graph Transformer Network for Interpretable Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "579_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Machine reading comprehension has aroused wide concerns, since it exploresthe potential of model for text understanding. To further equip the machinewith the reasoning capability, the challenging task of logical reasoning isproposed. Previous works on logical reasoning have proposed some strategies toextract the logical units from different aspects. However, there still remainsa challenge to model the long distance dependency among the logical units.Also, it is demanding to uncover the logical structures of the text and furtherfuse the discrete logic to the continuous text embedding. To tackle the aboveissues, we propose an end-to-end model Logiformer which utilizes a two-branchgraph transformer network for logical reasoning of text. Firstly, we introducedifferent extraction strategies to split the text into two sets of logicalunits, and construct the logical graph and the syntax graph respectively. Thelogical graph models the causal relations for the logical branch while thesyntax graph captures the co-occurrence relations for the syntax branch.Secondly, to model the long distance dependency, the node sequence from eachgraph is fed into the fully connected graph transformer structures. The twoadjacent matrices are viewed as the attention biases for the graph transformerlayers, which map the discrete logical structures to the continuous textembedding space. Thirdly, a dynamic gate mechanism and a question-awareself-attention module are introduced before the answer prediction to update thefeatures. The reasoning process provides the interpretability by employing thelogical units, which are consistent with human cognition. The experimentalresults show the superiority of our model, which outperforms thestate-of-the-art single model on two logical reasoning benchmarks."}], "temperature": 0.1}}
{"custom_id": "579_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Fangzhi Xu"}], "temperature": 0.1}}
{"custom_id": "580_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Facilitating automated conversion of scientific knowledge into scientific simulation models with the Machine Assisted Generation, Calibration, and Comparison (MAGCC) Framework"}], "temperature": 0.1}}
{"custom_id": "580_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The Machine Assisted Generation, Comparison, and Calibration (MAGCC)framework provides machine assistance and automation of recurrent crucial stepsand processes in the development, implementation, testing, and use ofscientific simulation models. MAGCC bridges systems for knowledge extractionvia natural language processing or extracted from existing mathematical modelsand provides a comprehensive workflow encompassing the composition ofscientific models and artificial intelligence (AI) assisted code generation.MAGCC accomplishes this through: 1) the development of a comprehensivelyexpressive formal knowledge representation knowledgebase, the StructuredScientific Knowledge Representation (SSKR) that encompasses all the types ofinformation needed to make any simulation model, 2) the use of an artificiallyintelligent logic reasoning system, the Computational Modeling Assistant (CMA),that takes information from the SSKR and generates, in a traceable fashion,model specifications across a range of simulation modeling methods, and 3) theuse of the CMA to generate executable code for a simulation model from thosemodel specifications. The MAGCC framework can be customized any scientificdomain, and future work will integrate newly developed code-generating AIsystems."}], "temperature": 0.1}}
{"custom_id": "580_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chase Cockrell"}], "temperature": 0.1}}
{"custom_id": "581_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Table-based Fact Verification with Self-adaptive Mixture of Experts"}], "temperature": 0.1}}
{"custom_id": "581_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The table-based fact verification task has recently gained widespreadattention and yet remains to be a very challenging problem. It inherentlyrequires informative reasoning over natural language together with differentnumerical and logical reasoning on tables (e.g., count, superlative,comparative). Considering that, we exploit mixture-of-experts and present inthis paper a new method: Self-adaptive Mixture-of-Experts Network (SaMoE).Specifically, we have developed a mixture-of-experts neural network torecognize and execute different types of reasoning -- the network is composedof multiple experts, each handling a specific part of the semantics forreasoning, whereas a management module is applied to decide the contribution ofeach expert network to the verification result. A self-adaptive method isdeveloped to teach the management module combining results of different expertsmore efficiently without external knowledge. The experimental resultsillustrate that our framework achieves 85.1% accuracy on the benchmark datasetTabFact, comparable with the previous state-of-the-art models. We hope ourframework can serve as a new baseline for table-based verification. Our code isavailable at https://github.com/THUMLP/SaMoE."}], "temperature": 0.1}}
{"custom_id": "581_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yuxuan Zhou"}], "temperature": 0.1}}
{"custom_id": "582_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning with Multi-Structure Commonsense Knowledge in Visual Dialog"}], "temperature": 0.1}}
{"custom_id": "582_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Visual Dialog requires an agent to engage in a conversation with humansgrounded in an image. Many studies on Visual Dialog focus on the understandingof the dialog history or the content of an image, while a considerable amountof commonsense-required questions are ignored. Handling these scenarios dependson logical reasoning that requires commonsense priors. How to capture relevantcommonsense knowledge complementary to the history and the image remains a keychallenge. In this paper, we propose a novel model by Reasoning withMulti-structure Commonsense Knowledge (RMK). In our model, the externalknowledge is represented with sentence-level facts and graph-level facts, toproperly suit the scenario of the composite of dialog history and image. On topof these multi-structure representations, our model can capture relevantknowledge and incorporate them into the vision and semantic features, viagraph-based interaction and transformer-based fusion. Experimental results andanalysis on VisDial v1.0 and VisDialCK datasets show that our proposed modeleffectively outperforms comparative methods."}], "temperature": 0.1}}
{"custom_id": "582_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Shunyu Zhang"}], "temperature": 0.1}}
{"custom_id": "583_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Enhancing Neural Mathematical Reasoning by Abductive Combination with Symbolic Library"}], "temperature": 0.1}}
{"custom_id": "583_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mathematical reasoning recently has been shown as a hard challenge for neuralsystems. Abilities including expression translation, logical reasoning, andmathematics knowledge acquiring appear to be essential to overcome thechallenge. This paper demonstrates that some abilities can be achieved throughabductive combination with discrete systems that have been programmed withhuman knowledge. On a mathematical reasoning dataset, we adopt the recentlyproposed abductive learning framework, and propose the ABL-Sym algorithm thatcombines the Transformer neural models with a symbolic mathematics library.ABL-Sym shows 9.73% accuracy improvement on the interpolation tasks and 47.22%accuracy improvement on the extrapolation tasks, over the state-of-the-artapproaches. Online demonstration: http://math.polixir.ai"}], "temperature": 0.1}}
{"custom_id": "583_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yangyang Hu"}], "temperature": 0.1}}
{"custom_id": "584_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Densely Connected Criss-Cross Attention Network for Document-level Relation Extraction"}], "temperature": 0.1}}
{"custom_id": "584_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Document-level relation extraction (RE) aims to identify relations betweentwo entities in a given document. Compared with its sentence-level counterpart,document-level RE requires complex reasoning. Previous research normallycompleted reasoning through information propagation on the mention-level orentity-level document-graph, but rarely considered reasoning at theentity-pair-level.In this paper, we propose a novel model, called DenselyConnected Criss-Cross Attention Network (Dense-CCNet), for document-level RE,which can complete logical reasoning at the entity-pair-level. Specifically,the Dense-CCNet performs entity-pair-level logical reasoning through theCriss-Cross Attention (CCA), which can collect contextual information inhorizontal and vertical directions on the entity-pair matrix to enhance thecorresponding entity-pair representation. In addition, we densely connectmultiple layers of the CCA to simultaneously capture the features of single-hopand multi-hop logical reasoning.We evaluate our Dense-CCNet model on threepublic document-level RE datasets, DocRED, CDR, and GDA. Experimental resultsdemonstrate that our model achieves state-of-the-art performance on these threedatasets."}], "temperature": 0.1}}
{"custom_id": "584_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Liang Zhang"}], "temperature": 0.1}}
{"custom_id": "585_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "AbductionRules: Training Transformers to Explain Unexpected Inputs"}], "temperature": 0.1}}
{"custom_id": "585_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Transformers have recently been shown to be capable of reliably performinglogical reasoning over facts and rules expressed in natural language, butabductive reasoning - inference to the best explanation of an unexpectedobservation - has been underexplored despite significant applications toscientific discovery, common-sense reasoning, and model interpretability.  We present AbductionRules, a group of natural language datasets designed totrain and test generalisable abduction over natural-language knowledge bases.We use these datasets to finetune pretrained Transformers and discuss theirperformance, finding that our models learned generalisable abductive techniquesbut also learned to exploit the structure of our data. Finally, we discuss theviability of this approach to abductive reasoning and ways in which it may beimproved in future work."}], "temperature": 0.1}}
{"custom_id": "585_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Nathan Young"}], "temperature": 0.1}}
{"custom_id": "586_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Neural-Symbolic Approach to Natural Language Understanding"}], "temperature": 0.1}}
{"custom_id": "586_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Deep neural networks, empowered by pre-trained language models, have achievedremarkable results in natural language understanding (NLU) tasks. However,their performances can drastically deteriorate when logical reasoning isneeded. This is because NLU in principle depends on not only analogicalreasoning, which deep neural networks are good at, but also logical reasoning.According to the dual-process theory, analogical reasoning and logicalreasoning are respectively carried out by System 1 and System 2 in the humanbrain. Inspired by the theory, we present a novel framework for NLU calledNeural-Symbolic Processor (NSP), which performs analogical reasoning based onneural processing and logical reasoning based on both neural and symbolicprocessing. As a case study, we conduct experiments on two NLU tasks, questionanswering (QA) and natural language inference (NLI), when numerical reasoning(a type of logical reasoning) is necessary. The experimental results show thatour method significantly outperforms state-of-the-art methods in both tasks."}], "temperature": 0.1}}
{"custom_id": "586_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhixuan Liu"}], "temperature": 0.1}}
{"custom_id": "587_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "FaiRR: Faithful and Robust Deductive Reasoning over Natural Language"}], "temperature": 0.1}}
{"custom_id": "587_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Transformers have been shown to be able to perform deductive reasoning on alogical rulebase containing rules and statements written in natural language.Recent works show that such models can also produce the reasoning steps (i.e.,the proof graph) that emulate the model's logical reasoning process. Currently,these black-box models generate both the proof graph and intermediateinferences within the same model and thus may be unfaithful. In this work, weframe the deductive logical reasoning task by defining three modularcomponents: rule selection, fact selection, and knowledge composition. The ruleand fact selection steps select the candidate rule and facts to be used andthen the knowledge composition combines them to generate new inferences. Thisensures model faithfulness by assured causal relation from the proof step tothe inference reasoning. To test our framework, we propose FaiRR (Faithful andRobust Reasoner) where the above three components are independently modeled bytransformers. We observe that FaiRR is robust to novel language perturbations,and is faster at inference than previous works on existing reasoning datasets.Additionally, in contrast to black-box generative models, the errors made byFaiRR are more interpretable due to the modular approach."}], "temperature": 0.1}}
{"custom_id": "587_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Soumya Sanyal"}], "temperature": 0.1}}
{"custom_id": "588_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "588_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Charts are very popular for analyzing data. When exploring charts, peopleoften ask a variety of complex reasoning questions that involve several logicaland arithmetic operations. They also commonly refer to visual features of achart in their questions. However, most existing datasets do not focus on suchcomplex reasoning questions as their questions are template-based and answerscome from a fixed-vocabulary. In this work, we present a large-scale benchmarkcovering 9.6K human-written questions as well as 23.1K questions generated fromhuman-written chart summaries. To address the unique challenges in ourbenchmark involving visual and logical reasoning over charts, we present twotransformer-based models that combine visual features and the data table of thechart in a unified way to answer questions. While our models achieve thestate-of-the-art results on the previous datasets as well as on our benchmark,the evaluation also reveals several challenges in answering complex reasoningquestions."}], "temperature": 0.1}}
{"custom_id": "588_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ahmed Masry"}], "temperature": 0.1}}
{"custom_id": "589_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "AdaLoGN: Adaptive Logic Graph Network for Reasoning-Based Machine Reading Comprehension"}], "temperature": 0.1}}
{"custom_id": "589_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent machine reading comprehension datasets such as ReClor and LogiQArequire performing logical reasoning over text. Conventional neural models areinsufficient for logical reasoning, while symbolic reasoners cannot directlyapply to text. To meet the challenge, we present a neural-symbolic approachwhich, to predict an answer, passes messages over a graph representing logicalrelations between text units. It incorporates an adaptive logic graph network(AdaLoGN) which adaptively infers logical relations to extend the graph and,essentially, realizes mutual and iterative reinforcement between neural andsymbolic reasoning. We also implement a novel subgraph-to-node message passingmechanism to enhance context-option interaction for answering multiple-choicequestions. Our approach shows promising results on ReClor and LogiQA."}], "temperature": 0.1}}
{"custom_id": "589_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xiao Li"}], "temperature": 0.1}}
{"custom_id": "590_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "What Makes Reading Comprehension Questions Difficult?"}], "temperature": 0.1}}
{"custom_id": "590_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "For a natural language understanding benchmark to be useful in research, ithas to consist of examples that are diverse and difficult enough todiscriminate among current and near-future state-of-the-art systems. However,we do not yet know how best to select text sources to collect a variety ofchallenging examples. In this study, we crowdsource multiple-choice readingcomprehension questions for passages taken from seven qualitatively distinctsources, analyzing what attributes of passages contribute to the difficulty andquestion types of the collected examples. To our surprise, we find that passagesource, length, and readability measures do not significantly affect questiondifficulty. Through our manual annotation of seven reasoning types, we observeseveral trends between passage sources and reasoning types, e.g., logicalreasoning is more often required in questions written for technical passages.These results suggest that when creating a new benchmark dataset, selecting adiverse set of passages can help ensure a diverse range of question types, butthat passage difficulty need not be a priority."}], "temperature": 0.1}}
{"custom_id": "590_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Saku Sugawara"}], "temperature": 0.1}}
{"custom_id": "591_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Neural Programming Language for the Reservoir Computer"}], "temperature": 0.1}}
{"custom_id": "591_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "From logical reasoning to mental simulation, biological and artificial neuralsystems possess an incredible capacity for computation. Such neural computersoffer a fundamentally novel computing paradigm by representing datacontinuously and processing information in a natively parallel and distributedmanner. To harness this computation, prior work has developed extensivetraining techniques to understand existing neural networks. However, the lackof a concrete and low-level programming language for neural networks precludesus from taking full advantage of a neural computing framework. Here, we providesuch a programming language using reservoir computing -- a simple recurrentneural network -- and close the gap between how we conceptualize and implementneural computers and silicon computers. By decomposing the reservoir's internalrepresentation and dynamics into a symbolic basis of its inputs, we define alow-level neural machine code that we use to program the reservoir to solvecomplex equations and store chaotic dynamical systems as random access memory(dRAM). Using this representation, we provide a fully distributed neuralimplementation of software virtualization and logical circuits, and evenprogram a playable game of pong inside of a reservoir computer. Taken together,we define a concrete, practical, and fully generalizable implementation ofneural computation."}], "temperature": 0.1}}
{"custom_id": "591_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jason Z. Kim"}], "temperature": 0.1}}
{"custom_id": "592_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "592_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning is of vital importance to natural language understanding.Previous studies either employ graph-based models to incorporate priorknowledge about logical relations, or introduce symbolic logic into neuralmodels through data augmentation. These methods, however, heavily depend onannotated training data, and thus suffer from over-fitting and poorgeneralization problems due to the dataset sparsity. To address these twoproblems, in this paper, we propose MERIt, a MEta-path guided contrastivelearning method for logical ReasonIng of text, to perform self-supervisedpre-training on abundant unlabeled text data. Two novel strategies serve asindispensable components of our method. In particular, a strategy based onmeta-path is devised to discover the logical structure in natural texts,followed by a counterfactual data augmentation strategy to eliminate theinformation shortcut induced by pre-training. The experimental results on twochallenging logical reasoning benchmarks, i.e., ReClor and LogiQA, demonstratethat our method outperforms the SOTA baselines with significant improvements."}], "temperature": 0.1}}
{"custom_id": "592_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Fangkai Jiao"}], "temperature": 0.1}}
{"custom_id": "593_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Towards Unifying Logical Entailment and Statistical Estimation"}], "temperature": 0.1}}
{"custom_id": "593_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This paper gives a generative model of the interpretation of formal logic fordata-driven logical reasoning. The key idea is to represent the interpretationas likelihood of a formula being true given a model of formal logic. Using thelikelihood, Bayes' theorem gives the posterior of the model being the casegiven the formula. The posterior represents an inverse interpretation of formallogic that seeks models making the formula true. The likelihood and posteriorcause Bayesian learning that gives the probability of the conclusion being truein the models where all the premises are true. This paper looks at statisticaland logical properties of the Bayesian learning. It is shown that thegenerative model is a unified theory of several different types of reasoning inlogic and statistics."}], "temperature": 0.1}}
{"custom_id": "593_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hiroyuki Kido"}], "temperature": 0.1}}
{"custom_id": "594_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "MUC-driven Feature Importance Measurement and Adversarial Analysis for Random Forest"}], "temperature": 0.1}}
{"custom_id": "594_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The broad adoption of Machine Learning (ML) in security-critical fieldsdemands the explainability of the approach. However, the research onunderstanding ML models, such as Random Forest (RF), is still in its infantstage. In this work, we leverage formal methods and logical reasoning todevelop a novel model-specific method for explaining the prediction of RF. Ourapproach is centered around Minimal Unsatisfiable Cores (MUC) and provides acomprehensive solution for feature importance, covering local and globalaspects, and adversarial sample analysis. Experimental results on severaldatasets illustrate the high quality of our feature importance measurement. Wealso demonstrate that our adversarial analysis outperforms the state-of-the-artmethod. Moreover, our method can produce a user-centered report, which helpsprovide recommendations in real-life applications."}], "temperature": 0.1}}
{"custom_id": "594_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Shucen Ma"}], "temperature": 0.1}}
{"custom_id": "595_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "ExAIS: Executable AI Semantics"}], "temperature": 0.1}}
{"custom_id": "595_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neural networks can be regarded as a new programming paradigm, i.e., insteadof building ever-more complex programs through (often informal) logicalreasoning in the programmers' mind, complex 'AI' systems are built byoptimising generic neural network models with big data. In this new paradigm,AI frameworks such as TensorFlow and PyTorch play a key role, which is asessential as the compiler for traditional programs. It is known that the lackof a proper semantics for programming languages (such as C), i.e., acorrectness specification for compilers, has contributed to many problematicprogram behaviours and security issues. While it is in general hard to have acorrectness specification for compilers due to the high complexity ofprogramming languages and their rapid evolution, we have a unique opportunityto do it right this time for neural networks (which have a limited set offunctions, and most of them have stable semantics). In this work, we report oureffort on providing a correctness specification of neural network frameworkssuch as TensorFlow. We specify the semantics of almost all TensorFlow layers inthe logical programming language Prolog. We demonstrate the usefulness of thesemantics through two applications. One is a fuzzing engine for TensorFlow,which features a strong oracle and a systematic way of generating valid neuralnetworks. The other is a model validation approach which enables consistent bugreporting for TensorFlow models."}], "temperature": 0.1}}
{"custom_id": "595_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Richard Schumi"}], "temperature": 0.1}}
{"custom_id": "596_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Automated Reasoning in Non-classical Logics in the TPTP World"}], "temperature": 0.1}}
{"custom_id": "596_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Non-classical logics are used in a wide spectrum of disciplines, includingartificial intelligence, computer science, mathematics, and philosophy. Thede-facto standard infrastructure for automated theorem proving, the TPTP World,currently supports only classical logics. Similar standards for non-classicallogic reasoning do not exist (yet). This hampers practical development ofreasoning systems, and limits their interoperability and application. Thispaper describes the latest extension of the TPTP World, which provideslanguages and infrastructure for reasoning in non-classical logics. Theextensions integrate seamlessly with the existing TPTP World."}], "temperature": 0.1}}
{"custom_id": "596_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Alexander Steen"}], "temperature": 0.1}}
{"custom_id": "597_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "End-to-end Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking"}], "temperature": 0.1}}
{"custom_id": "597_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Machine learning systems perform well on pattern matching tasks, but theirability to perform algorithmic or logical reasoning is not well understood. Oneimportant reasoning capability is algorithmic extrapolation, in which modelstrained only on small/simple reasoning problems can synthesize complexstrategies for large/complex problems at test time. Algorithmic extrapolationcan be achieved through recurrent systems, which can be iterated many times tosolve difficult reasoning problems. We observe that this approach fails toscale to highly complex problems because behavior degenerates when manyiterations are applied -- an issue we refer to as \"overthinking.\" We propose arecall architecture that keeps an explicit copy of the problem instance inmemory so that it cannot be forgotten. We also employ a progressive trainingroutine that prevents the model from learning behaviors that are specific toiteration number and instead pushes it to learn behaviors that can be repeatedindefinitely. These innovations prevent the overthinking problem, and enablerecurrent systems to solve extremely hard extrapolation tasks."}], "temperature": 0.1}}
{"custom_id": "597_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Arpit Bansal"}], "temperature": 0.1}}
{"custom_id": "598_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical Reasoning for Task Oriented Dialogue Systems"}], "temperature": 0.1}}
{"custom_id": "598_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "In recent years, large pretrained models have been used in dialogue systemsto improve successful task completion rates. However, lack of reasoningcapabilities of dialogue platforms make it difficult to provide relevant andfluent responses, unless the designers of a conversational experience spend aconsiderable amount of time implementing these capabilities in external rulebased modules. In this work, we propose a novel method to fine-tune pretrainedtransformer models such as Roberta and T5. to reason over a set of facts in agiven dialogue context. Our method includes a synthetic data generationmechanism which helps the model learn logical relations, such as comparisonbetween list of numerical values, inverse relations (and negation), inclusionand exclusion for categorical attributes, and application of a combination ofattributes over both numerical and categorical values, and spoken form fornumerical values, without need for additional training dataset. We show thatthe transformer based model can perform logical reasoning to answer questionswhen the dialogue context contains all the required information, otherwise itis able to extract appropriate constraints to pass to downstream components(e.g. a knowledge base) when partial information is available. We observe thattransformer based models such as UnifiedQA-T5 can be fine-tuned to performlogical reasoning (such as numerical and categorical attributes' comparison)over attributes that been seen in training time (e.g., accuracy of 90\\%+ forcomparison of smaller than $k_{\\max}$=5 values over heldout test dataset)."}], "temperature": 0.1}}
{"custom_id": "598_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Sajjad Beygi"}], "temperature": 0.1}}
{"custom_id": "599_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "VAEL: Bridging Variational Autoencoders and Probabilistic Logic Programming"}], "temperature": 0.1}}
{"custom_id": "599_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We present VAEL, a neuro-symbolic generative model integrating variationalautoencoders (VAE) with the reasoning capabilities of probabilistic logic (L)programming. Besides standard latent subsymbolic variables, our model exploitsa probabilistic logic program to define a further structured representation,which is used for logical reasoning. The entire process is end-to-enddifferentiable. Once trained, VAEL can solve new unseen generation tasks by (i)leveraging the previously acquired knowledge encoded in the neural componentand (ii) exploiting new logical programs on the structured latent space. Ourexperiments provide support on the benefits of this neuro-symbolic integrationboth in terms of task generalization and data efficiency. To the best of ourknowledge, this work is the first to propose a general-purpose end-to-endframework integrating probabilistic logic programming into a deep generativemodel."}], "temperature": 0.1}}
{"custom_id": "599_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Eleonora Misino"}], "temperature": 0.1}}
{"custom_id": "600_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neural Logic Analogy Learning"}], "temperature": 0.1}}
{"custom_id": "600_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Letter-string analogy is an important analogy learning task which seems to beeasy for humans but very challenging for machines. The main idea behind currentapproaches to solving letter-string analogies is to design heuristic rules forextracting analogy structures and constructing analogy mappings. However, onekey problem is that it is difficult to build a comprehensive and exhaustive setof analogy structures which can fully describe the subtlety of analogies. Thisproblem makes current approaches unable to handle complicated letter-stringanalogy problems. In this paper, we propose Neural logic analogy learning(Noan), which is a dynamic neural architecture driven by differentiable logicreasoning to solve analogy problems. Each analogy problem is converted intological expressions consisting of logical variables and basic logicaloperations (AND, OR, and NOT). More specifically, Noan learns the logicalvariables as vector embeddings and learns each logical operation as a neuralmodule. In this way, the model builds computational graph integrating neuralnetwork with logical reasoning to capture the internal logical structure of theinput letter strings. The analogy learning problem then becomes a True/Falseevaluation problem of the logical expressions. Experiments show that ourmachine learning-based Noan approach outperforms state-of-the-art approaches onstandard letter-string analogy benchmark datasets."}], "temperature": 0.1}}
{"custom_id": "600_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yujia Fan"}], "temperature": 0.1}}
{"custom_id": "601_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "PRIMA: Planner-Reasoner Inside a Multi-task Reasoning Agent"}], "temperature": 0.1}}
{"custom_id": "601_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We consider the problem of multi-task reasoning (MTR), where an agent cansolve multiple tasks via (first-order) logic reasoning. This capability isessential for human-like intelligence due to its strong generalizability andsimplicity for handling multiple tasks. However, a major challenge indeveloping effective MTR is the intrinsic conflict between reasoning capabilityand efficiency. An MTR-capable agent must master a large set of \"skills\" totackle diverse tasks, but executing a particular task at the inference stagerequires only a small subset of immediately relevant skills. How can wemaintain broad reasoning capability and also efficient specific-taskperformance? To address this problem, we propose a Planner-Reasoner frameworkcapable of state-of-the-art MTR capability and high efficiency. The Reasonermodels shareable (first-order) logic deduction rules, from which the Plannerselects a subset to compose into efficient reasoning paths. The entire model istrained in an end-to-end manner using deep reinforcement learning, andexperimental studies over a variety of domains validate its effectiveness."}], "temperature": 0.1}}
{"custom_id": "601_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Daoming Lyu"}], "temperature": 0.1}}
{"custom_id": "602_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning Like Program Executors"}], "temperature": 0.1}}
{"custom_id": "602_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning over natural language is a long-standing goal for the researchcommunity. However, studies have shown that existing language models areinadequate in reasoning. To address the issue, we present POET, a novelreasoning pre-training paradigm. Through pre-training language models withprograms and their execution results, POET empowers language models to harvestthe reasoning knowledge possessed by program executors via a data-drivenapproach. POET is conceptually simple and can be instantiated by differentkinds of program executors. In this paper, we showcase two simple instancesPOET-Math and POET-Logic, in addition to a complex instance, POET-SQL.Experimental results on six benchmarks demonstrate that POET can significantlyboost model performance in natural language reasoning, such as numericalreasoning, logical reasoning, and multi-hop reasoning. POET opens a new gate onreasoning-enhancement pre-training, and we hope our analysis would shed lighton the future research of reasoning like program executors."}], "temperature": 0.1}}
{"custom_id": "602_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Xinyu Pi"}], "temperature": 0.1}}
{"custom_id": "603_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Combining Commonsense Reasoning and Knowledge Acquisition to Guide Deep Learning in Robotics"}], "temperature": 0.1}}
{"custom_id": "603_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Algorithms based on deep network models are being used for many patternrecognition and decision-making tasks in robotics and AI. Training these modelsrequires a large labeled dataset and considerable computational resources,which are not readily available in many domains. Also, it is difficult toexplore the internal representations and reasoning mechanisms of these models.As a step towards addressing the underlying knowledge representation,reasoning, and learning challenges, the architecture described in this paperdraws inspiration from research in cognitive systems. As a motivating example,we consider an assistive robot trying to reduce clutter in any given scene byreasoning about the occlusion of objects and stability of object configurationsin an image of the scene. In this context, our architecture incrementallylearns and revises a grounding of the spatial relations between objects anduses this grounding to extract spatial information from input images.Non-monotonic logical reasoning with this information and incompletecommonsense domain knowledge is used to make decisions about stability andocclusion. For images that cannot be processed by such reasoning, regionsrelevant to the tasks at hand are automatically identified and used to traindeep network models to make the desired decisions. Image regions used to trainthe deep networks are also used to incrementally acquire previously unknownstate constraints that are merged with the existing knowledge for subsequentreasoning. Experimental evaluation performed using simulated and real-worldimages indicates that in comparison with baselines based just on deep networks,our architecture improves reliability of decision making and reduces the effortinvolved in training data-driven deep network models."}], "temperature": 0.1}}
{"custom_id": "603_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mohan Sridharan"}], "temperature": 0.1}}
{"custom_id": "604_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Scales and Hedges in a Logic with Analogous Semantics"}], "temperature": 0.1}}
{"custom_id": "604_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logics with analogous semantics, such as Fuzzy Logic, have a number ofexplanatory and application advantages, the most well-known being the abilityto help experts develop control systems. From a cognitive systems perspective,such languages also have the advantage of being grounded in perception. Forsocial decision making in humans, it is vital that logical conclusions aboutothers (cognitive empathy) are grounded in empathic emotion (affectiveempathy). Classical Fuzzy Logic, however, has several disadvantages: it is notobvious how complex formulae, e.g., the description of events in a text, can be(a) formed, (b) grounded, and (c) used in logical reasoning. The two-layeredContext Logic (CL) was designed to address these issue. Formally based on alattice semantics, like classical Fuzzy Logic, CL also features an analogoussemantics for complex fomulae. With the Activation Bit Vector Machine (ABVM),it has a simple and classical logical reasoning mechanism with an inherentimagery process based on the Vector Symbolic Architecture (VSA) model ofdistributed neuronal processing. This paper adds to the existing theory howscales, as necessary for adjective and verb semantics can be handled by thesystem."}], "temperature": 0.1}}
{"custom_id": "604_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hedda R. Schmidtke"}], "temperature": 0.1}}
{"custom_id": "605_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mechanizing Matching Logic In Coq"}], "temperature": 0.1}}
{"custom_id": "605_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Matching logic is a formalism for specifying, and reasoning about,mathematical structures, using patterns and pattern matching. Growing inpopularity, it has been used to define many logical systems such as separationlogic with recursive definitions and linear temporal logic. In addition, itserves as the logical foundation of the K semantic framework, which was used tobuild practical verifiers for a number of real-world languages. Despite being afundamental formal system accommodating substantial theories, matching logiclacks a general-purpose, machine-checked formalization. Hence, we formalizematching logic using the Coq proof assistant. Specifically, we create a newrepresentation of matching logic that uses a locally nameless encoding, and weformalize the syntax, semantics, and proof system of this representation in theCoq proof assistant. Crucially, we prove the soundness of the formalized proofsystem and provide a means to carry out interactive matching logic reasoning inCoq. We believe this work provides a previously unexplored avenue for reasoningabout matching logic, its models, and the proof system."}], "temperature": 0.1}}
{"custom_id": "605_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "P茅ter Bereczky"}], "temperature": 0.1}}
{"custom_id": "606_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Does Entity Abstraction Help Generative Transformers Reason?"}], "temperature": 0.1}}
{"custom_id": "606_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We study the utility of incorporating entity type abstractions intopre-trained Transformers and test these methods on four NLP tasks requiringdifferent forms of logical reasoning: (1) compositional language understandingwith text-based relational reasoning (CLUTRR), (2) abductive reasoning(ProofWriter), (3) multi-hop question answering (HotpotQA), and (4)conversational question answering (CoQA). We propose and empirically explorethree ways to add such abstraction: (i) as additional input embeddings, (ii) asa separate sequence to encode, and (iii) as an auxiliary prediction task forthe model. Overall, our analysis demonstrates that models with abstract entityknowledge performs better than without it. The best abstraction aware modelsachieved an overall accuracy of 88.8% and 91.8% compared to the baseline modelachieving 62.9% and 89.8% on CLUTRR and ProofWriter respectively. However, forHotpotQA and CoQA, we find that F1 scores improve by only 0.5% on average. Ourresults suggest that the benefit of explicit abstraction is significant informally defined logical reasoning settings requiring many reasoning hops, butpoint to the notion that it is less beneficial for NLP tasks having less formallogical structure."}], "temperature": 0.1}}
{"custom_id": "606_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Nicolas Gontier"}], "temperature": 0.1}}
{"custom_id": "607_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Modeling Associative Reasoning Processes"}], "temperature": 0.1}}
{"custom_id": "607_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The human capability to reason about one domain by using knowledge of otherdomains has been researched for more than 50 years, but models that areformally sound and predict cognitive process are sparse. We propose a formallysound method that models associative reasoning by adapting logical reasoningmechanisms. In particular it is shown that the combination with largecommensense knowledge within a single reasoning system demands for an efficientand powerful association technique. This approach is also used for modellingmind-wandering and the Remote Associates Test (RAT) for testing creativity. Ina general discussion we show implications of the model for a broad variety ofcognitive phenomena including consciousness."}], "temperature": 0.1}}
{"custom_id": "607_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Claudia Schon"}], "temperature": 0.1}}
{"custom_id": "608_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasoning in circles"}], "temperature": 0.1}}
{"custom_id": "608_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Circular proofs, introduced by Daniyar Shamkanov, are proofs in whichassumptions are allowed that are not axioms but do appear at least twice alonga branch. Shamkanov has shown that a formula belongs to the provability logicGL exactly if it has a circular proof in the modal logic K4. Shamkanov usesTait style proof systems and infinitary proofs. In this paper we prove the sameresult but then for sequent calculi and without the detour via infinitarysystems. We also obtain a mild generalisation of the result, implying that itsintuitionistic analogue holds as well."}], "temperature": 0.1}}
{"custom_id": "608_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Rosalie Iemhoff"}], "temperature": 0.1}}
{"custom_id": "609_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Graph Collaborative Reasoning"}], "temperature": 0.1}}
{"custom_id": "609_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Graphs can represent relational information among entities and graphstructures are widely used in many intelligent tasks such as search,recommendation, and question answering. However, most of the graph-structureddata in practice suffers from incompleteness, and thus link prediction becomesan important research problem. Though many models are proposed for linkprediction, the following two problems are still less explored: (1) Mostmethods model each link independently without making use of the richinformation from relevant links, and (2) existing models are mostly designedbased on associative learning and do not take reasoning into consideration.With these concerns, in this paper, we propose Graph Collaborative Reasoning(GCR), which can use the neighbor link information for relational reasoning ongraphs from logical reasoning perspectives. We provide a simple approach totranslate a graph structure into logical expressions, so that the linkprediction task can be converted into a neural logic reasoning problem. Weapply logical constrained neural modules to build the network architectureaccording to the logical expression and use back propagation to efficientlylearn the model parameters, which bridges differentiable learning and symbolicreasoning in a unified architecture. To show the effectiveness of our work, weconduct experiments on graph-related tasks such as link prediction andrecommendation based on commonly used benchmark datasets, and our graphcollaborative reasoning approach achieves state-of-the-art performance."}], "temperature": 0.1}}
{"custom_id": "609_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hanxiong Chen"}], "temperature": 0.1}}
{"custom_id": "610_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Two-stage Rule-induction Visual Reasoning on RPMs with an Application to Video Prediction"}], "temperature": 0.1}}
{"custom_id": "610_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Raven's Progressive Matrices (RPMs) are frequently used in evaluating human'svisual reasoning ability. Researchers have made considerable efforts indeveloping systems to automatically solve the RPM problem, often through ablack-box end-to-end convolutional neural network for both visual recognitionand logical reasoning tasks. Based on the two intrinsic natures of RPM problem,visual recognition and logical reasoning, we propose a Two-stage Rule-InductionVisual Reasoner (TRIVR), which consists of a perception module and a reasoningmodule, to tackle the challenges of real-world visual recognition andsubsequent logical reasoning tasks, respectively. For the reasoning module, wefurther propose a \"2+1\" formulation that models human's thinking in solvingRPMs and significantly reduces the model complexity. It derives a reasoningrule from each RPM sample, which is not feasible for existing methods. As aresult, the proposed reasoning module is capable of yielding a set of reasoningrules modeling human in solving the RPM problems. To validate the proposedmethod on real-world applications, an RPM-like Video Prediction (RVP) datasetis constructed, where visual reasoning is conducted on RPMs constructed usingreal-world video frames. Experimental results on various RPM-like datasetsdemonstrate that the proposed TRIVR achieves a significant and consistentperformance gain compared with the state-of-the-art models."}], "temperature": 0.1}}
{"custom_id": "610_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Wentao He"}], "temperature": 0.1}}
{"custom_id": "611_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SQALER: Scaling Question Answering by Decoupling Multi-Hop and Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "611_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "State-of-the-art approaches to reasoning and question answering overknowledge graphs (KGs) usually scale with the number of edges and can only beapplied effectively on small instance-dependent subgraphs. In this paper, weaddress this issue by showing that multi-hop and more complex logical reasoningcan be accomplished separately without losing expressive power. Motivated bythis insight, we propose an approach to multi-hop reasoning that scaleslinearly with the number of relation types in the graph, which is usuallysignificantly smaller than the number of edges or nodes. This produces a set ofcandidate solutions that can be provably refined to recover the solution to theoriginal problem. Our experiments on knowledge-based question answering showthat our approach solves the multi-hop MetaQA dataset, achieves a newstate-of-the-art on the more challenging WebQuestionsSP, is orders of magnitudemore scalable than competitive approaches, and can achieve compositionalgeneralization out of the training distribution."}], "temperature": 0.1}}
{"custom_id": "611_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mattia Atzeni"}], "temperature": 0.1}}
{"custom_id": "612_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Probabilistic Entity Representation Model for Reasoning over Knowledge Graphs"}], "temperature": 0.1}}
{"custom_id": "612_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning over Knowledge Graphs (KGs) is a fundamental technique thatcan provide efficient querying mechanism over large and incomplete databases.Current approaches employ spatial geometries such as boxes to learn queryrepresentations that encompass the answer entities and model the logicaloperations of projection and intersection. However, their geometry isrestrictive and leads to non-smooth strict boundaries, which further results inambiguous answer entities. Furthermore, previous works propose transformationtricks to handle unions which results in non-closure and, thus, cannot bechained in a stream. In this paper, we propose a Probabilistic EntityRepresentation Model (PERM) to encode entities as a Multivariate Gaussiandensity with mean and covariance parameters to capture its semantic positionand smooth decision boundary, respectively. Additionally, we also define theclosed logical operations of projection, intersection, and union that can beaggregated using an end-to-end objective function. On the logical queryreasoning problem, we demonstrate that the proposed PERM significantlyoutperforms the state-of-the-art methods on various public benchmark KGdatasets on standard evaluation metrics. We also evaluate PERM's competence ona COVID-19 drug-repurposing case study and show that our proposed work is ableto recommend drugs with substantially better F1 than current methods. Finally,we demonstrate the working of our PERM's query answering process through alow-dimensional visualization of the Gaussian representations."}], "temperature": 0.1}}
{"custom_id": "612_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Nurendra Choudhary"}], "temperature": 0.1}}
{"custom_id": "613_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Survey on State-of-the-art Techniques for Knowledge Graphs Construction and Challenges ahead"}], "temperature": 0.1}}
{"custom_id": "613_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Global datasphere is increasing fast, and it is expected to reach 175Zettabytes by 20251 . However, most of the content is unstructured and is notunderstandable by machines. Structuring this data into a knowledge graphenables multitudes of intelligent applications such as deep question answering,recommendation systems, semantic search, etc. The knowledge graph is anemerging technology that allows logical reasoning and uncovers new insightsusing content along with the context. Thereby, it provides necessary syntax andreasoning semantics that enable machines to solve complex healthcare, security,financial institutions, economics, and business problems. As an outcome,enterprises are putting their effort into constructing and maintainingknowledge graphs to support various downstream applications. Manual approachesare too expensive. Automated schemes can reduce the cost of building knowledgegraphs up to 15-250 times. This paper critiques state-of-the-art automatedtechniques to produce knowledge graphs of near-human quality autonomously.Additionally, it highlights different research issues that need to be addressedto deliver high-quality knowledge graphs"}], "temperature": 0.1}}
{"custom_id": "613_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ali Hur"}], "temperature": 0.1}}
{"custom_id": "614_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "ConditionalQA: A Complex Reading Comprehension Dataset with Conditional Answers"}], "temperature": 0.1}}
{"custom_id": "614_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We describe a Question Answering (QA) dataset that contains complex questionswith conditional answers, i.e. the answers are only applicable when certainconditions apply. We call this dataset ConditionalQA. In addition toconditional answers, the dataset also features: (1) long context documents withinformation that is related in logically complex ways; (2) multi-hop questionsthat require compositional logical reasoning; (3) a combination of extractivequestions, yes/no questions, questions with multiple answers, andnot-answerable questions; (4) questions asked without knowing the answers. Weshow that ConditionalQA is challenging for many of the existing QA models,especially in selecting answer conditions. We believe that this dataset willmotivate further research in answering complex questions over long documents.Data and leaderboard are publicly available at\\url{https://github.com/haitian-sun/ConditionalQA}."}], "temperature": 0.1}}
{"custom_id": "614_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Haitian Sun"}], "temperature": 0.1}}
{"custom_id": "615_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "RuleBert: Teaching Soft Rules to Pre-trained Language Models"}], "temperature": 0.1}}
{"custom_id": "615_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "While pre-trained language models (PLMs) are the go-to solution to tacklemany natural language processing problems, they are still very limited in theirability to capture and to use common-sense knowledge. In fact, even ifinformation is available in the form of approximate (soft) logical rules, it isnot clear how to transfer it to a PLM in order to improve its performance fordeductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs howto reason with soft Horn rules. We introduce a classification task where, givenfacts and soft rules, the PLM should return a prediction with a probability fora given hypothesis. We release the first dataset for this task, and we proposea revised loss function that enables the PLM to learn how to predict preciseprobabilities for the task. Our evaluation results show that the resultingfine-tuned models achieve very high performance, even on logical rules thatwere unseen at training. Moreover, we demonstrate that logical notionsexpressed by the rules are transferred to the fine-tuned model, yieldingstate-of-the-art results on external datasets."}], "temperature": 0.1}}
{"custom_id": "615_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mohammed Saeed"}], "temperature": 0.1}}
{"custom_id": "616_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neural Unification for Logic Reasoning over Natural Language"}], "temperature": 0.1}}
{"custom_id": "616_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Automated Theorem Proving (ATP) deals with the development of computerprograms being able to show that some conjectures (queries) are a logicalconsequence of a set of axioms (facts and rules). There exists severalsuccessful ATPs where conjectures and axioms are formally provided (e.g.formalised as First Order Logic formulas). Recent approaches, such as (Clark etal., 2020), have proposed transformer-based architectures for derivingconjectures given axioms expressed in natural language (English). Theconjecture is verified through a binary text classifier, where the transformersmodel is trained to predict the truth value of a conjecture given the axioms.The RuleTaker approach of (Clark et al., 2020) achieves appealing results bothin terms of accuracy and in the ability to generalize, showing that when themodel is trained with deep enough queries (at least 3 inference steps), thetransformers are able to correctly answer the majority of queries (97.6%) thatrequire up to 5 inference steps. In this work we propose a new architecture,namely the Neural Unifier, and a relative training procedure, which achievesstate-of-the-art results in term of generalisation, showing that mimicking awell-known inference procedure, the backward chaining, it is possible to answerdeep queries even when the model is trained only on shallow ones. The approachis demonstrated in experiments using a diverse set of benchmark data."}], "temperature": 0.1}}
{"custom_id": "616_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Gabriele Picco"}], "temperature": 0.1}}
{"custom_id": "617_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Geolog: Scalable Logic Programming on Spatial Data"}], "temperature": 0.1}}
{"custom_id": "617_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Spatial data is ubiquitous in our data-driven society. The Logic Programmingcommunity has been investigating the use of spatial data in different settings.Despite the success of this research, the Geographic Information System (GIS)community has rarely made use of these new approaches. This has mainly tworeasons. First, there is a lack of tools that tightly integrate logicalreasoning into state-of-the-art GIS software. Second, the scalability ofsolutions has often not been tested and hence, some solutions might work on toyexamples but do not scale well to real-world settings. The two maincontributions of this paper are (1) the Relation Based Programming paradigm,expressing rules on relations instead of individual entities, and (2) Geolog, atool for spatio-logical reasoning that can be installed on top of ArcMap, whichis an industry standard GIS. We evaluate our new Relation Based Programmingparadigm in four real-world scenarios and show that up to two orders ofmagnitude in performance gain can be achieved compared to the prevalent EntityBased Programming paradigm."}], "temperature": 0.1}}
{"custom_id": "617_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Tobias Grubenmann"}], "temperature": 0.1}}
{"custom_id": "618_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Combining Event Calculus and Description Logic Reasoning via Logic Programming"}], "temperature": 0.1}}
{"custom_id": "618_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The paper introduces a knowledge representation language that combines theevent calculus with description logic in a logic programming framework. Thepurpose is to provide the user with an expressive language for modelling andanalysing systems that evolve over time. The approach is exemplified with thelogic programming language as implemented in the Fusemate system. The paperextends Fusemate's rule language with a weakly DL-safe interface to thedescription logic $\\cal ALCIF$ and adapts the event calculus to this extendedlanguage. This way, time-stamped ABoxes can be manipulated as fluents in theevent calculus. All that is done in the frame of Fusemate's concept ofstratification by time. The paper provides conditions for soundness andcompleteness where appropriate. Using an elaborated example it demonstrates theinterplay of the event calculus, description logic and logic programming rulesfor computing possible models as plausible explanations of the current state ofthe modelled system."}], "temperature": 0.1}}
{"custom_id": "618_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Peter Baumgartner"}], "temperature": 0.1}}
{"custom_id": "619_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Counterfactual Adversarial Learning with Representation Interpolation"}], "temperature": 0.1}}
{"custom_id": "619_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Deep learning models exhibit a preference for statistical fitting overlogical reasoning. Spurious correlations might be memorized when there existsstatistical bias in training data, which severely limits the model performanceespecially in small data scenarios. In this work, we introduce CounterfactualAdversarial Training framework (CAT) to tackle the problem from a causalityperspective. Particularly, for a specific sample, CAT first generates acounterfactual representation through latent space interpolation in anadversarial manner, and then performs Counterfactual Risk Minimization (CRM) oneach original-counterfactual pair to adjust sample-wise loss weightdynamically, which encourages the model to explore the true causal effect.Extensive experiments demonstrate that CAT achieves substantial performanceimprovement over SOTA across different downstream tasks, including sentenceclassification, natural language inference and question answering."}], "temperature": 0.1}}
{"custom_id": "619_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Wei Wang"}], "temperature": 0.1}}
{"custom_id": "620_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Sinoledge: A Knowledge Engine based on Logical Reasoning and Distributed Micro Services"}], "temperature": 0.1}}
{"custom_id": "620_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We propose a knowledge engine called Sinoledge mainly for doctors,physicians, and researchers in medical field to organize thoughts, managereasoning process, test and deploy to production environments effortlessly. Ourproposal can be related to rule engine usually used in business or medicalfields. More importantly, our proposal provides a user-friendly interface, aneasy-maintain way of organizing knowledge, an understandable testingfunctionality and a highly available and efficient back-end architecture."}], "temperature": 0.1}}
{"custom_id": "620_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yining Huang"}], "temperature": 0.1}}
{"custom_id": "621_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "621_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "To quantitatively and intuitively explore the generalization ability ofpre-trained language models (PLMs), we have designed several tasks ofarithmetic and logical reasoning. We both analyse how well PLMs generalize whenthe test data is in the same distribution as the train data and when it isdifferent, for the latter analysis, we have also designed a cross-distributiontest set other than the in-distribution test set. We conduct experiments on oneof the most advanced and publicly released generative PLM - BART. Our researchfinds that the PLMs can easily generalize when the distribution is the same,however, it is still difficult for them to generalize out of the distribution."}], "temperature": 0.1}}
{"custom_id": "621_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Cunxiang Wang"}], "temperature": 0.1}}
{"custom_id": "622_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "From LSAT: The Progress and Challenges of Complex Reasoning"}], "temperature": 0.1}}
{"custom_id": "622_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Complex reasoning aims to draw a correct inference based on complex rules. Asa hallmark of human intelligence, it involves a degree of explicit readingcomprehension, interpretation of logical knowledge and complex ruleapplication. In this paper, we take a step forward in complex reasoning bysystematically studying the three challenging and domain-general tasks of theLaw School Admission Test (LSAT), including analytical reasoning, logicalreasoning and reading comprehension. We propose a hybrid reasoning system tointegrate these three tasks and achieve impressive overall performance on theLSAT tests. The experimental results demonstrate that our system endows itselfa certain complex reasoning ability, especially the fundamental readingcomprehension and challenging logical reasoning capacities. Further analysisalso shows the effectiveness of combining the pre-trained models with thetask-specific reasoning module, and integrating symbolic knowledge intodiscrete interpretable reasoning steps in complex reasoning. We further shed alight on the potential future directions, like unsupervised symbolic knowledgeextraction, model interpretability, few-shot learning and comprehensivebenchmark for complex reasoning."}], "temperature": 0.1}}
{"custom_id": "622_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Siyuan Wang"}], "temperature": 0.1}}
{"custom_id": "623_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Improving Coherence and Consistency in Neural Sequence Models with Dual-System, Neuro-Symbolic Reasoning"}], "temperature": 0.1}}
{"custom_id": "623_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Human reasoning can often be understood as an interplay between two systems:the intuitive and associative (\"System 1\") and the deliberative and logical(\"System 2\"). Neural sequence models -- which have been increasingly successfulat performing complex, structured tasks -- exhibit the advantages and failuremodes of System 1: they are fast and learn patterns from data, but are ofteninconsistent and incoherent. In this work, we seek a lightweight, training-freemeans of improving existing System 1-like sequence models by adding System2-inspired logical reasoning. We explore several variations on this theme inwhich candidate generations from a neural sequence model are examined forlogical consistency by a symbolic reasoning module, which can either accept orreject the generations. Our approach uses neural inference to mediate betweenthe neural System 1 and the logical System 2. Results in robust storygeneration and grounded instruction-following show that this approach canincrease the coherence and accuracy of neurally-based generations."}], "temperature": 0.1}}
{"custom_id": "623_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Maxwell Nye"}], "temperature": 0.1}}
{"custom_id": "624_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Towards Assurance-Driven Architectural Decomposition of Software Systems"}], "temperature": 0.1}}
{"custom_id": "624_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Computer systems are so complex, so they are usually designed and analyzed interms of layers of abstraction. Complexity is still a challenge facing logicalreasoning tools that are used to find software design flaws and implementationbugs. Abstraction is also a common technique for scaling those tools to morecomplex systems. However, the abstractions used in the design phase of systemsare in many cases different from those used for assurance. In this paper weargue that different software quality assurance techniques operate on differentaspects of software systems. To facilitate assurance, and for a smoothintegration of assurance tools into the Software Development Lifecycle (SDLC),we present a 4-dimensional meta-architecture that separates computational,coordination, and stateful software artifacts early on in the design stage. Weenumerate some of the design and assurance challenges that can be addressed bythis meta-architecture, and demonstrate it on the high-level design of a simplefile system."}], "temperature": 0.1}}
{"custom_id": "624_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ramy Shahin"}], "temperature": 0.1}}
{"custom_id": "625_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Techniques for Symbol Grounding with SATNet"}], "temperature": 0.1}}
{"custom_id": "625_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Many experts argue that the future of artificial intelligence is limited bythe field's ability to integrate symbolic logical reasoning into deep learningarchitectures. The recently proposed differentiable MAXSAT solver, SATNet, wasa breakthrough in its capacity to integrate with a traditional neural networkand solve visual reasoning problems. For instance, it can learn the rules ofSudoku purely from image examples. Despite its success, SATNet was shown tosuccumb to a key challenge in neurosymbolic systems known as the SymbolGrounding Problem: the inability to map visual inputs to symbolic variableswithout explicit supervision (\"label leakage\"). In this work, we present aself-supervised pre-training pipeline that enables SATNet to overcome thislimitation, thus broadening the class of problems that SATNet architectures cansolve to include datasets where no intermediary labels are available at all. Wedemonstrate that our method allows SATNet to attain full accuracy even with aharder problem setup that prevents any label leakage. We additionally introducea proofreading method that further improves the performance of SATNetarchitectures, beating the state-of-the-art on Visual Sudoku."}], "temperature": 0.1}}
{"custom_id": "625_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Sever Topan"}], "temperature": 0.1}}
{"custom_id": "626_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "SIRE: Separate Intra- and Inter-sentential Reasoning for Document-level Relation Extraction"}], "temperature": 0.1}}
{"custom_id": "626_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Document-level relation extraction has attracted much attention in recentyears. It is usually formulated as a classification problem that predictsrelations for all entity pairs in the document. However, previous worksindiscriminately represent intra- and inter-sentential relations in the sameway, confounding the different patterns for predicting them. Besides, theycreate a document graph and use paths between entities on the graph as cluesfor logical reasoning. However, not all entity pairs can be connected with apath and have the correct logical reasoning paths in their graph. Thus manycases of logical reasoning cannot be covered. This paper proposes an effectivearchitecture, SIRE, to represent intra- and inter-sentential relations indifferent ways. We design a new and straightforward form of logical reasoningmodule that can cover more logical reasoning chains. Experiments on the publicdatasets show SIRE outperforms the previous state-of-the-art methods. Furtheranalysis shows that our predictions are reliable and explainable. Our code isavailable at https://github.com/DreamInvoker/SIRE."}], "temperature": 0.1}}
{"custom_id": "626_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Shuang Zeng"}], "temperature": 0.1}}
{"custom_id": "627_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Discriminative Reasoning for Document-level Relation Extraction"}], "temperature": 0.1}}
{"custom_id": "627_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Document-level relation extraction (DocRE) models generally use graphnetworks to implicitly model the reasoning skill (i.e., pattern recognition,logical reasoning, coreference reasoning, etc.) related to the relation betweenone entity pair in a document. In this paper, we propose a novel discriminativereasoning framework to explicitly model the paths of these reasoning skillsbetween each entity pair in this document. Thus, a discriminative reasoningnetwork is designed to estimate the relation probability distribution ofdifferent reasoning paths based on the constructed graph and vectorizeddocument contexts for each entity pair, thereby recognizing their relation.Experimental results show that our method outperforms the previousstate-of-the-art performance on the large-scale DocRE dataset. The code ispublicly available at https://github.com/xwjim/DRN."}], "temperature": 0.1}}
{"custom_id": "627_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Wang Xu"}], "temperature": 0.1}}
{"custom_id": "628_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Volta at SemEval-2021 Task 9: Statement Verification and Evidence Finding with Tables using TAPAS and Transfer Learning"}], "temperature": 0.1}}
{"custom_id": "628_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Tables are widely used in various kinds of documents to present informationconcisely. Understanding tables is a challenging problem that requires anunderstanding of language and table structure, along with numerical and logicalreasoning. In this paper, we present our systems to solve Task 9 ofSemEval-2021: Statement Verification and Evidence Finding with Tables(SEM-TAB-FACTS). The task consists of two subtasks: (A) Given a table and astatement, predicting whether the table supports the statement and (B)Predicting which cells in the table provide evidence for/against the statement.We fine-tune TAPAS (a model which extends BERT's architecture to capturetabular structure) for both the subtasks as it has shown state-of-the-artperformance in various table understanding tasks. In subtask A, we evaluate howtransfer learning and standardizing tables to have a single header row improvesTAPAS' performance. In subtask B, we evaluate how different fine-tuningstrategies can improve TAPAS' performance. Our systems achieve an F1 score of67.34 in subtask A three-way classification, 72.89 in subtask A two-wayclassification, and 62.95 in subtask B."}], "temperature": 0.1}}
{"custom_id": "628_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Devansh Gautam"}], "temperature": 0.1}}
{"custom_id": "629_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "NeuralLog: Natural Language Inference with Joint Neural and Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "629_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Deep learning (DL) based language models achieve high performance on variousbenchmarks for Natural Language Inference (NLI). And at this time, symbolicapproaches to NLI are receiving less attention. Both approaches (symbolic andDL) have their advantages and weaknesses. However, currently, no methodcombines them in a system to solve the task of NLI. To merge symbolic and deeplearning methods, we propose an inference framework called NeuralLog, whichutilizes both a monotonicity-based logical inference engine and a neuralnetwork language model for phrase alignment. Our framework models the NLI taskas a classic search problem and uses the beam search algorithm to search foroptimal inference paths. Experiments show that our joint logic and neuralinference system improves accuracy on the NLI task and can achieve state-of-artaccuracy on the SICK and MED datasets."}], "temperature": 0.1}}
{"custom_id": "629_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zeming Chen"}], "temperature": 0.1}}
{"custom_id": "630_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Probabilistic Sufficient Explanations"}], "temperature": 0.1}}
{"custom_id": "630_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Understanding the behavior of learned classifiers is an important task, andvarious black-box explanations, logical reasoning approaches, andmodel-specific methods have been proposed. In this paper, we introduceprobabilistic sufficient explanations, which formulate explaining an instanceof classification as choosing the \"simplest\" subset of features such that onlyobserving those features is \"sufficient\" to explain the classification. Thatis, sufficient to give us strong probabilistic guarantees that the model willbehave similarly when all features are observed under the data distribution. Inaddition, we leverage tractable probabilistic reasoning tools such asprobabilistic circuits and expected predictions to design a scalable algorithmfor finding the desired explanations while keeping the guarantees intact. Ourexperiments demonstrate the effectiveness of our algorithm in findingsufficient explanations, and showcase its advantages compared to Anchors andlogical explanations."}], "temperature": 0.1}}
{"custom_id": "630_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Eric Wang"}], "temperature": 0.1}}
{"custom_id": "631_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text"}], "temperature": 0.1}}
{"custom_id": "631_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning of text requires understanding critical logical informationin the text and performing inference over them. Large-scale pre-trained modelsfor logical reasoning mainly focus on word-level semantics of text whilestruggling to capture symbolic logic. In this paper, we propose to understandlogical symbols and expressions in the text to arrive at the answer. Based onsuch logical information, we not only put forward a context extension frameworkbut also propose a data augmentation algorithm. The former extends the contextto cover implicit logical expressions following logical equivalence laws. Thelatter augments literally similar but logically different instances to bettercapture logical information, especially logical negative and conditionalrelationships. We conduct experiments on ReClor dataset. The results show thatour method achieves the state-of-the-art performance, and both logic-drivencontext extension framework and data augmentation algorithm can help improvethe accuracy. And our multi-model ensemble system is the first to surpass humanperformance on both EASY set and HARD set of ReClor."}], "temperature": 0.1}}
{"custom_id": "631_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Siyuan Wang"}], "temperature": 0.1}}
{"custom_id": "632_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Faithfully Explainable Recommendation via Neural Logic Reasoning"}], "temperature": 0.1}}
{"custom_id": "632_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Knowledge graphs (KG) have become increasingly important to endow modernrecommender systems with the ability to generate traceable reasoning paths toexplain the recommendation process. However, prior research rarely considersthe faithfulness of the derived explanations to justify the decision makingprocess. To the best of our knowledge, this is the first work that models andevaluates faithfully explainable recommendation under the framework of KGreasoning. Specifically, we propose neural logic reasoning for explainablerecommendation (LOGER) by drawing on interpretable logical rules to guide thepath reasoning process for explanation generation. We experiment on threelarge-scale datasets in the e-commerce domain, demonstrating the effectivenessof our method in delivering high-quality recommendations as well asascertaining the faithfulness of the derived explanation."}], "temperature": 0.1}}
{"custom_id": "632_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yaxin Zhu"}], "temperature": 0.1}}
{"custom_id": "633_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The General Theory of General Intelligence: A Pragmatic Patternist Perspective"}], "temperature": 0.1}}
{"custom_id": "633_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A multi-decade exploration into the theoretical foundations of artificial andnatural general intelligence, which has been expressed in a series of books andpapers and used to guide a series of practical and research-prototype softwaresystems, is reviewed at a moderate level of detail. The review coversunderlying philosophies (patternist philosophy of mind, foundationalphenomenological and logical ontology), formalizations of the concept ofintelligence, and a proposed high level architecture for AGI systems partlydriven by these formalizations and philosophies. The implementation of specificcognitive processes such as logical reasoning, program learning, clustering andattention allocation in the context and language of this high levelarchitecture is considered, as is the importance of a common (e.g. typedmetagraph based) knowledge representation for enabling \"cognitive synergy\"between the various processes. The specifics of human-like cognitivearchitecture are presented as manifestations of these general principles, andkey aspects of machine consciousness and machine ethics are also treated inthis context. Lessons for practical implementation of advanced AGI inframeworks such as OpenCog Hyperon are briefly considered."}], "temperature": 0.1}}
{"custom_id": "633_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ben Goertzel"}], "temperature": 0.1}}
{"custom_id": "634_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "DAGN: Discourse-Aware Graph Network for Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "634_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent QA with logical reasoning questions requires passage-level relationsamong the sentences. However, current approaches still focus on sentence-levelrelations interacting among tokens. In this work, we explore aggregatingpassage-level clues for solving logical reasoning QA by using discourse-basedinformation. We propose a discourse-aware graph network (DAGN) that reasonsrelying on the discourse structure of the texts. The model encodes discourseinformation as a graph with elementary discourse units (EDUs) and discourserelations, and learns the discourse-aware features via a graph network fordownstream QA tasks. Experiments are conducted on two logical reasoning QAdatasets, ReClor and LogiQA, and our proposed DAGN achieves competitiveresults. The source code is available at https://github.com/Eleanor-H/DAGN."}], "temperature": 0.1}}
{"custom_id": "634_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Yinya Huang"}], "temperature": 0.1}}
{"custom_id": "635_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and Execution"}], "temperature": 0.1}}
{"custom_id": "635_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Spatial-temporal reasoning is a challenging task in Artificial Intelligence(AI) due to its demanding but unique nature: a theoretic requirement onrepresenting and reasoning based on spatial-temporal knowledge in mind, and anapplied requirement on a high-level cognitive system capable of navigating andacting in space and time. Recent works have focused on an abstract reasoningtask of this kind -- Raven's Progressive Matrices (RPM). Despite theencouraging progress on RPM that achieves human-level performance in terms ofaccuracy, modern approaches have neither a treatment of human-like reasoning ongeneralization, nor a potential to generate answers. To fill in this gap, wepropose a neuro-symbolic Probabilistic Abduction and Execution (PrAE) learner;central to the PrAE learner is the process of probabilistic abduction andexecution on a probabilistic scene representation, akin to the mentalmanipulation of objects. Specifically, we disentangle perception and reasoningfrom a monolithic model. The neural visual perception frontend predictsobjects' attributes, later aggregated by a scene inference engine to produce aprobabilistic scene representation. In the symbolic logical reasoning backend,the PrAE learner uses the representation to abduce the hidden rules. An answeris predicted by executing the rules on the probabilistic representation. Theentire system is trained end-to-end in an analysis-by-synthesis manner withoutany visual attribute annotations. Extensive experiments demonstrate that thePrAE learner improves cross-configuration generalization and is capable ofrendering an answer, in contrast to prior works that merely make a categoricalchoice from candidates."}], "temperature": 0.1}}
{"custom_id": "635_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chi Zhang"}], "temperature": 0.1}}
{"custom_id": "636_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Context Transformer with Stacked Pointer Networks for Conversational Question Answering over Knowledge Graphs"}], "temperature": 0.1}}
{"custom_id": "636_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neural semantic parsing approaches have been widely used for QuestionAnswering (QA) systems over knowledge graphs. Such methods provide theflexibility to handle QA datasets with complex queries and a large number ofentities. In this work, we propose a novel framework named CARTON, whichperforms multi-task semantic parsing for handling the problem of conversationalquestion answering over a large-scale knowledge graph. Our framework consistsof a stack of pointer networks as an extension of a context transformer modelfor parsing the input question and the dialog history. The framework generatesa sequence of actions that can be executed on the knowledge graph. We evaluateCARTON on a standard dataset for complex sequential question answering on whichCARTON outperforms all baselines. Specifically, we observe performanceimprovements in F1-score on eight out of ten question types compared to theprevious state of the art. For logical reasoning questions, an improvement of11 absolute points is reached."}], "temperature": 0.1}}
{"custom_id": "636_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Joan Plepi"}], "temperature": 0.1}}
{"custom_id": "637_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Data augmentation by morphological mixup for solving Raven's Progressive Matrices"}], "temperature": 0.1}}
{"custom_id": "637_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Raven's Progressive Matrices (RPMs) are frequently used in testing human'svisual reasoning ability. Recent advances of RPM-like datasets and solutionmodels partially address the challenges of visually understanding the RPMquestions and logically reasoning the missing answers. In view of the poorgeneralization performance due to insufficient samples in RPM datasets, wepropose an effective scheme, namely Candidate Answer Morphological Mixup(CAM-Mix). CAM-Mix serves as a data augmentation strategy by gray-scale imagemorphological mixup, which regularizes various solution methods and overcomesthe model overfitting problem. By creating new negative candidate answerssemantically similar to the correct answers, a more accurate decision boundarycould be defined. By applying the proposed data augmentation method, asignificant and consistent performance improvement is achieved on variousRPM-like datasets compared with the state-of-the-art models."}], "temperature": 0.1}}
{"custom_id": "637_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Wentao He"}], "temperature": 0.1}}
{"custom_id": "638_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Extracting Optimal Explanations for Ensemble Trees via Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "638_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ensemble trees are a popular machine learning model which often yields highprediction performance when analysing structured data. Although individualsmall decision trees are deemed explainable by nature, an ensemble of largetrees is often difficult to understand. In this work, we propose an approachcalled optimised explanation (OptExplain) that faithfully extracts globalexplanations of ensemble trees using a combination of logical reasoning,sampling and optimisation. Building on top of this, we propose a method calledthe profile of equivalent classes (ProClass), which uses MAX-SAT to simplifythe explanation even further. Our experimental study on several datasets showsthat our approach can provide high-quality explanations to large ensemble treesmodels, and it betters recent top-performers."}], "temperature": 0.1}}
{"custom_id": "638_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Gelin Zhang"}], "temperature": 0.1}}
{"custom_id": "639_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The Cognition of Counterexample in Mathematics Students"}], "temperature": 0.1}}
{"custom_id": "639_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Studying Mathematics requires a synthesis of skills from a multitude ofacademic disciplines; logical reasoning being chief among them. This paperexplores mathematical logical preparedness of students entering first yearuniversity mathematics courses and also the effectiveness of using logicalfacility to predict successful course outcomes. We analyze data collected fromstudents enrolled at the University of Winnipeg in a pre-service course forhigh school teachers. We do find that, being able to successfully answerlogical questions, both before and after intervention, are significant inrelation to improved student outcomes."}], "temperature": 0.1}}
{"custom_id": "639_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Shannon Ezzat"}], "temperature": 0.1}}
{"custom_id": "640_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neural Sequence-to-grid Module for Learning Symbolic Rules"}], "temperature": 0.1}}
{"custom_id": "640_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical reasoning tasks over symbols, such as learning arithmetic operationsand computer program evaluations, have become challenges to deep learning. Inparticular, even state-of-the-art neural networks fail to achieve\\textit{out-of-distribution} (OOD) generalization of symbolic reasoning tasks,whereas humans can easily extend learned symbolic rules. To resolve thisdifficulty, we propose a neural sequence-to-grid (seq2grid) module, an inputpreprocessor that automatically segments and aligns an input sequence into agrid. As our module outputs a grid via a novel differentiable mapping, anyneural network structure taking a grid input, such as ResNet or TextCNN, can bejointly trained with our module in an end-to-end fashion. Extensive experimentsshow that neural networks having our module as an input preprocessor achieveOOD generalization on various arithmetic and algorithmic problems includingnumber sequence prediction problems, algebraic word problems, and computerprogram evaluation problems while other state-of-the-art sequence transductionmodels cannot. Moreover, we verify that our module enhances TextCNN to solvethe bAbI QA tasks without external memory."}], "temperature": 0.1}}
{"custom_id": "640_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Segwang Kim"}], "temperature": 0.1}}
{"custom_id": "641_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Programming and Reasoning with Partial Observability"}], "temperature": 0.1}}
{"custom_id": "641_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Computer programs are increasingly being deployed in partially-observableenvironments. A partially observable environment is an environment whose stateis not completely visible to the program, but from which the program receivespartial observations. Developers typically deal with partial observability bywriting a state estimator that, given observations, attempts to deduce thehidden state of the environment. In safety-critical domains, to formally verifysafety properties developers may write an environment model. The model capturesthe relationship between observations and hidden states and is used to provethe software correct.  In this paper, we present a new methodology for writing and verifyingprograms in partially observable environments. We present belief programming, aprogramming methodology where developers write an environment model that theprogram runtime automatically uses to perform state estimation. A beliefprogram dynamically updates and queries a belief state that captures thepossible states the environment could be in. To enable verification, we presentEpistemic Hoare Logic that reasons about the possible belief states of a beliefprogram the same way that classical Hoare logic reasons about the possiblestates of a program. We develop these concepts by defining a semantics and aprogram logic for a simple core language called BLIMP. In a case study, we showhow belief programming could be used to write and verify a controller for theMars Polar Lander in BLIMP. We present an implementation of BLIMP called CBLIMPand evaluate it to determine the feasibility of belief programming."}], "temperature": 0.1}}
{"custom_id": "641_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Eric Atkinson"}], "temperature": 0.1}}
{"custom_id": "642_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Closer Look at the Robustness of Vision-and-Language Pre-trained Models"}], "temperature": 0.1}}
{"custom_id": "642_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Large-scale pre-trained multimodal transformers, such as ViLBERT and UNITER,have propelled the state of the art in vision-and-language (V+L) research to anew level. Although achieving impressive performance on standard tasks, todate, it still remains unclear how robust these pre-trained models are. Toinvestigate, we conduct a host of thorough evaluations on existing pre-trainedmodels over 4 different types of V+L specific model robustness: (i) LinguisticVariation; (ii) Logical Reasoning; (iii) Visual Content Manipulation; and (iv)Answer Distribution Shift. Interestingly, by standard model finetuning,pre-trained V+L models already exhibit better robustness than manytask-specific state-of-the-art methods. To further enhance model robustness, wepropose Mango, a generic and efficient approach that learns a MultimodalAdversarial Noise GeneratOr in the embedding space to fool pre-trained V+Lmodels. Differing from previous studies focused on one specific type ofrobustness, Mango is task-agnostic, and enables universal performance lift forpre-trained models over diverse tasks designed to evaluate broad aspects ofrobustness. Comprehensive experiments demonstrate that Mango achieves new stateof the art on 7 out of 9 robustness benchmarks, surpassing existing methods bya significant margin. As the first comprehensive study on V+L robustness, thiswork puts robustness of pre-trained models into sharper focus, pointing newdirections for future study."}], "temperature": 0.1}}
{"custom_id": "642_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Linjie Li"}], "temperature": 0.1}}
{"custom_id": "643_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Bayes Meets Entailment and Prediction: Commonsense Reasoning with Non-monotonicity, Paraconsistency and Predictive Accuracy"}], "temperature": 0.1}}
{"custom_id": "643_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The recent success of Bayesian methods in neuroscience and artificialintelligence gives rise to the hypothesis that the brain is a Bayesian machine.Since logic and learning are both practices of the human brain, it leads toanother hypothesis that there is a Bayesian interpretation underlying bothlogical reasoning and machine learning. In this paper, we introduce agenerative model of logical consequence relations. It formalises the process ofhow the truth value of a sentence is probabilistically generated from theprobability distribution over states of the world. We show that the generativemodel characterises a classical consequence relation, paraconsistentconsequence relation and nonmonotonic consequence relation. In particular, thegenerative model gives a new consequence relation that outperforms them inreasoning with inconsistent knowledge. We also show that the generative modelgives a new classification algorithm that outperforms several representativealgorithms in predictive accuracy and complexity on the Kaggle Titanic dataset."}], "temperature": 0.1}}
{"custom_id": "643_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hiroyuki Kido"}], "temperature": 0.1}}
{"custom_id": "644_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neurosymbolic AI: The 3rd Wave"}], "temperature": 0.1}}
{"custom_id": "644_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Current advances in Artificial Intelligence (AI) and Machine Learning (ML)have achieved unprecedented impact across research communities and industry.Nevertheless, concerns about trust, safety, interpretability and accountabilityof AI were raised by influential thinkers. Many have identified the need forwell-founded knowledge representation and reasoning to be integrated with deeplearning and for sound explainability. Neural-symbolic computing has been anactive area of research for many years seeking to bring together robustlearning in neural networks with reasoning and explainability via symbolicrepresentations for network models. In this paper, we relate recent and earlyresearch results in neurosymbolic AI with the objective of identifying the keyingredients of the next wave of AI systems. We focus on research thatintegrates in a principled way neural network-based learning with symbolicknowledge representation and logical reasoning. The insights provided by 20years of neural-symbolic computing are shown to shed new light onto theincreasingly prominent role of trust, safety, interpretability andaccountability of AI. We also identify promising directions and challenges forthe next decade of AI research from the perspective of neural-symbolic systems."}], "temperature": 0.1}}
{"custom_id": "644_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Artur d'Avila Garcez"}], "temperature": 0.1}}
{"custom_id": "645_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Braid: Weaving Symbolic and Neural Knowledge into Coherent Logical Explanations"}], "temperature": 0.1}}
{"custom_id": "645_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Traditional symbolic reasoning engines, while attractive for their precisionand explicability, have a few major drawbacks: the use of brittle inferenceprocedures that rely on exact matching (unification) of logical terms, aninability to deal with uncertainty, and the need for a precompiled rule-base ofknowledge (the \"knowledge acquisition\" problem). To address these issues, wedevise a novel logical reasoner called Braid, that supports probabilisticrules, and uses the notion of custom unification functions and dynamic rulegeneration to overcome the brittle matching and knowledge-gap problem prevalentin traditional reasoners. In this paper, we describe the reasoning algorithmsused in Braid, and their implementation in a distributed task-based frameworkthat builds proof/explanation graphs for an input query. We use a simple QAexample from a children's story to motivate Braid's design and explain how thevarious components work together to produce a coherent logical explanation.Finally, we evaluate Braid on the ROC Story Cloze test and achieve close tostate-of-the-art results while providing frame-based explanations."}], "temperature": 0.1}}
{"custom_id": "645_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Aditya Kalyanpur"}], "temperature": 0.1}}
{"custom_id": "646_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neural Software Analysis"}], "temperature": 0.1}}
{"custom_id": "646_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Many software development problems can be addressed by program analysistools, which traditionally are based on precise, logical reasoning andheuristics to ensure that the tools are practical. Recent work has showntremendous success through an alternative way of creating developer tools,which we call neural software analysis. The key idea is to train a neuralmachine learning model on numerous code examples, which, once trained, makespredictions about previously unseen code. In contrast to traditional programanalysis, neural software analysis naturally handles fuzzy information, such ascoding conventions and natural language embedded in code, without relying onmanually encoded heuristics. This article gives an overview of neural softwareanalysis, discusses when to (not) use it, and presents three example analyses.The analyses address challenging software development problems: bug detection,type prediction, and code completion. The resulting tools complement andoutperform traditional program analyses, and are used in industrial practice."}], "temperature": 0.1}}
{"custom_id": "646_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Michael Pradel"}], "temperature": 0.1}}
{"custom_id": "647_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Natural Language Inference in Context -- Investigating Contextual Reasoning over Long Texts"}], "temperature": 0.1}}
{"custom_id": "647_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Natural language inference (NLI) is a fundamental NLP task, investigating theentailment relationship between two texts. Popular NLI datasets present thetask at sentence-level. While adequate for testing semantic representations,they fall short for testing contextual reasoning over long texts, which is anatural part of the human inference process. We introduce ConTRoL, a newdataset for ConTextual Reasoning over Long texts. Consisting of 8,325expert-designed \"context-hypothesis\" pairs with gold labels, ConTRoL is apassage-level NLI dataset with a focus on complex contextual reasoning typessuch as logical reasoning. It is derived from competitive selection andrecruitment test (verbal reasoning test) for police recruitment, with expertlevel quality. Compared with previous NLI benchmarks, the materials in ConTRoLare much more challenging, involving a range of reasoning types. Empiricalresults show that state-of-the-art language models perform by far worse thaneducated humans. Our dataset can also serve as a testing-set for downstreamtasks like Checking Factual Correctness of Summaries."}], "temperature": 0.1}}
{"custom_id": "647_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hanmeng Liu"}], "temperature": 0.1}}
{"custom_id": "648_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The P-T Probability Framework for Semantic Communication, Falsification, Confirmation, and Bayesian Reasoning"}], "temperature": 0.1}}
{"custom_id": "648_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Many researchers want to unify probability and logic by defining logicalprobability or probabilistic logic reasonably. This paper tries to unifystatistics and logic so that we can use both statistical probability andlogical probability at the same time. For this purpose, this paper proposes theP-T probability framework, which is assembled with Shannon's statisticalprobability framework for communication, Kolmogorov's probability axioms forlogical probability, and Zadeh's membership functions used as truth functions.Two kinds of probabilities are connected by an extended Bayes' theorem, withwhich we can convert a likelihood function and a truth function from one toanother. Hence, we can train truth functions (in logic) by samplingdistributions (in statistics). This probability framework was developed in theauthor's long-term studies on semantic information, statistical learning, andcolor vision. This paper first proposes the P-T probability framework andexplains different probabilities in it by its applications to semanticinformation theory. Then, this framework and the semantic information methodsare applied to statistical learning, statistical mechanics, hypothesisevaluation (including falsification), confirmation, and Bayesian reasoning.Theoretical applications illustrate the reasonability and practicability ofthis framework. This framework is helpful for interpretable AI. To interpretneural networks, we need further study."}], "temperature": 0.1}}
{"custom_id": "648_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Chenguang Lu"}], "temperature": 0.1}}
{"custom_id": "649_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Beta Embeddings for Multi-Hop Logical Reasoning in Knowledge Graphs"}], "temperature": 0.1}}
{"custom_id": "649_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "One of the fundamental problems in Artificial Intelligence is to performcomplex multi-hop logical reasoning over the facts captured by a knowledgegraph (KG). This problem is challenging, because KGs can be massive andincomplete. Recent approaches embed KG entities in a low dimensional space andthen use these embeddings to find the answer entities. However, it has been anoutstanding challenge of how to handle arbitrary first-order logic (FOL)queries as present methods are limited to only a subset of FOL operators. Inparticular, the negation operator is not supported. An additional limitation ofpresent methods is also that they cannot naturally model uncertainty. Here, wepresent BetaE, a probabilistic embedding framework for answering arbitrary FOLqueries over KGs. BetaE is the first method that can handle a complete set offirst-order logical operations: conjunction ($\\wedge$), disjunction ($\\vee$),and negation ($\\neg$). A key insight of BetaE is to use probabilisticdistributions with bounded support, specifically the Beta distribution, andembed queries/entities as distributions, which as a consequence allows us toalso faithfully model uncertainty. Logical operations are performed in theembedding space by neural operators over the probabilistic embeddings. Wedemonstrate the performance of BetaE on answering arbitrary FOL queries onthree large, incomplete KGs. While being more general, BetaE also increasesrelative performance by up to 25.4% over the current state-of-the-art KGreasoning methods that can only handle conjunctive queries without negation."}], "temperature": 0.1}}
{"custom_id": "649_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hongyu Ren"}], "temperature": 0.1}}
{"custom_id": "650_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Axiom Learning and Belief Tracing for Transparent Decision Making in Robotics"}], "temperature": 0.1}}
{"custom_id": "650_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A robot's ability to provide descriptions of its decisions and beliefspromotes effective collaboration with humans. Providing such transparency isparticularly challenging in integrated robot systems that includeknowledge-based reasoning methods and data-driven learning algorithms. Towardsaddressing this challenge, our architecture couples the complementary strengthsof non-monotonic logical reasoning, deep learning, and decision-tree induction.During reasoning and learning, the architecture enables a robot to provideon-demand relational descriptions of its decisions, beliefs, and the outcomesof hypothetical actions. These capabilities are grounded and evaluated in thecontext of scene understanding tasks and planning tasks performed usingsimulated images and images from a physical robot manipulating tabletopobjects."}], "temperature": 0.1}}
{"custom_id": "650_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Tiago Mota"}], "temperature": 0.1}}
{"custom_id": "651_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Measuring Systematic Generalization in Neural Proof Generation with Transformers"}], "temperature": 0.1}}
{"custom_id": "651_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We are interested in understanding how well Transformer language models(TLMs) can perform reasoning tasks when trained on knowledge encoded in theform of natural language. We investigate their systematic generalizationabilities on a logical reasoning task in natural language, which involvesreasoning over relationships between entities grounded in first-order logicalproofs. Specifically, we perform soft theorem-proving by leveraging TLMs togenerate natural language proofs. We test the generated proofs for logicalconsistency, along with the accuracy of the final inference. We observelength-generalization issues when evaluated on longer-than-trained sequences.However, we observe TLMs improve their generalization performance after beingexposed to longer, exhaustive proofs. In addition, we discover that TLMs areable to generalize better using backward-chaining proofs compared to theirforward-chaining counterparts, while they find it easier to generate forwardchaining proofs. We observe that models that are not trained to generate proofsare better at generalizing to problems based on longer proofs. This suggeststhat Transformers have efficient internal reasoning strategies that are harderto interpret. These results highlight the systematic generalization behavior ofTLMs in the context of logical reasoning, and we believe this work motivatesdeeper inspection of their underlying reasoning strategies."}], "temperature": 0.1}}
{"custom_id": "651_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Nicolas Gontier"}], "temperature": 0.1}}
{"custom_id": "652_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Deciding SHACL Shape Containment through Description Logics Reasoning (Extended Version)"}], "temperature": 0.1}}
{"custom_id": "652_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The Shapes Constraint Language (SHACL) allows for formalizing constraintsover RDF data graphs. A shape groups a set of constraints that may be fulfilledby nodes in the RDF graph. We investigate the problem of containment betweenSHACL shapes. One shape is contained in a second shape if every graph nodemeeting the constraints of the first shape also meets the constraints of thesecond. To decide shape containment, we map SHACL shape graphs into descriptionlogic axioms such that shape containment can be answered by description logicreasoning. We identify several, increasingly tight syntactic restrictions ofSHACL for which this approach becomes sound and complete."}], "temperature": 0.1}}
{"custom_id": "652_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Martin Leinberger"}], "temperature": 0.1}}
{"custom_id": "653_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neural Logic Reasoning"}], "temperature": 0.1}}
{"custom_id": "653_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent years have witnessed the success of deep neural networks in manyresearch areas. The fundamental idea behind the design of most neural networksis to learn similarity patterns from data for prediction and inference, whichlacks the ability of cognitive reasoning. However, the concrete ability ofreasoning is critical to many theoretical and practical problems. On the otherhand, traditional symbolic reasoning methods do well in making logicalinference, but they are mostly hard rule-based reasoning, which limits theirgeneralization ability to different tasks since difference tasks may requiredifferent rules. Both reasoning and generalization ability are important forprediction tasks such as recommender systems, where reasoning provides strongconnection between user history and target items for accurate prediction, andgeneralization helps the model to draw a robust user portrait over noisyinputs.  In this paper, we propose Logic-Integrated Neural Network (LINN) to integratethe power of deep learning and logic reasoning. LINN is a dynamic neuralarchitecture that builds the computational graph according to input logicalexpressions. It learns basic logical operations such as AND, OR, NOT as neuralmodules, and conducts propositional logical reasoning through the network forinference. Experiments on theoretical task show that LINN achieves significantperformance on solving logical equations and variables. Furthermore, we testour approach on the practical task of recommendation by formulating the taskinto a logical inference problem. Experiments show that LINN significantlyoutperforms state-of-the-art recommendation models in Top-K recommendation,which verifies the potential of LINN in practice."}], "temperature": 0.1}}
{"custom_id": "653_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Shaoyun Shi"}], "temperature": 0.1}}
{"custom_id": "654_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Semantic based model of Conceptual Work Products for formal verification of complex interactive systems"}], "temperature": 0.1}}
{"custom_id": "654_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Many clinical workflows depend on interactive computer systems for highlytechnical, conceptual work products, such as diagnoses, treatment plans, carecoordination, and case management. We describe an automatic logic reasoner toverify objective specifications for these highly technical, but abstract, workproducts that are essential to care. The conceptual work productsspecifications serve as a fundamental output requirement, which must be clearlystated, correct and solvable. There is strategic importance for suchspecifications because, in turn, they enable system model checking to verifythat machine functions taken with user procedures are actually able to achievethese abstract products. We chose case management of Multiple Sclerosis (MS)outpatients as our use case for its challenging complexity. As a first step, weillustrate how graphical class and state diagrams from UML can be developed andcritiqued with subject matter experts to serve as specifications of theconceptual work product of case management. A key feature is that thespecification must be declarative and thus independent of any process ortechnology. Our Work Domain Ontology with tools from Semantic Web is needed totranslate UML class and state diagrams for verification of solvability withautomatic reasoning. The solvable model will then be ready for subsequent usewith model checking on the system of human procedures and machine functions. Weused the expressive rule language SPARQL Inferencing Notation (SPIN) to developformal representations of the UML class diagram, the state machine, and theirinteractions. Using SPIN, we proved the consistency of the interactions ofstatic and dynamic concepts. We discussed how the new SPIN rule engine could beincorporated in the Object Management Group (OMG) Ontology Definition Metamodel(ODM)"}], "temperature": 0.1}}
{"custom_id": "654_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mohcine Madkour"}], "temperature": 0.1}}
{"custom_id": "655_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Few-shot Visual Reasoning with Meta-analogical Contrastive Learning"}], "temperature": 0.1}}
{"custom_id": "655_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "While humans can solve a visual puzzle that requires logical reasoning byobserving only few samples, it would require training over large amount of datafor state-of-the-art deep reasoning models to obtain similar performance on thesame task. In this work, we propose to solve such a few-shot (or low-shot)visual reasoning problem, by resorting to analogical reasoning, which is aunique human ability to identify structural or relational similarity betweentwo sets. Specifically, given training and test sets that contain the same typeof visual reasoning problems, we extract the structural relationships betweenelements in both domains, and enforce them to be as similar as possible withanalogical learning. We repeatedly apply this process with slightly modifiedqueries of the same problem under the assumption that it does not affect therelationship between a training and a test sample. This allows to learn therelational similarity between the two samples in an effective manner even witha single pair of samples. We validate our method on RAVEN dataset, on which itoutperforms state-of-the-art method, with larger gains when the training datais scarce. We further meta-learn our analogical contrastive learning model overthe same tasks with diverse attributes, and show that it generalizes to thesame visual reasoning problem with unseen attributes."}], "temperature": 0.1}}
{"custom_id": "655_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Youngsung Kim"}], "temperature": 0.1}}
{"custom_id": "656_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mapping computational thinking mindsets between educational levels with cognitive network science"}], "temperature": 0.1}}
{"custom_id": "656_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Computational thinking is a way of reasoning about the world in terms ofdata. This mindset channels number crunching toward an ambition to discoverknowledge through logic, models and simulations. Here we show how computationalcognitive science can be used to reconstruct and analyse the structure ofcomputational thinking mindsets (forma mentis in Latin) through complexnetworks. As a case study, we investigate cognitive networks tied to keyconcepts of computational thinking provided by: (i) 159 high school studentsenrolled in a science curriculum and (ii) 59 researchers in complex systems andsimulations. Researchers' reconstructed forma mentis highlighted a positivemindset about scientific modelling, semantically framing data and simulationsas ways of discovering nature. Students correctly identified different aspectsof logic reasoning but perceived \"computation\" as a distressing,anxiety-eliciting task, framed with math jargon and lacking links to real-worlddiscovery. Students' mindsets around \"data\", \"model\" and \"simulations\"critically revealed no awareness of numerical modelling as a way forunderstanding the world. Our findings provide evidence of a crippledcomputational thinking mindset in students, who acquire mathematical skillsthat are not channelled toward real-world discovery through coding. Thisunlinked knowledge ends up being perceived as distressing number-crunchingexpertise with no relevant outcome. The virtuous mindset of researchersreported here indicates that computational thinking can be restored by trainingstudents specifically in coding, modelling and simulations in relation todiscovering nature. Our approach opens innovative ways for quantifyingcomputational thinking and enhancing its development through mindsetreconstruction."}], "temperature": 0.1}}
{"custom_id": "656_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Massimo Stella"}], "temperature": 0.1}}
{"custom_id": "657_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning"}], "temperature": 0.1}}
{"custom_id": "657_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Machine reading is a fundamental task for testing the capability of naturallanguage understanding, which is closely related to human cognition in manyaspects. With the rising of deep learning techniques, algorithmic models rivalhuman performances on simple QA, and thus increasingly challenging machinereading datasets have been proposed. Though various challenges such as evidenceintegration and commonsense knowledge have been integrated, one of thefundamental capabilities in human reading, namely logical reasoning, is notfully investigated. We build a comprehensive dataset, named LogiQA, which issourced from expert-written questions for testing human Logical reasoning. Itconsists of 8,678 QA instances, covering multiple types of deductive reasoning.Results show that state-of-the-art neural models perform by far worse thanhuman ceiling. Our dataset can also serve as a benchmark for reinvestigatinglogical AI under the deep learning NLP setting. The dataset is freely availableat https://github.com/lgw863/LogiQA-dataset"}], "temperature": 0.1}}
{"custom_id": "657_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jian Liu"}], "temperature": 0.1}}
{"custom_id": "658_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Learning Syllogism with Euler Neural-Networks"}], "temperature": 0.1}}
{"custom_id": "658_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Traditional neural networks represent everything as a vector, and are able toapproximate a subset of logical reasoning to a certain degree. As basic logicrelations are better represented by topological relations between regions, wepropose a novel neural network that represents everything as a ball and is ableto learn topological configuration as an Euler diagram. So comes the name EulerNeural-Network (ENN). The central vector of a ball is a vector that can inheritrepresentation power of traditional neural network. ENN distinguishes fourspatial statuses between balls, namely, being disconnected, being partiallyoverlapped, being part of, being inverse part of. Within each status, idealvalues are defined for efficient reasoning. A novel back-propagation algorithmwith six Rectified Spatial Units (ReSU) can optimize an Euler diagramrepresenting logical premises, from which logical conclusion can be deduced. Incontrast to traditional neural network, ENN can precisely represent all 24different structures of Syllogism. Two large datasets are created: oneextracted from WordNet-3.0 covers all types of Syllogism reasoning, the otherextracted all family relations from DBpedia. Experiment results approve thesuperior power of ENN in logical representation and reasoning. Datasets andsource code are available upon request."}], "temperature": 0.1}}
{"custom_id": "658_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Tiansi Dong"}], "temperature": 0.1}}
{"custom_id": "659_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Computer-Aided Personalized Education"}], "temperature": 0.1}}
{"custom_id": "659_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The shortage of people trained in STEM fields is becoming acute, anduniversities and colleges are straining to satisfy this demand. In the case ofcomputer science, for instance, the number of US students taking introductorycourses has grown three-fold in the past decade. Recently, massive open onlinecourses (MOOCs) have been promoted as a way to ease this strain. This at bestprovides access to education. The bigger challenge though is coping withheterogeneous backgrounds of different students, retention, providing feedback,and assessment. Personalized education relying on computational tools canaddress this challenge.  While automated tutoring has been studied at different times in differentcommunities, recent advances in computing and education technology offerexciting opportunities to transform the manner in which students learn. Inparticular, at least three trends are significant. First, progress in logicalreasoning, data analytics, and natural language processing has led to tutoringtools for automatic assessment, personalized instruction including targetedfeedback, and adaptive content generation for a variety of subjects. Second,research in the science of learning and human-computer interaction is leadingto a better understanding of how different students learn, when and what typesof interventions are effective for different instructional goals, and how tomeasure the success of educational tools. Finally, the recent emergence ofonline education platforms, both in academia and industry, is leading to newopportunities for the development of a shared infrastructure. This CCC workshopbrought together researchers developing educational tools based on technologiessuch as logical reasoning and machine learning with researchers in education,human-computer interaction, and cognitive psychology."}], "temperature": 0.1}}
{"custom_id": "659_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Rajeev Alur"}], "temperature": 0.1}}
{"custom_id": "660_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Medical idioms for clinical Bayesian network development"}], "temperature": 0.1}}
{"custom_id": "660_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Bayesian Networks (BNs) are graphical probabilistic models that have provenpopular in medical applications. While numerous medical BNs have beenpublished, most are presented fait accompli without explanation of how thenetwork structure was developed or justification of why it represents thecorrect structure for the given medical application. This means that theprocess of building medical BNs from experts is typically ad hoc and offerslittle opportunity for methodological improvement. This paper proposesgenerally applicable and reusable medical reasoning patterns to aid thosedeveloping medical BNs. The proposed method complements and extends theidiom-based approach introduced by Neil, Fenton, and Nielsen in 2000. Wepropose instances of their generic idioms that are specific to medical BNs. Werefer to the proposed medical reasoning patterns as medical idioms. Inaddition, we extend the use of idioms to represent interventional andcounterfactual reasoning. We believe that the proposed medical idioms arelogical reasoning patterns that can be combined, reused and applied genericallyto help develop medical BNs. All proposed medical idioms have been illustratedusing medical examples on coronary artery disease. The method has also beenapplied to other ongoing BNs being developed with medical experts. Finally, weshow that applying the proposed medical idioms to published BN models resultsin models with a clearer structure."}], "temperature": 0.1}}
{"custom_id": "660_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Evangelia Kyrimi"}], "temperature": 0.1}}
{"custom_id": "661_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Matrix Shuffle-Exchange Networks for Hard 2D Tasks"}], "temperature": 0.1}}
{"custom_id": "661_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Convolutional neural networks have become the main tools for processingtwo-dimensional data. They work well for images, yet convolutions have alimited receptive field that prevents its applications to more complex 2Dtasks. We propose a new neural model, called Matrix Shuffle-Exchange network,that can efficiently exploit long-range dependencies in 2D data and hascomparable speed to a convolutional neural network. It is derived from NeuralShuffle-Exchange network and has $\\mathcal{O}( \\log{n})$ layers and$\\mathcal{O}( n^2 \\log{n})$ total time and space complexity for processing a $n\\times n$ data matrix. We show that the Matrix Shuffle-Exchange network iswell-suited for algorithmic and logical reasoning tasks on matrices and densegraphs, exceeding convolutional and graph neural network baselines. Itsdistinct advantage is the capability of retaining full long-range dependencymodelling when generalizing to larger instances - much larger than could beprocessed with models equipped with a dense attention mechanism."}], "temperature": 0.1}}
{"custom_id": "661_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Em墨ls Ozoli艈拧"}], "temperature": 0.1}}
{"custom_id": "662_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Logical Neural Networks"}], "temperature": 0.1}}
{"custom_id": "662_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We propose a novel framework seamlessly providing key properties of bothneural nets (learning) and symbolic logic (knowledge and reasoning). Everyneuron has a meaning as a component of a formula in a weighted real-valuedlogic, yielding a highly intepretable disentangled representation. Inference isomnidirectional rather than focused on predefined target variables, andcorresponds to logical reasoning, including classical first-order logic theoremproving as a special case. The model is end-to-end differentiable, and learningminimizes a novel loss function capturing logical contradiction, yieldingresilience to inconsistent knowledge. It also enables the open-world assumptionby maintaining bounds on truth values which can have probabilistic semantics,yielding resilience to incomplete knowledge."}], "temperature": 0.1}}
{"custom_id": "662_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Ryan Riegel"}], "temperature": 0.1}}
{"custom_id": "663_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "ReCO: A Large Scale Chinese Reading Comprehension Dataset on Opinion"}], "temperature": 0.1}}
{"custom_id": "663_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This paper presents the ReCO, a human-curated ChineseReading Comprehensiondataset on Opinion. The questions in ReCO are opinion based queries issued tothe commercial search engine. The passages are provided by the crowdworkers whoextract the support snippet from the retrieved documents. Finally, anabstractive yes/no/uncertain answer was given by the crowdworkers. The releaseof ReCO consists of 300k questions that to our knowledge is the largest inChinese reading comprehension. A prominent characteristic of ReCO is that inaddition to the original context paragraph, we also provided the supportevidence that could be directly used to answer the question. Quality analysisdemonstrates the challenge of ReCO that requires various types of reasoningskills, such as causal inference, logical reasoning, etc. Current QA modelsthat perform very well on many question answering problems, such as BERT, onlyachieve 77% accuracy on this dataset, a large margin behind humans nearly 92%performance, indicating ReCO presents a good challenge for machine readingcomprehension. The codes, datasets are freely available athttps://github.com/benywon/ReCO."}], "temperature": 0.1}}
{"custom_id": "663_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "BingningWang"}], "temperature": 0.1}}
{"custom_id": "664_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning"}], "temperature": 0.1}}
{"custom_id": "664_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Much progress has been made in semi-supervised learning (SSL) by combiningmethods that exploit different aspects of the data distribution, e.g.consistency regularisation relies on properties of $p(x)$, whereas entropyminimisation pertains to the label distribution $p(y|x)$. Focusing on thelatter, we present a probabilistic model for discriminative SSL, that mirrorsits classical generative counterpart. Under the assumption $y|x$ isdeterministic, the prior over latent variables becomes discrete. We show thatseveral well-known SSL methods can be interpreted as approximating this prior,and can be improved upon. We extend the discriminative model to neuro-symbolicSSL, where label features satisfy logical rules, by showing such rules relatedirectly to the above prior, thus justifying a family of methods that linkstatistical learning and logical reasoning, and unifying them with regular SSL."}], "temperature": 0.1}}
{"custom_id": "664_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Carl Allen"}], "temperature": 0.1}}
{"custom_id": "665_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Mathematical Reasoning via Self-supervised Skip-tree Training"}], "temperature": 0.1}}
{"custom_id": "665_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We examine whether self-supervised language modeling applied to mathematicalformulas enables logical reasoning. We suggest several logical reasoning tasksthat can be used to evaluate language models trained on formal mathematicalstatements, such as type inference, suggesting missing assumptions andcompleting equalities. To train language models for formal mathematics, wepropose a novel skip-tree task. We find that models trained on the skip-treetask show surprisingly strong mathematical reasoning abilities, and outperformmodels trained on standard skip-sequence tasks. We also analyze the models'ability to formulate new conjectures by measuring how often the predictions areprovable and useful in other proofs."}], "temperature": 0.1}}
{"custom_id": "665_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Markus N. Rabe"}], "temperature": 0.1}}
{"custom_id": "666_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Global Robustness Verification Networks"}], "temperature": 0.1}}
{"custom_id": "666_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "The wide deployment of deep neural networks, though achieving great successin many domains, has severe safety and reliability concerns. Existingadversarial attack generation and automatic verification techniques cannotformally verify whether a network is globally robust, i.e., the absence or notof adversarial examples in the input space. To address this problem, we developa global robustness verification framework with three components: 1) a novelrule-based ``back-propagation'' finding which input region is responsible forthe class assignment by logic reasoning; 2) a new network architecture SlidingDoor Network (SDN) enabling feasible rule-based ``back-propagation''; 3) aregion-based global robustness verification (RGRV) approach. Moreover, wedemonstrate the effectiveness of our approach on both synthetic and realdatasets."}], "temperature": 0.1}}
{"custom_id": "666_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Weidi Sun"}], "temperature": 0.1}}
{"custom_id": "667_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Scaling Exact Inference for Discrete Probabilistic Programs"}], "temperature": 0.1}}
{"custom_id": "667_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Probabilistic programming languages (PPLs) are an expressive means ofrepresenting and reasoning about probabilistic models. The computationalchallenge of probabilistic inference remains the primary roadblock for applyingPPLs in practice. Inference is fundamentally hard, so there is no one-size-fitsall solution. In this work, we target scalable inference for an important classof probabilistic programs: those whose probability distributions are discrete.Discrete distributions are common in many fields, including text analysis,network verification, artificial intelligence, and graph analysis, but theyprove to be challenging for existing PPLs.  We develop a domain-specific probabilistic programming language called Dicethat features a new approach to exact discrete probabilistic program inference.Dice exploits program structure in order to factorize inference, enabling us toperform exact inference on probabilistic programs with hundreds of thousands ofrandom variables. Our key technical contribution is a new reduction fromdiscrete probabilistic programs to weighted model counting (WMC). Thisreduction separates the structure of the distribution from its parameters,enabling logical reasoning tools to exploit that structure for probabilisticinference. We (1) show how to compositionally reduce Dice inference to WMC, (2)prove this compilation correct with respect to a denotational semantics, (3)empirically demonstrate the performance benefits over prior approaches, and (4)analyze the types of structure that allow Dice to scale to large probabilisticprograms."}], "temperature": 0.1}}
{"custom_id": "667_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Steven Holtzen"}], "temperature": 0.1}}
{"custom_id": "668_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neural Collaborative Reasoning"}], "temperature": 0.1}}
{"custom_id": "668_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Existing Collaborative Filtering (CF) methods are mostly designed based onthe idea of matching, i.e., by learning user and item embeddings from datausing shallow or deep models, they try to capture the associative relevancepatterns in data, so that a user embedding can be matched with relevant itemembeddings using designed or learned similarity functions. However, as acognition rather than a perception intelligent task, recommendation requiresnot only the ability of pattern recognition and matching from data, but alsothe ability of cognitive reasoning in data. In this paper, we propose toadvance Collaborative Filtering (CF) to Collaborative Reasoning (CR), whichmeans that each user knows part of the reasoning space, and they collaboratefor reasoning in the space to estimate preferences for each other. Technically,we propose a Neural Collaborative Reasoning (NCR) framework to bridge learningand reasoning. Specifically, we integrate the power of representation learningand logical reasoning, where representations capture similarity patterns indata from perceptual perspectives, and logic facilitates cognitive reasoningfor informed decision making. An important challenge, however, is to bridgedifferentiable neural networks and symbolic reasoning in a shared architecturefor optimization and inference. To solve the problem, we propose a modularizedreasoning architecture, which learns logical operations such as AND ($\\wedge$),OR ($\\vee$) and NOT ($\\neg$) as neural modules for implication reasoning($\\rightarrow$). In this way, logical expressions can be equivalently organizedas neural networks, so that logical reasoning and prediction can be conductedin a continuous space. Experiments on real-world datasets verified theadvantages of our framework compared with both shallow, deep and reasoningmodels."}], "temperature": 0.1}}
{"custom_id": "668_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hanxiong Chen"}], "temperature": 0.1}}
{"custom_id": "669_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Bayesian Entailment Hypothesis: How Brains Implement Monotonic and Non-monotonic Reasoning"}], "temperature": 0.1}}
{"custom_id": "669_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Recent success of Bayesian methods in neuroscience and artificialintelligence gives rise to the hypothesis that the brain is a Bayesian machine.Since logic, as the laws of thought, is a product and practice of the humanbrain, it leads to another hypothesis that there is a Bayesian algorithm anddata-structure for logical reasoning. In this paper, we give a Bayesian accountof entailment and characterize its abstract inferential properties. TheBayesian entailment is shown to be a monotonic consequence relation in anextreme case. In general, it is a sort of non-monotonic consequence relationwithout Cautious monotony or Cut. The preferential entailment, which is arepresentative non-monotonic consequence relation, is shown to be maximum aposteriori entailment, which is an approximation of the Bayesian entailment. Wefinally discuss merits of our proposals in terms of encoding preferences ondefaults, handling change and contradiction, and modeling human entailment."}], "temperature": 0.1}}
{"custom_id": "669_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Hiroyuki Kido"}], "temperature": 0.1}}
{"custom_id": "670_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Unifying Neural Learning and Symbolic Reasoning for Spinal Medical Report Generation"}], "temperature": 0.1}}
{"custom_id": "670_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Automated medical report generation in spine radiology, i.e., given spinalmedical images and directly create radiologist-level diagnosis reports tosupport clinical decision making, is a novel yet fundamental study in thedomain of artificial intelligence in healthcare. However, it is incrediblychallenging because it is an extremely complicated task that involves visualperception and high-level reasoning processes. In this paper, we propose theneural-symbolic learning (NSL) framework that performs human-like learning byunifying deep neural learning and symbolic logical reasoning for the spinalmedical report generation. Generally speaking, the NSL framework firstlyemploys deep neural learning to imitate human visual perception for detectingabnormalities of target spinal structures. Concretely, we design an adversarialgraph network that interpolates a symbolic graph reasoning module into agenerative adversarial network through embedding prior domain knowledge,achieving semantic segmentation of spinal structures with high complexity andvariability. NSL secondly conducts human-like symbolic logical reasoning thatrealizes unsupervised causal effect analysis of detected entities ofabnormalities through meta-interpretive learning. NSL finally fills thesediscoveries of target diseases into a unified template, successfully achievinga comprehensive medical report generation. When it employed in a real-worldclinical dataset, a series of empirical studies demonstrate its capacity onspinal medical report generation as well as show that our algorithm remarkablyexceeds existing methods in the detection of spinal structures. These indicateits potential as a clinical tool that contributes to computer-aided diagnosis."}], "temperature": 0.1}}
{"custom_id": "670_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Zhongyi Han"}], "temperature": 0.1}}
{"custom_id": "671_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Multi-Step Inference for Reasoning Over Paragraphs"}], "temperature": 0.1}}
{"custom_id": "671_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Complex reasoning over text requires understanding and chaining togetherfree-form predicates and logical connectives. Prior work has largely tried todo this either symbolically or with black-box transformers. We present a middleground between these two extremes: a compositional model reminiscent of neuralmodule networks that can perform chained logical reasoning. This model firstfinds relevant sentences in the context and then chains them together usingneural modules. Our model gives significant performance improvements (up to29\\% relative error reduction when comfibined with a reranker) on ROPES, arecently introduced complex reasoning dataset."}], "temperature": 0.1}}
{"custom_id": "671_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Jiangming Liu"}], "temperature": 0.1}}
{"custom_id": "672_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Extending Automated Deduction for Commonsense Reasoning"}], "temperature": 0.1}}
{"custom_id": "672_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Commonsense reasoning has long been considered as one of the holy grails ofartificial intelligence. Most of the recent progress in the field has beenachieved by novel machine learning algorithms for natural language processing.However, without incorporating logical reasoning, these algorithms remainarguably shallow. With some notable exceptions, developers of practicalautomated logic-based reasoners have mostly avoided focusing on the problem.The paper argues that the methods and algorithms used by existing automatedreasoners for classical first-order logic can be extended towards commonsensereasoning. Instead of devising new specialized logics we propose a framework ofextensions to the mainstream resolution-based search methods to make thesecapable of performing search tasks for practical commonsense reasoning withreasonable efficiency. The proposed extensions mostly rely on operating onordinary proof trees and are devised to handle commonsense knowledge basescontaining inconsistencies, default rules, taxonomies, topics, relevance,confidence and similarity measures. We claim that machine learning is bestsuited for the construction of commonsense knowledge bases while the extendedlogic-based methods would be well-suited for actually answering queries fromthese knowledge bases."}], "temperature": 0.1}}
{"custom_id": "672_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Tanel Tammet"}], "temperature": 0.1}}
{"custom_id": "673_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "From Statistical Relational to Neuro-Symbolic Artificial Intelligence"}], "temperature": 0.1}}
{"custom_id": "673_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neuro-symbolic and statistical relational artificial intelligence bothintegrate frameworks for learning with logical reasoning. This surveyidentifies several parallels across seven different dimensions between thesetwo fields. These cannot only be used to characterize and positionneuro-symbolic artificial intelligence approaches but also to identify a numberof directions for further research."}], "temperature": 0.1}}
{"custom_id": "673_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Luc De Raedt"}], "temperature": 0.1}}
{"custom_id": "674_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Reasonably complete forcing notions"}], "temperature": 0.1}}
{"custom_id": "674_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "We introduce more properties of forcing notions which imply that theirlambda-support iterations are lambda-proper, where lambda is an inaccessiblecardinal. This paper is a direct continuation of section A.2 ofmath.LO/0210205. As an application of our iteration result we show that it isconsistent that dominating numbers associated with two normal filters on lambdaare distinct."}], "temperature": 0.1}}
{"custom_id": "674_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Andrzej Roslanowski"}], "temperature": 0.1}}
{"custom_id": "675_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "On Successive Approximations To The Choice Problem and Logic"}], "temperature": 0.1}}
{"custom_id": "675_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This paper studies the formation of logical operations from pre-logicalprocesses. We are concerned with the reasons for certain mental processestaking form of logical reasoning and the underlying drives for consolidation oflogical operations in human mind. Starting from Piaget's approach to Logic(Piaget, 1956) we discuss whether the evolutionary adaptation can be such adriving force and whether the limits of human mind can result in the standardsystem of logical operations. The paper demonstrates that the classicaltwo-valued propositional logic can begin from a method of successiveapproximations applied to a decision-making problem within a framework ofSubject-in-an-environment survival. The presented results shed a new light onthe known model of human choice by Lefebvre (Lefebvre, 1991, 1995)."}], "temperature": 0.1}}
{"custom_id": "675_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Valeriy Bulitko"}], "temperature": 0.1}}
{"custom_id": "676_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Neural networks and logical reasoning systems. A translation table"}], "temperature": 0.1}}
{"custom_id": "676_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "A correspondence is established between the elements of logic reasoningsystems (knowledge bases, rules, inference and queries) and the hardware anddynamical operations of neural networks. The correspondence is framed as ageneral translation dictionary which, hopefully, will allow to go back andforth between symbolic and network formulations, a desirable step inlearning-oriented systems and multicomputer networks. In the framework of Hornclause logics it is found that atomic propositions with n arguments correspondto nodes with n-th order synapses, rules to synaptic intensity constraints,forward chaining to synaptic dynamics and queries either to simple nodeactivation or to a query tensor dynamics."}], "temperature": 0.1}}
{"custom_id": "676_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Joao Martins"}], "temperature": 0.1}}
{"custom_id": "677_Title", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Inferring Acceptance and Rejection in Dialogue by Default Rules of Inference"}], "temperature": 0.1}}
{"custom_id": "677_Summary", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "This paper discusses the processes by which conversants in a dialogue caninfer whether their assertions and proposals have been accepted or rejected bytheir conversational partners. It expands on previous work by showing thatlogical consistency is a necessary indicator of acceptance, but that it is notsufficient, and that logical inconsistency is sufficient as an indicator ofrejection, but it is not necessary. I show how conversants can use informationstructure and prosody as well as logical reasoning in distinguishing betweenacceptances and logically consistent rejections, and relate this work toprevious work on implicature and default reasoning by introducing three newclasses of rejection: {\\sc implicature rejections}, {\\sc epistemic rejections}and {\\sc deliberation rejections}. I show how these rejections are inferred asa result of default inferences, which, by other analyses, would have beenblocked by the context. In order to account for these facts, I propose a modelof the common ground that allows these default inferences to go through, andshow how the model, originally proposed to account for the various forms ofacceptance, can also model all types of rejection."}], "temperature": 0.1}}
{"custom_id": "677_First Author", "method": "POST", "url": "/v4/chat/completions", "body": {"model": "glm-4-flash", "messages": [{"role": "system", "content": "你是一个翻译家，你擅长把英文翻译成中文。现在开始， 每次你收到一个英文短语、句子或者段落，都把它翻译成地道的中文。"}, {"role": "user", "content": "Marilyn A. Walker"}], "temperature": 0.1}}
