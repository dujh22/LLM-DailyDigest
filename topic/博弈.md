# 博弈 Self-Play

## 强化学习

#### [SPIRAL：零和游戏自对弈成为语言模型推理训练的「免费午餐」](https://mp.weixin.qq.com/s/jAaM3hD46gFEFGFJdLVVJg)

2025-08-05

1. **研究方向为可扩展的自主提升，致力于构建能在未知环境中智能决策的自主智能体**
2. 通过基于结果的奖励机制，强化学习使模型能够发展出可泛化的推理策略，在复杂问题上取得了监督微调难以企及的进展。
3. 本文通过让模型在零和游戏中与自己对弈，自主发现并强化可泛化的推理模式，完全摆脱了对人工监督的依赖。
4. **论文标题：** SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning
5. 论文链接：https://huggingface.co/papers/2506.24119
6. 代码链接：https://github.com/spiral-rl/spiral
7. 研究团队的核心洞察是：如果强化学习能够从预训练语言模型中选择出可泛化的思维链（Chain-of-Thought, CoT）模式，那么游戏为这一过程提供了完美的**试炼场**：它们通过输赢结果提供廉价、可验证的奖励，无需人工标注。通过在这些游戏上进行自对弈，强化学习能够自动发现哪些 CoT 模式在多样化的竞争场景中获得成功，并逐步强化这些模式，创造了一个自主的推理能力提升系统。
8. 实验发现，不同游戏确实培养了专门化的认知能力：

- 井字棋专家在空间推理游戏 Snake 上达到 56% 胜率。
- 库恩扑克大师在概率游戏 Pig Dice 上取得惊人的 91.7% 胜率。
- 简单谈判专家在战略优化游戏上表现出色。

9. 更有趣的是，当结合多个游戏训练时，技能产生协同效应。
10. SPIRAL 验证了一个关键假设：预训练模型中已经包含了各种推理模式，强化学习的作用是从这些模式中筛选和强化那些真正可泛化的思维链。
11. 未来的研究开辟了新方向：
    - 混合博弈类型：结合零和、合作和混合动机游戏，可能培养更全面的推理能力。
    - 元游戏学习：让模型不仅玩游戏，还能创造新游戏，实现真正的创造性推理。
    - 跨模态游戏：将语言游戏扩展到包含视觉、音频等多模态信息，培养更丰富的认知能力。

## 智能体

策略改写「一战历史」！中科院开源全新博弈智能体框架DipLLM

2025-07-02 15:08:59 Wednesday｜ 策略改写「一战历史」！中科院开源全新博弈智能体框架DipLLM https://mp.weixin.qq.com/s/Hg7vHB_2ujfKSyvAcNjn6g

首个在复杂策略游戏Diplomacy中基于大语言模型微调的智能体框架，仅用Cicero 1.5%的训练数据就实现超越，展现出卓越的策略能力和样本效率。该框架通过自回归分解将复杂决策任务转化为序列化子任务，结合理论支持的均衡策略目标对LLM 进行高效微调，为构建更通用、高效的博弈智能体提供了新范式。

围棋、德州扑克曾是AI崛起的试炼场，从AlphaGo到Libratus，人工智能不断刷新策略上限。

Diplomacy：一款融合协作与竞争的七人博弈游戏，单轮动作空间高达10的64次方，其策略建模复杂度前所未有！

为此，Meta曾推出智能体Cicero[Meta, Science 2022]，结合人类数据与策略搜索，在该领域实现突破，但其方法高度依赖超大规模均衡搜索与重资源训练，难以扩展与迁移。

论文地址：https://arxiv.org/pdf/2506.09655

开源代码：https://github.com/KaiXIIM/dipllm
